[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes, AI and Deep Learning",
    "section": "",
    "text": "Preface\nWelcome to the fascinating world of Bayesian learning, artificial intelligence, and deep learning! This book is your guide to understanding these powerful tools and their applications in various fields. This book is a culmination of our experiences teaching these exciting fields to two distinct audiences: business school students at the University of Chicago and engineers at George Mason University.\nThis unique blend of perspectives allows us to present these complex concepts in a way that is accessible to data scientists, business professionals and technical experts. Whether you’re a manager seeking to leverage AI in your organization or an engineer building the next generation of intelligent systems, this book has something for you.\nTechniques discussed in this book emerged as a transformative force in modern society, and its impact on automation is undeniable. From self-driving cars to virtual assistants, these technologies are already a part of our daily lives. In the coming years, they will become even more ubiquitous, impacting every industry and aspect of our lives. Understanding these technologies is essential for anyone who wants to stay ahead of the curve.\nThrough its ability to learn, adapt, and make decisions, AI is accelerating the pace of automation across various industries and sectors. This impact is multifaceted, encompassing both positive and negative aspects that warrant careful consideration. AI algorithms can analyze vast amounts of data to identify patterns and trends, providing valuable insights for informed decision-making. This leads to better resource allocation, optimized processes, and improved outcomes across various domains. Chatbots and virtual assistants powered by AI can handle customer inquiries and provide support 24/7, offering a personalized and efficient experience. It even works on Jewish holidays! This improves customer satisfaction and loyalty, ultimately benefiting businesses.\nAs a result, AI enables the creation of entirely new business models and industries that were previously not possible. This disrupts traditional markets and creates opportunities for innovation and growth. AI is driving significant progress in fields like self-driving cars, personalized medicine, and space exploration. This has the potential to revolutionize these industries and improve lives in numerous ways.\nThe term AI has morphed over time. It was first coined in 1956 by John McCarthy, who defined it as “the science and engineering of making intelligent machines.” Since then, the field has evolved significantly, and the definition of AI has changed accordingly. Today, AI is a broad field that encompasses various subfields, including machine learning, deep learning, and natural language processing. These subfields are often used interchangeably, but they are not the same thing. Machine learning is a subfield of AI that focuses on algorithms that can learn from data. Deep learning is a subfield of machine learning that uses artificial neural networks to learn complex patterns and relationships in data. Natural language processing is a subfield of AI that focuses on algorithms that can understand and generate human language.\nSince 1956, the field of artificial intelligence (AI) has undergone significant transformations traditional AI was mostly focused on rule-based systems and boolean logic programming, with limited learning capabilities. It lead to them being brittle in changing environments. On the other hand, emerging AI is focused on modeling uncertainties, pattern matching, and deep learning. All of those are data-driven approaches. These approaches are more adaptable and can handle complex and unstructured data. They are also more data-dependent and lack interpretability.\n\n\n\n\n\n\n\n\n\nOld AI\n\n\n\nIf rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nNew AI\n\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\nThis book is based on the lecture notes from our courses, which have been refined and expanded over years of teaching. We have incorporated valuable feedback from students, both at the University of Chicago and George Mason University, to create a comprehensive and engaging learning experience. This book is organized into three parts:\n\nPart 1: Bayesian Learning: This part covers the basics of probability and Bayesian inference.\nPart 2: Artificial Intelligence: This part explores the core concepts of AI such and mostly focuses on pattern matching techniques such as decision trees and generalized linear models.\nPart 3: Deep Learning: This part delves into the world of deep learning, focusing on the architecture and training of deep neural networks. It covers convolutional neural networks, recurrent neural networks, and generative adversarial networks.\n\nThis work is inspired by the contributions of many great thinkers in the field of AI and machine learning. We acknowledge the foundational work of pioneers such as: Shannon (Reinforcement Learning), von Neumann (Theory of Choice), Bellman (Optimality and Learning).\nThe evolution of AI can be summarised in three stages:\n\nSearch. Started with one question and webpages ranked by relevance. Larry Page and Sergey Brin developed PageRank algorithm that uses power iterations to rank webpages by relevance. The statistical tools such as Kendall’s tau and Spearman’s rank correlation were used to measure the similarity between the ranking and the actual relevance.\nSuggestions. The first popular suggestion algorithm was developed by Netflix. It used collaborative filtering to recommend movies to users based on their viewing history and the viewing history of other users. The idea was to minimize the burden on the user to search for movies to watch.\nSummaries. Current AI systems like ChatGPT and Perplexity have revolutionized how we interact with information by excelling at summarization and generalization. These large language models can take vast amounts of complex information and distill it into clear, coherent summaries that capture the essential points while maintaining accuracy. They can generalize across different domains, connecting concepts from various fields and providing insights that might not be immediately obvious. For example, ChatGPT can summarize lengthy research papers into key findings, while Perplexity can synthesize information from multiple sources to provide comprehensive answers to complex questions. This ability to summarize and generalize represents a significant leap forward from simple search and recommendation systems, as these AI agents can now act as intelligent intermediaries that understand context, identify patterns, and present information in ways that are most useful to users.\n\nFirst it was one question and one answer, next it was suggestions. You do not know what to search for, collaborative filtering, recommender systems, and search engines. Now, it is summarise, you are the manager of AI agents agents interpret your request, formulate a plan and necessary questions, and then execute the plan. This is the future of AI, where agents can work together to solve complex problems and provide valuable insights.\nBayesian learning is a powerful statistical framework based on the work of Thomas Bayes. It provides a probabilistic approach to reasoning and learning, allowing us to update our beliefs about the world as we gather new data. This makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information. Artificial intelligence (AI) is a vast field that seeks to create intelligent agents capable of performing tasks that typically require human intelligence. These tasks can include perception, reasoning, learning, problem-solving, decision-making, and language processing. AI has made significant progress in recent years, driven by advances in computing power, data availability, and algorithms. Deep learning is a subfield of AI that uses artificial neural networks to learn from data. These networks are inspired by the structure and function of the human brain and have the ability to learn complex patterns and relationships in data. Deep learning has achieved remarkable results in various tasks such as image recognition, natural language processing, and machine translation.\nThe world of business and engineering are increasingly intertwined, as AI becomes an essential tool in both domains. This book bridges the gap between these disciplines by demonstrating how Bayesian learning, AI, and deep learning can be applied to address real-world challenges in:\n\nBusiness: Market analysis, customer segmentation, risk management, and strategic decision-making.\nEngineering: Robotics, image recognition, natural language processing, and data-driven automation.\n\nKey Features of This Book:\n\nAccessible explanations: We break down complex concepts into manageable chunks, using real-world examples and analogies to illustrate key principles.\nCase studies: We showcase practical applications of Bayesian learning, AI, and deep learning across diverse industries.\nHands-on exercises: We provide practical exercises and code examples to help you apply the concepts covered in the book to your own projects.\n\nJoining the AI Revolution:\nThe field of AI is rapidly evolving, and this book equips you with the knowledge and skills necessary to stay ahead of the curve. Whether you’re looking to enhance your business acumen or advance your engineering career, understanding the power of Bayesian learning, AI, and deep learning is crucial.\nWe invite you to join us on this exciting journey and discover the transformative potential of these powerful tools!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Generative AI\n“If you tell me precisely what it is a machine cannot do, then I can always make a machine which will do just that. John von Neumann, 1956”\nWhen you open an Amazon page there are many personal suggestions of goods to purchase. By analyzing previous product pages visited and purchases made by you and other people who have bought similar products Amazon uses AI and machine learning to predict what would of interest to you next time you shop.\nWhen you apply for a loan online, you typically get an immediate answer after filling an application. The information you provide, combined with your credit history pulled from a credit history bureau is used by a predictive model which can tell with high level of confidence whether you are to default on the loan or not.\nYou might ask, what is common among one of the most successful Internet retail company, finance industry and a phenomenal baseball team? All of these decisions use AI and methods of predictive analytics to improve the operations. They used historical observations combined with rigorous statistical analysis and efficient computer algorithms to predict future outcomes and change the decisions. The ability to collect and analyze complex data sets has been a prerogative of a small number of people for many year. It vital to have experience in data engineering, statistics, machine learning and probability. A data scientists has all of those skills. Current tools developed by industry and academic institutions makes data science profession accessible to a wider audience without requiring a training in a specific technical filed.\nOver the past decade, there has been an explosion of work, mostly applied, on deep learning. Applications of deep learning are everywhere. The main reason for this is that large Internet companies such as Google, Facebook, Amazon and Netflix increasingly displace traditional statistical and machine learning methods with deep learning techniques. Though, such companies are at the frontier of applying deep learning, virtually any industry can be impacted by applying deep learning (DL).\nData Science is a relatively new field that refers to sets of mathematical and statistical models, algorithms, and software that allow extracting patterns from data sets. The algorithms are the adoptions of applied mathematics techniques to specific computer architectures and the software implements those algorithms.\nPredictive analytics applies AI models to design predictive rules which then can be used by engineers and business for forecasting or what-if analysis. For example, a company that is interested in predicting sales as a result of advertisement campaign would use predictive model to identify the best way to allocate its marketing budget or a logistics company would use a predictive model to forecast demand for shipments to estimate the number of drivers it would need in the next few months.\nArtificial Intelligence has been around for decades. In fact the term AI was coined by a famous computer scientist John McCarthy in 1955. While being tightly connected to the field of robotics for many years, the AI concepts are widely applicable in other fields, including predictive analytics. Currently, the AI is understood as a set of mathematical tools that are used to develop algorithms that can perform tasks, typically done by humans, for example, drive a car or schedule a doctor’s appointment. This set of mathematical tools include probabilistic models, machine learning algorithms and deep learning. The previous successful applications included the victory of IBM’s DeepBlue over then world champion Garry Kasparov in 1997.\nTree search algorithms were developed by DeepBlue engineers to implement the chess robot. A modification was the addition of heuristics to cut branches of the tree that would not lead to a win. Those heuristics were designed by chess grand masters based on their intuition and previous experience. Vehicles in grand challenge also relied on traditional techniques such as Kalman filters and PID (proportional-integral-derivative) controllers that have been in use for many years.\nTwo distinguishing features of AI algorithms:\nA major difference between modern and historical AI algorithms is that most of the recent AI approaches rely on learning patterns from data. For example, DeepBlue algorithm was “hardcoded” and the human inputs were implemented as if-then statements by the IBM engineers. On the other hand, modern AlphaGo zero algorithm did not use any human inputs whatsoever and learned optimal strategies from a large data sets generated from self-plays. Although handcrafted systems were shown to perform well in some tasks, such as chess playing, the are hard to design for many complex applications, such as self-driving cars. On the other hand large data sets allow us to replace set of rules designed by engineers with a set of rules learned automatically from data. Thus, the learning algorithms, such as deep learning are at the core of the most of modern AI systems.\nThe main driving factor behind the growth of modern AI applications is the availability of massive and often unstructured data sets. Om the other hand, we now have appropriate computing power to develop computationally intensive AI algorithms. The three main modern AI enablers are:\nFitting complicated models to describe complicated patterns without overfitting requires millions or billions of data points. Two key ideas behind pattern-recognition systems are\nIn AI, prediction rules are often referred to as “models”. The process of using data to find a gooo prediction rule is often called “training the model”. With millions (or billions) of datapoints and fast pattern-matching skills, machines can find needles in a haystack proving insights for human health, transportation, ... etc.\nMachine learning (ML) arises from this question: could a computer go beyond “what we know how to order it to perform” and learn on its own how to perform a specified task? Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data? This question opens the door to a new programming paradigm. In classical programming, the paradigm of symbolic AI, humans input rules (a program) and data to be processed according to these rules, and out come answers. With machine learning, humans input data as well as the answers expected from the data, and out come the rules. These rules can then be applied to new data to produce original answers.\nA machine-learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task. For instance, if you wished to automate the task of tagging your vacation pictures, you could present a machine-learning system with many examples of pictures already tagged by humans, and the system would learn statistical rules for associating specific pictures to specific tags.\nAlthough machine learning only started to flourish in the 1990s, it has quickly become the most popular and most successful subfield of AI, a trend driven by the availability of faster hardware and larger datasets. Machine learning is tightly related to mathematical statistics, but it differs from statistics in several important ways. Unlike statistics, machine learning tends to deal with large, complex datasets (such as a dataset of millions of images, each consisting of tens of thousands of pixels) for which classical statistical analysis such as Bayesian analysis would be impractical. As a result, machine learning, and especially deep learning, exhibits comparatively little mathematical theory—maybe too little—and is engineering oriented. It’s a hands-on discipline in which ideas are proven empirically more often than theoretically.\nDeep learning DL is a type of machine learning which performs a sequence of transformations (filters) on a data. Output of each of those filters is called a factor in traditional statistical language and hidden feature in machine learning. Word deep means that there is a large number of filters that process the data. The power of this approach comes from the hierarchical nature of the model.\nThe three main factors driving AI are:\nThe widespread of mobile phones leads to generation of vast amounts of data. Besides images, users generate space and time trajectories, which are currently used to estimate and predict traffic, text messages, website clicking patterns, etc.\nDeep learning with many successful applications, has been frequently discussed in popular media. The popularity of the topic has led to hype people tend to think that deep learning techniques are capable to replace many of the human tasks, such as medical diagnostics, accountings. On the pessimistic side, people think that after a short hype, the DL techniques will disappoint and companies will stop funding R&D work on its development. However, the research on pushing this filed further is slow and it will take time before deep learning penetrates a wide range of industries. At any rate, the demand for data scientists in general and AI specialists has been increasing over the last few years with biggest markets being on silicon valley, NYC and Washington, DC(indeed 2018).\nThe field of predictive analytics was popularized by many famous competitions in which people compete to build the model with lowest prediction error. One of the first of this types of competitions was the Netflix prize. In 2009 Netflix payed $1 million to a team that developed the most accurate model for predicting movies a user would like to watch. At that time Netflix’s recommendation system generated 30 billion predictions per day. The initial goal of improving recommendation algorithm by 10 percent was overachieved by the winning team. The wining team used what is called an ensemble technique, which takes a weighted average from different prediction algorithms. Thus, the first lesson from this competition is that we typically need to build several predictive models to achieve a good results. On the other had, the model developed by the winning team was never used by Netflix due to complexity of those models and the fact that by the end of competition Netflix mostly shifted to streaming movies versus sending DVDs over mail. The second lesson is that simplicity and interpretability of models matters when they are deployed on a large scale. The third lesson, is that models need to adapt accordingly to meet the fast changing business requirements.\nDeep Learning’s (DL) growing popularity is summarized by the grown of products that Google is developing using DL. Figure 2 shows this immense growth. One key differentiating effect is that DL algorithms are scalable and can be implemented across the interned in apps such as YouTube and Gmail.\nApplications of Machine Learning/Deep Learning are endless, you just have to look at the right opportunity! There is a similar dynamics in popularity of deep learning search queries on Google. The growth is again exponential, although it is not yet close to popularity of traditional statistical techniques, such as linear regression analysis.\nMeanwhile, some ethical concurs are being raised as a result of growing popularity of AI. The most discussed thus far is the impact on the job market and many jobs being replaced by deep learning models. Although, some economic analysis (Acemoglu and Restrepo 2018) shows that while jobs displacement leads to reduced demand for labor and wages, it counteracted by a productivity effect and increases in demand for labor in non-automated tasks.\nThe algorithmic aspects of deep learning has existed for decades. In 1956, Kolmogorov has shown that any function can be represented as a superposition of univariate functions (this is exactly what deep learning does). In 1951 Robbins and Monro proposed stochastic approximations algorithms. This is the main technique for finding weights of a deep learning model today.\nBackpropagation algorithm for finding derivatives was first published and implemented by Werbos in 1974. In mid 1980s Schmidhuber studied many practical aspects of applying neural networks to real-life problems. Since the key ingredients of DL has been around for several decades, one could wonder why we observe a recent peak in popularity of those methods.\nOne of the strong driving forces is adoption of DL by internet companies that need to analyze large scale high dimensional datasets, such as human-written text, speech and images. Smartphone photography led to people uploading vast amounts of images to services like Instagram and Facebook. In 2012 more mobile devices were sold than PCs. The number of images shared on the Internet has skyrocketed as well. This can be see in products that Google is developing using DL.\nThe proliferation of smartphones globally has been one of the most dramatic technological adoptions in human history. From just 173 million smartphone users worldwide in 2010, the number exploded to over 6.8 billion users by 2023, representing nearly 86% of the global population. This exponential growth has been particularly pronounced in developing markets, where smartphones often serve as the primary gateway to the internet. Countries like India and China have seen smartphone penetration rates exceed 80%, while regions in Africa and Southeast Asia continue to show rapid adoption curves. The ubiquity of smartphones has fundamentally transformed how data is generated and consumed - these devices produce continuous streams of location data, user interactions, images, messages, and behavioral patterns that form the foundation for modern AI applications. The convergence of increasingly powerful mobile processors, high-resolution cameras, and always-on internet connectivity has created an unprecedented data generation ecosystem that feeds directly into the machine learning models powering everything from recommendation systems to autonomous vehicles.\nTherefore, data generated by Internet users creates a demand for techniques to analyze large scale data sets. Mathematical methodologies were in place for many years. One missing ingredient in the explosive nature of DL popularity is the availability of computing power. DL models are computationally hungry, trial and error process is required to build a useful model. Sometimes hundreds or thousands of different models are required to be evaluated before choosing one to be used in an application. Training models can be computationally expensive, we are usually talking about large amounts of training data that need to be analyzed to build a model.\nThe adoption rate of AI technologies, particularly generative AI like ChatGPT, has shattered all previous records for technology adoption. While it took the internet 7 years to reach 100 million users, the telephone 75 years, and television 13 years, ChatGPT achieved this milestone in just 2 months after its launch in November 2022. This unprecedented speed of adoption reflects not just the accessibility of AI tools, but also their immediate utility across diverse user needs. Unlike previous innovations that required significant infrastructure changes or learning curves, AI chatbots could be accessed through simple web interfaces and provided immediate value for tasks ranging from writing assistance to problem-solving. The viral nature of AI adoption has been further accelerated by social media demonstrations and word-of-mouth sharing of impressive AI capabilities, creating a network effect that compounds the growth rate. This rapid adoption suggests that AI represents a fundamentally different type of technological shift - one that augments human capabilities rather than replacing existing systems entirely. The chart below illustrates the explosive growth potential of AI technologies.\nThe first generation of AI models was fundamentally enabled by the availability of powerful GPU chips, which provided the parallel processing capabilities necessary to train deep neural networks on large datasets. The breakthrough in deep learning around 2012, including innovations like AlexNet for image recognition, would not have been possible without GPUs that could perform thousands of matrix operations simultaneously. Current AI models, including ChatGPT, Claude, and other large language models, continue to rely primarily on GPUs for both training and inference. Modern AI training clusters consist of thousands of interconnected GPUs working together for weeks or months to process the enormous datasets required for today’s sophisticated models. While some companies have developed specialized AI chips like Google’s TPUs, GPUs remain the dominant platform for AI development due to their versatility, widespread availability, and established software ecosystems.\nThe gaming industry was one of the earliest drivers of GPU development, as game developers demanded increasingly sophisticated graphics rendering capabilities to create immersive virtual worlds with realistic lighting, textures, and physics simulations. Companies like NVIDIA and AMD invested heavily in parallel processing architectures optimized for the matrix operations required to render complex 3D scenes in real-time. The rise of cryptocurrency mining, particularly Bitcoin and Ethereum, created an unexpected second wave of GPU demand as miners discovered that graphics cards were far more efficient than traditional CPUs for the repetitive hash calculations required by proof-of-work algorithms. This mining boom drove massive investments in GPU manufacturing capacity and spurred innovations in memory bandwidth and energy efficiency. More recently, the explosion of AI-generated video content has created a third major demand driver, as video generation models require enormous computational power to process and synthesize high-resolution video frames. The convergence of these three use cases - gaming graphics, cryptocurrency mining, and AI video generation - has accelerated GPU development far beyond what any single application could have achieved alone, creating the powerful hardware infrastructure that now enables training of large language models and other AI applications.\nTable 1 illustrates the dramatic evolution of GPU performance over two decades, from early graphics cards to specialized AI accelerators. The data shows exponential growth in computational power: from the modest 0.23 TeraFLOPS of the 2006 GeForce 7900 GTX to the projected 100 PetaFLOPS (FP4) of the 2027 Rubin Ultra - representing a performance increase of over 400,000x. Here FP4 is a lower precision (4-bit) floating-point arithmetic that is used for AI workloads. It is an alternative to FP32 (32-bit) floating-point arithmetic that is used for general purpose computing.\nMemory capacity has similarly exploded from 0.5GB to a projected 1TB. Modern GPUs have evolved from simple graphics processors to sophisticated AI-optimized architectures featuring specialized tensor cores, mixed-precision arithmetic (FP8/FP4), and massive high-bandwidth memory systems. The transition from traditional FP32 floating-point operations to lower-precision AI workloads (FP8/FP4) has enabled unprecedented computational throughput measured in PetaFLOPS and ExaFLOPS scales, making current and future GPUs the primary engines driving the deep learning revolution and large language model training.\nNow AI models are the main consumers of those processors. The more popular of those are ChatGPT-4, Anthropic’s Claude and Perplexity. ChatGPT-4 is based on the transformer architecture. It is able to handle long conversations and maintain better context over multiple turns. It is stronger in creative writing, technical writing, reasoning tasks, and code generation. It has better performance on logic-heavy tasks and answering technical queries. It is mainly used for chatbots, automated content creation, code writing, customer support, and more advanced AI tasks.\nOpenAI, the company behind ChatGPT, has experienced remarkable growth in both valuation and revenue. As of late 2024, OpenAI reached a valuation of $157 billion following its latest funding round, making it one of the most valuable private companies in the world. The company’s annual recurring revenue (ARR) has grown exponentially, reaching approximately $3.7 billion in 2024, driven primarily by ChatGPT subscriptions and API usage. OpenAI has raised over $13 billion in total funding, with major investors including Microsoft, which has invested $13 billion and maintains a strategic partnership that includes exclusive cloud computing arrangements. This rapid financial growth reflects the massive demand for generative AI capabilities across industries and the transformative potential of large language models.\nClaudeis the main competitor of OpenAI. It is supported by Amazon and excels at complex reasoning tasks, problem-solving, and in-depth analysis across a wide range of domains. Claude can write, debug, and explain code in many programming languages. It can analyze images and documents in addition to text and can engage in various conversation styles, from formal analysis to creative writing to casual discussion.\nAmazon has made a significant strategic investment in Anthropic, Claude’s creator, committing up to $4 billion to advance AI safety research and development. This partnership positions Amazon Web Services (AWS) as Anthropic’s primary cloud provider while giving Amazon a minority ownership stake in the company. Unlike ChatGPT, which excels in creative writing and general-purpose conversations, Claude is specifically designed with a focus on safety, harmlessness, and nuanced reasoning. Claude demonstrates superior performance in tasks requiring careful analysis, ethical reasoning, and handling sensitive topics. It employs Constitutional AI training methods that make it more reliable in avoiding harmful outputs and better at acknowledging uncertainty when it doesn’t know something. Recent advances in Claude 3.7 and Claude 4.0 have introduced groundbreaking multimodal capabilities, allowing these models to process and analyze images, documents, and code with unprecedented accuracy. Claude 4.0 represents a significant leap forward in mathematical reasoning, coding assistance, and complex problem-solving tasks, with performance improvements of 40-60% over previous versions in benchmark evaluations. These newer models feature enhanced “thinking” processes that are more transparent, often explaining their reasoning step-by-step with greater depth and clarity, which makes them particularly valuable for educational applications, research assistance, and professional analysis where understanding the AI’s decision-making process is crucial. Claude 4.0 also introduces improved long-context understanding, capable of processing documents up to 200,000 tokens, and demonstrates remarkable advances in scientific reasoning and technical writing. This approach has made Claude increasingly popular among researchers, academics, and professionals who require more thoughtful and contextually aware AI assistance.\nPerplexity synthesizes information from multiple sources and presents it with proper citations. Each response includes references for easy verification. It functions as a conversational search engine. Perplexity has emerged as a formidable competitor to Google Search by offering a fundamentally different approach to information discovery. Unlike traditional search engines that provide links to websites, Perplexity acts as an AI-powered research assistant that directly answers questions while citing sources. The company has attracted significant investment, including backing from Amazon founder Jeff Bezos, who participated in Perplexity’s $74 million Series B funding round in 2024. This strategic investment reflects growing confidence in AI-first search alternatives that could disrupt Google’s longstanding dominance in the search market.\nThe company has also developed innovative partnerships with major brands like Marriott and Nike, demonstrating how AI search can be integrated into enterprise applications. Marriott has explored using Perplexity’s technology to enhance customer service by providing instant, cited answers about hotel amenities, local attractions, and booking policies. Similarly, Nike has experimented with Perplexity’s capabilities to help customers find specific product information, sizing guides, and availability across different locations. These enterprise partnerships showcase Perplexity’s potential to move beyond general web search into specialized, domain-specific applications.\nPerplexity’s advertising model differs significantly from Google’s traditional approach. Rather than displaying ads alongside search results, Perplexity is exploring sponsored answers and branded content integration that maintains the conversational flow while clearly identifying commercial partnerships. This approach could prove less intrusive than traditional search advertising while providing new revenue streams. The company’s growth trajectory and enterprise adoption suggest it could pose a meaningful challenge to Google’s search monopoly, particularly among users who prefer direct answers over browsing multiple websites.\nThe explosive growth of Large Language Models (LLMs) like ChatGPT, Claude, and Perplexity has been fundamentally enabled by the vast repositories of digital text that have accumulated over the past three decades. The “fuel” powering these sophisticated AI systems comes from an unprecedented collection of human knowledge digitized and made accessible through the internet. Wikipedia alone contains over 60 million articles across hundreds of languages, representing one of humanity’s largest collaborative knowledge projects. Web crawling technologies have systematically captured billions of web pages, blog posts, news articles, and forum discussions, creating massive text corpora that encode diverse writing styles, domains of expertise, and forms of human expression. The digitization of literature through projects like Google Books and Internet Archive has made millions of books searchable and processable, from classical literature to technical manuals. Social media platforms have contributed streams of conversational text, while academic databases provide formal scientific and scholarly writing. This digital text explosion created training datasets containing trillions of words - orders of magnitude larger than what any human could read in multiple lifetimes. By processing these enormous text collections through transformer architectures, LLMs learned statistical patterns of language use, absorbing grammar, syntax, semantics, and even reasoning patterns embedded in human writing. The models discovered how words relate to each other, how concepts connect across different contexts, and how to generate coherent, contextually appropriate responses by predicting the most likely next word given preceding text. This approach allowed AI systems to develop surprisingly sophisticated language understanding and generation capabilities without explicit programming of linguistic rules, instead learning the deep structure of human communication from the collective digital footprint of our species.\nThe mathematical operations used for manipulating and rendering images are the same as those used in deep learning models. Researchers started to used graphical processing units (GPUs) (a.k.a graphics cards) to train deep learning models in 2010s. The wide availability of GPUs made deep learning modeling accessible for a large number of researchers and engineers and eventually led to popularity of DL. Recently, several competitive hardware architectures were developed by large companies like Google, which uses its own TPU (Tensor Processing Units) as well as smaller start-ups.\nThis course will focus on practical and theoretical aspects of predicting using deep learning models. Currently, deep learning techniques are almost exclusively used for image analysis and natural language processing and are practiced by a handful number of scientists and engineers with most of them being trained in computer science. However, modern methodologies, software and availability of cloud computing make deep learning accessible to a wide range of data scientists who would typically use more traditional predictive models such as generalized linear regression or tree-based methods.\nA unified approach to analyze and apply deep learning models to a wide range or problems that arise in business and engineering is required. To make this happen, we will bring together ideas from probability and statistics, optimization, scalable linear algebra and high performance computing. Although, deep learning models are very interesting to study from methodological point of view, the most important aspect of those is the predictive power unseen before with more traditional models. Ability to learn very complex patterns in data and generate accurate predictions make the deep learning a useful and exciting methodology to use, we hope to convey that excitement. This set of notes is self-contained and has a set of references for a reader interested in learning further.\nAlthough basics of probability, statistics and linear algebra will be revisited, it is targeted towards students who have completed a course in introductory statistics and high school calculus. We will make extensive use of computational tools, such as R language, as well as PyTorch and TensorFlow libraries for predictive modeling, both for illustration and in homework problems.\nThere are many aspects of data analysis that do not deal with building predictive models, for example data processing and labeling can require significant human resources(Hermann and Balso 2017; Baylor et al. 2017).\nThe landscape of Artificial Intelligence is rapidly being reshaped by the rise of Generative AI (Gen AI). As of 2025, Gen AI has moved beyond hype and into practical application across a multitude of personal and professional domains. A recent article in the Harvard Business Review, “How People Are Really Using Gen AI in 2025” by Marc Zao-Sanders, highlights this shift, noting that user interest has significantly increased and investment in AI is skyrocketing.\nThe article reveals a fascinating trend: a move from purely technical applications towards more emotive and personal uses. The top use cases in 2025 reflect this, with “Therapy/companionship” leading the list. Other prominent uses include “Organizing my life,” “Finding purpose,” “Enhanced learning,” and “Generating code (for pros).” This indicates that individuals are leveraging Gen AI not just for productivity, but also for personal development and well-being.\nSome concrete examples of how people are using Gen AI, as cited in the article, include:\nThe article also points to the increasing sophistication of Gen AI users, who are developing a deeper understanding of the technology’s capabilities and limitations, including concerns around data privacy and the potential for over-reliance.\nBelow is an image from the HBR article summarizing the top 10 use cases:\nSource: Marc Zao-Sanders, “How People Are Really Using Gen AI in 2025,” Harvard Business Review, April 9, 2025, https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025.\nThe continued evolution of Gen AI promises even more sophisticated applications in the future, moving from providing information to taking action (agentic behavior).\nThe computer therapist is not something new. In 1966, Joseph Weizenbaum created ELIZA, a computer program that could simulate a conversation with a psychotherapist. ELIZA used simple pattern matching to respond to user inputs, creating the illusion of understanding. While it was a groundbreaking achievement at the time, it lacked true comprehension and relied on scripted responses.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#generative-ai",
    "href": "00-intro.html#generative-ai",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Therapy/Companionship: Providing accessible mental health support and a sense of connection, especially in regions with limited access to human therapists. Users find AI to be available 24/7 and non-judgmental.\nOrganizing My Life: Creating timelines for tasks, planning daily habits, and managing personal projects.\nEnhanced Learning: Using AI as a study guide to explain complex topics and reinforce learning.\nHealthier Living: Generating meal plans based on specific dietary needs and macro calculations.\nCreating Travel Itineraries: Planning detailed vacations, including finding rustic accommodations and hidden gems while optimizing travel time.\nDisputing Fines: Drafting appeal letters for things like parking tickets.\n\n\n\n\n\n\nTop 10 Gen AI Use Cases in 2025. Source: Harvard Business Review, “How People Are Really Using Gen AI in 2025”, April 9, 2025.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#agi-and-aiq",
    "href": "00-intro.html#agi-and-aiq",
    "title": "Principles of Data Science",
    "section": "AGI and AIQ",
    "text": "AGI and AIQ\n\n“I visualize a time when we will be to robots what dogs are to humans. And I am rooting for the machines.” - Claude Shannon\n\n\n“Let us suppose we have set up a machine with certain initial instruction tables, so constructed that these tables might on occasion, if good reason arose, modify those tables. One can imagine that after the machine had been operating for some time, the instructions would have altered out of all recognition, but nevertheless still be such that one would have to admit that the machine was still doing very worthwhile calculations. Possibly it might still be getting results of the type desired when the machine was first set up, but in a much more efficient manner. In such a case one would have to admit that the progress of the machine had not been foreseen when its original instructions were put in. It would be like a pupil who had learnt much from his master, but had added much more by his own work. When this happens I feel that one is obliged to regard the machine as showing intelligence.” – Alan Turing\n\nPeople, organizations and markets. AI does the organization and hence connects people to markets faster and simplesly. Hence it creates economic values. Most of th recessions in the 19th centurey was a result of not being able to get goods to markets quick enough which led to banking crises. AI accelerates speed to market. It reates growth. The age of abundance is here.\nSkynet and terminator\nTransfer learning\nOlga comments (Toloka)\n\nChat does not know what it does not know\nStill need humans and their skills\nLike co-pilot, we need collaboration between humans and AI, humans became managers\nBefore people would build many classifiers for a specific task. The economics of the model there is one big winner. They combine all the models together.\nNeed humans for ground truth, for labeling data, for training models\nAI is very good at decomposing and planning, and humans are not as good at executing the plan, because it is against their intuition.\n\nAndrej Karpathy’s talk, “Software Is Changing (Again),” explores how large language models (LLMs) are fundamentally transforming the way software is developed and used. He describes this new era as “Software 3.0,” where natural language becomes the primary programming interface and LLMs act as a new kind of computer and compares it to the previous generations of software development approaches sumamrised in the table below.\n\n\n\n\n\n\n\n\n\nParadigm\n“Program” is…\nDeveloper’s main job\nCanonical depot\n\n\n\n\nSoftware 1.0\nHand-written code\nWrite logic\nGitHub\n\n\nSoftware 2.0\nNeural-net weights\nCurate data & train\nHugging Face / Model Atlas\n\n\nSoftware 3.0\nNatural-language prompts\nCompose/police English instructions\nPrompt libraries\n\n\n\nCurrenlty LLMs are collaborative partners that can augment human abilities, democratizing software creation and allowing people without traditional programming backgrounds to build complex applications simply by describing what they want in plain English.\nPolson and Scott (2018) have predicted that human-machine interaction will be the next frontier of AI.\nOlga sais that humans are callable function.\nThe same will happen to university professors. They will become lablers for content. And simply will be responsible for clicking yes, when content is appropriate and no, when it is not.\nHal Varian’s 2010 paper “Computer Mediated Transactions” Varian (2010) provides a foundational framework for understanding how computers can automate routine tasks and decision-making processes, reducing transaction costs and increasing efficiency. This includes automated pricing, inventory management, and customer service systems. He talks about system that can coordinate between multiple parties by providing real-time information sharing and communication platforms. This enables more complex multi-party transactions and supply chain management.\nThis framework remains highly relevant for understanding modern AI and machine learning applications in business, as these technologies represent the next evolution of computer-mediated transactions, enabling even more sophisticated automation, coordination, and communication capabilities.\nIn his talk on “Why are LLMs not Better at Finding Proofs?”, Timothy Gowers discusses that while large language models (LLMs) can display some sensible reasoning—such as narrowing down the search space in a problem—they tend to falter when they get stuck, relying too heavily on intelligent guesswork rather than systematic problem-solving. Unlike humans, who typically respond to a failed attempt with a targeted adjustment based on what went wrong, LLMs often just make another guess that isn’t clearly informed by previous failures. He also highlights a key difference in approach: humans usually build up to a solution incrementally, constructing examples that satisfy parts of the problem and then refining their approach based on the requirements. For example, when trying to prove an existential statement, a human might first find examples satisfying one condition, then look for ways to satisfy additional conditions, adjusting parameters as needed. LLMs, by contrast, are more likely to skip these intermediate steps and try to jump directly to the final answer, missing the structured, iterative reasoning that characterizes human problem-solving.\nWhile there are indeed limitations to what current large language models can solve, particularly in areas requiring systematic mathematical reasoning, they continue to demonstrate remarkable capabilities in solving complex problems through alternative approaches. A notable example is the application of deep learning to the classical three-body problem in physics, a problem that has challenged mathematicians and physicists for centuries. Traditional analytical methods have struggled to find closed-form solutions for the three-body problem, but deep neural networks have shown surprising success in approximating solutions through pattern recognition and optimization techniques. These neural networks can learn the underlying dynamics from training data and generate accurate predictions for orbital trajectories, even when analytical solutions remain elusive. This success demonstrates that the trial-and-error approach, when combined with sophisticated pattern recognition capabilities, can lead to practical solutions for problems that have resisted traditional mathematical approaches. The key insight is that while these methods may not provide the elegant closed-form solutions that mathematicians prefer, they offer valuable computational tools that can advance scientific understanding and enable practical applications in fields ranging from astrophysics to spacecraft navigation.\n\n\n\n\nAcemoglu, Daron, and Pascual Restrepo. 2018. “Artificial Intelligence, Automation and Work.” National Bureau of Economic Research.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, et al. 2017. “Tfx: A Tensorflow-Based Production-Scale Machine Learning Platform.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016. “End to End Learning for Self-Driving Cars.” arXiv Preprint arXiv:1604.07316. https://arxiv.org/abs/1604.07316.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo: Uber’s Machine Learning Platform.”\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in Artificial Intelligence.”\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg Mori. 2017. “Learning Person Trajectory Representations for Team Activity Analysis.” arXiv Preprint arXiv:1706.00893. https://arxiv.org/abs/1706.00893.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How People and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V McConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018. “Prediction of Cardiovascular Risk Factors from Retinal Fundus Photographs via Deep Learning.” Nature Biomedical Engineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.” arXiv. https://arxiv.org/abs/1712.01815.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and TD-Gammon.” Communications of the ACM 38 (3): 58–68.\n\n\nVarian, Hal R. 2010. “Computer Mediated Transactions.” American Economic Review 100 (2): 1–10.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu, Yeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of Structured Information from Street View Imagery.” arXiv Preprint arXiv:1704.03549. https://arxiv.org/abs/1704.03549.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "17-theoryai.html",
    "href": "17-theoryai.html",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "",
    "text": "1.1 Normal Means Problem\nAs we have seen in the previous chapters, the development of learning from data algorithms has been driven by two fundamental paradigms: the classical frequentist approach centered around maximum likelihood estimation (MLE) and the Bayesian approach grounded in decision theory. This chapter explores how these seemingly distinct methodologies converge in the modern theory of AI, particularly through the lens of regularization and model selection.\nMaximum likelihood estimation represents the cornerstone of classical statistical inference. Given observed data \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n\\) and a parametric model \\(f_{\\theta}(x)\\), the MLE principle seeks to find the parameter values that maximize the likelihood function: \\[\n\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} \\mathcal{L}(\\theta; \\mathcal{D}) = \\arg\\max_{\\theta} \\prod_{i=1}^n p(y_i | x_i, \\theta)\n\\]\nThis approach has several appealing properties: it provides consistent estimators under mild conditions, achieves the Cramér-Rao lower bound asymptotically, and offers a principled framework for parameter estimation. However, MLE has well-documented limitations, particularly in high-dimensional settings MLE can lead to overfitting, poor generalization, and numerical instability. Furthermore, as was shown by Stein’s paradox, MLE can be inadmissible. Meaning, there are other estimators that have lower risk than the MLE. We will start this chapter with the normal means problem and show how MLE can be inadmissible.\nConsider the vector of means case where \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\). We have \\[\ny_i \\mid \\theta_i \\sim N(\\theta_i, \\sigma^2), ~ i=1,\\ldots,p &gt; 2\n\\tag{1.1}\\]\nThe goal is to estimate the vector of means \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\) and we can achieve this by borrowing strength across the observations. This is also a proxy for non-parametric regression, where \\(\\theta_i = f(x_i)\\). Also typically \\(y_i\\) is a mean of \\(n\\) observations, i.e. \\(y_i = \\frac{1}{n} \\sum_{j=1}^n x_{ij}\\). Much has been written on the properties of the Bayes risk as a function of \\(n\\) and \\(p\\). Much work has also been done on the asymptotic properties of the Bayes risk as \\(n\\) and \\(p\\) grow to infinity.\nThe goal is to estimate the vector \\(\\theta\\) using a squared loss \\[\n\\mathcal{L}(\\theta, \\hat{\\theta}) = \\sum_{i=1}^p (\\theta_i - \\hat{\\theta}_i)^2,\n\\] where \\(\\hat \\theta\\) is the vector of estimates. Now, we will compare the MLE estimate and what is called the James-Stein estimate. A principled way to evaluate the performance of an estimator is to average its loss over the data, this metric is called the risk. The MLE estimate \\(\\hat \\theta_{i} = y_i\\) has a constant risk \\(p\\) \\[\nR(\\theta,\\hat \\theta ) = \\E[y]{\\mathcal{L}\\left(\\theta, \\hat \\theta\\right) } = p.\n\\] Here expectation is over the data given by distribution Equation 1.1. The estimate is map (rule) from the data to the parameter space \\(\\hat \\theta = \\hat \\theta(y)\\).\nBayesian inference offers a fundamentally different perspective by incorporating prior knowledge and quantifying uncertainty through probability distributions. The Bayesian approach begins with a prior distribution \\(p(\\theta)\\) over the parameter space and updates this belief using Bayes’ rule: \\[\np(\\theta | y) = \\frac{p(y | \\theta) p(\\theta)}{p(y)}\n\\]\nThe Bayes estimator is the value \\(\\hat \\theta^{B}\\) that minimizes the Bayes risk, the expected loss: \\[\n\\hat \\theta^{B} = \\arg\\min_{\\hat \\theta(y)} R(\\pi, \\hat \\theta(y))\n\\] Here \\(\\pi\\) is the prior distribution of \\(\\theta\\) and \\(R(\\pi, \\hat \\theta(y))\\) is the Bayes risk defined as: \\[\nR(\\pi, \\hat{\\theta}(y)) = \\mathbb{E}_{\\theta \\sim \\pi} \\left[ \\mathbb{E}_{y\\mid \\theta} \\left[ \\mathcal{L}(\\theta, \\hat{\\theta}(y)) \\right] \\right].\n\\tag{1.2}\\] For squared error loss, this yields the posterior mean \\(\\E{\\theta \\mid y}\\), while for absolute error loss, it gives the posterior median.\nFor the normal means problem with squared error loss, this becomes: \\[\nR(\\pi, \\hat{\\theta}(y)) = \\int \\left( \\int (\\theta - \\hat{\\theta}(y))^2 p(y|\\theta) dy \\right) \\pi(\\theta) d\\theta\n\\]\nThe Bayes risk quantifies the expected performance of an estimator, taking into account both the uncertainty in the data and the prior uncertainty about the parameter. It serves as a benchmark for comparing different estimators: an estimator with lower Bayes risk is preferred under the chosen prior and loss function. In particular, the Bayes estimator achieves the minimum possible Bayes risk for the given prior and loss.\nIn 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution that has uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein’s rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson’s chi-square test with results from a computer simulation.\nIn each of these examples, the mean square error of these rules is less than half that of the sample mean. This result is paradoxical because it contradicts the elementary law of statistical theory. The philosophical implications of Stein’s paradox are also significant. It has influenced the development of shrinkage estimators and has connections to Bayesianism and model selection criteria.\nThe James-Stein estimator is a shrinkage estimator that shrinks the MLE towards the prior mean. The prior mean is typically the sample mean of the data. The James-Stein estimator is given by \\[\n\\hat \\theta_{i}^{JS} = (1 - \\lambda) \\hat \\theta_{i}^{MLE} + \\lambda \\bar y,\n\\] where \\(\\lambda\\) is a shrinkage parameter and \\(\\bar y\\) is the sample mean of the data. The shrinkage parameter is typically chosen to minimize the risk of the estimator.\nFollowing Efron and Morris (1975), we can view the James-Stein estimator through the lens of empirical Bayes methodology. Efron and Morris demonstrate that Stein’s seemingly paradoxical result has a natural interpretation when viewed as an empirical Bayes procedure that estimates the prior distribution from the data itself.\nConsider the hierarchical model: \\[\n\\begin{aligned}\ny_i | \\theta_i &\\sim N(\\theta_i, \\sigma^2) \\\\\n\\theta_i | \\mu, \\tau^2 &\\sim N(\\mu, \\tau^2)\n\\end{aligned}\n\\]\nThe marginal distribution of \\(y_i\\) is then \\(y_i \\sim N(\\mu, \\sigma^2 + \\tau^2)\\). In the empirical Bayes approach, we estimate the hyperparameters \\(\\mu\\) and \\(\\tau^2\\) from the marginal likelihood:\n\\[\nm(y | \\mu, \\tau^2) = \\prod_{i=1}^p \\frac{1}{\\sqrt{2\\pi(\\sigma^2 + \\tau^2)}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2(\\sigma^2 + \\tau^2)}\\right)\n\\]\nThe maximum marginal likelihood estimators are: \\[\n\\hat{\\mu} = \\bar{y} = \\frac{1}{p}\\sum_{i=1}^p y_i\n\\] \\[\n\\hat{\\tau}^2 = \\max\\left(0, \\frac{1}{p}\\sum_{i=1}^p (y_i - \\bar{y})^2 - \\sigma^2\\right)\n\\]\nThe empirical Bayes estimator then becomes: \\[\n\\hat{\\theta}_i^{EB} = \\frac{\\hat{\\tau}^2}{\\sigma^2 + \\hat{\\tau}^2} y_i + \\frac{\\sigma^2}{\\sigma^2 + \\hat{\\tau}^2} \\hat{\\mu}\n\\]\nThis can be rewritten as: \\[\n\\hat{\\theta}_i^{EB} = \\left(1 - \\frac{\\sigma^2}{\\sigma^2 + \\hat{\\tau}^2}\\right) y_i + \\frac{\\sigma^2}{\\sigma^2 + \\hat{\\tau}^2} \\bar{y}\n\\]\nWhen \\(\\mu = 0\\) and using the estimate \\(\\hat{\\tau}^2 = \\max(0, \\|y\\|^2/p - \\sigma^2)\\), this reduces to a form closely related to the James-Stein estimator: \\[\n\\hat{\\theta}_i^{JS} = \\left(1 - \\frac{(p-2)\\sigma^2}{\\|y\\|^2}\\right) y_i\n\\]\nEfron and Morris show that the empirical Bayes interpretation provides insight into why the James-Stein estimator dominates the MLE. The key insight is that the MLE implicitly assumes an improper flat prior \\(\\pi(\\theta) \\propto 1\\), which leads to poor risk properties in high dimensions.\nThe risk of the MLE is constant: \\[\nR(\\theta, \\hat{\\theta}^{MLE}) = \\mathbb{E}[\\|\\hat{\\theta}^{MLE} - \\theta\\|^2] = p\\sigma^2\n\\]\nIn contrast, the James-Stein estimator has risk: \\[\nR(\\theta, \\hat{\\theta}^{JS}) = p\\sigma^2 - (p-2)\\sigma^2 \\mathbb{E}\\left[\\frac{1}{\\|\\theta + \\epsilon\\|^2/\\sigma^2}\\right]\n\\]\nwhere \\(\\epsilon \\sim N(0, \\sigma^2 I)\\). Since the second term is always positive, we have: \\[\nR(\\theta, \\hat{\\theta}^{JS}) &lt; R(\\theta, \\hat{\\theta}^{MLE}) \\quad \\forall \\theta \\in \\mathbb{R}^p, \\quad p \\geq 3\n\\]\nThis uniform domination demonstrates the inadmissibility of the MLE under squared error loss for \\(p \\geq 3\\).\nThe James-Stein estimator is not the only shrinkage estimator that dominates the MLE. Other shrinkage estimators, such as the ridge regression estimator, also have lower risk than the MLE. The key insight is that shrinkage estimators can leverage prior information to improve estimation accuracy, especially in high-dimensional settings.\nNote, that we used the empirical Bayes version of the definition of risk. Full Bayes approach incorporates both the data and the prior distribution of the parameter as in Equation 1.2.\nIn the normal means problem, the Bayes risk can be explicitly calculated due to the conjugacy of the normal prior and likelihood, and it illustrates how incorporating prior information (via shrinkage) can lead to estimators with lower overall risk compared to the MLE, especially in high-dimensional settings.\nFrom a historical perspective, James-Stein (a.k.a \\(L^2\\)-regularisation, Stein (1964)) is only a global shrinkage rule–in the sense that there are no local parameters to learn about sparsity. A simple sparsity example shows the issue with \\(L^2\\)-regularisation. Consider the sparse \\(r\\)-spike shows the problem with focusing solely on rules with the same shrinkage weight (albeit benefiting from pooling of information).\nLet the true parameter value be \\(\\theta_p = \\left ( \\sqrt{d/p} , \\ldots , \\sqrt{d/p} , 0 , \\ldots , 0 \\right )\\). James-Stein is equivalent to the model \\[\ny_i = \\theta_i + \\epsilon_i \\; \\mathrm{ and} \\; \\theta_i \\sim \\mathcal{N} \\left ( 0 , \\tau^2 \\right )\n\\]\nA better approach to address sparsity is to use a local shrinkage estimator, such as the horseshoe prior. The horseshoe prior is particularly effective for sparse signals, as it allows for strong shrinkage of noise while preserving signals.\nThe horseshoe prior is defined as: \\[\n\\theta_i \\sim N(0, \\sigma^2 \\tau^2 \\lambda_i^2), \\quad \\lambda_i \\sim C^+(0, 1), \\quad \\tau \\sim C^+(0, 1)\n\\]\nHere, \\(\\lambda_i\\) is a local shrinkage parameter, and \\(\\tau\\) is a global shrinkage parameter. The half-Cauchy distribution \\(C^+\\) ensures heavy tails, allowing for adaptive shrinkage. We will discuss the horseshoe prior in more detail later in this section.\nThe motivation for the shrinkage prior rather than a flat uniform prior are the following probabilistic arguments. They have an ability to balance signal detection and noise suppression in high-dimensional settings. Unlike flat uniform priors, shrinkage priors adaptively shrink small signals towards zero while preserving large signals. This behavior is crucial for sparse estimation problems, where most parameters are expected to be zero or near-zero. The James-Stein procedure is an example of global shrinkage, when the overall sparsity level across all parameters is controlled, ensuring that the majority of parameters are shrunk towards zero. Later in this section we will discuss local shrinkage priors, such as the horseshoe prior, which allow individual parameters to escape shrinkage if they represent significant signals.\nIn summary, flat uniform priors (MLE) fail to provide adequate regularization in high-dimensional settings, leading to poor risk properties and overfitting. By incorporating probabilistic arguments and hierarchical structures, shrinkage priors offer a principled approach to regularization that aligns with Bayesian decision theory and modern statistical practice.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#normal-means-problem",
    "href": "17-theoryai.html#normal-means-problem",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "",
    "text": "Example 1.1 (Example: James-Stein for Baseball Batting Averages) We reproduce the baseball batting average example from Efron and Morris (1977). Data below has the number of hits for 18 baseball player after 45 at-beat in 1970 season\n\n# Data source: https://www1.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt\nbaseball = read.csv(\"../data/EfronMorrisBB.txt\", sep = \"\\t\", stringsAsFactors = FALSE) %&gt;% select(LastName,AtBats,BattingAverage,SeasonAverage)\n\nNow, we can eatimate overall mean and variance\n\nmu_hat &lt;- mean(baseball$BattingAverage)\nsigma2_hat &lt;- var(baseball$BattingAverage)\n\nAs well as the posterior mean for each player (James-Stein estimator)\n\nbaseball &lt;- baseball %&gt;%\n  mutate(\n    JS = (sigma2_hat / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * mu_hat +\n      ((BattingAverage * (1 - BattingAverage) / AtBats) / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * BattingAverage\n  )\nkable(baseball)\n\n\n\n\nLastName\nAtBats\nBattingAverage\nSeasonAverage\nJS\n\n\n\n\nClemente\n45\n0.40\n0.35\n0.34\n\n\nRobinson\n45\n0.38\n0.31\n0.32\n\n\nHoward\n45\n0.36\n0.28\n0.31\n\n\nJohnstone\n45\n0.33\n0.24\n0.30\n\n\nBerry\n45\n0.31\n0.28\n0.29\n\n\nSpencer\n45\n0.31\n0.27\n0.29\n\n\nKessinger\n45\n0.29\n0.27\n0.28\n\n\nAlvarado\n45\n0.27\n0.22\n0.27\n\n\nSanto\n45\n0.24\n0.27\n0.26\n\n\nSwaboda\n45\n0.24\n0.23\n0.26\n\n\nPetrocelli\n45\n0.22\n0.26\n0.25\n\n\nRodriguez\n45\n0.22\n0.22\n0.25\n\n\nScott\n45\n0.22\n0.30\n0.25\n\n\nUnser\n45\n0.22\n0.26\n0.25\n\n\nWilliams\n45\n0.22\n0.25\n0.25\n\n\nCampaneris\n45\n0.20\n0.28\n0.24\n\n\nMunson\n45\n0.18\n0.30\n0.23\n\n\nAlvis\n45\n0.16\n0.18\n0.22\n\n\n\n\n\nPlot below shows the observed averages vs. James-Stein estimate\n\nggplot(baseball, aes(x = BattingAverage, y = JS)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"Observed Batting Average\",\n    y = \"James-Stein Estimate\",\n    title = \"Empirical Bayes Shrinkage of Batting Averages\"\n  )\n\n\n\n\n\n\n\n\nCalculate mean squared error (MSE) for observed and James-Stein estimates\n\nmse_observed &lt;- mean((baseball$BattingAverage - mu_hat)^2)\nmse_js &lt;- mean((baseball$JS - mu_hat)^2)\n\ncat(sprintf(\"MSE (Observed): %.6f\\n\", mse_observed))\n\n## MSE (Observed): 0.004584\n\ncat(sprintf(\"MSE (James-Stein): %.6f\\n\", mse_js))\n\n## MSE (James-Stein): 0.001031\n\n\nWe can see that the James-Stein estimator has a lower MSE than the observed batting averages. This is a demonstration of Stein’s paradox, where the James-Stein estimator, which shrinks the estimates towards the overall mean, performs better than the naive sample mean estimator.\n\na = matrix(rep(1:3, nrow(baseball)), 3, nrow(baseball))\nb = matrix(c(baseball$BattingAverage, baseball$SeasonAverage, baseball$JS),    3, nrow(baseball), byrow=TRUE)\n\nmatplot(a, b, pch=\" \", ylab=\"predicted average\", xaxt=\"n\", xlim=c(0.5, 3.1), ylim=c(0.13, 0.42))\nmatlines(a, b)\ntext(rep(0.7, nrow(baseball)), baseball$BattingAverage, baseball$LastName, cex=0.6)\ntext(1, 0.14, \"First 45\\nat bats\", cex=0.5)\ntext(2, 0.14, \"Average\\nof remainder\", cex=0.5)\ntext(3, 0.14, \"J-S\\nestimator\", cex=0.5)\n\n\n\n\n\n\n\n\nNow if we look at the season dynamics for Clemente\n\n# Data source: https://www.baseball-almanac.com/players/hittinglogs.php?p=clemero01&y=1970\ncl = read.csv(\"../data/clemente.csv\")\nx = cumsum(cl$AB)\ny = cumsum(cl$H)/cumsum(cl$AB)\n# Plot x,y startind from index 2\nind = c(1,2)\nplot(x[-ind],y[-ind], type='o', ylab=\"Betting Average\", xlab=\"Number at Bats\")\n# Add horizontal line for season average 145/412 and add text above line `Seaosn Average`\ntext(200, 145/412 + 0.005, \"Season Average\", col = \"red\")\nabline(h = 145/412, col = \"red\", lty = 2)\n# Ted williams record is .406 in in 1941, so you know the first data points are noise\ntext(200, baseball$JS[1] + 0.005, \"JS\", col = \"red\")\nabline(h = baseball$JS[1], col = \"red\", lty = 2)\ntext(200, baseball$BattingAverage[1] + 0.005, \"After 45 Bets\", col = \"red\")\nabline(h = baseball$BattingAverage[1], col = \"red\", lty = 2)",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#maximum-aposteriori-estimation-map-and-regularization",
    "href": "17-theoryai.html#maximum-aposteriori-estimation-map-and-regularization",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.2 Maximum Aposteriori Estimation (MAP) and Regularization",
    "text": "1.2 Maximum Aposteriori Estimation (MAP) and Regularization\nIn the previous sections, we have seen how the Bayesian approach provides a principled framework for parameter estimation through the use of prior distributions and the minimization of Bayes risk. However, in many practical scenarios, we may not have access to a full Bayesian model or the computational resources to perform Bayesian inference. This is where the concept of MAP or regularization comes into play. It also sometimes called a poor man’s Bayesian approach.\nGiven input-output pairs \\((x_i,y_i)\\), MAP learns the funvtion \\(f\\) that maps inputs \\(x_i\\) to outputs \\(y_i\\) by minimizing \\[\n\\sum_{i=1}^N  \\mathcal{L}(y_i,f(x_i)) + \\lambda \\phi(f) \\rightarrow \\text{minimize}_{f}.\n\\] The first term is the loss function that measures the difference between the predicted output \\(f(x_i)\\) and the true output \\(y_i\\). The second term is a regularization term that penalizes complex functions \\(f\\) to prevent overfitting. The parameter \\(\\lambda\\) controls the trade-off between fitting the data well and keeping the function simple. In the case when \\(f\\) is a parametric model, then we simply replace \\(f\\) with the parameters \\(\\theta\\) of the model, and the regularization term becomes a penalty on the parameters.\nThe loss is simply a negative log-likelihood from a probabilistic model specified for the data generating process. For example, when \\(y\\) is numeric and \\(y_i \\mid x_i \\sim N(f(x_i),\\sigma^2)\\), we get the squared loss \\(\\mathcal{L}(y,f(x)) = (y-f(x))^2\\). When \\(y_i\\in \\{0,1\\}\\) is binary, we use the logistic loss \\(\\mathcal{L}(y,f(x)) = \\log(1+\\exp(-yf(x)))\\).\nThe penalty term \\(\\lambda \\phi(f)\\) discourages complex functions \\(f\\). Then, we can think of regularization as a technique to incorporate some prior knowledge about parameters of the model into the estimation process. Consider an example when regularization allows us to solve a hard problem of filtering noisy traffic data.\n\nExample 1.2 (Traffic) Consider traffic flow speed measured by an in-ground sensor installed on interstate I-55 near Chicago. Speed measurements are noisy and prone to have outliers. Figure 1.1 shows speed measured data, averaged over five minute intervals on one of the weekdays.\n\n\n\n\n\n\nFigure 1.1: Speed profile over 24 hour period on I-55, on October 22, 2013\n\n\n\nThe statistical model is \\[\ny_t = f_t + \\epsilon_t, ~ \\epsilon_t \\sim N(0,\\sigma^2), ~ t=1,\\ldots,n,\n\\] where \\(y_t\\) is the speed measurement at time \\(t\\), \\(f_t\\) is the true underlying speed at time \\(t\\), and \\(\\epsilon_t\\) is the measurement noise. There are two sources of noise. The first is the measurement noise, caused by inhalant nature of the sensor’s hardware. The second source is due to sampling error, vehicles observed on a specific lane where senor installed might not represent well traffic in other lanes. A naive MLE approach woule be to estimate the speed profile \\(f = (f_1, \\ldots, f_n)\\) by minimizing the squared loss \\[\n\\hat f = \\arg\\min_{f} \\sum_{t=1}^{n} (y_t - f_t)^2.\n\\] However, the minima of this loss function is 0 and corresponds to the case when \\(\\hat f_t = y_t\\) for all \\(t\\). We have learned nothing about the speed profile, and the estimate is simply the noisy observation \\(y_t\\). In this case, we have no way to distinguish between the true speed profile and the noise.\nHowever, we can use regularization and bring some prior knowledge about traffic speed profiles to improve the estimate of the speed profile and to remove the noise.\nSpecifically, we will use a trend filtering approach. Under this approach, we assume that the speed profile \\(f\\) is a piece-wise linear function of time, and we want to find a function that captures the underlying trend while ignoring the noise. The regularization term \\(\\phi(f)\\) is then the second difference of the speed profile, \\[\n\\lambda \\sum_{t=1}^{n-1}|f_{t-1} - 2f_t + f_{t+1}|\n\\] which penalizes the “kinks” in the speed profile. The value of this penalty is zero, when \\(f_{t-1}, f_t, f_{t+1}\\) lie on a straight line, and it increases when the speed profile has a kink. The parameter \\(\\lambda\\) is a regularization parameter that controls the strength of the penalty.\nTrend filtering penalized function is then \\[\n(1/2) \\sum_{t=1}^{n}(y_t - f_t)^2 + \\lambda \\sum_{t=1}^{n-1}|f_{t-1} - 2f_t + f_{t+1}|,\n\\] which is a variation of a well-know Hodrick-Prescott filter.\nThis approach requires us to choose the regularization parameter \\(\\lambda\\). A small value of \\(\\lambda\\) will lead to a function that fits the data well, but may not capture the underlying trend. A large value of \\(\\lambda\\) will lead to a function that captures the underlying trend, but may not fit the data well. The optimal value of \\(\\lambda\\) can be chosen using cross-validation or other model selection techniques. The left panel of ?fig-traffic shows the trend filtering for different values of \\(\\lambda \\in \\{5,50,500\\}\\). The right panel shows the optimal value of \\(\\lambda\\) chosen by cross-validation (by visual inspection).\n\n\n\n\n\n\nTrend filter for different penalty\n\n\n\n\n\n\n\nTrend Filtering for Traffic Speed Data\n\n\n\n\n\n\nThere is a duality between using regularization term in optimisation problem and assuming a prior distribution over the parameters of the model \\(f\\). Given the likelihood \\(L(y_i,f(x_i))\\), the posterior is given by Bayes’ rule: \\[\np(f \\mid y_i, x_i) = \\frac{\\prod_{i=1}^n L(y_i,f(x_i)) p(f)}{p(y_i \\mid x_i)}.\n\\] If we take the negative log of this posterior, we get: \\[\n-\\log p(f \\mid y_i, x_i) = - \\sum_{i=1}^n \\log L(y_i,f(x_i)) - \\log p(f) + \\log p(y_i \\mid x_i).\n\\] Since loss is the negative log-likelihood \\(-\\log L(y_i,f(x_i))  = \\mathcal{L}(y_i,f(x_i))\\), the posterior maximization is equivalent to minimizing the following regularized loss function: \\[\n\\sum_{i=1}^n \\mathcal{L}(y_i,f(x_i)) + \\log p(f).\n\\] The last term \\(\\log p(y_i \\mid x_i)\\) does not depend on \\(f\\) and can be ignored in the optimization problem. Thus, the equivalence is given by: \\[\n\\lambda \\phi(f) = -\\log p(f),\n\\] where \\(\\phi(f)\\) is the penalty term that corresponds to the prior distribution of \\(f\\). Below we will consider several choices for the prior distribution of \\(f\\) and the corresponding penalty term \\(\\phi(f)\\) commonly used in practice.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#ridge-regression",
    "href": "17-theoryai.html#ridge-regression",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.3 Ridge Regression",
    "text": "1.3 Ridge Regression\nThe ridge regression uses a gaussian prior on the parameters of the model \\(f\\), which leads to a squared penalty term. Specifically, we assume that the parameters \\(\\beta\\) of the model \\(f(x) = x^T\\beta\\) are distributed as: \\[\n\\beta \\sim N(0, \\sigma^2 I),\n\\] where \\(I\\) is the identity matrix. The prior distribution of \\(\\beta\\) is a multivariate normal distribution with mean 0 and covariance \\(\\sigma^2 I\\). The negative log of this prior distribution is given by: \\[\n-\\log p(\\beta) = \\frac{1}{2\\sigma^2} \\|\\beta||_2^2 + \\text{const},\n\\] where \\(\\|\\beta||_2^2 = \\sum_{j=1}^p \\beta_j^2\\) is the squared 2-norm of the vector \\(\\beta\\). The regularization term \\(\\phi(f)\\) is then given by: \\[\n\\phi(f) = \\frac{1}{2\\sigma^2} \\|\\beta||_2^2.  \n\\] This leads to the following optimization problem: \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad ||y- X\\beta||_2^2 + \\lambda ||\\beta||_2^2,\n\\] where \\(\\lambda = 1/\\sigma^2\\) is the regularization parameter that controls the strength of the prior. The solution to this optimization problem is given by: \\[\n\\hat{\\beta}_{\\text{ridge}} = ( X^T X + \\lambda I )^{-1} X^T y.\n\\] The regularization parameter \\(\\lambda\\) is related to the variance of the prior distribution. When \\(\\lambda=0\\), the function \\(f\\) is the maximum likelihood estimate of the parameters. When \\(\\lambda\\) is large, the function \\(f\\) is the prior mean of the parameters. When \\(\\lambda\\) is infinite, the function \\(f\\) is the prior mode of the parameters.\nNotice, that the OLS estimate (invented by Gauss) is a special case of ridge regression when \\(\\lambda = 0\\): \\[\n\\hat{\\beta}_{\\text{OLS}} = ( X^T X )^{-1} X^T y.\n\\]\nThe original motivation for ridge regularisation was to address the problem of numerical instability in the OLS solution when the design matrix \\(X\\) is ill-conditioned, i.e. when \\(X^T X\\) is close to singular. In this case, the OLS solution can be very sensitive to small perturbations in the data, leading to large variations in the estimated coefficients \\(\\hat{\\beta}\\). This is particularly problematic when the number of features \\(p\\) is large, as the condition number of \\(X^T X\\) can grow rapidly with \\(p\\). The ridge regression solution stabilizes the OLS solution by adding a small positive constant \\(\\lambda\\) to the diagonal of the \\(X^T X\\) matrix, which improves the condition number and makes the solution more robust to noise in the data. The additional term \\(\\lambda I\\) simply shifts the eigenvalues of \\(X^T X\\) away from zero, thus improving the numerical stability of the inversion.\nAnother way to think and write the objective function of Ridge as the following constrained optimization problem: \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad ||y- X\\beta||_2^2 \\quad \\text{subject to} \\quad ||\\beta||_2^2 \\leq t,\n\\] where \\(t\\) is a positive constant that controls the size of the coefficients \\(\\beta\\). This formulation emphasizes the idea that ridge regression is a form of regularization that constrains the size of the coefficients, preventing them from growing too large and leading to overfitting. The constraint \\(||\\beta||_2^2 \\leq t\\) can be interpreted as a budget on the size of the coefficients, where larger values of \\(t\\) allow for larger coefficients and more complex models.\nConstraint on the model parameters (and the original Ridge estimator) was proposed by Tikhonov et al. (1943) for solving inverse problems to “discover” physical laws from observations. The norm of the \\(\\beta\\) vector would usually represent amount of energy required. Many processes in nature are energy minimizing!\nAgain, the tuning parameter \\(\\lambda\\) controls trade-off between how well model fits the data and how small \\(\\beta\\)s are. Different values of \\(\\lambda\\) will lead to different models. We select \\(\\lambda\\) using cross validation.\n\nExample 1.3 (Shrinkage) Consider a simulated data with \\(n=50\\), \\(p=30\\), and \\(\\sigma^2=1\\). The true model is linear with \\(10\\) large coefficients between \\(0.5\\) and \\(1\\).\nOur approximators \\(\\hat f_{\\beta}\\) is a linear regression. We can empirically calculate the bias by calculating the empirical squared loss \\(1/n||y -\\hat y||_2^2\\) and variance can be empirically calculated as \\(1/n\\sum  (\\bar{\\hat{y}} - \\hat y_i)\\)\nBias squared \\(\\mathrm{Bias}(\\hat{y})^2=0.006\\) and variance \\(\\Var{\\hat{y}} =0.627\\). Thus, the prediction error = \\(1 + 0.006 + 0.627 = 1.633\\)\nWe’ll do better by shrinking the coefficients to reduce the variance. Let’s estimate, how big a gain will we get with Ridge?\n\n\n\nTrue model coefficients\n\n\nNow we see the accuracy of the model as a function of \\(\\lambda\\)\n\n\n\nPrediction error as a function of \\(\\lambda\\)\n\n\nRidge Regression At best: Bias squared \\(=0.077\\) and variance \\(=0.402\\).\nPrediction error = \\(1 + 0.077 + 0.403 = 1.48\\)\n\n\n\nRidge\n\n\n\n\n1.3.1 Kernel View of Ridge Regression\nAnother interesting view stems from what is called the push-through matrix identity: \\[\n(aI + UV)^{-1}U = U(aI + VU)^{-1}\n\\] for \\(a\\), \\(U\\), \\(V\\) such that the products are well-defined and the inverses exist. We can obtain this from \\(U(aI + VU) = (aI + UV)U\\), followed by multiplication by \\((aI + UV)^{-1}\\) on the left and the right. Applying the identity above to the ridge regression solution with \\(a = \\lambda\\), \\(U = X^T\\), and \\(V = X\\), we obtain an alternative form for the ridge solution: \\[\n\\hat{\\beta} = X^T (XX^T + \\lambda I)^{-1} Y.\n\\] This is often referred to as the kernel form of the ridge estimator. From this, we can see that the ridge fit can be expressed as \\[\nX\\hat{\\beta} = XX^T (XX^T + \\lambda I)^{-1} Y.\n\\] What does this remind you of? This is precisely \\(K(K + \\lambda I)^{-1}Y\\) where \\(K = XX^T\\), which, recall, is the fit from RKHS regression with a linear kernel \\(k(x, z) = x^T z\\). Therefore, we can think of RKHS regression as generalizing ridge regression by replacing the standard linear inner product with a general kernel. (Indeed, RKHS regression is often called kernel ridge regression.) \\[\n(aI + UV)^{-1}U = U(aI + VU)^{-1}\n\\] for \\(a\\), \\(U\\), \\(V\\) such that the products are well-defined and the inverses exist. We can obtain this from \\(U(aI + VU) = (aI + UV)U\\), followed by multiplication by \\((aI + UV)^{-1}\\) on the left and the right. Applying the identity above to the ridge regression solution with \\(a = \\lambda\\), \\(U = X^T\\), and \\(V = X\\), we obtain an alternative form for the ridge solution: \\[\n\\hat{\\beta} = X^T (XX^T + \\lambda I)^{-1} Y.\n\\] This is often referred to as the kernel form of the ridge estimator. From this, we can see that the ridge fit can be expressed as \\[\nX\\hat{\\beta} = XX^T (XX^T + \\lambda I)^{-1} Y.\n\\] What does this remind you of? This is precisely \\(K(K + \\lambda I)^{-1}Y\\) where \\(K = XX^T\\), which, recall, is the fit from RKHS regression with a linear kernel \\(k(x, z) = x^T z\\). Therefore, we can think of RKHS regression as generalizing ridge regression by replacing the standard linear inner product with a general kernel. (Indeed, RKHS regression is often called kernel ridge regression.)",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#lasso-regression",
    "href": "17-theoryai.html#lasso-regression",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.4 Lasso Regression",
    "text": "1.4 Lasso Regression\nThe Lasso (Least Absolute Shrinkage and Selection Operator) regression uses a Laplace prior on the parameters of the model \\(f\\), which leads to an \\(\\ell_1\\) penalty term. Specifically, we assume that the parameters \\(\\beta\\) of the model \\(f(x) = x^T\\beta\\) are distributed as: \\[\n\\beta_j \\sim \\text{Laplace}(0, b) \\quad \\text{independently for } j = 1, \\ldots, p,\n\\] where \\(b &gt; 0\\) is the scale parameter. The Laplace distribution has the probability density function: \\[\np(\\beta_j \\mid b) = \\frac{1}{2b}\\exp\\left(-\\frac{|\\beta_j|}{b}\\right).\n\\] The negative log of this prior distribution is given by: \\[\n-\\log p(\\beta) = \\frac{1}{b} \\|\\beta\\|_1 + \\text{const},\n\\] where \\(\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|\\) is the \\(\\ell_1\\)-norm of the vector \\(\\beta\\). The regularization term \\(\\phi(f)\\) is then given by: \\[\n\\phi(f) = \\frac{1}{b} \\|\\beta\\|_1.  \n\\] This leads to the following optimization problem: \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad \\|y- X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1,\n\\] where \\(\\lambda = 2\\sigma^2/b\\) is the regularization parameter that controls the strength of the prior. Unlike ridge regression, the Lasso optimization problem does not have a closed-form solution due to the non-differentiable nature of the \\(\\ell_1\\) penalty. However, efficient algorithms such as coordinate descent and proximal gradient methods can be used to solve it.\nThe key distinguishing feature of Lasso is its ability to perform automatic variable selection. The \\(\\ell_1\\) penalty encourages sparsity in the coefficient vector \\(\\hat{\\beta}\\), meaning that many coefficients will be exactly zero. This property makes Lasso particularly useful for high-dimensional problems where feature selection is important.\nWhen \\(\\lambda=0\\), the Lasso reduces to the ordinary least squares (OLS) estimate. As \\(\\lambda\\) increases, more coefficients are driven to exactly zero, resulting in a sparser model. When \\(\\lambda\\) is very large, all coefficients become zero.\nThe geometric intuition behind Lasso’s sparsity-inducing property comes from the constraint formulation. We can write the Lasso problem as: \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad \\|y- X\\beta\\|_2^2 \\quad \\text{subject to} \\quad \\|\\beta\\|_1 \\leq t,\n\\] where \\(t\\) is a positive constant that controls the sparsity of the solution. The constraint region \\(\\|\\beta\\|_1 \\leq t\\) forms a diamond (in 2D) or rhombus-shaped region with sharp corners at the coordinate axes. The optimal solution often occurs at these corners, where some coefficients are exactly zero.\nFrom a Bayesian perspective, the Lasso estimator corresponds to the maximum a posteriori (MAP) estimate under independent Laplace priors on the coefficients. We use Bayes rule to calculate the posterior as a product of Normal likelihood and Laplace prior: \\[\n\\log p(\\beta \\mid y, b) \\propto -\\|y-X\\beta\\|_2^2 - \\frac{2\\sigma^2}{b}\\|\\beta\\|_1.\n\\] For fixed \\(\\sigma^2\\) and \\(b&gt;0\\), the posterior mode is equivalent to the Lasso estimate with \\(\\lambda = 2\\sigma^2/b\\). Large variance \\(b\\) of the prior is equivalent to small penalty weight \\(\\lambda\\) in the Lasso objective function.\nOne of the most popular algorithms for solving the Lasso problem is coordinate descent. The algorithm iteratively updates each coefficient while holding all others fixed. For the \\(j\\)-th coefficient, the update rule is: \\[\n\\hat{\\beta}_j \\leftarrow \\text{soft}\\left(\\frac{1}{n}\\sum_{i=1}^n x_{ij}(y_i - \\sum_{k \\neq j} x_{ik}\\hat{\\beta}_k), \\frac{\\lambda}{n}\\right),\n\\] where the soft-thresholding operator is defined as: \\[\n\\text{soft}(z, \\gamma) = \\text{sign}(z)(|z| - \\gamma)_+ = \\begin{cases}\nz - \\gamma & \\text{if } z &gt; \\gamma \\\\\n0 & \\text{if } |z| \\leq \\gamma \\\\\nz + \\gamma & \\text{if } z &lt; -\\gamma\n\\end{cases}\n\\]\n\nExample 1.6 (Sparsity and Variable Selection)  \n\n# Generate simulated data\nset.seed(123)\nn &lt;- 100  # number of observations\np &lt;- 20   # number of predictors\nsigma &lt;- 1  # noise level\n\n# Create design matrix with some correlation structure\nX &lt;- matrix(rnorm(n * p), n, p)\n# Add some correlation between predictors\nfor(i in 2:p) {\n  X[, i] &lt;- 0.5 * X[, i-1] + sqrt(0.75) * X[, i]\n}\n\n# True coefficients - sparse signal\nbeta_true &lt;- c(3, -2, 1.5, 0, 0, 2, 0, 0, 0, -1, rep(0, 10))\nsparse_indices &lt;- which(beta_true != 0)\n\n# Generate response\ny &lt;- X %*% beta_true + sigma * rnorm(n)\n\n# Fit LASSO path using glmnet\nlibrary(glmnet)\nlasso_fit &lt;- glmnet(X, y, alpha = 1)\n\n# Plot coefficient paths\nplot(lasso_fit, xvar = \"lambda\", label = TRUE)\ntitle(\"LASSO Coefficient Paths\")\n\n\n\n\n\n\n\n# Cross-validation to select optimal lambda\ncv_lasso &lt;- cv.glmnet(X, y, alpha = 1, nfolds = 10)\n\n# Plot cross-validation curve\nplot(cv_lasso)\ntitle(\"Cross-Validation for LASSO\")\n\n\n\n\n\n\n\n# Extract coefficients at optimal lambda\nlambda_min &lt;- cv_lasso$lambda.min\nlambda_1se &lt;- cv_lasso$lambda.1se\n\ncoef_min &lt;- coef(lasso_fit, s = lambda_min)\ncoef_1se &lt;- coef(lasso_fit, s = lambda_1se)\n\n# Compare estimates with true values\ncomparison &lt;- data.frame(\n  True = c(0, beta_true),  # Include intercept\n  LASSO_min = as.vector(coef_min),\n  LASSO_1se = as.vector(coef_1se)\n)\nrownames(comparison) &lt;- c(\"Intercept\", paste0(\"X\", 1:p))\n\n# Print comparison table\nkable(round(comparison, 3), caption = \"Comparison of True and Estimated Coefficients\")\n\n\nComparison of True and Estimated Coefficients\n\n\n\nTrue\nLASSO_min\nLASSO_1se\n\n\n\n\nIntercept\n0.0\n-0.12\n-0.07\n\n\nX1\n3.0\n2.92\n2.67\n\n\nX2\n-2.0\n-2.11\n-1.79\n\n\nX3\n1.5\n1.66\n1.42\n\n\nX4\n0.0\n0.00\n0.00\n\n\nX5\n0.0\n0.01\n0.01\n\n\nX6\n2.0\n1.97\n1.83\n\n\nX7\n0.0\n0.06\n0.00\n\n\nX8\n0.0\n-0.10\n0.00\n\n\nX9\n0.0\n-0.04\n-0.03\n\n\nX10\n-1.0\n-1.02\n-0.87\n\n\nX11\n0.0\n0.12\n0.00\n\n\nX12\n0.0\n0.07\n0.00\n\n\nX13\n0.0\n-0.11\n0.00\n\n\nX14\n0.0\n0.00\n0.00\n\n\nX15\n0.0\n0.04\n0.00\n\n\nX16\n0.0\n0.00\n0.00\n\n\nX17\n0.0\n0.00\n0.00\n\n\nX18\n0.0\n-0.03\n0.00\n\n\nX19\n0.0\n0.07\n0.00\n\n\nX20\n0.0\n-0.13\n0.00\n\n\n\n\n# Visualization of coefficient estimates\nlibrary(reshape2)\nlibrary(ggplot2)\n\n# Melt data for plotting\nplot_data &lt;- melt(comparison, id.vars = NULL)\nplot_data$Variable &lt;- rep(rownames(comparison), 3)\nplot_data$Variable &lt;- factor(plot_data$Variable, levels = rownames(comparison))\n\nggplot(plot_data, aes(x = Variable, y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Coefficient Estimates Comparison\", \n       y = \"Coefficient Value\", \n       fill = \"Method\") +\n  scale_fill_brewer(type = \"qual\", palette = \"Set2\")\n\n\n\n\n\n\n\n# Calculate prediction errors\npred_min &lt;- predict(lasso_fit, newx = X, s = lambda_min)\npred_1se &lt;- predict(lasso_fit, newx = X, s = lambda_1se)\n\nmse_min &lt;- mean((y - pred_min)^2)\nmse_1se &lt;- mean((y - pred_1se)^2)\n\ncat(\"Mean Squared Error (lambda.min):\", round(mse_min, 3), \"\\n\")\n\n## Mean Squared Error (lambda.min): 0.68\n\ncat(\"Mean Squared Error (lambda.1se):\", round(mse_1se, 3), \"\\n\")\n\n## Mean Squared Error (lambda.1se): 0.85\n\n# Variable selection performance\nselected_min &lt;- which(coef_min[-1] != 0)  # Exclude intercept\nselected_1se &lt;- which(coef_1se[-1] != 0)  # Exclude intercept\n\ncat(\"\\nTrue non-zero coefficients:\", sparse_indices, \"\\n\")\n\n## \n## True non-zero coefficients: 1 2 3 6 10\n\ncat(\"Selected by LASSO (lambda.min):\", selected_min, \"\\n\")\n\n## Selected by LASSO (lambda.min): 1 2 3 5 6 7 8 9 10 11 12 13 15 18 19 20\n\ncat(\"Selected by LASSO (lambda.1se):\", selected_1se, \"\\n\")\n\n## Selected by LASSO (lambda.1se): 1 2 3 5 6 9 10\n\n# Calculate selection metrics\ntrue_positives_min &lt;- length(intersect(sparse_indices, selected_min))\nfalse_positives_min &lt;- length(setdiff(selected_min, sparse_indices))\nfalse_negatives_min &lt;- length(setdiff(sparse_indices, selected_min))\n\nprecision_min &lt;- true_positives_min / max(1, length(selected_min))\nrecall_min &lt;- true_positives_min / length(sparse_indices)\n\ncat(\"\\nSelection Performance (lambda.min):\\n\")\n\n## \n## Selection Performance (lambda.min):\n\ncat(\"Precision:\", round(precision_min, 3), \"\\n\")\n\n## Precision: 0.31\n\ncat(\"Recall:\", round(recall_min, 3), \"\\n\")\n\n## Recall: 1\n\n\nThe coefficient paths plot shows how LASSO coefficients shrink toward zero as the regularization parameter lambda increases. The colored lines represent different predictors, demonstrating LASSO’s variable selection property. The cross-validation plot reveals the bias-variance tradeoff: - lambda.min gives the minimum CV error (best predictive performance) - lambda.1se provides a more parsimonious model (within 1 SE of minimum) Performance metrics show LASSO’s effectiveness: - High precision and recall indicate successful variable selection - Lower MSE compared to unregularized models demonstrates regularization benefits - The comparison table reveals LASSO correctly identifies most true signals while setting noise variables to zero The coefficient estimates visualization clearly shows: - True coefficients (ground truth) - LASSO estimates at different lambda values - How regularization affects both signal preservation and noise suppression Key insights: 1. LASSO successfully performs automatic variable selection 2. The method balances model complexity with predictive accuracy 3. Cross-validation provides principled selection of regularization strength 4. Results demonstrate the practical value of sparsity-inducing priors ::\n\n1.4.1 Scale Mixture Representation\nThe Laplace distribution can be represented as a scale mixture of Normal distributions (Andrews and Mallows 1974): \\[\n\\begin{aligned}\n\\beta_j \\mid \\sigma^2,\\tau_j &\\sim N(0,\\tau_j^2\\sigma^2)\\\\\n\\tau_j^2 \\mid \\alpha &\\sim \\text{Exp}(\\alpha^2/2)\\\\\n\\sigma^2 &\\sim \\pi(\\sigma^2).\n\\end{aligned}\n\\] We can show equivalence by integrating out \\(\\tau_j\\): \\[\np(\\beta_j\\mid \\sigma^2,\\alpha) = \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi \\tau_j\\sigma^2}}\\exp\\left(-\\frac{\\beta_j^2}{2\\sigma^2\\tau_j^2}\\right)\\frac{\\alpha^2}{2}\\exp\\left(-\\frac{\\alpha^2\\tau_j^2}{2}\\right)d\\tau_j = \\frac{\\alpha}{2\\sigma}\\exp\\left(-\\frac{\\alpha|\\beta_j|}{\\sigma}\\right).\n\\] This representation allows for efficient Gibbs sampling algorithms that can automatically tune the regularization parameter through the hierarchical Bayesian framework.\n\n\n1.5 Penalty and Regularisation\nThe problem of finding a good model boils down to finding \\(\\phi\\) that minimize some form of Bayes risk for the problem at hand.\nThere are a number of commonly used penalty functions (a.k.a. log prior density). For example, the $ l^2$-norm corresponds to s normal prior. The resulting Bayes rule will take the form of a shrinkage estimator, a weighted combination between data and prior beliefs about the parameter. An $ l^1 $-norm will induce a sparse solution in the estimator and can be used an a variable selection operator. The $ l_0 $-norm directly induces a subset selection procedure.\nThe amount of regularisation \\(\\lambda\\) gauges the trade-off between the compromise between the observed data and the initial prior beliefs.\nThere are two main approaches to finding a good model:\n\nFull Bayes: This approach places a prior distribution on the parameters and computes the full posterior distribution.\nRegularization Methods: These approaches add penalty terms to the objective function to control model complexity.\n\nNow, let’s look at those two approaches in more detail.\nThe full Bayes approach is to place a prior distribution on the parameters and compute the full posterior distribution using the Bayes rule: \\[\np( \\theta | y ) = \\frac{ f( y | \\theta ) p( \\theta ) }{ m(y) },\n\\] here \\[\nm(y) = \\int f( y| \\theta ) p( \\theta ) d \\theta\n\\] Here \\(m(y)\\) is the marginal beliefs about the data. This can also be used to choose the amount of regularisation via the type II maximum likelihood estimator (MMLE) defined by \\[\n\\hat{\\tau} = \\arg \\max \\log m( y | \\tau )\n\\] where again $ m( y | ) = f( y | ) p( | ) $.\nFor example, in the normal-normal model, with \\(\\mu=0\\), we can integrate out the high dimensional \\(\\theta\\) and find \\(m(y | \\tau)\\) in closed form as \\(y_i \\sim N(0, \\sigma^2 + \\tau^2)\\) \\[\nm( y | \\tau ) = ( 2 \\pi)^{-n/2} ( \\sigma^2 + \\tau^2 )^{- n/2}  \\exp \\left ( - \\frac{ \\sum y_i^2 }{ 2 ( \\sigma^2 + \\tau^2) }\n\\] The original JS estimator shrinks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want o shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS is \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), which serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate proper from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\nCommon examples include ridge regression (L2 penalty), lasso regression (L1 penalty), and elastic net (combination of L1 and L2 penalties).\nRather than having to perform high dimensional integration with the likes of MCMC etc, a common approach is to use a maximum a posteriori (MAP) estimator defined by \\[\n\\hat{\\theta} = \\arg \\max \\log p ( \\theta | y )\n\\] This can directly lead to sparsity as in the case of \\(\\ell_1\\)-norm optimisation.\n\n\n1.6 \\(\\ell_2\\) Shrinkage.\nThe original JS estimator shranks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want o shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS uses \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), which serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate proper prior from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\n\n1.6.1 Sparse \\(r\\)-spike problem\nFor the sparse \\(r\\)-spike problem we require a different rule. For a sparse signal, however, \\(\\hat \\theta_{JS}\\) performs poorly when the true parameter is an \\(r\\)-spike where \\(\\theta_r\\) has \\(r\\) coordinates at \\(\\sqrt{p/r}\\) and the rest set at zero with norm \\({\\Vert \\theta_r \\Vert}^2 =p\\).\nThe classical risk satisfies \\(R \\left ( \\hat \\theta_{JS} , \\theta_r \\right ) \\geq p/2\\) where the simple thresholding rule \\(\\sqrt{2 \\ln p}\\) performs with risk \\(\\sqrt{\\ln p}\\) in the \\(r\\)-spike sparse case even though it is inadmissible in MSE for a non-sparse signal. Then is due to the fact that for \\(\\theta_p\\) we have \\[\n\\frac{p \\Vert \\theta \\Vert^2}{p + \\Vert \\theta \\Vert^2} \\leq R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\leq\n2 + \\frac{p \\Vert \\theta \\Vert^2}{ d + \\Vert \\theta \\Vert^2}.\n\\] This implies that \\(R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\geq (p/2)\\). Hence, simple thresholding rule beats James-Stein this with a risk given by \\(\\sqrt{\\log p }\\). This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.\nA Bayes rule that inherits good MSE properties but also simultaneously provides asymptotic minimax estimation risk for sparse signals. HS estimator uniformly dominates the traditional sample mean estimator in MSE and has good posterior concentration properties for nearly black objects. Specifically, the horseshoe estimator attains asymptotically minimax risk rate \\[\n\\sup_{ \\theta \\in l_0[p_n] } \\;\n\\mathbb{E}_{ y | \\theta } \\|\\hat y_{hs} - \\theta \\|^2 \\asymp\np_n \\log \\left ( n / p_n \\right ).\n\\] The “worst’’ \\(\\theta\\) is obtained at the maximum difference between \\(\\left| \\hat \\theta_{HS} - y \\right|\\) where \\(\\hat \\theta_{HS} = \\mathbb{E}(\\theta|y)\\) can be interpreted as a Bayes posterior mean (optimal under Bayes MSE).\nOne such estimator that achieves the optimal minimax rate is the horseshoe estimator proposed by Carvalho, Polson, and Scott (2010).\n\n\n1.6.2 Efron Example\nEfron provide an example which shows the importance of specifying priors in high dimensions. The key idea behind James-Stein shrinkage is that one when one can “borrow strength” across components. In this sense the multivariate parameter estimation problem is easier than the univariate one.\nStein’s phenomenon where \\(y_i | \\theta_i \\sim N(\\theta_i, 1)\\) and \\(\\theta_i \\sim N(0, \\tau^2)\\) where \\(\\tau \\rightarrow \\infty\\) illustrates this point well. This leads to the improper “non-informative” uniform prior. The corresponding generalized Bayes rule is the vector of means—which we know is inadmissible. so no regularisation leads to an estimator with poor risk property.\nLet \\(\\|y\\| = \\sum_{i=1}^p y_i^2\\). Then, we can make the following probabilistic statements from the model, \\[\nP\\left( \\| y \\| &gt; \\| \\theta \\| \\right) &gt; \\frac{1}{2}\n\\] Now for the posterior, this inequallty is reversed under a flat Lebesgue measure, \\[\nP\\left( \\| \\theta \\| &gt; \\| y \\| \\; | \\; y \\right) &gt; \\frac{1}{2}\n\\] which is in conflict with the classical statement. This is a property of the prior which leads to a poor rule (the overall average) and risk.\nThe shrinkage rule (a.k.a. normal prior) where \\(\\tau^2\\) is “estimated” from the data avoids this conflict. More precisely, we have \\[\n\\hat{\\theta}(y) = \\left( 1 - \\frac{k-2}{\\|y\\|^2} \\right) y \\quad \\text{and} \\quad E\\left( \\| \\hat{\\theta} - \\theta \\| \\right) &lt; k, \\; \\forall \\theta.\n\\] Hence, when \\(\\|y\\|^2\\) is small the shrinkage factor is more extreme. For example, if \\(k=10\\), \\(\\|y\\|^2=12\\), then \\(\\hat{\\theta} = (1/3) y\\). Now we have the more intuitive result that \\(P\\left(\\|\\theta\\| &gt; \\|y\\| \\; | \\; y\\right) &lt; \\frac{1}{2}\\).\nThis shows that careful specification of default priors matter in high dimensions is necessary.\n\n\n\n1.7 \\(\\ell_1\\) Sparsity\n\n\n1.8 \\(\\ell_0\\) Subset Selection\nThe canonical problem is estimaiton of the normal means problem. Here we have \\(y_i = \\theta_i + e_i,~i=1,\\ldots,p\\) and \\(e_i \\sim N(0, \\sigma^2)\\). The goal is to estimate the vector of means \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\). This is also a proxy for non-parametric regression, where \\(\\theta_i = f(x_i)\\). Aslo typically \\(y_i\\) is a mean of \\(n\\) observations, i.e. \\(y_i = \\frac{1}{n} \\sum_{j=1}^n x_{ij}\\). ## James-Stein Estimator The classic James-Stein shrinkage rule, \\(\\hat y_{js}\\), uniformly dominates the traditional sample mean estimator, \\(\\hat{\\theta}\\), for all values of the true parameter \\(\\theta\\). In classical MSE risk terms: \\[\nR(\\hat y_{js}, \\theta) \\defeq E_{y|\\theta} {\\Vert \\hat y_{js} - \\theta \\Vert}^2 &lt; p\n    = E_{y|\\theta} {\\Vert y - \\theta \\Vert}^2, \\;\\;\\; \\forall \\theta\n\\] For a sparse signal, however, \\(\\hat y_{js}\\) performs poorly when the true parameter is an \\(r\\)-spike where \\(\\theta_r\\) has \\(r\\) coordinates at \\(\\sqrt{p/r}\\) and the rest set at zero with norm \\({\\Vert \\theta_r \\Vert}^2 =p\\).\nThe classical risk satisfies \\(R \\left ( \\hat y_{js} , \\theta_r \\right ) \\geq p/2\\) where the simple thresholding rule \\(\\sqrt{2 \\ln p}\\) performs with risk \\(\\sqrt{\\ln p}\\) in the \\(r\\)-spike sparse case even though it is inadmissible in MSE for a non-sparse signal.\n\n\n1.9 R-spike Problem\nFrom a historical perspective, James-Stein (a.k.a \\(L^2\\)-regularisation)(Stein 1964) is only a global shrinkage rule–in the sense that there are no local parameters to learn about sparsity. A simple sparsity example shows the issue with \\(L^2\\)-regularisation. Consider the sparse \\(r\\)-spike shows the problem with focusing solely on rules with the same shrinkage weight (albeit benefiting from pooling of information).\nLet the true parameter value be \\(\\theta_p = \\left ( \\sqrt{d/p} , \\ldots , \\sqrt{d/p} , 0 , \\ldots , 0 \\right )\\). James-Stein is equivalent to the model \\[\ny_i = \\theta_i + \\epsilon_i \\; \\mathrm{ and} \\; \\theta_i \\sim \\mathcal{N} \\left ( 0 , \\tau^2 \\right )\n\\] This dominates the plain MLE but loses admissibility! This is due to the fact that a “plug-in” estimate of global shrinkage \\(\\hat{\\tau}\\) is used. Tiao and Tan’s original “closed-form” analysis is particularly relevant here. They point out that the mode of \\(p(\\tau^2|y)\\) is zero exactly when the shrinkage weight turns negative (their condition 6.6). From a risk perspective \\(E \\Vert \\hat{\\theta}^{JS} - \\theta \\Vert \\leq p , \\forall \\theta\\) showing the inadmissibility of the MLE. At origin the risk is \\(2\\), but! \\[\n\\frac{p \\Vert \\theta \\Vert^2}{p + \\Vert \\theta \\Vert^2} \\leq R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\leq\n2 + \\frac{p \\Vert \\theta \\Vert^2}{ d + \\Vert \\theta \\Vert^2}.\n\\] This implies that \\(R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\geq (p/2)\\). Hence, simple thresholding rule beats James-Stein this with a risk given by \\(\\sqrt{\\log p }\\). This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.\nThe horseshoe estimator, which we will discuss in more detail later, \\(\\hat y_{hs}\\), was proposed by Carvalho, Polson, and Scott (2010) to provide a Bayes rule that inherits good MSE properties but also simultaneously provides asymptotic minimax estimation risk for sparse signals. HS estimator uniformly dominates the traditional sample mean estimator in MSE and has good posterior concentration properties for nearly black objects. Specifically, the horseshoe estimator attains asymptotically minimax risk rate \\[\n\\sup_{ \\theta \\in l_0[p_n] } \\;\n\\mathbb{E}_{ y | \\theta } \\|\\hat y_{hs} - \\theta \\|^2 \\asymp\np_n \\log \\left ( n / p_n \\right ).\n\\] The “worst’’ \\(\\theta\\) is obtained at the maximum difference between \\(\\left|\\hat{y}_{hs} - y\\right|\\) where \\(\\hat{y}_{hs} = \\mathbb{E}(\\theta|y)\\) can be interpreted as a Bayes posterior mean (optimal under Bayes MSE).\n\n\n1.10 \\(\\ell_2\\) Shrinkage\n\nExample 1.4 (Stein’s Paradox) Stein’s paradox, as explained Efron and Morris (1977), is a phenomenon in statistics that challenges our intuitive understanding of estimation. The paradox arises when trying to estimate the mean of a multivariate normal distribution. Traditionally, the best guess about the future is usually obtained by computing the average of past events. However, Charles Stein showed that there are circumstances where there are estimators better than the arithmetic average. This is what’s known as Stein’s paradox.\nIn 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution that has uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein’s rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson’s chi-square test with results from a computer simulation.\nIn each of these examples, the mean square error of these rules is less than half that of the sample mean. This result is paradoxical because it contradicts the elementary law of statistical theory. The philosophical implications of Stein’s paradox are also significant. It has influenced the development of shrinkage estimators and has connections to Bayesianism and model selection criteria.\nSuppose that we have \\(n\\) independent observations \\(y_{1},\\ldots,y_{n}\\) from a \\(N\\left(  \\theta,\\sigma^{2}\\right)\\) distribution. The maximum likelihood estimator is \\(\\widehat{\\theta}=\\bar{y}\\), the sample mean. The Bayes estimator is the posterior mean, \\[\n\\widehat{\\theta}=\\mathbb{E}\\left[  \\theta\\mid y\\right]  =\\frac{\\sigma^{2}}{\\sigma^{2}+n}% \\bar{y}.\n\\] The Bayes estimator is a shrinkage estimator, it shrinks the MLE towards the prior mean. The amount of shrinkage is determined by the ratio of the variance of the prior and the variance of the likelihood. The Bayes estimator is also a function of the MLE \\[\n\\widehat{\\theta}=\\frac{\\sigma^{2}}{\\sigma^{2}+n}\\bar{y}+\\frac{n}{\\sigma^{2}+n}\\widehat{\\theta}.\n\\] This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior. The Bayes estimator is a function of the MLE \\[\n\\widehat{\\theta}=\\frac{\\sigma^{2}}{\\sigma^{2}+n}\\bar{y}+\\frac{n}{\\sigma^{2}+n}\\widehat{\\theta}.\n\\] This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior.\nThe original JS estimator shranks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and MOrris and Lindley showed that you want o shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS is \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), whcih serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate propr from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\nWe reproduce the baseball bartting average example from Efron and Morris (1977). Data below has the number of hits for 18 baseball player after 45 at-beat in 1970 season\n\n# Data source: https://www1.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt\nbaseball = read.csv(\"../data/EfronMorrisBB.txt\", sep = \"\\t\", stringsAsFactors = FALSE) %&gt;% select(LastName,AtBats,BattingAverage,SeasonAverage)\n\nNow, we can eatimate overall mean and variance\n\nmu_hat &lt;- mean(baseball$BattingAverage)\nsigma2_hat &lt;- var(baseball$BattingAverage)\n\nAs well as the osterior mean for each player (James-Stein estimator)\n\nbaseball &lt;- baseball %&gt;%\n  mutate(\n    JS = (sigma2_hat / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * mu_hat +\n      ((BattingAverage * (1 - BattingAverage) / AtBats) / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * BattingAverage\n  )\nkable(baseball)\n\n\n\n\nLastName\nAtBats\nBattingAverage\nSeasonAverage\nJS\n\n\n\n\nClemente\n45\n0.40\n0.35\n0.34\n\n\nRobinson\n45\n0.38\n0.31\n0.32\n\n\nHoward\n45\n0.36\n0.28\n0.31\n\n\nJohnstone\n45\n0.33\n0.24\n0.30\n\n\nBerry\n45\n0.31\n0.28\n0.29\n\n\nSpencer\n45\n0.31\n0.27\n0.29\n\n\nKessinger\n45\n0.29\n0.27\n0.28\n\n\nAlvarado\n45\n0.27\n0.22\n0.27\n\n\nSanto\n45\n0.24\n0.27\n0.26\n\n\nSwaboda\n45\n0.24\n0.23\n0.26\n\n\nPetrocelli\n45\n0.22\n0.26\n0.25\n\n\nRodriguez\n45\n0.22\n0.22\n0.25\n\n\nScott\n45\n0.22\n0.30\n0.25\n\n\nUnser\n45\n0.22\n0.26\n0.25\n\n\nWilliams\n45\n0.22\n0.25\n0.25\n\n\nCampaneris\n45\n0.20\n0.28\n0.24\n\n\nMunson\n45\n0.18\n0.30\n0.23\n\n\nAlvis\n45\n0.16\n0.18\n0.22\n\n\n\n\n\nPlot below shows the observed averages vs. James-Stein estimate\n\nggplot(baseball, aes(x = BattingAverage, y = JS)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"Observed Batting Average\",\n    y = \"James-Stein Estimate\",\n    title = \"Empirical Bayes Shrinkage of Batting Averages (2016)\"\n  )\n\n\n\n\n\n\n\n\nCalculate mean squared error (MSE) for observed and James-Stein estimates\n\nmse_observed &lt;- mean((baseball$BattingAverage - mu_hat)^2)\nmse_js &lt;- mean((baseball$JS - mu_hat)^2)\n\ncat(sprintf(\"MSE (Observed): %.6f\\n\", mse_observed))\n\n## MSE (Observed): 0.004584\n\ncat(sprintf(\"MSE (James-Stein): %.6f\\n\", mse_js))\n\n## MSE (James-Stein): 0.001031\n\n\nWe can see that the James-Stein estimator has a lower MSE than the observed batting averages. This is a demonstration of Stein’s paradox, where the James-Stein estimator, which shrinks the estimates towards the overall mean, performs better than the naive sample mean estimator.\n\na = matrix(rep(1:3, nrow(baseball)), 3, nrow(baseball))\nb = matrix(c(baseball$BattingAverage, baseball$SeasonAverage, baseball$JS),    3, nrow(baseball), byrow=TRUE)\n\nmatplot(a, b, pch=\" \", ylab=\"predicted average\", xaxt=\"n\", xlim=c(0.5, 3.1), ylim=c(0.13, 0.42))\nmatlines(a, b)\ntext(rep(0.7, nrow(baseball)), baseball$BattingAverage, baseball$LastName, cex=0.6)\ntext(1, 0.14, \"First 45\\nat bats\", cex=0.5)\ntext(2, 0.14, \"Average\\nof remainder\", cex=0.5)\ntext(3, 0.14, \"J-S\\nestimator\", cex=0.5)\n\n\n\n\n\n\n\n\nNow if we look at the season dynamics for Clemente\n\n# Data source: https://www.baseball-almanac.com/players/hittinglogs.php?p=clemero01&y=1970\ncl = read.csv(\"../data/clemente.csv\")\nx = cumsum(cl$AB)\ny = cumsum(cl$H)/cumsum(cl$AB)\n# Plot x,y startind from index 2\nind = c(1,2)\nplot(x[-ind],y[-ind], type='o', ylab=\"Betting Average\", xlab=\"Number at Bats\")\n# Add horizontal line for season average 145/412 and add text above line `Seaosn Average`\ntext(200, 145/412 + 0.005, \"Season Average\", col = \"red\")\nabline(h = 145/412, col = \"red\", lty = 2)\n# Ted williams record is .406 in in 1941, so you know the first data points are noise\ntext(200, baseball$JS[1] + 0.005, \"JS\", col = \"red\")\nabline(h = baseball$JS[1], col = \"red\", lty = 2)\ntext(200, baseball$BattingAverage[1] + 0.005, \"After 45 Bets\", col = \"red\")\nabline(h = baseball$BattingAverage[1], col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nThe motivation for the shrinkage prior rather than a flat uniform prior are the following probabilistic arguments.\n\nExample 1.5 (Efron Example) Efron shows the importance of priors in high dimensions when one can “borrow strength” (a.k.a. regularisation) across components.\nStein’s phenomenon where \\(y_i | \\theta_i \\sim N(\\theta_i, 1)\\) and \\(\\theta_i \\sim N(0, \\tau^2)\\) illustrates this point well. From the model,\n\\[\nP\\left( \\| y \\| &gt; \\| \\theta \\| \\right) &gt; \\frac{1}{2}\n\\]\nUnder a flat Lebesgue measure, this inequality is reversed in the posterior, namely\n\\[\nP\\left( \\| \\theta \\| &gt; \\| y \\| \\; | \\; y \\right) &gt; \\frac{1}{2}\n\\]\nIn conflict with the classical statement. However, if we use Stein’s rule (posterior where \\(\\tau^2\\) is estimated via empirical Bayes) we have\n\\[\n\\hat{\\theta}(y) = \\left( 1 - \\frac{k-2}{\\|y\\|^2} \\right) y \\quad \\text{and} \\quad E\\left( \\| \\hat{\\theta} - \\theta \\| \\right) &lt; k, \\; \\forall \\theta.\n\\]\nHence, when \\(\\|y\\|^2\\) is small the shrinkage factor is more extreme.\nFor example, if \\(k=10\\), \\(\\|y\\|^2=12\\), then \\(\\hat{\\theta} = (1/3) y\\). Now we have the more intuitive result:\n\\[\nP\\left( \\| \\theta \\| &gt; \\| y \\| \\; | \\; y \\right) &lt; \\frac{1}{2}\n\\]\nShowing that default priors matter in high dimensions.\n\n\n\n1.11 \\(\\ell_1\\) Sparsity\n\n\n1.12 \\(\\ell_0\\) Subset Selection\n\n\n1.13 Bayesain Model Selection via Regularisation\nFrom Bayesian perspective regularization is nothing but incorporation of prior information into the model. Remember, that a Bayesian model is specified by likelihood and prior distributions. Bayesian regularization methods include the Bayesian bridge, horseshoe regularization, Bayesian lasso, Bayesian elastic net, spike-and-slab lasso, and global-local shrinkage priors. Bayesian \\(\\ell_0\\) regularization is an attractive solution for high dimensional variable selection as it directly penalizes the number of predictors. The caveat is the need to search over all possible model combinations, as a full solution requires enumeration over all possible models which is NP-hard.\nIn Bayesian approach, regularization requires the specification of a loss, denoted by \\(\\mathcal{L}\\left(\\beta\\right)\\) and a penalty function, denoted by \\(\\phi_{\\lambda}(\\beta)\\), where \\(\\lambda\\) is a global regularization parameter. From a Bayesian perspective, \\(\\mathcal{L}\\left(\\beta\\right)\\) and \\(\\phi_{\\lambda}(\\beta)\\) correspond to the negative logarithms of the likelihood and prior distribution, respectively. Regularization leads to an maximum a posteriori (MAP) optimization problem of the form \\[\n\\underset{\\beta \\in R^p}{\\mathrm{minimize}\\quad}\n\\mathcal{L}\\left(\\beta\\right) + \\phi_{\\lambda}(\\beta) \\; .\n\\] Taking a probabilistic approach leads to a Bayesian hierarchical model \\[\np(y \\mid \\beta) \\propto \\exp\\{-\\mathcal{L}(\\beta)\\} \\; , \\quad p(\\beta) \\propto \\exp\\{ -\\phi_{\\lambda}(\\beta) \\} \\ .\n\\] The solution to the minimization problem estimated by regularization corresponds to the posterior mode, \\(\\hat{\\beta} = \\mathrm{ arg \\; max}_\\beta \\; p( \\beta|y)\\), where \\(p(\\beta|y)\\) denotes the posterior distribution. Consider a normal mean problem with \\[\n\\label{eqn:linreg}\ny = \\theta+ e \\ , \\ \\  \\text{where } e \\sim N(0, \\sigma^2),~-\\infty \\le \\theta \\le \\infty \\ .\n\\] What prior \\(p(\\theta)\\) should we place on \\(\\theta\\) to be able to separate the “signal” \\(\\theta\\) from “noise” \\(e\\), when we know that there is a good chance that \\(\\theta\\) is sparse (i.e. equal to zero). In the multivariate case we have \\(y_i = \\theta_i + e_i\\) and sparseness is measured by the number of zeros in \\(\\theta = (\\theta_1\\ldots,\\theta_p)\\). The Bayesan Lasso assumes double exponential (a.k.a Laplace) prior distribution where \\[\np(\\theta_i \\mid b) = 0.5b\\exp(-|\\theta|/b).\n\\] We use Bayes rule to calculate the posterior as a product of Normal likelihood and Laplace prior \\[\n\\log p(\\theta \\mid y, b) \\propto ||y-\\theta||_2^2 + \\dfrac{2\\sigma^2}{b}||\\theta||_1.\n\\] For fixed \\(\\sigma^2\\) and \\(b&gt;0\\) the posterior mode is equivalent to the Lasso estimate with \\(\\lambda = 2\\sigma^2/b\\). Large variance \\(b\\) of the prior is equivalent to the small penalty weight \\(\\lambda\\) in the Lasso objective function.\n\n\n1.14 Shrinkage (\\(\\ell_2\\) Norm)\nWe can estimate the risk bounds of \\(\\ell_2\\) Norm regularisation. The classic bias-variance tradeoff is given by the MSE risk bound. \\[\nR(\\theta,\\hat \\theta) = E_{y|\\theta} \\left [ \\Vert \\hat \\theta - \\theta \\Vert^2 \\right ] = \\Vert \\hat \\theta - \\theta \\Vert^2 + E_{y|\\theta} \\left [ \\Vert \\hat \\theta - \\mathbb{E}(\\hat \\theta) \\Vert^2 \\right ]\n\\]\nIn a case of multiple parameters, the Stein bound is \\[\nR(\\theta,\\hat \\theta_{JS}) &lt; R(\\theta,\\hat \\theta_{MLE}) \\;\\;\\; \\forall \\theta \\in \\mathbb{R}^p, \\;\\;\\; p \\geq 3.\n\\] In an applied problem the gap in risk between MLE and JS estimators can be large. For example, in the normal means problem with \\(p=100\\) and \\(n=100\\), the risk of the MLE is \\(R(\\theta,\\hat \\theta_{MLE}) = 100\\) while the risk of the JS estimator is \\(R(\\theta,\\hat \\theta_{JS}) = 1.5\\). The JS estimator is 67 times more efficient than the MLE. The JS estimator is also minimax optimal in the sense that it attains the minimax risk bound for the normal means problem. The minimax risk bound is the smallest risk that can be attained by any estimator.\nJS is a specific estimate and it motivates the ridge estimator. The ridge estimator is a shrinkage estimator with the penalty function being the squared norm of the parameter vector. The ridge estimator is \\[\n\\hat \\theta_{ridge} = \\left (  X^T X + \\lambda I \\right )^{-1} X^T y\n\\] where \\(\\lambda\\) is the regularization parameter.\n\n\n1.15 Sparsity (\\(\\ell_1\\) Norm)\nHigh-dimensional predictor selection and sparse signal recovery are routine statistical and machine learning tasks. There is a vast and growing literator with lasso (leat absolute selection) being the gold standard. Horseshoe priors are the Bayesian counterpart. Sparsity relies on the property of a few large signals among many (nearly zero) noisy observations. The goal is to find a niddle in the haystack. Suppose that we observe data from Normal means model \\[\ny_i \\mid \\theta_i \\sim N(\\theta_i,1),~ 1\\le i\\le p, ~ \\theta = (\\theta_1,\\ldots,\\theta_p),\n\\] where parameter \\(\\theta\\) lies in the ball \\[\n||\\theta||_{\\ell_0} = \\{\\theta : \\text{number of  }\\theta_i \\ne 0 \\le p_n\\}.\n\\]\nEven threshholding can beat MLE, when the signal is sparse. The thresholding estimator is \\[\n\\hat \\theta_{thr} = \\left \\{ \\begin{array}{ll} \\hat \\theta_i & \\mbox{if} \\; \\hat \\theta_i &gt; \\sqrt{2 \\ln p} \\\\ 0 & \\mbox{otherwise} \\end{array} \\right .\n\\]\nSparse signal detection provides a challenge to statistical methodology; consider the classical normal means inference problem. Suppose that we observe data from the probability model \\(( y_i | \\theta_i ) \\sim N( \\theta_i,1)\\). We wish to provide an estimator \\(\\hat y_{hs}\\) for the vector of normal means \\(\\theta = ( \\theta_1, \\ldots , \\theta_p )\\). Sparsity occurs when a large portion of the parameter vector contains zeroes. The ’’ultra-sparse`` or “nearly black” vector case occurs when \\(p_n\\), denoting the number of non-zero parameter values, and for \\(\\theta \\in l_0 [ p_n]\\), which denotes the set \\(\\# ( \\theta_i \\neq 0 ) \\leq p_n\\) where \\(p_n = o(n)\\) where \\(p_n \\rightarrow \\infty\\) as \\(n \\rightarrow \\infty\\).\nThe predictive rule is often represented by a mathematical model, such as a linear regression model or a neural network. The model is trained using historical data, which consists of observed inputs and outputs \\((x_1,y_1),\\ldots, (x_n,y_n)\\).\nThe model is then used to predict the output \\(y\\) for new inputs \\(x\\). The accuracy of the model is evaluated using a performance metric, such as the mean squared error or the mean absolute error. The model is then updated and retrained using new data to improve its accuracy. This process is repeated until the model achieves the desired level of accuracy.\n\n\n\n1.16 LASSO\nThe Laplace distribution can be represented as scale mixture of Normal distribution(Andrews and Mallows 1974) \\[\n\\begin{aligned}\n\\theta_i \\mid \\sigma^2,\\tau \\sim &N(0,\\tau^2\\sigma^2)\\\\\n\\tau^2  \\mid \\alpha \\sim &\\exp (\\alpha^2/2)\\\\\n\\sigma^2 \\sim & \\pi(\\sigma^2).\\end{aligned}\n\\] We can show equivalence by integrating out \\(\\tau\\) \\[\np(\\theta_i\\mid \\sigma^2,\\alpha) =  \\int_{0}^{\\infty} \\dfrac{1}{\\sqrt{2\\pi \\tau}}\\exp\\left(-\\dfrac{\\theta_i^2}{2\\sigma^2\\tau}\\right)\\dfrac{\\alpha^2}{2}\\exp\\left(-\\dfrac{\\alpha^2\\tau}{2}\\right)d\\tau = \\dfrac{\\alpha}{2\\sigma}\\exp(-\\alpha/\\sigma|\\theta_i|).\n\\] Thus it is a Laplace distribution with location 0 and scale \\(\\alpha/\\sigma\\). Representation of Laplace prior is a scale Normal mixture allows us to apply an efficient numerical algorithm for computing samples from the posterior distribution. This algorithms is called a Gibbs sample and it iteratively samples from \\(\\theta \\mid a,y\\) and \\(b\\mid \\theta,y\\) to estimate joint distribution over \\((\\hat \\theta, \\hat b)\\). Thus, we so not need to apply cross-validation to find optimal value of \\(b\\), the Bayesian algorithm does it “automatically”. We will discuss Gibbs algorithm later in the book.\nWhen prior is Normal \\(\\theta_i \\sim N(0,\\sigma_{\\theta}^2)\\), the posterior mode is equivalent to the ridge estimate. The relation between variance of the prior and the penalty weight in ridge regression is inverse proportional \\(\\lambda\\propto 1/\\sigma_{\\theta}^2\\).\n\n\n1.17 Subset Selection (\\(\\ell_0\\) Norm)\nSkike-and-slab (George and McCulloh) or Bernoulli-Gaussian (Polson Sun)\n\n\n1.18 Bridge (\\(\\ell_{\\alpha}\\))\nThis is a non-convex penalty when \\(0&lt;\\alpha&lt;1\\). It is an NP-hard problem. When \\(\\alpha=1\\) or \\(\\alpha=2\\) we have optimisation problems that are “solvable” for large scale cases. However, when \\(0\\le \\alpha&lt;1\\) the current optimisation algorithms won’t work.\nThe real killer is that you can use data to estimate \\(\\alpha\\) and \\(\\lambda\\) (let the data speak for itself) Box and Tiao (1992).\nBayesian analogue of the bridge estimator in regression is \\[\ny = X\\beta + \\epsilon\n\\]\nfor some unknown vector \\(\\beta = (\\beta_1, \\ldots, \\beta_p)'\\). Given choices of \\(\\alpha \\in (0,1]\\) and \\(\\nu \\in \\mathbb{R}^+\\), the bridge estimator \\(\\hat{\\beta}\\) is the minimizer of\n\\[\nQ_y(\\beta) = \\frac{1}{2} \\|y - X\\beta\\|^2 + \\nu \\sum_{j=1}^p |\\beta_j|^\\alpha.\n\\tag{1.3}\\]\nThis bridges a class of shrinkage and selection operators, with the best-subset-selection penalty at one end, and the \\(\\ell_1\\) (or lasso) penalty at the other. An early reference to this class of models can be found in Frank and Friedman (1993), with recent papers focusing on model-selection asymptotics, along with strategies for actually computing the estimator (Huang, Horowitz, and Ma (2008), Mazumder, and and Hastie (2011)).\nBridge approach differs from this line of work in adopting a Bayesian perspective on bridge estimation. Specifically, we treat\n\\[\np(\\beta \\mid y) \\propto \\exp\\{-Q_y(\\beta)\\}\n\\]\nas a posterior distribution having the minimizer of Equation 1.3 as its global mode. This posterior arises in assuming a Gaussian likelihood for \\(y\\), along with a prior for \\(\\beta\\) that decomposes as a product of independent exponential-power priors (Box and Tiao (1992)):\n\\[\np(\\beta \\mid \\alpha, \\nu) \\propto \\prod_{j=1}^p \\exp\\left(-\\left|\\frac{\\beta_j}{\\tau}\\right|^\\alpha\\right), \\quad \\tau = \\nu^{-1/\\alpha}. \\tag{2}\n\\]\nRather than minimizing (1), we proceed by constructing a Markov chain having the joint posterior for \\(\\beta\\) as its stationary distribution.\n\n1.18.1 Spike-and-Slab Prior\nOur Bayesian formulation of allows to specify a wide range of range of regularized formulations for a regression problem. In this section we consider a Bayesian model for variable selection. Consider a linear regression problem \\[\ny = \\beta_1x_1+\\ldots+\\beta_px_p + e \\ , \\ \\  \\text{where } e \\sim N(0, \\sigma^2),~-\\infty \\le \\beta_i \\le \\infty \\ .\n\\] We would like to solve the problem of variable selections, i.e. identify which input variables \\(x_i\\) to be used in our model. The gold standard for Bayesian variable selection are spike-and-slab priors, or Bernoulli-Gaussian mixtures. Whilst spike-and-slab priors provide full model uncertainty quantification, they can be hard to scale to very high dimensional problems and can have poor sparsity properties. On the other hand, techniques like proximal algorithms can solve non-convex optimization problems which are fast and scalable, although they generally don’t provide a full assessment of model uncertainty.\nTo perform a model selection, we would like to specify a prior distribution \\(p\\left(\\beta\\right)\\), which imposes a sparsity assumption on \\(\\beta\\), where only a small portion of all \\(\\beta_i\\)’s are non-zero. In other words, \\(\\|\\beta\\|_0 = k \\ll p\\), where \\(\\|\\beta\\|_0 \\defeq \\#\\{i : \\beta_i\\neq0\\}\\), the cardinality of the support of \\(\\beta\\), also known as the \\(\\ell_0\\) (pseudo)norm of \\(\\beta\\). A multivariate Gaussian prior (\\(l_2\\) norm) leads to poor sparsity properties in this situation. Sparsity-inducing prior distributions for \\(\\beta\\) can be constructed to impose sparsity include the double exponential (lasso).\nUnder spike-and-slab, each \\(\\beta_i\\) exchangeably follows a mixture prior consisting of \\(\\delta_0\\), a point mass at \\(0\\), and a Gaussian distribution centered at zero. Hence we write,\n\\[\n\\label{eqn:ss}\n\\beta_i | \\theta, \\sigma_\\beta^2 \\sim (1-\\theta)\\delta_0 + \\theta N\\left(0, \\sigma_\\beta^2\\right) \\ .\n\\] Here \\(\\theta\\in \\left(0, 1\\right)\\) controls the overall sparsity in \\(\\beta\\) and \\(\\sigma_\\beta^2\\) accommodates non-zero signals. This family is termed as the Bernoulli-Gaussian mixture model in the signal processing community.\nA useful re-parameterization, the parameters \\(\\beta\\) is given by two independent random variable vectors \\(\\gamma = \\left(\\gamma_1, \\ldots, \\gamma_p\\right)'\\) and \\(\\alpha = \\left(\\alpha_1, \\ldots, \\alpha_p\\right)'\\) such that \\(\\beta_i  =  \\gamma_i\\alpha_i\\), with probabilistic structure \\[\n\\label{eq:bg}\n\\begin{array}{rcl}\n\\gamma_i\\mid\\theta & \\sim & \\text{Bernoulli}(\\theta) \\ ;\n\\\\\n\\alpha_i \\mid \\sigma_\\beta^2 &\\sim & N\\left(0, \\sigma_\\beta^2\\right) \\ .\n\\\\\n\\end{array}\n\\] Since \\(\\gamma_i\\) and \\(\\alpha_i\\) are independent, the joint prior density becomes \\[\np\\left(\\gamma_i, \\alpha_i \\mid \\theta, \\sigma_\\beta^2\\right) =\n\\theta^{\\gamma_i}\\left(1-\\theta\\right)^{1-\\gamma_i}\\frac{1}{\\sqrt{2\\pi}\\sigma_\\beta}\\exp\\left\\{-\\frac{\\alpha_i^2}{2\\sigma_\\beta^2}\\right\\}\n\\ , \\ \\ \\ \\text{for } 1\\leq i\\leq p \\ .\n\\] The indicator \\(\\gamma_i\\in \\{0, 1\\}\\) can be viewed as a dummy variable to indicate whether \\(\\beta_i\\) is included in the model.\nLet \\(S = \\{i: \\gamma_i = 1\\} \\subseteq \\{1, \\ldots, p\\}\\) be the “active set\" of \\(\\gamma\\), and \\(\\|\\gamma\\|_0 = \\sum\\limits_{i = 1}^p\\gamma_i\\) be its cardinality. The joint prior on the vector \\(\\{\\gamma, \\alpha\\}\\) then factorizes as \\[\n\\begin{array}{rcl}\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2\\right) & = & \\prod\\limits_{i = 1}^p p\\left(\\alpha_i, \\gamma_i \\mid \\theta, \\sigma_\\beta^2\\right) \\\\\n& = &\n\\theta^{\\|\\gamma\\|_0}\n\\left(1-\\theta\\right)^{p - \\|\\gamma\\|_0}\n\\left(2\\pi\\sigma_\\beta^2\\right)^{-\\frac p2}\\exp\\left\\{-\\frac1{2\\sigma_\\beta^2}\\sum\\limits_{i = 1}^p\\alpha_i^2\\right\\} \\ .\n\\end{array}\n\\]\nLet \\(X_\\gamma \\defeq \\left[X_i\\right]_{i \\in S}\\) be the set of “active explanatory variables\" and \\(\\alpha_\\gamma \\defeq \\left(\\alpha_i\\right)'_{i \\in S}\\) be their corresponding coefficients. We can write \\(X\\beta = X_\\gamma \\alpha_\\gamma\\). The likelihood can be expressed in terms of \\(\\gamma\\), \\(\\alpha\\) as \\[\np\\left(y \\mid \\gamma, \\alpha, \\theta, \\sigma_e^2\\right)\n=\n\\left(2\\pi\\sigma_e^2\\right)^{-\\frac n2}\n\\exp\\left\\{\n-\\frac1{2\\sigma_e^2}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n\\right\\} \\ .\n\\]\nUnder this re-parameterization by \\(\\left\\{\\gamma, \\alpha\\right\\}\\), the posterior is given by\n\\[\n\\begin{array}{rcl}\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2, \\sigma_e^2, y\\right) & \\propto &\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2\\right)\np\\left(y \\mid \\gamma, \\alpha, \\theta, \\sigma_e^2\\right)\\\\\n& \\propto &\n\\exp\\left\\{-\\frac1{2\\sigma_e^2}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n-\\frac1{2\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2\n-\\log\\left(\\frac{1-\\theta}{\\theta}\\right)\n\\left\\|\\gamma\\right\\|_0\n\\right\\} \\ .\n\\end{array}\n\\] Our goal then is to find the regularized maximum a posterior (MAP) estimator \\[\n\\arg\\max\\limits_{\\gamma, \\alpha}p\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2, \\sigma_e^2, y \\right) \\ .\n\\] By construction, the \\(\\gamma\\) \\(\\in\\left\\{0, 1\\right\\}^p\\) will directly perform variable selection. Spike-and-slab priors, on the other hand, will sample the full posterior and calculate the posterior probability of variable inclusion. Finding the MAP estimator is equivalent to minimizing over \\(\\left\\{\\gamma, \\alpha\\right\\}\\) the regularized least squares objective function\n\\[\n\\min\\limits_{\\gamma, \\alpha}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n+ \\frac{\\sigma_e^2}{\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2\n+ 2\\sigma_e^2\\log\\left(\\frac{1-\\theta}{\\theta}\\right)\n\\left\\|\\gamma\\right\\|_0 \\ .\n\\tag{1.4}\\] This objective possesses several interesting properties:\n\nThe first term is essentially the least squares loss function.\nThe second term looks like a ridge regression penalty and has connection with the signal-to-noise ratio (SNR) \\(\\sigma_\\beta^2/\\sigma_e^2\\). Smaller SNR will be more likely to shrink the estimates towards \\(0\\). If \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the prior uncertainty on the size of non-zero coefficients is much larger than the noise level, that is, the SNR is sufficiently large, this term can be ignored. This is a common assumption in spike-and-slab framework in that people usually want \\(\\sigma_\\beta \\to \\infty\\) or to be “sufficiently large\" in order to avoid imposing harsh shrinkage to non-zero signals.\nIf we further assume that \\(\\theta &lt; \\frac12\\), meaning that the coefficients are known to be sparse a priori, then \\(\\log\\left(\\left(1-\\theta\\right) / \\theta\\right) &gt; 0\\), and the third term can be seen as an \\(\\ell_0\\) regularization.\n\nTherefore, our Bayesian objective inference is connected to \\(\\ell_0\\)-regularized least squares, which we summarize in the following proposition.\n(Spike-and-slab MAP & \\(\\ell_0\\) regularization)\nFor some \\(\\lambda &gt; 0\\), assuming \\(\\theta &lt; \\frac12\\), \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the Bayesian MAP estimate defined by Equation 1.4 is equivalent to the \\(\\ell_0\\) regularized least squares objective, for some \\(\\lambda &gt; 0\\), \\[\n\\min\\limits_{\\beta}\n\\frac12\\left\\|y - X\\beta\\right\\|_2^2\n+ \\lambda\n\\left\\|\\beta\\right\\|_0 \\ .\n\\tag{1.5}\\]\nFirst, assuming that \\[\n\\theta &lt; \\frac12, \\ \\ \\  \\sigma_\\beta^2 \\gg \\sigma_e^2, \\ \\ \\  \\frac{\\sigma_e^2}{\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2 \\to 0 \\ ,\n\\] gives us an objective function of the form \\[\n\\min\\limits_{\\gamma, \\alpha}\n\\frac12 \\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n+ \\lambda\n\\left\\|\\gamma\\right\\|_0,  \\ \\ \\ \\  \\text{where } \\lambda \\defeq \\sigma_e^2\\log\\left(\\left(1-\\theta\\right) / \\theta\\right) &gt; 0 \\ .\n\\tag{1.6}\\]\nEquation Equation 1.6 can be seen as a variable selection version of equation Equation 1.5. The interesting fact is that Equation 1.5 and Equation 1.6 are equivalent. To show this, we need only to check that the optimal solution to Equation 1.5 corresponds to a feasible solution to Equation 1.6 and vice versa. This is explained as follows.\nOn the one hand, assuming \\(\\hat\\beta\\) is an optimal solution to Equation 1.5, then we can correspondingly define \\(\\hat\\gamma_i \\defeq I\\left\\{\\hat\\beta_i \\neq 0\\right\\}\\), \\(\\hat\\alpha_i \\defeq \\hat\\beta_i\\), such that \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) is feasible to Equation 1.6 and gives the same objective value as \\(\\hat\\beta\\) gives Equation 1.5.\nOn the other hand, assuming \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) is optimal to Equation 1.6, implies that we must have all of the elements in \\(\\hat\\alpha_\\gamma\\) should be non-zero, otherwise a new \\(\\tilde\\gamma_i \\defeq I\\left\\{\\hat\\alpha_i \\neq 0\\right\\}\\) will give a lower objective value of Equation 1.6. As a result, if we define \\(\\hat\\beta_i \\defeq \\hat\\gamma_i\\hat\\alpha_i\\), \\(\\hat\\beta\\) will be feasible to Equation 1.5 and gives the same objective value as \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) gives Equation 1.6.\n\n\n\n1.19 Horseshoe Prior\n\n\n\n\n\n\n\n\n\nThe sparse normal means problem is concerned with inference for the parameter vector \\(\\theta = ( \\theta_1 , \\ldots , \\theta_p )\\) where we observe data \\(y_i = \\theta_i + \\epsilon_i\\) where the level of sparsity might be unknown. From both a theoretical and empirical viewpoint, regularized estimators have won the day. This still leaves open the question of how does specify a penalty, denoted by \\(\\pi_{HS}\\), (a.k.a. log-prior, \\(- \\log p_{HS}\\))? Lasso simply uses an \\(L^1\\)-norm, \\(\\sum_{i=1}^K | \\theta_i |\\), as opposed to the horseshoe prior which (essentially) uses the penalty \\[\n\\pi_{HS} ( \\theta_i | \\tau ) = - \\log p_{HS} ( \\theta_i | \\tau ) = - \\log \\log \\left ( 1 + \\frac{2 \\tau^2}{\\theta_i^2} \\right ) .\n\\] The motivation for the horseshoe penalty arises from the analysis of the prior mass and influence on the posterior in both the tail and behaviour at the origin. The latter is the key determinate of the sparsity properties of the estimator.\nThe horseshoe Carvalho, Polson, and Scott (2010) is a Bayesian method for ‘needle-in-a-haystack’ type problems where there is some sparsity, meaning that there are some signals amid mostly noise.\nWe introduce the horseshoe in the context of the normal means model, which is given by \\[Y_i = \\beta_i + \\varepsilon_i, \\quad i = 1, \\ldots, n,\\] with \\(\\varepsilon_i\\) i.i.d. \\(\\mathcal{N}(0, \\sigma^2)\\). The horseshoe prior is given by \\[\\begin{align*}\n\\beta_i &\\sim \\mathcal{N}(0, \\sigma^2 \\tau^2 \\lambda_i^2)\\\\\n\\lambda_i &\\sim C^+(0, 1),\n\\end{align*}\\] where \\(C^+\\) denotes the half-Cauchy distribution. Optionally, hyperpriors on \\(\\tau\\) and \\(\\sigma\\) may be specified, as is described further in the next two sections.\nTo illustrate the shrinkage behaviour of the horseshoe, let’s plot the posterior mean for \\(\\beta_i\\) as a function of \\(y_i\\) for three different values of \\(\\tau\\).\n\nlibrary(horseshoe)\nlibrary(ggplot2)\ntau.values &lt;- c(0.005, 0.05, 0.5)\ny.values &lt;- seq(-5, 5, length = 100)\ndf &lt;- data.frame(tau = rep(tau.values, each = length(y.values)),\n                 y = rep(y.values, 3),\n                 post.mean = c(HS.post.mean(y.values, tau = tau.values[1], Sigma2=1), \n                               HS.post.mean(y.values, tau = tau.values[2], Sigma2=1), \n                               HS.post.mean(y.values, tau = tau.values[3], Sigma2=1)) )\n\nggplot(data = df, aes(x = y, y = post.mean, group = tau, color = factor(tau))) + \n  geom_line(size = 1.5) + \n  scale_color_brewer(palette=\"Dark2\") + \n  geom_abline(lty = 2) + geom_hline(yintercept = 0, colour = \"grey\") + \n  theme_classic() + ylab(\"\") + labs(color = \"Tau\") +\n  ggtitle(\"Horseshoe posterior mean for three values of tau\") \n\n\n\n\n\n\n\n\nSmaller values of \\(\\tau\\) lead to stronger shrinkage behaviour of the horseshoe. Observations that are in absolute value at most equal to \\(\\sqrt{2\\sigma^2\\log(1/\\tau)}\\) are shrunk to values close to zero (Van der Pas et al (2014)). For larger observed values, the horseshoe posterior mean will tend to the identity (that is, barely any shrinkage, the estimate will be very close to the observed value). The optimal value of \\(\\tau\\) is the proportion of true signals. This value is typically not known in practice but can be estimated, as described further in the next sections.\n\n\n1.20 The normal means problem\nThe normal means model is: \\[Y_i = \\beta_i + \\varepsilon_i, \\quad i = 1, \\ldots, n,\\] with \\(\\varepsilon_i\\) i.i.d. \\(\\mathcal{N}(0, \\sigma^2)\\).\nFirst, we will be computing the posterior mean only, with known variance \\(\\sigma^2\\) The function HS.post.mean computes the posterior mean of \\((\\beta_1, \\ldots, \\beta_n)\\). It does not require MCMC and is suitable when only an estimate of the vector \\((\\beta_1, \\ldots, \\beta_n)\\) is desired. In case uncertainty quantification or variable selection is also of interest, or no good value for \\(\\sigma^2\\) is available, please see below for the function HS.normal.means.\nThe function HS.post.mean requires the observed outcomes, a value for \\(\\tau\\) and a value for \\(\\sigma\\). Ideally, \\(\\tau\\) should be equal to the proportion of nonzero \\(\\beta_i\\)’s. Typically, this proportion is unknown, in which case it is recommended to use the function HS.MMLE to find the marginal maximum likelihood estimator for \\(\\tau\\).\nAs an example, we generate 50 data points, the first 10 of which are coming from true signals. The first 10 \\(\\beta_i\\)’s are equal to five and the remaining \\(\\beta_i\\)’s are equal to zero. Let’s first plot the true parameters (black) and observations (blue).\n\ndf &lt;- data.frame(index = 1:50,\n                 truth &lt;- c(rep(5, 10), rep(0, 40)),\n                 y &lt;- truth + rnorm(50) #observations\n                 )\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  ggtitle(\"Black = truth, Blue = observations\")\n\n\n\n\n\n\n\n\nWe estimate \\(\\tau\\) using the MMLE, using the known variance.\n\n(tau.est &lt;- HS.MMLE(df$y, Sigma2 = 1))\n\n## [1] 0.96\n\n\nWe then use this estimate of \\(\\tau\\) to find the posterior mean, and add it to the plot in red.\n\npost.mean &lt;- HS.post.mean(df$y, tau.est, 1)\ndf$post.mean &lt;- post.mean\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean), size = 2, col = \"red\") +\n  ggtitle(\"Black = truth, Blue = observations, Red = estimates\")\n\n\n\n\n\n\n\n\nIf the posterior variance is of interest, the function HS.post.var can be used. It takes the same arguments as HS.post.mean.\n\n1.20.1 Posterior mean, credible intervals and variable selection, possibly unknown \\(\\sigma^2\\)\nThe function HS.normal.means is the main function to use for the normal means problem. It uses MCMC and results in an object that contains all MCMC samples as well as the posterior mean for all parameters (\\(\\beta_i\\)’s, \\(\\tau\\), \\(\\sigma\\)), the posterior median for the \\(\\beta_i\\)’s, and credible intervals for the \\(\\beta_i\\)’s.\nThe key choices to make are:\n\nHow to handle \\(\\tau\\). The recommended option is “truncatedCauchy” (a half-Cauchy prior truncated to \\([1/n, 1]\\)). See the manual for other options.\nHow to handle \\(\\sigma\\). The recommended option is “Jeffreys” (Jeffrey’s prior). See the manual for other options.\n\nOther options that can be set by the user are the level of the credible intervals (default is 95%), and the number of MCMC samples (default is 1000 burn-in samples and then 5000 more).\nLet’s continue the example from the previous section. We first create a ‘horseshoe object’.\n\nhs.object &lt;- HS.normal.means(df$y, method.tau = \"truncatedCauchy\", method.sigma = \"Jeffreys\")\n\nWe extract the posterior mean of the \\(\\beta_i\\)’s and plot them in red.\n\ndf$post.mean.full &lt;- hs.object$BetaHat\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full), size = 2, col = \"red\") +\n  ggtitle(\"Black = truth, Blue = observations, Red = estimates\")\n\n\n\n\n\n\n\n\nWe plot the marginal credible intervals (and remove the observations from the plot for clarity).\n\ndf$lower.CI &lt;- hs.object$LeftCI\ndf$upper.CI &lt;- hs.object$RightCI\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full), size = 2, col = \"red\") +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI), width = .1, col = \"red\") +\n  ggtitle(\"Black = truth, Red = estimates with 95% credible intervals\")\n\n\n\n\n\n\n\n\nFinally, we perform variable selection using HS.var.select. In the normal means problem, we can use two decision rules. We will illustrate them both. The first method checks whether zero is contained in the credible interval, as studied by Van der Pas et al (2017).\n\ndf$selected.CI &lt;- HS.var.select(hs.object, df$y, method = \"intervals\")\n\nThe result is a vector of zeroes and ones, with the ones indicating that the observations is suspected to correspond to an actual signal. We now plot the results, coloring the estimates/intervals blue if a signal is detected and red otherwise.\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full, col = factor(selected.CI)), \n             size = 2) +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI, col = factor(selected.CI)),\n                width = .1) +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Black = truth, Blue = selected as signal, Red = selected as noise\")\n\n\n\n\n\n\n\n\nThe other variable selection method is the thresholding method of Carvalho et al (2010). The posterior mean can be written as \\(c_iy_i\\) where \\(y_i\\) is the observation and \\(c_i\\) some number between 0 and 1. A variable is selected if \\(c_i \\geq c\\) for some user-selected threshold \\(c\\) (default is \\(c = 0.5\\)). In the example:\n\ndf$selected.thres &lt;- HS.var.select(hs.object, df$y, method = \"threshold\")\n\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full, col = factor(selected.thres)), \n             size = 2) +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI, col = factor(selected.thres)),\n                width = .1) +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Black = truth, Blue = selected as signal, Red = selected as noise\")\n\n\n\n\n\n\n\n\n\n\n\n1.21 Polya-Gamma\nBayesian inference for logistic regression has long been recognized as a computationally challenging problem due to the analytically inconvenient form of the binomial likelihood function(Polson, Scott, and Windle 2013). While the probit model enjoys simple latent-variable methods for posterior sampling, the logistic model has historically required more complex approaches involving multiple layers of auxiliary variables or approximations(Polson, Scott, and Windle 2013). The breakthrough work of Polson, Scott, and Windle (2013) introduced a revolutionary data-augmentation strategy using a novel class of distributions called Pólya-Gamma distributions, which enables simple and exact Gibbs sampling for Bayesian logistic regression(Polson, Scott, and Windle 2013).\nThis methodology represents a significant advancement in Bayesian computation, providing a direct analog to the Albert and Chib (1993) method for probit regression while maintaining both exactness and simplicity(Polson, Scott, and Windle 2013). The approach has proven particularly valuable for complex hierarchical models where traditional Metropolis-Hastings samplers are difficult to tune and implement effectively(Polson, Scott, and Windle 2013).\n\n\n\n\n\n\nKey Innovation\n\n\n\nThe Pólya-Gamma methodology provides exact Gibbs sampling for Bayesian logistic regression, eliminating the need for complex Metropolis-Hastings tuning while maintaining theoretical guarantees.\n\n\n\n\n1.22 The Pólya-Gamma Distribution\nThe Pólya-Gamma distribution, denoted as PG(b,c), is carefully constructed as a subset of infinite convolutions of gamma distributions(Polson, Scott, and Windle 2013). A random variable X follows a Pólya-Gamma distribution with parameters b &gt; 0 and c ∈ ℝ if:\n\\[X \\stackrel{d}{=} \\frac{1}{2\\pi^2} \\sum_{k=1}^{\\infty} \\frac{g_k}{(k-1/2)^2 + c^2/(4\\pi^2)}\\]\nwhere \\(g_k \\sim \\text{Ga}(b,1)\\) are independent gamma random variables, and \\(\\stackrel{d}{=}\\) indicates equality in distribution(Polson, Scott, and Windle 2013).\nThe Pólya-Gamma family exhibits several remarkable properties that make it ideal for data augmentation:\n\nLaplace Transform: For \\(\\omega \\sim \\text{PG}(b,0)\\), the Laplace transform is \\(E\\{\\exp(-\\omega t)\\} = \\cosh^{-b}(\\sqrt{t}/2)\\)(Polson, Scott, and Windle 2013)\nExponential Tilting: The general PG(b,c) distribution arises through exponential tilting of the PG(b,0) density:\n\n\\[p(x|b,c) = \\frac{\\exp(-c^2x/2)p(x|b,0)}{E[\\exp(-c^2\\omega/2)]}\\]\nwhere the expectation is taken with respect to PG(b,0)(Polson, Scott, and Windle 2013)\n\nConvolution Property: The family is closed under convolution for random variates with the same tilting parameter(Polson, Scott, and Windle 2013)\nKnown Moments: All finite moments are available in closed form, with the expectation given by:\n\n\\[E(\\omega) = \\frac{b}{2c}\\tanh(c/2) = \\frac{b}{2c}\\frac{e^c-1}{1+e^c}\\]\n\n\n\n\n\n\nComputational Advantage\n\n\n\nThe known moments and convolution properties make the Pólya-Gamma distribution computationally tractable and theoretically well-behaved.\n\n\n\n1.22.1 The Data-Augmentation Strategy\nThe core of the Pólya-Gamma methodology rests on a fundamental integral identity that represents binomial likelihoods as mixtures of Gaussians(Polson, Scott, and Windle 2013). The key theorem states:\n\nTheorem 1: For b &gt; 0 and a ∈ ℝ, the following integral identity holds:\n\\[\\frac{(e^\\psi)^a}{(1+e^\\psi)^b} = 2^{-b}e^{\\kappa\\psi} \\int_0^{\\infty} e^{-\\omega\\psi^2/2} p(\\omega) d\\omega\\]\nwhere \\(\\kappa = a - b/2\\), and \\(p(\\omega)\\) is the density of \\(\\omega \\sim \\text{PG}(b,0)\\)(Polson, Scott, and Windle 2013).\nMoreover, the conditional distribution \\(p(\\omega|\\psi)\\) is also in the Pólya-Gamma class: \\((\\omega|\\psi) \\sim \\text{PG}(b,\\psi)\\)(Polson, Scott, and Windle 2013).\n\n\n\n1.22.2 Gibbs Sampling Algorithm\nThis integral identity leads directly to a simple two-step Gibbs sampler for Bayesian logistic regression(Polson, Scott, and Windle 2013). For a dataset with observations \\(y_i \\sim \\text{Binom}(n_i, 1/(1+e^{-\\psi_i}))\\) where \\(\\psi_i = x_i^T\\beta\\), and a Gaussian prior \\(\\beta \\sim N(b,B)\\), the algorithm iterates:\n\nSample auxiliary variables: \\((\\omega_i|\\beta) \\sim \\text{PG}(n_i, x_i^T\\beta)\\) for each observation\nSample parameters: \\((\\beta|y,\\omega) \\sim N(m_\\omega, V_\\omega)\\) where:\n\n\\(V_\\omega = (X^T\\Omega X + B^{-1})^{-1}\\)\n\\(m_\\omega = V_\\omega(X^T\\kappa + B^{-1}b)\\)\n\\(\\kappa = (y_1-n_1/2, \\ldots, y_n-n_n/2)\\)\n\\(\\Omega = \\text{diag}(\\omega_1, \\ldots, \\omega_n)\\)\n\n\nThis approach requires only Gaussian draws for the main parameters and Pólya-Gamma draws for a single layer of latent variables, making it significantly simpler than previous methods(Polson, Scott, and Windle 2013).\n\n\n1.22.3 The PG(1,z) Sampler\nThe practical success of the Pólya-Gamma method depends on efficient simulation of Pólya-Gamma random variables(Polson, Scott, and Windle 2013). The authors developed a sophisticated accept-reject sampler based on the alternating-series method of Devroye (1986)(Devroye 1986). For the fundamental PG(1,c) case, the sampler:\n\nUses exponential and inverse-Gaussian draws as proposals\nAchieves acceptance probability uniformly bounded below at 0.99919\nRequires no tuning for optimal performance\nEvaluates acceptance using iterative partial sums\n\n\n\n1.22.4 General PG(b,z) Sampling\nFor integer values of b, PG(b,z) random variables are generated by summing b independent PG(1,z) draws, exploiting the convolution property(Polson, Scott, and Windle 2013). This approach maintains efficiency for moderate values of b, though computational cost scales linearly with the total number of counts in negative binomial applications(Polson, Scott, and Windle 2013).\n\n\n\n1.23 Implementation with BayesLogit Package\n\n1.23.1 Package Overview\nThe BayesLogit package provides efficient tools for sampling from the Pólya-Gamma distribution(Windle 2023). The current version (2.1) focuses on core functionality: sampling from the Pólya-Gamma distribution through the rpg() function and its variants(Windle 2023).\n\n\n1.23.2 Core Functions\nThe package offers several sampling methods:\n\nrpg(): Main function that automatically selects the best method\nrpg.devroye(): Devroye-like method for integer h values\nrpg.gamma(): Sum of gammas method (slower but works for all parameters)\nrpg.sp(): Saddlepoint approximation method\n\n\n\n1.23.3 Installation and Basic Usage\n\n# Install from CRAN\ninstall.packages(\"BayesLogit\")\nlibrary(BayesLogit)\n\n# Basic usage examples\n# Sample from PG(1, 0)\nsamples1 &lt;- rpg(1000, h=1, z=0)\n\n# Sample with tilting parameter\nsamples2 &lt;- rpg(1000, h=1, z=2.5)\n\n# Multiple shape parameters\nh_values &lt;- c(1, 2, 3)\nz_values &lt;- c(1, 2, 3)\nsamples3 &lt;- rpg(100, h=h_values, z=z_values)\n\n\n\n1.23.4 Implementing Bayesian Logistic Regression\nHere’s a complete implementation of Bayesian logistic regression using the Pólya-Gamma methodology:\n\n# Bayesian Logistic Regression with Pólya-Gamma Data Augmentation\nbayesian_logit_pg &lt;- function(y, X, n_iter=5000, burn_in=1000) {\n  n &lt;- length(y)\n  p &lt;- ncol(X)\n  \n  # Prior specification (weakly informative)\n  beta_prior_mean &lt;- rep(0, p)\n  beta_prior_prec &lt;- diag(0.01, p)  # Precision matrix\n  \n  # Storage for samples\n  beta_samples &lt;- matrix(0, n_iter, p)\n  omega_samples &lt;- matrix(0, n_iter, n)\n  \n  # Initialize\n  beta &lt;- rep(0, p)\n  \n  for(iter in 1:n_iter) {\n    # Step 1: Sample omega (auxiliary variables)\n    psi &lt;- X %*% beta\n    omega &lt;- rpg(n, h=1, z=psi)\n    \n    # Step 2: Sample beta (regression coefficients)\n    # Posterior precision and mean\n    V_omega &lt;- solve(t(X) %*% diag(omega) %*% X + beta_prior_prec)\n    kappa &lt;- y - 0.5\n    m_omega &lt;- V_omega %*% (t(X) %*% kappa + beta_prior_prec %*% beta_prior_mean)\n    \n    # Sample from multivariate normal\n    beta &lt;- mvrnorm(1, m_omega, V_omega)\n    \n    # Store samples\n    beta_samples[iter, ] &lt;- beta\n    omega_samples[iter, ] &lt;- omega\n  }\n  \n  # Return samples after burn-in\n  list(\n    beta = beta_samples[(burn_in+1):n_iter, ],\n    omega = omega_samples[(burn_in+1):n_iter, ],\n    n_samples = n_iter - burn_in\n  )\n}\n\n# Example usage with simulated data\nset.seed(123)\nn &lt;- 100\nX &lt;- cbind(1, matrix(rnorm(n*2), n, 2))  # Intercept + 2 predictors\nbeta_true &lt;- c(-0.5, 1.2, -0.8)\nlogits &lt;- X %*% beta_true\nprobs &lt;- 1/(1 + exp(-logits))\ny &lt;- rbinom(n, 1, probs)\n\n# Fit model\nresults &lt;- bayesian_logit_pg(y, X, n_iter=3000, burn_in=500)\n\n# Posterior summaries\nposterior_means &lt;- colMeans(results$beta)\nposterior_sds &lt;- apply(results$beta, 2, sd)\n\nComputational Advantages\nExtensive benchmarking studies demonstrate the superior performance of the Pólya-Gamma method across various scenarios(Polson, Scott, and Windle 2013):\n\nSimple logistic models: Competitive with well-tuned Metropolis-Hastings samplers\nHierarchical models: Significantly outperforms alternative methods\nMixed models: Provides substantial efficiency gains over traditional approaches\nSpatial models: Shows dramatic improvements for Gaussian process spatial models\n\nTheoretical Guarantees\nThe Pólya-Gamma Gibbs sampler enjoys strong theoretical properties(Polson, Scott, and Windle 2013):\n\nUniform ergodicity: Proven by Choi and Hobert (2013), guaranteeing convergence and central limit theorems for Monte Carlo averages(Polson, Scott, and Windle 2013)\nNo tuning required: Unlike Metropolis-Hastings methods, the sampler requires no manual tuning\nExact sampling: Produces draws from the correct posterior distribution without approximation\n\n\n\n\n\n\n\nImportant Note\n\n\n\nThe theoretical guarantees hold under standard regularity conditions, and the method requires proper prior specification for optimal performance.\n\n\nBeyond Binary Logistic Regression\nThe Pólya-Gamma methodology extends naturally to various related models(Polson, Scott, and Windle 2013):\n\nNegative binomial regression: Direct application using the same data-augmentation scheme\nMultinomial logistic models: Extended through partial difference of random utility models(Windle, Polson, and Scott 2014)\nMixed effects models: Seamless incorporation of random effects structures\nSpatial models: Efficient inference for spatial count data models\n\n\n\n1.23.5 Modern Applications\nRecent developments have expanded the methodology’s applicability[Windle, Polson, and Scott (2014)](Zhang, Datta, and Banerjee 2018):\n\nGaussian process classification: Scalable variational approaches using Pólya-Gamma augmentation\nDeep learning: Integration with neural network architectures for Bayesian deep learning\nState-space models: Application to dynamic binary time series models\n\nThe Pólya-Gamma methodology represents a fundamental advancement in Bayesian computation for logistic models, combining theoretical elegance with practical efficiency(Polson, Scott, and Windle 2013). Its introduction of the Pólya-Gamma distribution class and the associated data-augmentation strategy has enabled routine application of Bayesian methods to complex hierarchical models that were previously computationally prohibitive(Polson, Scott, and Windle 2013).\nThe BayesLogit package provides researchers and practitioners with efficient, well-tested implementations of these methods(Windle 2023). The combination of exact inference, computational efficiency, and theoretical guarantees makes the Pólya-Gamma approach the method of choice for Bayesian logistic regression in most practical applications(Polson, Scott, and Windle 2013).\nAs computational demands continue to grow and models become increasingly complex, the Pólya-Gamma methodology’s advantages become even more pronounced, establishing it as an essential tool in the modern Bayesian statistician’s toolkit (Tiao (2019)). Ongoing research continues to extend the Pólya-Gamma methodology to new domains, including high-dimensional settings, nonparametric models, and integration with modern machine learning frameworks.\n\n\n\n\n\n\n\nAndrews, D. F., and C. L. Mallows. 1974. “Scale Mixtures of Normal Distributions.” Journal of the Royal Statistical Society. Series B (Methodological) 36 (1): 99–102. https://www.jstor.org/stable/2984774.\n\n\nBox, George E. P., and George C. Tiao. 1992. Bayesian Inference in Statistical Analysis. New York: Wiley-Interscience.\n\n\nCarvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010. “The Horseshoe Estimator for Sparse Signals.” Biometrika, asq017.\n\n\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation. Springer Science & Business Media.\n\n\nDiaconis, P., and D. Ylvisaker. 1983. “Quantifying Prior Opinion.”\n\n\nEfron, Bradley, and Carl Morris. 1975. “Data Analysis Using Stein’s Estimator and Its Generalizations.” Journal of the American Statistical Association 70 (350): 311–19.\n\n\n———. 1977. “Stein’s Paradox in Statistics.” Scientific American 236 (5): 119–27.\n\n\nFrank, Ildiko E., and Jerome H. Friedman. 1993. “A Statistical View of Some Chemometrics Regression Tools.” Technometrics 35 (2): 109–35. https://www.jstor.org/stable/1269656.\n\n\nHuang, Jian, Joel L. Horowitz, and Shuangge Ma. 2008. “Asymptotic Properties of Bridge Estimators in Sparse High-Dimensional Regression Models.” The Annals of Statistics 36 (2): 587–613.\n\n\nMazumder, Rahul, Friedman, and Trevor and Hastie. 2011. “SparseNet: Coordinate Descent With Nonconvex Penalties.” Journal of the American Statistical Association 106 (495): 1125–38.\n\n\nPolson, Nicholas G., James G. Scott, and Jesse Windle. 2013. “Bayesian Inference for Logistic Models Using Pólya–Gamma Latent Variables.” Journal of the American Statistical Association 108 (504): 1339–49.\n\n\nStein, Charles. 1964. “Inadmissibility of the Usual Estimator for the Variance of a Normal Distribution with Unknown Mean.” Annals of the Institute of Statistical Mathematics 16 (1): 155–60.\n\n\nTiao, Louis. 2019. “Pólya-Gamma Bayesian Logistic Regression.” Blog post.\n\n\nTikhonov, Andrey Nikolayevich et al. 1943. “On the Stability of Inverse Problems.” In Dokl. Akad. Nauk Sssr, 39:195–98.\n\n\nWindle, Jesse. 2023. “BayesLogit: Bayesian Logistic Regression.” R package version 2.1.\n\n\nWindle, Jesse, Nicholas G. Polson, and James G. Scott. 2014. “Sampling Polya-Gamma Random Variates: Alternate and Approximate Techniques.” arXiv. https://arxiv.org/abs/1405.0506.\n\n\nZhang, Yichi, Anirban Datta, and Sudipto Banerjee. 2018. “Scalable Gaussian Process Classification with Pólya-Gamma Data Augmentation.” arXiv Preprint arXiv:1802.06383. https://arxiv.org/abs/1802.06383.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#penalty-and-regularisation",
    "href": "17-theoryai.html#penalty-and-regularisation",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.5 Penalty and Regularisation",
    "text": "1.5 Penalty and Regularisation\nThe problem of finding a good model boils down to finding \\(\\phi\\) that minimize some form of Bayes risk for the problem at hand.\nThere are a number of commonly used penalty functions (a.k.a. log prior density). For example, the $ l^2$-norm corresponds to s normal prior. The resulting Bayes rule will take the form of a shrinkage estimator, a weighted combination between data and prior beliefs about the parameter. An $ l^1 $-norm will induce a sparse solution in the estimator and can be used an a variable selection operator. The $ l_0 $-norm directly induces a subset selection procedure.\nThe amount of regularisation \\(\\lambda\\) gauges the trade-off between the compromise between the observed data and the initial prior beliefs.\nThere are two main approaches to finding a good model:\n\nFull Bayes: This approach places a prior distribution on the parameters and computes the full posterior distribution.\nRegularization Methods: These approaches add penalty terms to the objective function to control model complexity.\n\nNow, let’s look at those two approaches in more detail.\nThe full Bayes approach is to place a prior distribution on the parameters and compute the full posterior distribution using the Bayes rule: \\[\np( \\theta | y ) = \\frac{ f( y | \\theta ) p( \\theta ) }{ m(y) },\n\\] here \\[\nm(y) = \\int f( y| \\theta ) p( \\theta ) d \\theta\n\\] Here \\(m(y)\\) is the marginal beliefs about the data. This can also be used to choose the amount of regularisation via the type II maximum likelihood estimator (MMLE) defined by \\[\n\\hat{\\tau} = \\arg \\max \\log m( y | \\tau )\n\\] where again $ m( y | ) = f( y | ) p( | ) $.\nFor example, in the normal-normal model, with \\(\\mu=0\\), we can integrate out the high dimensional \\(\\theta\\) and find \\(m(y | \\tau)\\) in closed form as \\(y_i \\sim N(0, \\sigma^2 + \\tau^2)\\) \\[\nm( y | \\tau ) = ( 2 \\pi)^{-n/2} ( \\sigma^2 + \\tau^2 )^{- n/2}  \\exp \\left ( - \\frac{ \\sum y_i^2 }{ 2 ( \\sigma^2 + \\tau^2) }\n\\] The original JS estimator shrinks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want o shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS is \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), which serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate proper from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\nCommon examples include ridge regression (L2 penalty), lasso regression (L1 penalty), and elastic net (combination of L1 and L2 penalties).\nRather than having to perform high dimensional integration with the likes of MCMC etc, a common approach is to use a maximum a posteriori (MAP) estimator defined by \\[\n\\hat{\\theta} = \\arg \\max \\log p ( \\theta | y )\n\\] This can directly lead to sparsity as in the case of \\(\\ell_1\\)-norm optimisation.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#ell_2-shrinkage.",
    "href": "17-theoryai.html#ell_2-shrinkage.",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.6 \\(\\ell_2\\) Shrinkage.",
    "text": "1.6 \\(\\ell_2\\) Shrinkage.\nThe original JS estimator shranks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want o shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS uses \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), which serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate proper prior from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\n\n1.6.1 Sparse \\(r\\)-spike problem\nFor the sparse \\(r\\)-spike problem we require a different rule. For a sparse signal, however, \\(\\hat \\theta_{JS}\\) performs poorly when the true parameter is an \\(r\\)-spike where \\(\\theta_r\\) has \\(r\\) coordinates at \\(\\sqrt{p/r}\\) and the rest set at zero with norm \\({\\Vert \\theta_r \\Vert}^2 =p\\).\nThe classical risk satisfies \\(R \\left ( \\hat \\theta_{JS} , \\theta_r \\right ) \\geq p/2\\) where the simple thresholding rule \\(\\sqrt{2 \\ln p}\\) performs with risk \\(\\sqrt{\\ln p}\\) in the \\(r\\)-spike sparse case even though it is inadmissible in MSE for a non-sparse signal. Then is due to the fact that for \\(\\theta_p\\) we have \\[\n\\frac{p \\Vert \\theta \\Vert^2}{p + \\Vert \\theta \\Vert^2} \\leq R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\leq\n2 + \\frac{p \\Vert \\theta \\Vert^2}{ d + \\Vert \\theta \\Vert^2}.\n\\] This implies that \\(R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\geq (p/2)\\). Hence, simple thresholding rule beats James-Stein this with a risk given by \\(\\sqrt{\\log p }\\). This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.\nA Bayes rule that inherits good MSE properties but also simultaneously provides asymptotic minimax estimation risk for sparse signals. HS estimator uniformly dominates the traditional sample mean estimator in MSE and has good posterior concentration properties for nearly black objects. Specifically, the horseshoe estimator attains asymptotically minimax risk rate \\[\n\\sup_{ \\theta \\in l_0[p_n] } \\;\n\\mathbb{E}_{ y | \\theta } \\|\\hat y_{hs} - \\theta \\|^2 \\asymp\np_n \\log \\left ( n / p_n \\right ).\n\\] The “worst’’ \\(\\theta\\) is obtained at the maximum difference between \\(\\left| \\hat \\theta_{HS} - y \\right|\\) where \\(\\hat \\theta_{HS} = \\mathbb{E}(\\theta|y)\\) can be interpreted as a Bayes posterior mean (optimal under Bayes MSE).\nOne such estimator that achieves the optimal minimax rate is the horseshoe estimator proposed by Carvalho, Polson, and Scott (2010).\n\n\n1.6.2 Efron Example\nEfron provide an example which shows the importance of specifying priors in high dimensions. The key idea behind James-Stein shrinkage is that one when one can “borrow strength” across components. In this sense the multivariate parameter estimation problem is easier than the univariate one.\nStein’s phenomenon where \\(y_i | \\theta_i \\sim N(\\theta_i, 1)\\) and \\(\\theta_i \\sim N(0, \\tau^2)\\) where \\(\\tau \\rightarrow \\infty\\) illustrates this point well. This leads to the improper “non-informative” uniform prior. The corresponding generalized Bayes rule is the vector of means—which we know is inadmissible. so no regularisation leads to an estimator with poor risk property.\nLet \\(\\|y\\| = \\sum_{i=1}^p y_i^2\\). Then, we can make the following probabilistic statements from the model, \\[\nP\\left( \\| y \\| &gt; \\| \\theta \\| \\right) &gt; \\frac{1}{2}\n\\] Now for the posterior, this inequallty is reversed under a flat Lebesgue measure, \\[\nP\\left( \\| \\theta \\| &gt; \\| y \\| \\; | \\; y \\right) &gt; \\frac{1}{2}\n\\] which is in conflict with the classical statement. This is a property of the prior which leads to a poor rule (the overall average) and risk.\nThe shrinkage rule (a.k.a. normal prior) where \\(\\tau^2\\) is “estimated” from the data avoids this conflict. More precisely, we have \\[\n\\hat{\\theta}(y) = \\left( 1 - \\frac{k-2}{\\|y\\|^2} \\right) y \\quad \\text{and} \\quad E\\left( \\| \\hat{\\theta} - \\theta \\| \\right) &lt; k, \\; \\forall \\theta.\n\\] Hence, when \\(\\|y\\|^2\\) is small the shrinkage factor is more extreme. For example, if \\(k=10\\), \\(\\|y\\|^2=12\\), then \\(\\hat{\\theta} = (1/3) y\\). Now we have the more intuitive result that \\(P\\left(\\|\\theta\\| &gt; \\|y\\| \\; | \\; y\\right) &lt; \\frac{1}{2}\\).\nThis shows that careful specification of default priors matter in high dimensions is necessary.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#ell_1-sparsity",
    "href": "17-theoryai.html#ell_1-sparsity",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.7 \\(\\ell_1\\) Sparsity",
    "text": "1.7 \\(\\ell_1\\) Sparsity",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#ell_0-subset-selection",
    "href": "17-theoryai.html#ell_0-subset-selection",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.8 \\(\\ell_0\\) Subset Selection",
    "text": "1.8 \\(\\ell_0\\) Subset Selection\nThe canonical problem is estimaiton of the normal means problem. Here we have \\(y_i = \\theta_i + e_i,~i=1,\\ldots,p\\) and \\(e_i \\sim N(0, \\sigma^2)\\). The goal is to estimate the vector of means \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\). This is also a proxy for non-parametric regression, where \\(\\theta_i = f(x_i)\\). Aslo typically \\(y_i\\) is a mean of \\(n\\) observations, i.e. \\(y_i = \\frac{1}{n} \\sum_{j=1}^n x_{ij}\\). ## James-Stein Estimator The classic James-Stein shrinkage rule, \\(\\hat y_{js}\\), uniformly dominates the traditional sample mean estimator, \\(\\hat{\\theta}\\), for all values of the true parameter \\(\\theta\\). In classical MSE risk terms: \\[\nR(\\hat y_{js}, \\theta) \\defeq E_{y|\\theta} {\\Vert \\hat y_{js} - \\theta \\Vert}^2 &lt; p\n    = E_{y|\\theta} {\\Vert y - \\theta \\Vert}^2, \\;\\;\\; \\forall \\theta\n\\] For a sparse signal, however, \\(\\hat y_{js}\\) performs poorly when the true parameter is an \\(r\\)-spike where \\(\\theta_r\\) has \\(r\\) coordinates at \\(\\sqrt{p/r}\\) and the rest set at zero with norm \\({\\Vert \\theta_r \\Vert}^2 =p\\).\nThe classical risk satisfies \\(R \\left ( \\hat y_{js} , \\theta_r \\right ) \\geq p/2\\) where the simple thresholding rule \\(\\sqrt{2 \\ln p}\\) performs with risk \\(\\sqrt{\\ln p}\\) in the \\(r\\)-spike sparse case even though it is inadmissible in MSE for a non-sparse signal.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#r-spike-problem",
    "href": "17-theoryai.html#r-spike-problem",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.9 R-spike Problem",
    "text": "1.9 R-spike Problem\nFrom a historical perspective, James-Stein (a.k.a \\(L^2\\)-regularisation)(Stein 1964) is only a global shrinkage rule–in the sense that there are no local parameters to learn about sparsity. A simple sparsity example shows the issue with \\(L^2\\)-regularisation. Consider the sparse \\(r\\)-spike shows the problem with focusing solely on rules with the same shrinkage weight (albeit benefiting from pooling of information).\nLet the true parameter value be \\(\\theta_p = \\left ( \\sqrt{d/p} , \\ldots , \\sqrt{d/p} , 0 , \\ldots , 0 \\right )\\). James-Stein is equivalent to the model \\[\ny_i = \\theta_i + \\epsilon_i \\; \\mathrm{ and} \\; \\theta_i \\sim \\mathcal{N} \\left ( 0 , \\tau^2 \\right )\n\\] This dominates the plain MLE but loses admissibility! This is due to the fact that a “plug-in” estimate of global shrinkage \\(\\hat{\\tau}\\) is used. Tiao and Tan’s original “closed-form” analysis is particularly relevant here. They point out that the mode of \\(p(\\tau^2|y)\\) is zero exactly when the shrinkage weight turns negative (their condition 6.6). From a risk perspective \\(E \\Vert \\hat{\\theta}^{JS} - \\theta \\Vert \\leq p , \\forall \\theta\\) showing the inadmissibility of the MLE. At origin the risk is \\(2\\), but! \\[\n\\frac{p \\Vert \\theta \\Vert^2}{p + \\Vert \\theta \\Vert^2} \\leq R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\leq\n2 + \\frac{p \\Vert \\theta \\Vert^2}{ d + \\Vert \\theta \\Vert^2}.\n\\] This implies that \\(R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\geq (p/2)\\). Hence, simple thresholding rule beats James-Stein this with a risk given by \\(\\sqrt{\\log p }\\). This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.\nThe horseshoe estimator, which we will discuss in more detail later, \\(\\hat y_{hs}\\), was proposed by Carvalho, Polson, and Scott (2010) to provide a Bayes rule that inherits good MSE properties but also simultaneously provides asymptotic minimax estimation risk for sparse signals. HS estimator uniformly dominates the traditional sample mean estimator in MSE and has good posterior concentration properties for nearly black objects. Specifically, the horseshoe estimator attains asymptotically minimax risk rate \\[\n\\sup_{ \\theta \\in l_0[p_n] } \\;\n\\mathbb{E}_{ y | \\theta } \\|\\hat y_{hs} - \\theta \\|^2 \\asymp\np_n \\log \\left ( n / p_n \\right ).\n\\] The “worst’’ \\(\\theta\\) is obtained at the maximum difference between \\(\\left|\\hat{y}_{hs} - y\\right|\\) where \\(\\hat{y}_{hs} = \\mathbb{E}(\\theta|y)\\) can be interpreted as a Bayes posterior mean (optimal under Bayes MSE).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#ell_2-shrinkage",
    "href": "17-theoryai.html#ell_2-shrinkage",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.10 \\(\\ell_2\\) Shrinkage",
    "text": "1.10 \\(\\ell_2\\) Shrinkage\n\nExample 1.4 (Stein’s Paradox) Stein’s paradox, as explained Efron and Morris (1977), is a phenomenon in statistics that challenges our intuitive understanding of estimation. The paradox arises when trying to estimate the mean of a multivariate normal distribution. Traditionally, the best guess about the future is usually obtained by computing the average of past events. However, Charles Stein showed that there are circumstances where there are estimators better than the arithmetic average. This is what’s known as Stein’s paradox.\nIn 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution that has uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein’s rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson’s chi-square test with results from a computer simulation.\nIn each of these examples, the mean square error of these rules is less than half that of the sample mean. This result is paradoxical because it contradicts the elementary law of statistical theory. The philosophical implications of Stein’s paradox are also significant. It has influenced the development of shrinkage estimators and has connections to Bayesianism and model selection criteria.\nSuppose that we have \\(n\\) independent observations \\(y_{1},\\ldots,y_{n}\\) from a \\(N\\left(  \\theta,\\sigma^{2}\\right)\\) distribution. The maximum likelihood estimator is \\(\\widehat{\\theta}=\\bar{y}\\), the sample mean. The Bayes estimator is the posterior mean, \\[\n\\widehat{\\theta}=\\mathbb{E}\\left[  \\theta\\mid y\\right]  =\\frac{\\sigma^{2}}{\\sigma^{2}+n}% \\bar{y}.\n\\] The Bayes estimator is a shrinkage estimator, it shrinks the MLE towards the prior mean. The amount of shrinkage is determined by the ratio of the variance of the prior and the variance of the likelihood. The Bayes estimator is also a function of the MLE \\[\n\\widehat{\\theta}=\\frac{\\sigma^{2}}{\\sigma^{2}+n}\\bar{y}+\\frac{n}{\\sigma^{2}+n}\\widehat{\\theta}.\n\\] This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior. The Bayes estimator is a function of the MLE \\[\n\\widehat{\\theta}=\\frac{\\sigma^{2}}{\\sigma^{2}+n}\\bar{y}+\\frac{n}{\\sigma^{2}+n}\\widehat{\\theta}.\n\\] This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior.\nThe original JS estimator shranks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and MOrris and Lindley showed that you want o shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS is \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), whcih serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate propr from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\nWe reproduce the baseball bartting average example from Efron and Morris (1977). Data below has the number of hits for 18 baseball player after 45 at-beat in 1970 season\n\n# Data source: https://www1.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt\nbaseball = read.csv(\"../data/EfronMorrisBB.txt\", sep = \"\\t\", stringsAsFactors = FALSE) %&gt;% select(LastName,AtBats,BattingAverage,SeasonAverage)\n\nNow, we can eatimate overall mean and variance\n\nmu_hat &lt;- mean(baseball$BattingAverage)\nsigma2_hat &lt;- var(baseball$BattingAverage)\n\nAs well as the osterior mean for each player (James-Stein estimator)\n\nbaseball &lt;- baseball %&gt;%\n  mutate(\n    JS = (sigma2_hat / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * mu_hat +\n      ((BattingAverage * (1 - BattingAverage) / AtBats) / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * BattingAverage\n  )\nkable(baseball)\n\n\n\n\nLastName\nAtBats\nBattingAverage\nSeasonAverage\nJS\n\n\n\n\nClemente\n45\n0.40\n0.35\n0.34\n\n\nRobinson\n45\n0.38\n0.31\n0.32\n\n\nHoward\n45\n0.36\n0.28\n0.31\n\n\nJohnstone\n45\n0.33\n0.24\n0.30\n\n\nBerry\n45\n0.31\n0.28\n0.29\n\n\nSpencer\n45\n0.31\n0.27\n0.29\n\n\nKessinger\n45\n0.29\n0.27\n0.28\n\n\nAlvarado\n45\n0.27\n0.22\n0.27\n\n\nSanto\n45\n0.24\n0.27\n0.26\n\n\nSwaboda\n45\n0.24\n0.23\n0.26\n\n\nPetrocelli\n45\n0.22\n0.26\n0.25\n\n\nRodriguez\n45\n0.22\n0.22\n0.25\n\n\nScott\n45\n0.22\n0.30\n0.25\n\n\nUnser\n45\n0.22\n0.26\n0.25\n\n\nWilliams\n45\n0.22\n0.25\n0.25\n\n\nCampaneris\n45\n0.20\n0.28\n0.24\n\n\nMunson\n45\n0.18\n0.30\n0.23\n\n\nAlvis\n45\n0.16\n0.18\n0.22\n\n\n\n\n\nPlot below shows the observed averages vs. James-Stein estimate\n\nggplot(baseball, aes(x = BattingAverage, y = JS)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"Observed Batting Average\",\n    y = \"James-Stein Estimate\",\n    title = \"Empirical Bayes Shrinkage of Batting Averages (2016)\"\n  )\n\n\n\n\n\n\n\n\nCalculate mean squared error (MSE) for observed and James-Stein estimates\n\nmse_observed &lt;- mean((baseball$BattingAverage - mu_hat)^2)\nmse_js &lt;- mean((baseball$JS - mu_hat)^2)\n\ncat(sprintf(\"MSE (Observed): %.6f\\n\", mse_observed))\n\n## MSE (Observed): 0.004584\n\ncat(sprintf(\"MSE (James-Stein): %.6f\\n\", mse_js))\n\n## MSE (James-Stein): 0.001031\n\n\nWe can see that the James-Stein estimator has a lower MSE than the observed batting averages. This is a demonstration of Stein’s paradox, where the James-Stein estimator, which shrinks the estimates towards the overall mean, performs better than the naive sample mean estimator.\n\na = matrix(rep(1:3, nrow(baseball)), 3, nrow(baseball))\nb = matrix(c(baseball$BattingAverage, baseball$SeasonAverage, baseball$JS),    3, nrow(baseball), byrow=TRUE)\n\nmatplot(a, b, pch=\" \", ylab=\"predicted average\", xaxt=\"n\", xlim=c(0.5, 3.1), ylim=c(0.13, 0.42))\nmatlines(a, b)\ntext(rep(0.7, nrow(baseball)), baseball$BattingAverage, baseball$LastName, cex=0.6)\ntext(1, 0.14, \"First 45\\nat bats\", cex=0.5)\ntext(2, 0.14, \"Average\\nof remainder\", cex=0.5)\ntext(3, 0.14, \"J-S\\nestimator\", cex=0.5)\n\n\n\n\n\n\n\n\nNow if we look at the season dynamics for Clemente\n\n# Data source: https://www.baseball-almanac.com/players/hittinglogs.php?p=clemero01&y=1970\ncl = read.csv(\"../data/clemente.csv\")\nx = cumsum(cl$AB)\ny = cumsum(cl$H)/cumsum(cl$AB)\n# Plot x,y startind from index 2\nind = c(1,2)\nplot(x[-ind],y[-ind], type='o', ylab=\"Betting Average\", xlab=\"Number at Bats\")\n# Add horizontal line for season average 145/412 and add text above line `Seaosn Average`\ntext(200, 145/412 + 0.005, \"Season Average\", col = \"red\")\nabline(h = 145/412, col = \"red\", lty = 2)\n# Ted williams record is .406 in in 1941, so you know the first data points are noise\ntext(200, baseball$JS[1] + 0.005, \"JS\", col = \"red\")\nabline(h = baseball$JS[1], col = \"red\", lty = 2)\ntext(200, baseball$BattingAverage[1] + 0.005, \"After 45 Bets\", col = \"red\")\nabline(h = baseball$BattingAverage[1], col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nThe motivation for the shrinkage prior rather than a flat uniform prior are the following probabilistic arguments.\n\nExample 1.5 (Efron Example) Efron shows the importance of priors in high dimensions when one can “borrow strength” (a.k.a. regularisation) across components.\nStein’s phenomenon where \\(y_i | \\theta_i \\sim N(\\theta_i, 1)\\) and \\(\\theta_i \\sim N(0, \\tau^2)\\) illustrates this point well. From the model,\n\\[\nP\\left( \\| y \\| &gt; \\| \\theta \\| \\right) &gt; \\frac{1}{2}\n\\]\nUnder a flat Lebesgue measure, this inequality is reversed in the posterior, namely\n\\[\nP\\left( \\| \\theta \\| &gt; \\| y \\| \\; | \\; y \\right) &gt; \\frac{1}{2}\n\\]\nIn conflict with the classical statement. However, if we use Stein’s rule (posterior where \\(\\tau^2\\) is estimated via empirical Bayes) we have\n\\[\n\\hat{\\theta}(y) = \\left( 1 - \\frac{k-2}{\\|y\\|^2} \\right) y \\quad \\text{and} \\quad E\\left( \\| \\hat{\\theta} - \\theta \\| \\right) &lt; k, \\; \\forall \\theta.\n\\]\nHence, when \\(\\|y\\|^2\\) is small the shrinkage factor is more extreme.\nFor example, if \\(k=10\\), \\(\\|y\\|^2=12\\), then \\(\\hat{\\theta} = (1/3) y\\). Now we have the more intuitive result:\n\\[\nP\\left( \\| \\theta \\| &gt; \\| y \\| \\; | \\; y \\right) &lt; \\frac{1}{2}\n\\]\nShowing that default priors matter in high dimensions.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#ell_1-sparsity-1",
    "href": "17-theoryai.html#ell_1-sparsity-1",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.11 \\(\\ell_1\\) Sparsity",
    "text": "1.11 \\(\\ell_1\\) Sparsity",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#ell_0-subset-selection-1",
    "href": "17-theoryai.html#ell_0-subset-selection-1",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.12 \\(\\ell_0\\) Subset Selection",
    "text": "1.12 \\(\\ell_0\\) Subset Selection",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#bayesain-model-selection-via-regularisation",
    "href": "17-theoryai.html#bayesain-model-selection-via-regularisation",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.13 Bayesain Model Selection via Regularisation",
    "text": "1.13 Bayesain Model Selection via Regularisation\nFrom Bayesian perspective regularization is nothing but incorporation of prior information into the model. Remember, that a Bayesian model is specified by likelihood and prior distributions. Bayesian regularization methods include the Bayesian bridge, horseshoe regularization, Bayesian lasso, Bayesian elastic net, spike-and-slab lasso, and global-local shrinkage priors. Bayesian \\(\\ell_0\\) regularization is an attractive solution for high dimensional variable selection as it directly penalizes the number of predictors. The caveat is the need to search over all possible model combinations, as a full solution requires enumeration over all possible models which is NP-hard.\nIn Bayesian approach, regularization requires the specification of a loss, denoted by \\(\\mathcal{L}\\left(\\beta\\right)\\) and a penalty function, denoted by \\(\\phi_{\\lambda}(\\beta)\\), where \\(\\lambda\\) is a global regularization parameter. From a Bayesian perspective, \\(\\mathcal{L}\\left(\\beta\\right)\\) and \\(\\phi_{\\lambda}(\\beta)\\) correspond to the negative logarithms of the likelihood and prior distribution, respectively. Regularization leads to an maximum a posteriori (MAP) optimization problem of the form \\[\n\\underset{\\beta \\in R^p}{\\mathrm{minimize}\\quad}\n\\mathcal{L}\\left(\\beta\\right) + \\phi_{\\lambda}(\\beta) \\; .\n\\] Taking a probabilistic approach leads to a Bayesian hierarchical model \\[\np(y \\mid \\beta) \\propto \\exp\\{-\\mathcal{L}(\\beta)\\} \\; , \\quad p(\\beta) \\propto \\exp\\{ -\\phi_{\\lambda}(\\beta) \\} \\ .\n\\] The solution to the minimization problem estimated by regularization corresponds to the posterior mode, \\(\\hat{\\beta} = \\mathrm{ arg \\; max}_\\beta \\; p( \\beta|y)\\), where \\(p(\\beta|y)\\) denotes the posterior distribution. Consider a normal mean problem with \\[\n\\label{eqn:linreg}\ny = \\theta+ e \\ , \\ \\  \\text{where } e \\sim N(0, \\sigma^2),~-\\infty \\le \\theta \\le \\infty \\ .\n\\] What prior \\(p(\\theta)\\) should we place on \\(\\theta\\) to be able to separate the “signal” \\(\\theta\\) from “noise” \\(e\\), when we know that there is a good chance that \\(\\theta\\) is sparse (i.e. equal to zero). In the multivariate case we have \\(y_i = \\theta_i + e_i\\) and sparseness is measured by the number of zeros in \\(\\theta = (\\theta_1\\ldots,\\theta_p)\\). The Bayesan Lasso assumes double exponential (a.k.a Laplace) prior distribution where \\[\np(\\theta_i \\mid b) = 0.5b\\exp(-|\\theta|/b).\n\\] We use Bayes rule to calculate the posterior as a product of Normal likelihood and Laplace prior \\[\n\\log p(\\theta \\mid y, b) \\propto ||y-\\theta||_2^2 + \\dfrac{2\\sigma^2}{b}||\\theta||_1.\n\\] For fixed \\(\\sigma^2\\) and \\(b&gt;0\\) the posterior mode is equivalent to the Lasso estimate with \\(\\lambda = 2\\sigma^2/b\\). Large variance \\(b\\) of the prior is equivalent to the small penalty weight \\(\\lambda\\) in the Lasso objective function.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#shrinkage-ell_2-norm",
    "href": "17-theoryai.html#shrinkage-ell_2-norm",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.14 Shrinkage (\\(\\ell_2\\) Norm)",
    "text": "1.14 Shrinkage (\\(\\ell_2\\) Norm)\nWe can estimate the risk bounds of \\(\\ell_2\\) Norm regularisation. The classic bias-variance tradeoff is given by the MSE risk bound. \\[\nR(\\theta,\\hat \\theta) = E_{y|\\theta} \\left [ \\Vert \\hat \\theta - \\theta \\Vert^2 \\right ] = \\Vert \\hat \\theta - \\theta \\Vert^2 + E_{y|\\theta} \\left [ \\Vert \\hat \\theta - \\mathbb{E}(\\hat \\theta) \\Vert^2 \\right ]\n\\]\nIn a case of multiple parameters, the Stein bound is \\[\nR(\\theta,\\hat \\theta_{JS}) &lt; R(\\theta,\\hat \\theta_{MLE}) \\;\\;\\; \\forall \\theta \\in \\mathbb{R}^p, \\;\\;\\; p \\geq 3.\n\\] In an applied problem the gap in risk between MLE and JS estimators can be large. For example, in the normal means problem with \\(p=100\\) and \\(n=100\\), the risk of the MLE is \\(R(\\theta,\\hat \\theta_{MLE}) = 100\\) while the risk of the JS estimator is \\(R(\\theta,\\hat \\theta_{JS}) = 1.5\\). The JS estimator is 67 times more efficient than the MLE. The JS estimator is also minimax optimal in the sense that it attains the minimax risk bound for the normal means problem. The minimax risk bound is the smallest risk that can be attained by any estimator.\nJS is a specific estimate and it motivates the ridge estimator. The ridge estimator is a shrinkage estimator with the penalty function being the squared norm of the parameter vector. The ridge estimator is \\[\n\\hat \\theta_{ridge} = \\left (  X^T X + \\lambda I \\right )^{-1} X^T y\n\\] where \\(\\lambda\\) is the regularization parameter.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#sparsity-ell_1-norm",
    "href": "17-theoryai.html#sparsity-ell_1-norm",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.15 Sparsity (\\(\\ell_1\\) Norm)",
    "text": "1.15 Sparsity (\\(\\ell_1\\) Norm)\nHigh-dimensional predictor selection and sparse signal recovery are routine statistical and machine learning tasks. There is a vast and growing literator with lasso (leat absolute selection) being the gold standard. Horseshoe priors are the Bayesian counterpart. Sparsity relies on the property of a few large signals among many (nearly zero) noisy observations. The goal is to find a niddle in the haystack. Suppose that we observe data from Normal means model \\[\ny_i \\mid \\theta_i \\sim N(\\theta_i,1),~ 1\\le i\\le p, ~ \\theta = (\\theta_1,\\ldots,\\theta_p),\n\\] where parameter \\(\\theta\\) lies in the ball \\[\n||\\theta||_{\\ell_0} = \\{\\theta : \\text{number of  }\\theta_i \\ne 0 \\le p_n\\}.\n\\]\nEven threshholding can beat MLE, when the signal is sparse. The thresholding estimator is \\[\n\\hat \\theta_{thr} = \\left \\{ \\begin{array}{ll} \\hat \\theta_i & \\mbox{if} \\; \\hat \\theta_i &gt; \\sqrt{2 \\ln p} \\\\ 0 & \\mbox{otherwise} \\end{array} \\right .\n\\]\nSparse signal detection provides a challenge to statistical methodology; consider the classical normal means inference problem. Suppose that we observe data from the probability model \\(( y_i | \\theta_i ) \\sim N( \\theta_i,1)\\). We wish to provide an estimator \\(\\hat y_{hs}\\) for the vector of normal means \\(\\theta = ( \\theta_1, \\ldots , \\theta_p )\\). Sparsity occurs when a large portion of the parameter vector contains zeroes. The ’’ultra-sparse`` or “nearly black” vector case occurs when \\(p_n\\), denoting the number of non-zero parameter values, and for \\(\\theta \\in l_0 [ p_n]\\), which denotes the set \\(\\# ( \\theta_i \\neq 0 ) \\leq p_n\\) where \\(p_n = o(n)\\) where \\(p_n \\rightarrow \\infty\\) as \\(n \\rightarrow \\infty\\).\nThe predictive rule is often represented by a mathematical model, such as a linear regression model or a neural network. The model is trained using historical data, which consists of observed inputs and outputs \\((x_1,y_1),\\ldots, (x_n,y_n)\\).\nThe model is then used to predict the output \\(y\\) for new inputs \\(x\\). The accuracy of the model is evaluated using a performance metric, such as the mean squared error or the mean absolute error. The model is then updated and retrained using new data to improve its accuracy. This process is repeated until the model achieves the desired level of accuracy.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#lasso",
    "href": "17-theoryai.html#lasso",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.16 LASSO",
    "text": "1.16 LASSO\nThe Laplace distribution can be represented as scale mixture of Normal distribution(Andrews and Mallows 1974) \\[\n\\begin{aligned}\n\\theta_i \\mid \\sigma^2,\\tau \\sim &N(0,\\tau^2\\sigma^2)\\\\\n\\tau^2  \\mid \\alpha \\sim &\\exp (\\alpha^2/2)\\\\\n\\sigma^2 \\sim & \\pi(\\sigma^2).\\end{aligned}\n\\] We can show equivalence by integrating out \\(\\tau\\) \\[\np(\\theta_i\\mid \\sigma^2,\\alpha) =  \\int_{0}^{\\infty} \\dfrac{1}{\\sqrt{2\\pi \\tau}}\\exp\\left(-\\dfrac{\\theta_i^2}{2\\sigma^2\\tau}\\right)\\dfrac{\\alpha^2}{2}\\exp\\left(-\\dfrac{\\alpha^2\\tau}{2}\\right)d\\tau = \\dfrac{\\alpha}{2\\sigma}\\exp(-\\alpha/\\sigma|\\theta_i|).\n\\] Thus it is a Laplace distribution with location 0 and scale \\(\\alpha/\\sigma\\). Representation of Laplace prior is a scale Normal mixture allows us to apply an efficient numerical algorithm for computing samples from the posterior distribution. This algorithms is called a Gibbs sample and it iteratively samples from \\(\\theta \\mid a,y\\) and \\(b\\mid \\theta,y\\) to estimate joint distribution over \\((\\hat \\theta, \\hat b)\\). Thus, we so not need to apply cross-validation to find optimal value of \\(b\\), the Bayesian algorithm does it “automatically”. We will discuss Gibbs algorithm later in the book.\nWhen prior is Normal \\(\\theta_i \\sim N(0,\\sigma_{\\theta}^2)\\), the posterior mode is equivalent to the ridge estimate. The relation between variance of the prior and the penalty weight in ridge regression is inverse proportional \\(\\lambda\\propto 1/\\sigma_{\\theta}^2\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#subset-selection-ell_0-norm",
    "href": "17-theoryai.html#subset-selection-ell_0-norm",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.17 Subset Selection (\\(\\ell_0\\) Norm)",
    "text": "1.17 Subset Selection (\\(\\ell_0\\) Norm)\nSkike-and-slab (George and McCulloh) or Bernoulli-Gaussian (Polson Sun)",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#bridge-ell_alpha",
    "href": "17-theoryai.html#bridge-ell_alpha",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.18 Bridge (\\(\\ell_{\\alpha}\\))",
    "text": "1.18 Bridge (\\(\\ell_{\\alpha}\\))\nThis is a non-convex penalty when \\(0&lt;\\alpha&lt;1\\). It is an NP-hard problem. When \\(\\alpha=1\\) or \\(\\alpha=2\\) we have optimisation problems that are “solvable” for large scale cases. However, when \\(0\\le \\alpha&lt;1\\) the current optimisation algorithms won’t work.\nThe real killer is that you can use data to estimate \\(\\alpha\\) and \\(\\lambda\\) (let the data speak for itself) Box and Tiao (1992).\nBayesian analogue of the bridge estimator in regression is \\[\ny = X\\beta + \\epsilon\n\\]\nfor some unknown vector \\(\\beta = (\\beta_1, \\ldots, \\beta_p)'\\). Given choices of \\(\\alpha \\in (0,1]\\) and \\(\\nu \\in \\mathbb{R}^+\\), the bridge estimator \\(\\hat{\\beta}\\) is the minimizer of\n\\[\nQ_y(\\beta) = \\frac{1}{2} \\|y - X\\beta\\|^2 + \\nu \\sum_{j=1}^p |\\beta_j|^\\alpha.\n\\tag{1.3}\\]\nThis bridges a class of shrinkage and selection operators, with the best-subset-selection penalty at one end, and the \\(\\ell_1\\) (or lasso) penalty at the other. An early reference to this class of models can be found in Frank and Friedman (1993), with recent papers focusing on model-selection asymptotics, along with strategies for actually computing the estimator (Huang, Horowitz, and Ma (2008), Mazumder, and and Hastie (2011)).\nBridge approach differs from this line of work in adopting a Bayesian perspective on bridge estimation. Specifically, we treat\n\\[\np(\\beta \\mid y) \\propto \\exp\\{-Q_y(\\beta)\\}\n\\]\nas a posterior distribution having the minimizer of Equation 1.3 as its global mode. This posterior arises in assuming a Gaussian likelihood for \\(y\\), along with a prior for \\(\\beta\\) that decomposes as a product of independent exponential-power priors (Box and Tiao (1992)):\n\\[\np(\\beta \\mid \\alpha, \\nu) \\propto \\prod_{j=1}^p \\exp\\left(-\\left|\\frac{\\beta_j}{\\tau}\\right|^\\alpha\\right), \\quad \\tau = \\nu^{-1/\\alpha}. \\tag{2}\n\\]\nRather than minimizing (1), we proceed by constructing a Markov chain having the joint posterior for \\(\\beta\\) as its stationary distribution.\n\n1.18.1 Spike-and-Slab Prior\nOur Bayesian formulation of allows to specify a wide range of range of regularized formulations for a regression problem. In this section we consider a Bayesian model for variable selection. Consider a linear regression problem \\[\ny = \\beta_1x_1+\\ldots+\\beta_px_p + e \\ , \\ \\  \\text{where } e \\sim N(0, \\sigma^2),~-\\infty \\le \\beta_i \\le \\infty \\ .\n\\] We would like to solve the problem of variable selections, i.e. identify which input variables \\(x_i\\) to be used in our model. The gold standard for Bayesian variable selection are spike-and-slab priors, or Bernoulli-Gaussian mixtures. Whilst spike-and-slab priors provide full model uncertainty quantification, they can be hard to scale to very high dimensional problems and can have poor sparsity properties. On the other hand, techniques like proximal algorithms can solve non-convex optimization problems which are fast and scalable, although they generally don’t provide a full assessment of model uncertainty.\nTo perform a model selection, we would like to specify a prior distribution \\(p\\left(\\beta\\right)\\), which imposes a sparsity assumption on \\(\\beta\\), where only a small portion of all \\(\\beta_i\\)’s are non-zero. In other words, \\(\\|\\beta\\|_0 = k \\ll p\\), where \\(\\|\\beta\\|_0 \\defeq \\#\\{i : \\beta_i\\neq0\\}\\), the cardinality of the support of \\(\\beta\\), also known as the \\(\\ell_0\\) (pseudo)norm of \\(\\beta\\). A multivariate Gaussian prior (\\(l_2\\) norm) leads to poor sparsity properties in this situation. Sparsity-inducing prior distributions for \\(\\beta\\) can be constructed to impose sparsity include the double exponential (lasso).\nUnder spike-and-slab, each \\(\\beta_i\\) exchangeably follows a mixture prior consisting of \\(\\delta_0\\), a point mass at \\(0\\), and a Gaussian distribution centered at zero. Hence we write,\n\\[\n\\label{eqn:ss}\n\\beta_i | \\theta, \\sigma_\\beta^2 \\sim (1-\\theta)\\delta_0 + \\theta N\\left(0, \\sigma_\\beta^2\\right) \\ .\n\\] Here \\(\\theta\\in \\left(0, 1\\right)\\) controls the overall sparsity in \\(\\beta\\) and \\(\\sigma_\\beta^2\\) accommodates non-zero signals. This family is termed as the Bernoulli-Gaussian mixture model in the signal processing community.\nA useful re-parameterization, the parameters \\(\\beta\\) is given by two independent random variable vectors \\(\\gamma = \\left(\\gamma_1, \\ldots, \\gamma_p\\right)'\\) and \\(\\alpha = \\left(\\alpha_1, \\ldots, \\alpha_p\\right)'\\) such that \\(\\beta_i  =  \\gamma_i\\alpha_i\\), with probabilistic structure \\[\n\\label{eq:bg}\n\\begin{array}{rcl}\n\\gamma_i\\mid\\theta & \\sim & \\text{Bernoulli}(\\theta) \\ ;\n\\\\\n\\alpha_i \\mid \\sigma_\\beta^2 &\\sim & N\\left(0, \\sigma_\\beta^2\\right) \\ .\n\\\\\n\\end{array}\n\\] Since \\(\\gamma_i\\) and \\(\\alpha_i\\) are independent, the joint prior density becomes \\[\np\\left(\\gamma_i, \\alpha_i \\mid \\theta, \\sigma_\\beta^2\\right) =\n\\theta^{\\gamma_i}\\left(1-\\theta\\right)^{1-\\gamma_i}\\frac{1}{\\sqrt{2\\pi}\\sigma_\\beta}\\exp\\left\\{-\\frac{\\alpha_i^2}{2\\sigma_\\beta^2}\\right\\}\n\\ , \\ \\ \\ \\text{for } 1\\leq i\\leq p \\ .\n\\] The indicator \\(\\gamma_i\\in \\{0, 1\\}\\) can be viewed as a dummy variable to indicate whether \\(\\beta_i\\) is included in the model.\nLet \\(S = \\{i: \\gamma_i = 1\\} \\subseteq \\{1, \\ldots, p\\}\\) be the “active set\" of \\(\\gamma\\), and \\(\\|\\gamma\\|_0 = \\sum\\limits_{i = 1}^p\\gamma_i\\) be its cardinality. The joint prior on the vector \\(\\{\\gamma, \\alpha\\}\\) then factorizes as \\[\n\\begin{array}{rcl}\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2\\right) & = & \\prod\\limits_{i = 1}^p p\\left(\\alpha_i, \\gamma_i \\mid \\theta, \\sigma_\\beta^2\\right) \\\\\n& = &\n\\theta^{\\|\\gamma\\|_0}\n\\left(1-\\theta\\right)^{p - \\|\\gamma\\|_0}\n\\left(2\\pi\\sigma_\\beta^2\\right)^{-\\frac p2}\\exp\\left\\{-\\frac1{2\\sigma_\\beta^2}\\sum\\limits_{i = 1}^p\\alpha_i^2\\right\\} \\ .\n\\end{array}\n\\]\nLet \\(X_\\gamma \\defeq \\left[X_i\\right]_{i \\in S}\\) be the set of “active explanatory variables\" and \\(\\alpha_\\gamma \\defeq \\left(\\alpha_i\\right)'_{i \\in S}\\) be their corresponding coefficients. We can write \\(X\\beta = X_\\gamma \\alpha_\\gamma\\). The likelihood can be expressed in terms of \\(\\gamma\\), \\(\\alpha\\) as \\[\np\\left(y \\mid \\gamma, \\alpha, \\theta, \\sigma_e^2\\right)\n=\n\\left(2\\pi\\sigma_e^2\\right)^{-\\frac n2}\n\\exp\\left\\{\n-\\frac1{2\\sigma_e^2}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n\\right\\} \\ .\n\\]\nUnder this re-parameterization by \\(\\left\\{\\gamma, \\alpha\\right\\}\\), the posterior is given by\n\\[\n\\begin{array}{rcl}\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2, \\sigma_e^2, y\\right) & \\propto &\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2\\right)\np\\left(y \\mid \\gamma, \\alpha, \\theta, \\sigma_e^2\\right)\\\\\n& \\propto &\n\\exp\\left\\{-\\frac1{2\\sigma_e^2}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n-\\frac1{2\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2\n-\\log\\left(\\frac{1-\\theta}{\\theta}\\right)\n\\left\\|\\gamma\\right\\|_0\n\\right\\} \\ .\n\\end{array}\n\\] Our goal then is to find the regularized maximum a posterior (MAP) estimator \\[\n\\arg\\max\\limits_{\\gamma, \\alpha}p\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2, \\sigma_e^2, y \\right) \\ .\n\\] By construction, the \\(\\gamma\\) \\(\\in\\left\\{0, 1\\right\\}^p\\) will directly perform variable selection. Spike-and-slab priors, on the other hand, will sample the full posterior and calculate the posterior probability of variable inclusion. Finding the MAP estimator is equivalent to minimizing over \\(\\left\\{\\gamma, \\alpha\\right\\}\\) the regularized least squares objective function\n\\[\n\\min\\limits_{\\gamma, \\alpha}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n+ \\frac{\\sigma_e^2}{\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2\n+ 2\\sigma_e^2\\log\\left(\\frac{1-\\theta}{\\theta}\\right)\n\\left\\|\\gamma\\right\\|_0 \\ .\n\\tag{1.4}\\] This objective possesses several interesting properties:\n\nThe first term is essentially the least squares loss function.\nThe second term looks like a ridge regression penalty and has connection with the signal-to-noise ratio (SNR) \\(\\sigma_\\beta^2/\\sigma_e^2\\). Smaller SNR will be more likely to shrink the estimates towards \\(0\\). If \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the prior uncertainty on the size of non-zero coefficients is much larger than the noise level, that is, the SNR is sufficiently large, this term can be ignored. This is a common assumption in spike-and-slab framework in that people usually want \\(\\sigma_\\beta \\to \\infty\\) or to be “sufficiently large\" in order to avoid imposing harsh shrinkage to non-zero signals.\nIf we further assume that \\(\\theta &lt; \\frac12\\), meaning that the coefficients are known to be sparse a priori, then \\(\\log\\left(\\left(1-\\theta\\right) / \\theta\\right) &gt; 0\\), and the third term can be seen as an \\(\\ell_0\\) regularization.\n\nTherefore, our Bayesian objective inference is connected to \\(\\ell_0\\)-regularized least squares, which we summarize in the following proposition.\n(Spike-and-slab MAP & \\(\\ell_0\\) regularization)\nFor some \\(\\lambda &gt; 0\\), assuming \\(\\theta &lt; \\frac12\\), \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the Bayesian MAP estimate defined by Equation 1.4 is equivalent to the \\(\\ell_0\\) regularized least squares objective, for some \\(\\lambda &gt; 0\\), \\[\n\\min\\limits_{\\beta}\n\\frac12\\left\\|y - X\\beta\\right\\|_2^2\n+ \\lambda\n\\left\\|\\beta\\right\\|_0 \\ .\n\\tag{1.5}\\]\nFirst, assuming that \\[\n\\theta &lt; \\frac12, \\ \\ \\  \\sigma_\\beta^2 \\gg \\sigma_e^2, \\ \\ \\  \\frac{\\sigma_e^2}{\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2 \\to 0 \\ ,\n\\] gives us an objective function of the form \\[\n\\min\\limits_{\\gamma, \\alpha}\n\\frac12 \\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n+ \\lambda\n\\left\\|\\gamma\\right\\|_0,  \\ \\ \\ \\  \\text{where } \\lambda \\defeq \\sigma_e^2\\log\\left(\\left(1-\\theta\\right) / \\theta\\right) &gt; 0 \\ .\n\\tag{1.6}\\]\nEquation Equation 1.6 can be seen as a variable selection version of equation Equation 1.5. The interesting fact is that Equation 1.5 and Equation 1.6 are equivalent. To show this, we need only to check that the optimal solution to Equation 1.5 corresponds to a feasible solution to Equation 1.6 and vice versa. This is explained as follows.\nOn the one hand, assuming \\(\\hat\\beta\\) is an optimal solution to Equation 1.5, then we can correspondingly define \\(\\hat\\gamma_i \\defeq I\\left\\{\\hat\\beta_i \\neq 0\\right\\}\\), \\(\\hat\\alpha_i \\defeq \\hat\\beta_i\\), such that \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) is feasible to Equation 1.6 and gives the same objective value as \\(\\hat\\beta\\) gives Equation 1.5.\nOn the other hand, assuming \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) is optimal to Equation 1.6, implies that we must have all of the elements in \\(\\hat\\alpha_\\gamma\\) should be non-zero, otherwise a new \\(\\tilde\\gamma_i \\defeq I\\left\\{\\hat\\alpha_i \\neq 0\\right\\}\\) will give a lower objective value of Equation 1.6. As a result, if we define \\(\\hat\\beta_i \\defeq \\hat\\gamma_i\\hat\\alpha_i\\), \\(\\hat\\beta\\) will be feasible to Equation 1.5 and gives the same objective value as \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) gives Equation 1.6.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#horseshoe-prior",
    "href": "17-theoryai.html#horseshoe-prior",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.19 Horseshoe Prior",
    "text": "1.19 Horseshoe Prior\n\n\n\n\n\n\n\n\n\nThe sparse normal means problem is concerned with inference for the parameter vector \\(\\theta = ( \\theta_1 , \\ldots , \\theta_p )\\) where we observe data \\(y_i = \\theta_i + \\epsilon_i\\) where the level of sparsity might be unknown. From both a theoretical and empirical viewpoint, regularized estimators have won the day. This still leaves open the question of how does specify a penalty, denoted by \\(\\pi_{HS}\\), (a.k.a. log-prior, \\(- \\log p_{HS}\\))? Lasso simply uses an \\(L^1\\)-norm, \\(\\sum_{i=1}^K | \\theta_i |\\), as opposed to the horseshoe prior which (essentially) uses the penalty \\[\n\\pi_{HS} ( \\theta_i | \\tau ) = - \\log p_{HS} ( \\theta_i | \\tau ) = - \\log \\log \\left ( 1 + \\frac{2 \\tau^2}{\\theta_i^2} \\right ) .\n\\] The motivation for the horseshoe penalty arises from the analysis of the prior mass and influence on the posterior in both the tail and behaviour at the origin. The latter is the key determinate of the sparsity properties of the estimator.\nThe horseshoe Carvalho, Polson, and Scott (2010) is a Bayesian method for ‘needle-in-a-haystack’ type problems where there is some sparsity, meaning that there are some signals amid mostly noise.\nWe introduce the horseshoe in the context of the normal means model, which is given by \\[Y_i = \\beta_i + \\varepsilon_i, \\quad i = 1, \\ldots, n,\\] with \\(\\varepsilon_i\\) i.i.d. \\(\\mathcal{N}(0, \\sigma^2)\\). The horseshoe prior is given by \\[\\begin{align*}\n\\beta_i &\\sim \\mathcal{N}(0, \\sigma^2 \\tau^2 \\lambda_i^2)\\\\\n\\lambda_i &\\sim C^+(0, 1),\n\\end{align*}\\] where \\(C^+\\) denotes the half-Cauchy distribution. Optionally, hyperpriors on \\(\\tau\\) and \\(\\sigma\\) may be specified, as is described further in the next two sections.\nTo illustrate the shrinkage behaviour of the horseshoe, let’s plot the posterior mean for \\(\\beta_i\\) as a function of \\(y_i\\) for three different values of \\(\\tau\\).\n\nlibrary(horseshoe)\nlibrary(ggplot2)\ntau.values &lt;- c(0.005, 0.05, 0.5)\ny.values &lt;- seq(-5, 5, length = 100)\ndf &lt;- data.frame(tau = rep(tau.values, each = length(y.values)),\n                 y = rep(y.values, 3),\n                 post.mean = c(HS.post.mean(y.values, tau = tau.values[1], Sigma2=1), \n                               HS.post.mean(y.values, tau = tau.values[2], Sigma2=1), \n                               HS.post.mean(y.values, tau = tau.values[3], Sigma2=1)) )\n\nggplot(data = df, aes(x = y, y = post.mean, group = tau, color = factor(tau))) + \n  geom_line(size = 1.5) + \n  scale_color_brewer(palette=\"Dark2\") + \n  geom_abline(lty = 2) + geom_hline(yintercept = 0, colour = \"grey\") + \n  theme_classic() + ylab(\"\") + labs(color = \"Tau\") +\n  ggtitle(\"Horseshoe posterior mean for three values of tau\") \n\n\n\n\n\n\n\n\nSmaller values of \\(\\tau\\) lead to stronger shrinkage behaviour of the horseshoe. Observations that are in absolute value at most equal to \\(\\sqrt{2\\sigma^2\\log(1/\\tau)}\\) are shrunk to values close to zero (Van der Pas et al (2014)). For larger observed values, the horseshoe posterior mean will tend to the identity (that is, barely any shrinkage, the estimate will be very close to the observed value). The optimal value of \\(\\tau\\) is the proportion of true signals. This value is typically not known in practice but can be estimated, as described further in the next sections.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#the-normal-means-problem",
    "href": "17-theoryai.html#the-normal-means-problem",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.20 The normal means problem",
    "text": "1.20 The normal means problem\nThe normal means model is: \\[Y_i = \\beta_i + \\varepsilon_i, \\quad i = 1, \\ldots, n,\\] with \\(\\varepsilon_i\\) i.i.d. \\(\\mathcal{N}(0, \\sigma^2)\\).\nFirst, we will be computing the posterior mean only, with known variance \\(\\sigma^2\\) The function HS.post.mean computes the posterior mean of \\((\\beta_1, \\ldots, \\beta_n)\\). It does not require MCMC and is suitable when only an estimate of the vector \\((\\beta_1, \\ldots, \\beta_n)\\) is desired. In case uncertainty quantification or variable selection is also of interest, or no good value for \\(\\sigma^2\\) is available, please see below for the function HS.normal.means.\nThe function HS.post.mean requires the observed outcomes, a value for \\(\\tau\\) and a value for \\(\\sigma\\). Ideally, \\(\\tau\\) should be equal to the proportion of nonzero \\(\\beta_i\\)’s. Typically, this proportion is unknown, in which case it is recommended to use the function HS.MMLE to find the marginal maximum likelihood estimator for \\(\\tau\\).\nAs an example, we generate 50 data points, the first 10 of which are coming from true signals. The first 10 \\(\\beta_i\\)’s are equal to five and the remaining \\(\\beta_i\\)’s are equal to zero. Let’s first plot the true parameters (black) and observations (blue).\n\ndf &lt;- data.frame(index = 1:50,\n                 truth &lt;- c(rep(5, 10), rep(0, 40)),\n                 y &lt;- truth + rnorm(50) #observations\n                 )\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  ggtitle(\"Black = truth, Blue = observations\")\n\n\n\n\n\n\n\n\nWe estimate \\(\\tau\\) using the MMLE, using the known variance.\n\n(tau.est &lt;- HS.MMLE(df$y, Sigma2 = 1))\n\n## [1] 0.96\n\n\nWe then use this estimate of \\(\\tau\\) to find the posterior mean, and add it to the plot in red.\n\npost.mean &lt;- HS.post.mean(df$y, tau.est, 1)\ndf$post.mean &lt;- post.mean\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean), size = 2, col = \"red\") +\n  ggtitle(\"Black = truth, Blue = observations, Red = estimates\")\n\n\n\n\n\n\n\n\nIf the posterior variance is of interest, the function HS.post.var can be used. It takes the same arguments as HS.post.mean.\n\n1.20.1 Posterior mean, credible intervals and variable selection, possibly unknown \\(\\sigma^2\\)\nThe function HS.normal.means is the main function to use for the normal means problem. It uses MCMC and results in an object that contains all MCMC samples as well as the posterior mean for all parameters (\\(\\beta_i\\)’s, \\(\\tau\\), \\(\\sigma\\)), the posterior median for the \\(\\beta_i\\)’s, and credible intervals for the \\(\\beta_i\\)’s.\nThe key choices to make are:\n\nHow to handle \\(\\tau\\). The recommended option is “truncatedCauchy” (a half-Cauchy prior truncated to \\([1/n, 1]\\)). See the manual for other options.\nHow to handle \\(\\sigma\\). The recommended option is “Jeffreys” (Jeffrey’s prior). See the manual for other options.\n\nOther options that can be set by the user are the level of the credible intervals (default is 95%), and the number of MCMC samples (default is 1000 burn-in samples and then 5000 more).\nLet’s continue the example from the previous section. We first create a ‘horseshoe object’.\n\nhs.object &lt;- HS.normal.means(df$y, method.tau = \"truncatedCauchy\", method.sigma = \"Jeffreys\")\n\nWe extract the posterior mean of the \\(\\beta_i\\)’s and plot them in red.\n\ndf$post.mean.full &lt;- hs.object$BetaHat\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full), size = 2, col = \"red\") +\n  ggtitle(\"Black = truth, Blue = observations, Red = estimates\")\n\n\n\n\n\n\n\n\nWe plot the marginal credible intervals (and remove the observations from the plot for clarity).\n\ndf$lower.CI &lt;- hs.object$LeftCI\ndf$upper.CI &lt;- hs.object$RightCI\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full), size = 2, col = \"red\") +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI), width = .1, col = \"red\") +\n  ggtitle(\"Black = truth, Red = estimates with 95% credible intervals\")\n\n\n\n\n\n\n\n\nFinally, we perform variable selection using HS.var.select. In the normal means problem, we can use two decision rules. We will illustrate them both. The first method checks whether zero is contained in the credible interval, as studied by Van der Pas et al (2017).\n\ndf$selected.CI &lt;- HS.var.select(hs.object, df$y, method = \"intervals\")\n\nThe result is a vector of zeroes and ones, with the ones indicating that the observations is suspected to correspond to an actual signal. We now plot the results, coloring the estimates/intervals blue if a signal is detected and red otherwise.\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full, col = factor(selected.CI)), \n             size = 2) +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI, col = factor(selected.CI)),\n                width = .1) +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Black = truth, Blue = selected as signal, Red = selected as noise\")\n\n\n\n\n\n\n\n\nThe other variable selection method is the thresholding method of Carvalho et al (2010). The posterior mean can be written as \\(c_iy_i\\) where \\(y_i\\) is the observation and \\(c_i\\) some number between 0 and 1. A variable is selected if \\(c_i \\geq c\\) for some user-selected threshold \\(c\\) (default is \\(c = 0.5\\)). In the example:\n\ndf$selected.thres &lt;- HS.var.select(hs.object, df$y, method = \"threshold\")\n\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full, col = factor(selected.thres)), \n             size = 2) +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI, col = factor(selected.thres)),\n                width = .1) +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Black = truth, Blue = selected as signal, Red = selected as noise\")",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#polya-gamma",
    "href": "17-theoryai.html#polya-gamma",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.21 Polya-Gamma",
    "text": "1.21 Polya-Gamma\nBayesian inference for logistic regression has long been recognized as a computationally challenging problem due to the analytically inconvenient form of the binomial likelihood function(Polson, Scott, and Windle 2013). While the probit model enjoys simple latent-variable methods for posterior sampling, the logistic model has historically required more complex approaches involving multiple layers of auxiliary variables or approximations(Polson, Scott, and Windle 2013). The breakthrough work of Polson, Scott, and Windle (2013) introduced a revolutionary data-augmentation strategy using a novel class of distributions called Pólya-Gamma distributions, which enables simple and exact Gibbs sampling for Bayesian logistic regression(Polson, Scott, and Windle 2013).\nThis methodology represents a significant advancement in Bayesian computation, providing a direct analog to the Albert and Chib (1993) method for probit regression while maintaining both exactness and simplicity(Polson, Scott, and Windle 2013). The approach has proven particularly valuable for complex hierarchical models where traditional Metropolis-Hastings samplers are difficult to tune and implement effectively(Polson, Scott, and Windle 2013).\n\n\n\n\n\n\nKey Innovation\n\n\n\nThe Pólya-Gamma methodology provides exact Gibbs sampling for Bayesian logistic regression, eliminating the need for complex Metropolis-Hastings tuning while maintaining theoretical guarantees.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#the-pólya-gamma-distribution",
    "href": "17-theoryai.html#the-pólya-gamma-distribution",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.22 The Pólya-Gamma Distribution",
    "text": "1.22 The Pólya-Gamma Distribution\nThe Pólya-Gamma distribution, denoted as PG(b,c), is carefully constructed as a subset of infinite convolutions of gamma distributions(Polson, Scott, and Windle 2013). A random variable X follows a Pólya-Gamma distribution with parameters b &gt; 0 and c ∈ ℝ if:\n\\[X \\stackrel{d}{=} \\frac{1}{2\\pi^2} \\sum_{k=1}^{\\infty} \\frac{g_k}{(k-1/2)^2 + c^2/(4\\pi^2)}\\]\nwhere \\(g_k \\sim \\text{Ga}(b,1)\\) are independent gamma random variables, and \\(\\stackrel{d}{=}\\) indicates equality in distribution(Polson, Scott, and Windle 2013).\nThe Pólya-Gamma family exhibits several remarkable properties that make it ideal for data augmentation:\n\nLaplace Transform: For \\(\\omega \\sim \\text{PG}(b,0)\\), the Laplace transform is \\(E\\{\\exp(-\\omega t)\\} = \\cosh^{-b}(\\sqrt{t}/2)\\)(Polson, Scott, and Windle 2013)\nExponential Tilting: The general PG(b,c) distribution arises through exponential tilting of the PG(b,0) density:\n\n\\[p(x|b,c) = \\frac{\\exp(-c^2x/2)p(x|b,0)}{E[\\exp(-c^2\\omega/2)]}\\]\nwhere the expectation is taken with respect to PG(b,0)(Polson, Scott, and Windle 2013)\n\nConvolution Property: The family is closed under convolution for random variates with the same tilting parameter(Polson, Scott, and Windle 2013)\nKnown Moments: All finite moments are available in closed form, with the expectation given by:\n\n\\[E(\\omega) = \\frac{b}{2c}\\tanh(c/2) = \\frac{b}{2c}\\frac{e^c-1}{1+e^c}\\]\n\n\n\n\n\n\nComputational Advantage\n\n\n\nThe known moments and convolution properties make the Pólya-Gamma distribution computationally tractable and theoretically well-behaved.\n\n\n\n1.22.1 The Data-Augmentation Strategy\nThe core of the Pólya-Gamma methodology rests on a fundamental integral identity that represents binomial likelihoods as mixtures of Gaussians(Polson, Scott, and Windle 2013). The key theorem states:\n\nTheorem 1: For b &gt; 0 and a ∈ ℝ, the following integral identity holds:\n\\[\\frac{(e^\\psi)^a}{(1+e^\\psi)^b} = 2^{-b}e^{\\kappa\\psi} \\int_0^{\\infty} e^{-\\omega\\psi^2/2} p(\\omega) d\\omega\\]\nwhere \\(\\kappa = a - b/2\\), and \\(p(\\omega)\\) is the density of \\(\\omega \\sim \\text{PG}(b,0)\\)(Polson, Scott, and Windle 2013).\nMoreover, the conditional distribution \\(p(\\omega|\\psi)\\) is also in the Pólya-Gamma class: \\((\\omega|\\psi) \\sim \\text{PG}(b,\\psi)\\)(Polson, Scott, and Windle 2013).\n\n\n\n1.22.2 Gibbs Sampling Algorithm\nThis integral identity leads directly to a simple two-step Gibbs sampler for Bayesian logistic regression(Polson, Scott, and Windle 2013). For a dataset with observations \\(y_i \\sim \\text{Binom}(n_i, 1/(1+e^{-\\psi_i}))\\) where \\(\\psi_i = x_i^T\\beta\\), and a Gaussian prior \\(\\beta \\sim N(b,B)\\), the algorithm iterates:\n\nSample auxiliary variables: \\((\\omega_i|\\beta) \\sim \\text{PG}(n_i, x_i^T\\beta)\\) for each observation\nSample parameters: \\((\\beta|y,\\omega) \\sim N(m_\\omega, V_\\omega)\\) where:\n\n\\(V_\\omega = (X^T\\Omega X + B^{-1})^{-1}\\)\n\\(m_\\omega = V_\\omega(X^T\\kappa + B^{-1}b)\\)\n\\(\\kappa = (y_1-n_1/2, \\ldots, y_n-n_n/2)\\)\n\\(\\Omega = \\text{diag}(\\omega_1, \\ldots, \\omega_n)\\)\n\n\nThis approach requires only Gaussian draws for the main parameters and Pólya-Gamma draws for a single layer of latent variables, making it significantly simpler than previous methods(Polson, Scott, and Windle 2013).\n\n\n1.22.3 The PG(1,z) Sampler\nThe practical success of the Pólya-Gamma method depends on efficient simulation of Pólya-Gamma random variables(Polson, Scott, and Windle 2013). The authors developed a sophisticated accept-reject sampler based on the alternating-series method of Devroye (1986)(Devroye 1986). For the fundamental PG(1,c) case, the sampler:\n\nUses exponential and inverse-Gaussian draws as proposals\nAchieves acceptance probability uniformly bounded below at 0.99919\nRequires no tuning for optimal performance\nEvaluates acceptance using iterative partial sums\n\n\n\n1.22.4 General PG(b,z) Sampling\nFor integer values of b, PG(b,z) random variables are generated by summing b independent PG(1,z) draws, exploiting the convolution property(Polson, Scott, and Windle 2013). This approach maintains efficiency for moderate values of b, though computational cost scales linearly with the total number of counts in negative binomial applications(Polson, Scott, and Windle 2013).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#implementation-with-bayeslogit-package",
    "href": "17-theoryai.html#implementation-with-bayeslogit-package",
    "title": "1  Theory of AI: From MLE to Bayesian Regularization",
    "section": "1.23 Implementation with BayesLogit Package",
    "text": "1.23 Implementation with BayesLogit Package\n\n1.23.1 Package Overview\nThe BayesLogit package provides efficient tools for sampling from the Pólya-Gamma distribution(Windle 2023). The current version (2.1) focuses on core functionality: sampling from the Pólya-Gamma distribution through the rpg() function and its variants(Windle 2023).\n\n\n1.23.2 Core Functions\nThe package offers several sampling methods:\n\nrpg(): Main function that automatically selects the best method\nrpg.devroye(): Devroye-like method for integer h values\nrpg.gamma(): Sum of gammas method (slower but works for all parameters)\nrpg.sp(): Saddlepoint approximation method\n\n\n\n1.23.3 Installation and Basic Usage\n\n# Install from CRAN\ninstall.packages(\"BayesLogit\")\nlibrary(BayesLogit)\n\n# Basic usage examples\n# Sample from PG(1, 0)\nsamples1 &lt;- rpg(1000, h=1, z=0)\n\n# Sample with tilting parameter\nsamples2 &lt;- rpg(1000, h=1, z=2.5)\n\n# Multiple shape parameters\nh_values &lt;- c(1, 2, 3)\nz_values &lt;- c(1, 2, 3)\nsamples3 &lt;- rpg(100, h=h_values, z=z_values)\n\n\n\n1.23.4 Implementing Bayesian Logistic Regression\nHere’s a complete implementation of Bayesian logistic regression using the Pólya-Gamma methodology:\n\n# Bayesian Logistic Regression with Pólya-Gamma Data Augmentation\nbayesian_logit_pg &lt;- function(y, X, n_iter=5000, burn_in=1000) {\n  n &lt;- length(y)\n  p &lt;- ncol(X)\n  \n  # Prior specification (weakly informative)\n  beta_prior_mean &lt;- rep(0, p)\n  beta_prior_prec &lt;- diag(0.01, p)  # Precision matrix\n  \n  # Storage for samples\n  beta_samples &lt;- matrix(0, n_iter, p)\n  omega_samples &lt;- matrix(0, n_iter, n)\n  \n  # Initialize\n  beta &lt;- rep(0, p)\n  \n  for(iter in 1:n_iter) {\n    # Step 1: Sample omega (auxiliary variables)\n    psi &lt;- X %*% beta\n    omega &lt;- rpg(n, h=1, z=psi)\n    \n    # Step 2: Sample beta (regression coefficients)\n    # Posterior precision and mean\n    V_omega &lt;- solve(t(X) %*% diag(omega) %*% X + beta_prior_prec)\n    kappa &lt;- y - 0.5\n    m_omega &lt;- V_omega %*% (t(X) %*% kappa + beta_prior_prec %*% beta_prior_mean)\n    \n    # Sample from multivariate normal\n    beta &lt;- mvrnorm(1, m_omega, V_omega)\n    \n    # Store samples\n    beta_samples[iter, ] &lt;- beta\n    omega_samples[iter, ] &lt;- omega\n  }\n  \n  # Return samples after burn-in\n  list(\n    beta = beta_samples[(burn_in+1):n_iter, ],\n    omega = omega_samples[(burn_in+1):n_iter, ],\n    n_samples = n_iter - burn_in\n  )\n}\n\n# Example usage with simulated data\nset.seed(123)\nn &lt;- 100\nX &lt;- cbind(1, matrix(rnorm(n*2), n, 2))  # Intercept + 2 predictors\nbeta_true &lt;- c(-0.5, 1.2, -0.8)\nlogits &lt;- X %*% beta_true\nprobs &lt;- 1/(1 + exp(-logits))\ny &lt;- rbinom(n, 1, probs)\n\n# Fit model\nresults &lt;- bayesian_logit_pg(y, X, n_iter=3000, burn_in=500)\n\n# Posterior summaries\nposterior_means &lt;- colMeans(results$beta)\nposterior_sds &lt;- apply(results$beta, 2, sd)\n\nComputational Advantages\nExtensive benchmarking studies demonstrate the superior performance of the Pólya-Gamma method across various scenarios(Polson, Scott, and Windle 2013):\n\nSimple logistic models: Competitive with well-tuned Metropolis-Hastings samplers\nHierarchical models: Significantly outperforms alternative methods\nMixed models: Provides substantial efficiency gains over traditional approaches\nSpatial models: Shows dramatic improvements for Gaussian process spatial models\n\nTheoretical Guarantees\nThe Pólya-Gamma Gibbs sampler enjoys strong theoretical properties(Polson, Scott, and Windle 2013):\n\nUniform ergodicity: Proven by Choi and Hobert (2013), guaranteeing convergence and central limit theorems for Monte Carlo averages(Polson, Scott, and Windle 2013)\nNo tuning required: Unlike Metropolis-Hastings methods, the sampler requires no manual tuning\nExact sampling: Produces draws from the correct posterior distribution without approximation\n\n\n\n\n\n\n\nImportant Note\n\n\n\nThe theoretical guarantees hold under standard regularity conditions, and the method requires proper prior specification for optimal performance.\n\n\nBeyond Binary Logistic Regression\nThe Pólya-Gamma methodology extends naturally to various related models(Polson, Scott, and Windle 2013):\n\nNegative binomial regression: Direct application using the same data-augmentation scheme\nMultinomial logistic models: Extended through partial difference of random utility models(Windle, Polson, and Scott 2014)\nMixed effects models: Seamless incorporation of random effects structures\nSpatial models: Efficient inference for spatial count data models\n\n\n\n1.23.5 Modern Applications\nRecent developments have expanded the methodology’s applicability[Windle, Polson, and Scott (2014)](Zhang, Datta, and Banerjee 2018):\n\nGaussian process classification: Scalable variational approaches using Pólya-Gamma augmentation\nDeep learning: Integration with neural network architectures for Bayesian deep learning\nState-space models: Application to dynamic binary time series models\n\nThe Pólya-Gamma methodology represents a fundamental advancement in Bayesian computation for logistic models, combining theoretical elegance with practical efficiency(Polson, Scott, and Windle 2013). Its introduction of the Pólya-Gamma distribution class and the associated data-augmentation strategy has enabled routine application of Bayesian methods to complex hierarchical models that were previously computationally prohibitive(Polson, Scott, and Windle 2013).\nThe BayesLogit package provides researchers and practitioners with efficient, well-tested implementations of these methods(Windle 2023). The combination of exact inference, computational efficiency, and theoretical guarantees makes the Pólya-Gamma approach the method of choice for Bayesian logistic regression in most practical applications(Polson, Scott, and Windle 2013).\nAs computational demands continue to grow and models become increasingly complex, the Pólya-Gamma methodology’s advantages become even more pronounced, establishing it as an essential tool in the modern Bayesian statistician’s toolkit (Tiao (2019)). Ongoing research continues to extend the Pólya-Gamma methodology to new domains, including high-dimensional settings, nonparametric models, and integration with modern machine learning frameworks.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Theory of AI: From MLE to Bayesian Regularization</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acemoglu, Daron, and Pascual Restrepo. 2018. “Artificial\nIntelligence, Automation and Work.” National Bureau of Economic\nResearch.\n\n\nAndrews, D. F., and C. L. Mallows. 1974. “Scale\nMixtures of Normal Distributions.”\nJournal of the Royal Statistical Society. Series B\n(Methodological) 36 (1): 99–102. https://www.jstor.org/stable/2984774.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo,\nZakaria Haque, Salem Haykal, et al. 2017. “Tfx: A\nTensorflow-Based Production-Scale Machine Learning Platform.” In\nProceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard\nFirner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016.\n“End to End Learning for Self-Driving Cars.” arXiv\nPreprint arXiv:1604.07316. https://arxiv.org/abs/1604.07316.\n\n\nBox, George E. P., and George C. Tiao. 1992. Bayesian\nInference in Statistical Analysis. New\nYork: Wiley-Interscience.\n\n\nCarvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010.\n“The Horseshoe Estimator for Sparse Signals.”\nBiometrika, asq017.\n\n\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation.\nSpringer Science & Business Media.\n\n\nDiaconis, P., and D. Ylvisaker. 1983. “Quantifying Prior\nOpinion.”\n\n\nEfron, Bradley, and Carl Morris. 1975. “Data Analysis Using\nStein’s Estimator and Its\nGeneralizations.” Journal of the American\nStatistical Association 70 (350): 311–19.\n\n\n———. 1977. “Stein’s Paradox in Statistics.” Scientific\nAmerican 236 (5): 119–27.\n\n\nFrank, Ildiko E., and Jerome H. Friedman. 1993. “A\nStatistical View of Some Chemometrics Regression\nTools.” Technometrics 35 (2): 109–35. https://www.jstor.org/stable/1269656.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo:\nUber’s Machine Learning Platform.”\n\n\nHuang, Jian, Joel L. Horowitz, and Shuangge Ma. 2008. “Asymptotic\nProperties of Bridge Estimators in Sparse High-Dimensional Regression\nModels.” The Annals of Statistics 36 (2): 587–613.\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in\nArtificial Intelligence.”\n\n\nMazumder, Rahul, Friedman, and Trevor and Hastie. 2011. “SparseNet:\nCoordinate Descent With Nonconvex Penalties.”\nJournal of the American Statistical Association 106 (495):\n1125–38.\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg\nMori. 2017. “Learning Person Trajectory Representations for Team\nActivity Analysis.” arXiv Preprint arXiv:1706.00893. https://arxiv.org/abs/1706.00893.\n\n\nPolson, Nicholas G., James G. Scott, and Jesse Windle. 2013.\n“Bayesian Inference for Logistic\nModels Using Pólya–Gamma Latent\nVariables.” Journal of the American Statistical\nAssociation 108 (504): 1339–49.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How\nPeople and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V\nMcConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018.\n“Prediction of Cardiovascular Risk Factors from Retinal Fundus\nPhotographs via Deep Learning.” Nature Biomedical\nEngineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,\nMatthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering\nChess and Shogi by Self-Play with\na General Reinforcement Learning Algorithm.” arXiv.\nhttps://arxiv.org/abs/1712.01815.\n\n\nStein, Charles. 1964. “Inadmissibility of the Usual Estimator for\nthe Variance of a Normal Distribution with Unknown Mean.”\nAnnals of the Institute of Statistical Mathematics 16 (1):\n155–60.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and\nTD-Gammon.” Communications of the ACM 38\n(3): 58–68.\n\n\nTiao, Louis. 2019. “Pólya-Gamma Bayesian\nLogistic Regression.” Blog post.\n\n\nTikhonov, Andrey Nikolayevich et al. 1943. “On the Stability of\nInverse Problems.” In Dokl. Akad. Nauk Sssr, 39:195–98.\n\n\nVarian, Hal R. 2010. “Computer Mediated\nTransactions.” American Economic Review 100 (2):\n1–10.\n\n\nWindle, Jesse. 2023. “BayesLogit:\nBayesian Logistic Regression.” R package version\n2.1.\n\n\nWindle, Jesse, Nicholas G. Polson, and James G. Scott. 2014.\n“Sampling Polya-Gamma Random Variates: Alternate and\nApproximate Techniques.” arXiv. https://arxiv.org/abs/1405.0506.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu,\nYeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of\nStructured Information from Street View Imagery.” arXiv\nPreprint arXiv:1704.03549. https://arxiv.org/abs/1704.03549.\n\n\nZhang, Yichi, Anirban Datta, and Sudipto Banerjee. 2018. “Scalable\nGaussian Process Classification with Pólya-Gamma Data\nAugmentation.” arXiv Preprint arXiv:1802.06383. https://arxiv.org/abs/1802.06383.",
    "crumbs": [
      "References"
    ]
  }
]