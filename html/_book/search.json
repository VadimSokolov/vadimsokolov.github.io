[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes, AI and Deep Learning",
    "section": "",
    "text": "Preface\nWelcome to the fascinating world of Bayesian learning, artificial intelligence, and deep learning! This book is your guide to understanding these powerful tools and their applications in various fields. This book is a culmination of our experiences teaching these exciting fields to two distinct audiences: business school students at the University of Chicago and engineers at George Mason University.\nThis unique blend of perspectives allows us to present these complex concepts in a way that is accessible to data scientists, business professionals and technical experts. Whether you’re a manager seeking to leverage AI in your organization or an engineer building the next generation of intelligent systems, this book has something for you.\nTechniques discussed in this book emerged as a transformative force in modern society, and its impact on automation is undeniable. From self-driving cars to virtual assistants, these technologies are already a part of our daily lives. In the coming years, they will become even more ubiquitous, impacting every industry and aspect of our lives. Understanding these technologies is essential for anyone who wants to stay ahead of the curve.\nThrough its ability to learn, adapt, and make decisions, AI is accelerating the pace of automation across various industries and sectors. This impact is multifaceted, encompassing both positive and negative aspects that warrant careful consideration. AI algorithms can analyze vast amounts of data to identify patterns and trends, providing valuable insights for informed decision-making. This leads to better resource allocation, optimized processes, and improved outcomes across various domains. Chatbots and virtual assistants powered by AI can handle customer inquiries and provide support 24/7, offering a personalized and efficient experience. It even works on Jewish holidays! This improves customer satisfaction and loyalty, ultimately benefiting businesses.\nAs a result, AI enables the creation of entirely new business models and industries that were previously not possible. This disrupts traditional markets and creates opportunities for innovation and growth. AI is driving significant progress in fields like self-driving cars, personalized medicine, and space exploration. This has the potential to revolutionize these industries and improve lives in numerous ways.\nThe term AI has morphed over time. It was first coined in 1956 by John McCarthy, who defined it as “the science and engineering of making intelligent machines.” Since then, the field has evolved significantly, and the definition of AI has changed accordingly. Today, AI is a broad field that encompasses various subfields, including machine learning, deep learning, and natural language processing. These subfields are often used interchangeably, but they are not the same thing. Machine learning is a subfield of AI that focuses on algorithms that can learn from data. Deep learning is a subfield of machine learning that uses artificial neural networks to learn complex patterns and relationships in data. Natural language processing is a subfield of AI that focuses on algorithms that can understand and generate human language.\nSince 1956, the field of artificial intelligence (AI) has undergone significant transformations traditional AI was mostly focused on rule-based systems and boolean logic programming, with limited learning capabilities. It lead to them being brittle in changing environments. On the other hand, emerging AI is focused on modeling uncertainties, pattern matching, and deep learning. All of those are data-driven approaches. These approaches are more adaptable and can handle complex and unstructured data. They are also more data-dependent and lack interpretability.\n\n\n\n\n\n\n\n\n\n\n\n\nOld AI\n\n\n\nIf rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nNew AI\n\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\nThis book is based on the lecture notes from our courses, which have been refined and expanded over years of teaching. We have incorporated valuable feedback from students, both at the University of Chicago and George Mason University, to create a comprehensive and engaging learning experience. This book is organized into three parts:\n\nPart 1: Bayesian Learning: This part covers the basics of probability and Bayesian inference.\nPart 2: Artificial Intelligence: This part explores the core concepts of AI such and mostly focuses on pattern matching techniques such as decision trees and generalized linear models.\nPart 3: Deep Learning: This part delves into the world of deep learning, focusing on the architecture and training of deep neural networks. It covers convolutional neural networks, recurrent neural networks, and generative adversarial networks.\n\nThis work is inspired by the contributions of many great thinkers in the field of AI and machine learning. We acknowledge the foundational work of pioneers such as: Shannon (Reinforcement Learning), von Neumann (Theory of Choice), Bellman (Optimality and Learning).\nThe evolution of AI can be summarised in three stages:\n\nSearch. Started with one question and webpages ranked by relevance. Larry Page and Sergey Brin developed PageRank algorithm that uses power iterations to rank webpages by relevance. The statistical tools such as Kendall’s tau and Spearman’s rank correlation were used to measure the similarity between the ranking and the actual relevance.\nSuggestions. The first popular suggestion algorithm was developed by Netflix. It used collaborative filtering to recommend movies to users based on their viewing history and the viewing history of other users. The idea was to minimize the burden on the user to search for movies to watch.\nSummaries. Current AI systems like ChatGPT and Perplexity have revolutionized how we interact with information by excelling at summarization and generalization. These large language models can take vast amounts of complex information and distill it into clear, coherent summaries that capture the essential points while maintaining accuracy. They can generalize across different domains, connecting concepts from various fields and providing insights that might not be immediately obvious. For example, ChatGPT can summarize lengthy research papers into key findings, while Perplexity can synthesize information from multiple sources to provide comprehensive answers to complex questions. This ability to summarize and generalize represents a significant leap forward from simple search and recommendation systems, as these AI agents can now act as intelligent intermediaries that understand context, identify patterns, and present information in ways that are most useful to users.\n\nFirst it was one question and one answer, next it was suggestions. You do not know what to search for, collaborative filtering, recommender systems, and search engines. Now, it is summarise, you are the manager of AI agents agents interpret your request, formulate a plan and necessary questions, and then execute the plan. This is the future of AI, where agents can work together to solve complex problems and provide valuable insights.\nBayesian learning is a powerful statistical framework based on the work of Thomas Bayes. It provides a probabilistic approach to reasoning and learning, allowing us to update our beliefs about the world as we gather new data. This makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information. Artificial intelligence (AI) is a vast field that seeks to create intelligent agents capable of performing tasks that typically require human intelligence. These tasks can include perception, reasoning, learning, problem-solving, decision-making, and language processing. AI has made significant progress in recent years, driven by advances in computing power, data availability, and algorithms. Deep learning is a subfield of AI that uses artificial neural networks to learn from data. These networks are inspired by the structure and function of the human brain and have the ability to learn complex patterns and relationships in data. Deep learning has achieved remarkable results in various tasks such as image recognition, natural language processing, and machine translation.\nThe world of business and engineering are increasingly intertwined, as AI becomes an essential tool in both domains. This book bridges the gap between these disciplines by demonstrating how Bayesian learning, AI, and deep learning can be applied to address real-world challenges in:\n\nBusiness: Market analysis, customer segmentation, risk management, and strategic decision-making.\nEngineering: Robotics, image recognition, natural language processing, and data-driven automation.\n\nKey Features of This Book:\n\nAccessible explanations: We break down complex concepts into manageable chunks, using real-world examples and analogies to illustrate key principles.\nCase studies: We showcase practical applications of Bayesian learning, AI, and deep learning across diverse industries.\nHands-on exercises: We provide practical exercises and code examples to help you apply the concepts covered in the book to your own projects.\n\nJoining the AI Revolution:\nThe field of AI is rapidly evolving, and this book equips you with the knowledge and skills necessary to stay ahead of the curve. Whether you’re looking to enhance your business acumen or advance your engineering career, understanding the power of Bayesian learning, AI, and deep learning is crucial.\nWe invite you to join us on this exciting journey and discover the transformative potential of these powerful tools!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Generative AI\n“If you tell me precisely what it is a machine cannot do, then I can always make a machine which will do just that. John von Neumann, 1956”\nWhen you open an Amazon page there are many personal suggestions of goods to purchase. By analyzing previous product pages visited and purchases made by you and other people who have bought similar products Amazon uses AI and machine learning to predict what would of interest to you next time you shop.\nWhen you apply for a loan online, you typically get an immediate answer after filling an application. The information you provide, combined with your credit history pulled from a credit history bureau is used by a predictive model which can tell with high level of confidence whether you are to default on the loan or not.\nYou might ask, what is common among one of the most successful Internet retail company, finance industry and a phenomenal baseball team? All of these decisions use AI and methods of predictive analytics to improve the operations. They used historical observations combined with rigorous statistical analysis and efficient computer algorithms to predict future outcomes and change the decisions. The ability to collect and analyze complex data sets has been a prerogative of a small number of people for many year. It vital to have experience in data engineering, statistics, machine learning and probability. A data scientists has all of those skills. Current tools developed by industry and academic institutions makes data science profession accessible to a wider audience without requiring a training in a specific technical filed.\nOver the past decade, there has been an explosion of work, mostly applied, on deep learning. Applications of deep learning are everywhere. The main reason for this is that large Internet companies such as Google, Facebook, Amazon and Netflix increasingly displace traditional statistical and machine learning methods with deep learning techniques. Though, such companies are at the frontier of applying deep learning, virtually any industry can be impacted by applying deep learning (DL).\nData Science is a relatively new field that refers to sets of mathematical and statistical models, algorithms, and software that allow extracting patterns from data sets. The algorithms are the adoptions of applied mathematics techniques to specific computer architectures and the software implements those algorithms.\nPredictive analytics applies AI models to design predictive rules which then can be used by engineers and business for forecasting or what-if analysis. For example, a company that is interested in predicting sales as a result of advertisement campaign would use predictive model to identify the best way to allocate its marketing budget or a logistics company would use a predictive model to forecast demand for shipments to estimate the number of drivers it would need in the next few months.\nArtificial Intelligence has been around for decades. In fact the term AI was coined by a famous computer scientist John McCarthy in 1955. While being tightly connected to the field of robotics for many years, the AI concepts are widely applicable in other fields, including predictive analytics. Currently, the AI is understood as a set of mathematical tools that are used to develop algorithms that can perform tasks, typically done by humans, for example, drive a car or schedule a doctor’s appointment. This set of mathematical tools include probabilistic models, machine learning algorithms and deep learning. The previous successful applications included the victory of IBM’s DeepBlue over then world champion Garry Kasparov in 1997.\nTree search algorithms were developed by DeepBlue engineers to implement the chess robot. A modification was the addition of heuristics to cut branches of the tree that would not lead to a win. Those heuristics were designed by chess grand masters based on their intuition and previous experience. Vehicles in grand challenge also relied on traditional techniques such as Kalman filters and PID (proportional-integral-derivative) controllers that have been in use for many years.\nTwo distinguishing features of AI algorithms:\nA major difference between modern and historical AI algorithms is that most of the recent AI approaches rely on learning patterns from data. For example, DeepBlue algorithm was “hardcoded” and the human inputs were implemented as if-then statements by the IBM engineers. On the other hand, modern AlphaGo zero algorithm did not use any human inputs whatsoever and learned optimal strategies from a large data sets generated from self-plays. Although handcrafted systems were shown to perform well in some tasks, such as chess playing, the are hard to design for many complex applications, such as self-driving cars. On the other hand large data sets allow us to replace set of rules designed by engineers with a set of rules learned automatically from data. Thus, the learning algorithms, such as deep learning are at the core of the most of modern AI systems.\nThe main driving factor behind the growth of modern AI applications is the availability of massive and often unstructured data sets. Om the other hand, we now have appropriate computing power to develop computationally intensive AI algorithms. The three main modern AI enablers are:\nFitting complicated models to describe complicated patterns without overfitting requires millions or billions of data points. Two key ideas behind pattern-recognition systems are\nIn AI, prediction rules are often referred to as “models”. The process of using data to find a gooo prediction rule is often called “training the model”. With millions (or billions) of datapoints and fast pattern-matching skills, machines can find needles in a haystack proving insights for human health, transportation, ... etc.\nMachine learning (ML) arises from this question: could a computer go beyond “what we know how to order it to perform” and learn on its own how to perform a specified task? Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data? This question opens the door to a new programming paradigm. In classical programming, the paradigm of symbolic AI, humans input rules (a program) and data to be processed according to these rules, and out come answers. With machine learning, humans input data as well as the answers expected from the data, and out come the rules. These rules can then be applied to new data to produce original answers.\nA machine-learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task. For instance, if you wished to automate the task of tagging your vacation pictures, you could present a machine-learning system with many examples of pictures already tagged by humans, and the system would learn statistical rules for associating specific pictures to specific tags.\nAlthough machine learning only started to flourish in the 1990s, it has quickly become the most popular and most successful subfield of AI, a trend driven by the availability of faster hardware and larger datasets. Machine learning is tightly related to mathematical statistics, but it differs from statistics in several important ways. Unlike statistics, machine learning tends to deal with large, complex datasets (such as a dataset of millions of images, each consisting of tens of thousands of pixels) for which classical statistical analysis such as Bayesian analysis would be impractical. As a result, machine learning, and especially deep learning, exhibits comparatively little mathematical theory—maybe too little—and is engineering oriented. It’s a hands-on discipline in which ideas are proven empirically more often than theoretically.\nDeep learning DL is a type of machine learning which performs a sequence of transformations (filters) on a data. Output of each of those filters is called a factor in traditional statistical language and hidden feature in machine learning. Word deep means that there is a large number of filters that process the data. The power of this approach comes from the hierarchical nature of the model.\nThe three main factors driving AI are:\nThe widespread of mobile phones leads to generation of vast amounts of data. Besides images, users generate space and time trajectories, which are currently used to estimate and predict traffic, text messages, website clicking patterns, etc.\nDeep learning with many successful applications, has been frequently discussed in popular media. The popularity of the topic has led to hype people tend to think that deep learning techniques are capable to replace many of the human tasks, such as medical diagnostics, accountings. On the pessimistic side, people think that after a short hype, the DL techniques will disappoint and companies will stop funding R&D work on its development. However, the research on pushing this filed further is slow and it will take time before deep learning penetrates a wide range of industries. At any rate, the demand for data scientists in general and AI specialists has been increasing over the last few years with biggest markets being on silicon valley, NYC and Washington, DC(indeed 2018).\nThe field of predictive analytics was popularized by many famous competitions in which people compete to build the model with lowest prediction error. One of the first of this types of competitions was the Netflix prize. In 2009 Netflix payed $1 million to a team that developed the most accurate model for predicting movies a user would like to watch. At that time Netflix’s recommendation system generated 30 billion predictions per day. The initial goal of improving recommendation algorithm by 10 percent was overachieved by the winning team. The wining team used what is called an ensemble technique, which takes a weighted average from different prediction algorithms. Thus, the first lesson from this competition is that we typically need to build several predictive models to achieve a good results. On the other had, the model developed by the winning team was never used by Netflix due to complexity of those models and the fact that by the end of competition Netflix mostly shifted to streaming movies versus sending DVDs over mail. The second lesson is that simplicity and interpretability of models matters when they are deployed on a large scale. The third lesson, is that models need to adapt accordingly to meet the fast changing business requirements.\nDeep Learning’s (DL) growing popularity is summarized by the grown of products that Google is developing using DL. Figure 2 shows this immense growth. One key differentiating effect is that DL algorithms are scalable and can be implemented across the interned in apps such as YouTube and Gmail.\nApplications of Machine Learning/Deep Learning are endless, you just have to look at the right opportunity! There is a similar dynamics in popularity of deep learning search queries on Google. The growth is again exponential, although it is not yet close to popularity of traditional statistical techniques, such as linear regression analysis.\nMeanwhile, some ethical concurs are being raised as a result of growing popularity of AI. The most discussed thus far is the impact on the job market and many jobs being replaced by deep learning models. Although, some economic analysis (Acemoglu and Restrepo 2018) shows that while jobs displacement leads to reduced demand for labor and wages, it counteracted by a productivity effect and increases in demand for labor in non-automated tasks.\nThe algorithmic aspects of deep learning has existed for decades. In 1956, Kolmogorov has shown that any function can be represented as a superposition of univariate functions (this is exactly what deep learning does). In 1951 Robbins and Monro proposed stochastic approximations algorithms. This is the main technique for finding weights of a deep learning model today.\nBackpropagation algorithm for finding derivatives was first published and implemented by Werbos in 1974. In mid 1980s Schmidhuber studied many practical aspects of applying neural networks to real-life problems. Since the key ingredients of DL has been around for several decades, one could wonder why we observe a recent peak in popularity of those methods.\nOne of the strong driving forces is adoption of DL by internet companies that need to analyze large scale high dimensional datasets, such as human-written text, speech and images. Smartphone photography led to people uploading vast amounts of images to services like Instagram and Facebook. In 2012 more mobile devices were sold than PCs. The number of images shared on the Internet has skyrocketed as well. This can be see in products that Google is developing using DL.\nThe proliferation of smartphones globally has been one of the most dramatic technological adoptions in human history. From just 173 million smartphone users worldwide in 2010, the number exploded to over 6.8 billion users by 2023, representing nearly 86% of the global population. This exponential growth has been particularly pronounced in developing markets, where smartphones often serve as the primary gateway to the internet. Countries like India and China have seen smartphone penetration rates exceed 80%, while regions in Africa and Southeast Asia continue to show rapid adoption curves. The ubiquity of smartphones has fundamentally transformed how data is generated and consumed - these devices produce continuous streams of location data, user interactions, images, messages, and behavioral patterns that form the foundation for modern AI applications. The convergence of increasingly powerful mobile processors, high-resolution cameras, and always-on internet connectivity has created an unprecedented data generation ecosystem that feeds directly into the machine learning models powering everything from recommendation systems to autonomous vehicles.\nTherefore, data generated by Internet users creates a demand for techniques to analyze large scale data sets. Mathematical methodologies were in place for many years. One missing ingredient in the explosive nature of DL popularity is the availability of computing power. DL models are computationally hungry, trial and error process is required to build a useful model. Sometimes hundreds or thousands of different models are required to be evaluated before choosing one to be used in an application. Training models can be computationally expensive, we are usually talking about large amounts of training data that need to be analyzed to build a model.\nThe adoption rate of AI technologies, particularly generative AI like ChatGPT, has shattered all previous records for technology adoption. While it took the internet 7 years to reach 100 million users, the telephone 75 years, and television 13 years, ChatGPT achieved this milestone in just 2 months after its launch in November 2022. This unprecedented speed of adoption reflects not just the accessibility of AI tools, but also their immediate utility across diverse user needs. Unlike previous innovations that required significant infrastructure changes or learning curves, AI chatbots could be accessed through simple web interfaces and provided immediate value for tasks ranging from writing assistance to problem-solving. The viral nature of AI adoption has been further accelerated by social media demonstrations and word-of-mouth sharing of impressive AI capabilities, creating a network effect that compounds the growth rate. This rapid adoption suggests that AI represents a fundamentally different type of technological shift - one that augments human capabilities rather than replacing existing systems entirely. The chart below illustrates the explosive growth potential of AI technologies.\nThe first generation of AI models was fundamentally enabled by the availability of powerful GPU chips, which provided the parallel processing capabilities necessary to train deep neural networks on large datasets. The breakthrough in deep learning around 2012, including innovations like AlexNet for image recognition, would not have been possible without GPUs that could perform thousands of matrix operations simultaneously. Current AI models, including ChatGPT, Claude, and other large language models, continue to rely primarily on GPUs for both training and inference. Modern AI training clusters consist of thousands of interconnected GPUs working together for weeks or months to process the enormous datasets required for today’s sophisticated models. While some companies have developed specialized AI chips like Google’s TPUs, GPUs remain the dominant platform for AI development due to their versatility, widespread availability, and established software ecosystems.\nThe gaming industry was one of the earliest drivers of GPU development, as game developers demanded increasingly sophisticated graphics rendering capabilities to create immersive virtual worlds with realistic lighting, textures, and physics simulations. Companies like NVIDIA and AMD invested heavily in parallel processing architectures optimized for the matrix operations required to render complex 3D scenes in real-time. The rise of cryptocurrency mining, particularly Bitcoin and Ethereum, created an unexpected second wave of GPU demand as miners discovered that graphics cards were far more efficient than traditional CPUs for the repetitive hash calculations required by proof-of-work algorithms. This mining boom drove massive investments in GPU manufacturing capacity and spurred innovations in memory bandwidth and energy efficiency. More recently, the explosion of AI-generated video content has created a third major demand driver, as video generation models require enormous computational power to process and synthesize high-resolution video frames. The convergence of these three use cases - gaming graphics, cryptocurrency mining, and AI video generation - has accelerated GPU development far beyond what any single application could have achieved alone, creating the powerful hardware infrastructure that now enables training of large language models and other AI applications.\nTable 1 illustrates the dramatic evolution of GPU performance over two decades, from early graphics cards to specialized AI accelerators. The data shows exponential growth in computational power: from the modest 0.23 TeraFLOPS of the 2006 GeForce 7900 GTX to the projected 100 PetaFLOPS (FP4) of the 2027 Rubin Ultra - representing a performance increase of over 400,000x. Here FP4 is a lower precision (4-bit) floating-point arithmetic that is used for AI workloads. It is an alternative to FP32 (32-bit) floating-point arithmetic that is used for general purpose computing.\nMemory capacity has similarly exploded from 0.5GB to a projected 1TB. Modern GPUs have evolved from simple graphics processors to sophisticated AI-optimized architectures featuring specialized tensor cores, mixed-precision arithmetic (FP8/FP4), and massive high-bandwidth memory systems. The transition from traditional FP32 floating-point operations to lower-precision AI workloads (FP8/FP4) has enabled unprecedented computational throughput measured in PetaFLOPS and ExaFLOPS scales, making current and future GPUs the primary engines driving the deep learning revolution and large language model training.\nNow AI models are the main consumers of those processors. The more popular of those are ChatGPT-4, Anthropic’s Claude and Perplexity. ChatGPT-4 is based on the transformer architecture. It is able to handle long conversations and maintain better context over multiple turns. It is stronger in creative writing, technical writing, reasoning tasks, and code generation. It has better performance on logic-heavy tasks and answering technical queries. It is mainly used for chatbots, automated content creation, code writing, customer support, and more advanced AI tasks.\nOpenAI, the company behind ChatGPT, has experienced remarkable growth in both valuation and revenue. As of late 2024, OpenAI reached a valuation of $157 billion following its latest funding round, making it one of the most valuable private companies in the world. The company’s annual recurring revenue (ARR) has grown exponentially, reaching approximately $3.7 billion in 2024, driven primarily by ChatGPT subscriptions and API usage. OpenAI has raised over $13 billion in total funding, with major investors including Microsoft, which has invested $13 billion and maintains a strategic partnership that includes exclusive cloud computing arrangements. This rapid financial growth reflects the massive demand for generative AI capabilities across industries and the transformative potential of large language models.\nClaudeis the main competitor of OpenAI. It is supported by Amazon and excels at complex reasoning tasks, problem-solving, and in-depth analysis across a wide range of domains. Claude can write, debug, and explain code in many programming languages. It can analyze images and documents in addition to text and can engage in various conversation styles, from formal analysis to creative writing to casual discussion.\nAmazon has made a significant strategic investment in Anthropic, Claude’s creator, committing up to $4 billion to advance AI safety research and development. This partnership positions Amazon Web Services (AWS) as Anthropic’s primary cloud provider while giving Amazon a minority ownership stake in the company. Unlike ChatGPT, which excels in creative writing and general-purpose conversations, Claude is specifically designed with a focus on safety, harmlessness, and nuanced reasoning. Claude demonstrates superior performance in tasks requiring careful analysis, ethical reasoning, and handling sensitive topics. It employs Constitutional AI training methods that make it more reliable in avoiding harmful outputs and better at acknowledging uncertainty when it doesn’t know something. Recent advances in Claude 3.7 and Claude 4.0 have introduced groundbreaking multimodal capabilities, allowing these models to process and analyze images, documents, and code with unprecedented accuracy. Claude 4.0 represents a significant leap forward in mathematical reasoning, coding assistance, and complex problem-solving tasks, with performance improvements of 40-60% over previous versions in benchmark evaluations. These newer models feature enhanced “thinking” processes that are more transparent, often explaining their reasoning step-by-step with greater depth and clarity, which makes them particularly valuable for educational applications, research assistance, and professional analysis where understanding the AI’s decision-making process is crucial. Claude 4.0 also introduces improved long-context understanding, capable of processing documents up to 200,000 tokens, and demonstrates remarkable advances in scientific reasoning and technical writing. This approach has made Claude increasingly popular among researchers, academics, and professionals who require more thoughtful and contextually aware AI assistance.\nPerplexity synthesizes information from multiple sources and presents it with proper citations. Each response includes references for easy verification. It functions as a conversational search engine. Perplexity has emerged as a formidable competitor to Google Search by offering a fundamentally different approach to information discovery. Unlike traditional search engines that provide links to websites, Perplexity acts as an AI-powered research assistant that directly answers questions while citing sources. The company has attracted significant investment, including backing from Amazon founder Jeff Bezos, who participated in Perplexity’s $74 million Series B funding round in 2024. This strategic investment reflects growing confidence in AI-first search alternatives that could disrupt Google’s longstanding dominance in the search market.\nThe company has also developed innovative partnerships with major brands like Marriott and Nike, demonstrating how AI search can be integrated into enterprise applications. Marriott has explored using Perplexity’s technology to enhance customer service by providing instant, cited answers about hotel amenities, local attractions, and booking policies. Similarly, Nike has experimented with Perplexity’s capabilities to help customers find specific product information, sizing guides, and availability across different locations. These enterprise partnerships showcase Perplexity’s potential to move beyond general web search into specialized, domain-specific applications.\nPerplexity’s advertising model differs significantly from Google’s traditional approach. Rather than displaying ads alongside search results, Perplexity is exploring sponsored answers and branded content integration that maintains the conversational flow while clearly identifying commercial partnerships. This approach could prove less intrusive than traditional search advertising while providing new revenue streams. The company’s growth trajectory and enterprise adoption suggest it could pose a meaningful challenge to Google’s search monopoly, particularly among users who prefer direct answers over browsing multiple websites.\nThe explosive growth of Large Language Models (LLMs) like ChatGPT, Claude, and Perplexity has been fundamentally enabled by the vast repositories of digital text that have accumulated over the past three decades. The “fuel” powering these sophisticated AI systems comes from an unprecedented collection of human knowledge digitized and made accessible through the internet. Wikipedia alone contains over 60 million articles across hundreds of languages, representing one of humanity’s largest collaborative knowledge projects. Web crawling technologies have systematically captured billions of web pages, blog posts, news articles, and forum discussions, creating massive text corpora that encode diverse writing styles, domains of expertise, and forms of human expression. The digitization of literature through projects like Google Books and Internet Archive has made millions of books searchable and processable, from classical literature to technical manuals. Social media platforms have contributed streams of conversational text, while academic databases provide formal scientific and scholarly writing. This digital text explosion created training datasets containing trillions of words - orders of magnitude larger than what any human could read in multiple lifetimes. By processing these enormous text collections through transformer architectures, LLMs learned statistical patterns of language use, absorbing grammar, syntax, semantics, and even reasoning patterns embedded in human writing. The models discovered how words relate to each other, how concepts connect across different contexts, and how to generate coherent, contextually appropriate responses by predicting the most likely next word given preceding text. This approach allowed AI systems to develop surprisingly sophisticated language understanding and generation capabilities without explicit programming of linguistic rules, instead learning the deep structure of human communication from the collective digital footprint of our species.\nThe mathematical operations used for manipulating and rendering images are the same as those used in deep learning models. Researchers started to used graphical processing units (GPUs) (a.k.a graphics cards) to train deep learning models in 2010s. The wide availability of GPUs made deep learning modeling accessible for a large number of researchers and engineers and eventually led to popularity of DL. Recently, several competitive hardware architectures were developed by large companies like Google, which uses its own TPU (Tensor Processing Units) as well as smaller start-ups.\nThis course will focus on practical and theoretical aspects of predicting using deep learning models. Currently, deep learning techniques are almost exclusively used for image analysis and natural language processing and are practiced by a handful number of scientists and engineers with most of them being trained in computer science. However, modern methodologies, software and availability of cloud computing make deep learning accessible to a wide range of data scientists who would typically use more traditional predictive models such as generalized linear regression or tree-based methods.\nA unified approach to analyze and apply deep learning models to a wide range or problems that arise in business and engineering is required. To make this happen, we will bring together ideas from probability and statistics, optimization, scalable linear algebra and high performance computing. Although, deep learning models are very interesting to study from methodological point of view, the most important aspect of those is the predictive power unseen before with more traditional models. Ability to learn very complex patterns in data and generate accurate predictions make the deep learning a useful and exciting methodology to use, we hope to convey that excitement. This set of notes is self-contained and has a set of references for a reader interested in learning further.\nAlthough basics of probability, statistics and linear algebra will be revisited, it is targeted towards students who have completed a course in introductory statistics and high school calculus. We will make extensive use of computational tools, such as R language, as well as PyTorch and TensorFlow libraries for predictive modeling, both for illustration and in homework problems.\nThere are many aspects of data analysis that do not deal with building predictive models, for example data processing and labeling can require significant human resources(Hermann and Balso 2017; Baylor et al. 2017).\nThe landscape of Artificial Intelligence is rapidly being reshaped by the rise of Generative AI (Gen AI). As of 2025, Gen AI has moved beyond hype and into practical application across a multitude of personal and professional domains. A recent article in the Harvard Business Review, “How People Are Really Using Gen AI in 2025” by Marc Zao-Sanders, highlights this shift, noting that user interest has significantly increased and investment in AI is skyrocketing.\nThe article reveals a fascinating trend: a move from purely technical applications towards more emotive and personal uses. The top use cases in 2025 reflect this, with “Therapy/companionship” leading the list. Other prominent uses include “Organizing my life,” “Finding purpose,” “Enhanced learning,” and “Generating code (for pros).” This indicates that individuals are leveraging Gen AI not just for productivity, but also for personal development and well-being.\nSome concrete examples of how people are using Gen AI, as cited in the article, include:\nThe article also points to the increasing sophistication of Gen AI users, who are developing a deeper understanding of the technology’s capabilities and limitations, including concerns around data privacy and the potential for over-reliance.\nBelow is an image from the HBR article summarizing the top 10 use cases:\nSource: Marc Zao-Sanders, “How People Are Really Using Gen AI in 2025,” Harvard Business Review, April 9, 2025, https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025.\nThe continued evolution of Gen AI promises even more sophisticated applications in the future, moving from providing information to taking action (agentic behavior).\nThe computer therapist is not something new. In 1966, Joseph Weizenbaum created ELIZA, a computer program that could simulate a conversation with a psychotherapist. ELIZA used simple pattern matching to respond to user inputs, creating the illusion of understanding. While it was a groundbreaking achievement at the time, it lacked true comprehension and relied on scripted responses.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#generative-ai",
    "href": "00-intro.html#generative-ai",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Therapy/Companionship: Providing accessible mental health support and a sense of connection, especially in regions with limited access to human therapists. Users find AI to be available 24/7 and non-judgmental.\nOrganizing My Life: Creating timelines for tasks, planning daily habits, and managing personal projects.\nEnhanced Learning: Using AI as a study guide to explain complex topics and reinforce learning.\nHealthier Living: Generating meal plans based on specific dietary needs and macro calculations.\nCreating Travel Itineraries: Planning detailed vacations, including finding rustic accommodations and hidden gems while optimizing travel time.\nDisputing Fines: Drafting appeal letters for things like parking tickets.\n\n\n\n\n\n\nTop 10 Gen AI Use Cases in 2025. Source: Harvard Business Review, “How People Are Really Using Gen AI in 2025”, April 9, 2025.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#agi-and-aiq",
    "href": "00-intro.html#agi-and-aiq",
    "title": "Principles of Data Science",
    "section": "AGI and AIQ",
    "text": "AGI and AIQ\n\n“I visualize a time when we will be to robots what dogs are to humans. And I am rooting for the machines.” - Claude Shannon\n\n\n“Let us suppose we have set up a machine with certain initial instruction tables, so constructed that these tables might on occasion, if good reason arose, modify those tables. One can imagine that after the machine had been operating for some time, the instructions would have altered out of all recognition, but nevertheless still be such that one would have to admit that the machine was still doing very worthwhile calculations. Possibly it might still be getting results of the type desired when the machine was first set up, but in a much more efficient manner. In such a case one would have to admit that the progress of the machine had not been foreseen when its original instructions were put in. It would be like a pupil who had learnt much from his master, but had added much more by his own work. When this happens I feel that one is obliged to regard the machine as showing intelligence.” – Alan Turing\n\nPeople, organizations and markets. AI does the organization and hence connects people to markets faster and simplesly. Hence it creates economic values. Most of th recessions in the 19th centurey was a result of not being able to get goods to markets quick enough which led to banking crises. AI accelerates speed to market. It reates growth. The age of abundance is here.\nSkynet and terminator\nTransfer learning\nOlga comments (Toloka)\n\nChat does not know what it does not know\nStill need humans and their skills\nLike co-pilot, we need collaboration between humans and AI, humans became managers\nBefore people would build many classifiers for a specific task. The economics of the model there is one big winner. They combine all the models together.\nNeed humans for ground truth, for labeling data, for training models\nAI is very good at decomposing and planning, and humans are not as good at executing the plan, because it is against their intuition.\n\nAndrej Karpathy’s talk, “Software Is Changing (Again),” explores how large language models (LLMs) are fundamentally transforming the way software is developed and used. He describes this new era as “Software 3.0,” where natural language becomes the primary programming interface and LLMs act as a new kind of computer and compares it to the previous generations of software development approaches sumamrised in the table below.\n\n\n\n\n\n\n\n\n\nParadigm\n“Program” is…\nDeveloper’s main job\nCanonical depot\n\n\n\n\nSoftware 1.0\nHand-written code\nWrite logic\nGitHub\n\n\nSoftware 2.0\nNeural-net weights\nCurate data & train\nHugging Face / Model Atlas\n\n\nSoftware 3.0\nNatural-language prompts\nCompose/police English instructions\nPrompt libraries\n\n\n\nCurrenlty LLMs are collaborative partners that can augment human abilities, democratizing software creation and allowing people without traditional programming backgrounds to build complex applications simply by describing what they want in plain English.\nPolson and Scott (2018) have predicted that human-machine interaction will be the next frontier of AI.\nOlga sais that humans are callable function.\nThe same will happen to university professors. They will become lablers for content. And simply will be responsible for clicking yes, when content is appropriate and no, when it is not.\nHal Varian’s 2010 paper “Computer Mediated Transactions” Varian (2010) provides a foundational framework for understanding how computers can automate routine tasks and decision-making processes, reducing transaction costs and increasing efficiency. This includes automated pricing, inventory management, and customer service systems. He talks about system that can coordinate between multiple parties by providing real-time information sharing and communication platforms. This enables more complex multi-party transactions and supply chain management.\nThis framework remains highly relevant for understanding modern AI and machine learning applications in business, as these technologies represent the next evolution of computer-mediated transactions, enabling even more sophisticated automation, coordination, and communication capabilities.\nIn his talk on “Why are LLMs not Better at Finding Proofs?”, Timothy Gowers discusses that while large language models (LLMs) can display some sensible reasoning—such as narrowing down the search space in a problem—they tend to falter when they get stuck, relying too heavily on intelligent guesswork rather than systematic problem-solving. Unlike humans, who typically respond to a failed attempt with a targeted adjustment based on what went wrong, LLMs often just make another guess that isn’t clearly informed by previous failures. He also highlights a key difference in approach: humans usually build up to a solution incrementally, constructing examples that satisfy parts of the problem and then refining their approach based on the requirements. For example, when trying to prove an existential statement, a human might first find examples satisfying one condition, then look for ways to satisfy additional conditions, adjusting parameters as needed. LLMs, by contrast, are more likely to skip these intermediate steps and try to jump directly to the final answer, missing the structured, iterative reasoning that characterizes human problem-solving.\nWhile there are indeed limitations to what current large language models can solve, particularly in areas requiring systematic mathematical reasoning, they continue to demonstrate remarkable capabilities in solving complex problems through alternative approaches. A notable example is the application of deep learning to the classical three-body problem in physics, a problem that has challenged mathematicians and physicists for centuries. Traditional analytical methods have struggled to find closed-form solutions for the three-body problem, but deep neural networks have shown surprising success in approximating solutions through pattern recognition and optimization techniques. These neural networks can learn the underlying dynamics from training data and generate accurate predictions for orbital trajectories, even when analytical solutions remain elusive. This success demonstrates that the trial-and-error approach, when combined with sophisticated pattern recognition capabilities, can lead to practical solutions for problems that have resisted traditional mathematical approaches. The key insight is that while these methods may not provide the elegant closed-form solutions that mathematicians prefer, they offer valuable computational tools that can advance scientific understanding and enable practical applications in fields ranging from astrophysics to spacecraft navigation.\n\n\n\n\nAcemoglu, Daron, and Pascual Restrepo. 2018. “Artificial Intelligence, Automation and Work.” National Bureau of Economic Research.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, et al. 2017. “Tfx: A Tensorflow-Based Production-Scale Machine Learning Platform.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016. “End to End Learning for Self-Driving Cars.” arXiv Preprint arXiv:1604.07316. https://arxiv.org/abs/1604.07316.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo: Uber’s Machine Learning Platform.”\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in Artificial Intelligence.”\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg Mori. 2017. “Learning Person Trajectory Representations for Team Activity Analysis.” arXiv Preprint arXiv:1706.00893. https://arxiv.org/abs/1706.00893.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How People and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V McConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018. “Prediction of Cardiovascular Risk Factors from Retinal Fundus Photographs via Deep Learning.” Nature Biomedical Engineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.” arXiv. https://arxiv.org/abs/1712.01815.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and TD-Gammon.” Communications of the ACM 38 (3): 58–68.\n\n\nVarian, Hal R. 2010. “Computer Mediated Transactions.” American Economic Review 100 (2): 1–10.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu, Yeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of Structured Information from Street View Imagery.” arXiv Preprint arXiv:1704.03549. https://arxiv.org/abs/1704.03549.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "01-prob.html",
    "href": "01-prob.html",
    "title": "1  Probability and Uncertainty",
    "section": "",
    "text": "1.1 Bernoulli’s Problem\nProbability deals with randomness. The art of data science is being able to “separate” signal from noise. For example, we need to account for randomness in human behavior. A random phenomena, by its very nature, means a precise prediction of an outcome has to be described by a distribution. Surprisingly random events typically have statistical regularity in many ways. For example, if we flip a coin, it would be hard to predict the outcome (head or tail) on an individual flip, but if we flip a coin many times and count the proportion of heads, the average will converge to something close to \\(1/2\\). This is called the law of large numbers.\nProbability is a language that lets you communicate information about uncertain outcomes and events. By assigning a numeric value between zero and one to an event, or a collection of outcomes, in its simplest form, probability measures how likely an event is to occur.\nOur goal here is to introduce you to the concepts of probability, conditional probability and their governing rules. The crowning being bayes rule for updating conditional probabilities. Understanding these concepts serves as a basis for more complex data analysis and machine learning algorithms. Building probabilistic models has many challenges and real world application. You are about to learn about practical examples from fields as diverse as medical diagnosis, chess games to racetrack odds.\nWe start by defining probabilities of a finite number of events. An axiomatic approach was proposed by Kolmogorov. This approach is very powerful and allows us to derive many important results and rules for calculating probabilities. Furthermore, in this chapter, we will discuss the notion of conditional probability and independence as well as tools for summarizing the distribution of a random variable, namely expectation and variance.\nThe study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same (statistical regularity). The first rigorous treatment of probability was presented by Jakob Bernoulli in his paper “Ars Conjectandi” (art of guesses) where he claims that to makes a guess is the same thing as to measure a probability.\nBernoulli considered the following problem. Suppose that we observe \\(m\\) successes and \\(n\\) failures of an event \\(A\\), out of total \\(N=m+n\\) trials. How do we assign a probability \\(P(A)\\) to the event \\(A\\)? A classic definition of the probability (due to Jakob Bernoulli) is the ratio of number of favorable outcomes \\(m\\) to the total number of outcomes \\(N\\), which is the sum of \\(m\\) and the number of unfavorable outcomes \\(n\\) \\[\nP = \\dfrac{m}{m+n} = \\dfrac{m}{N}.\n\\]\nMoreover, can we can we construct a law of succession? What is the probability that the next trial is to be success, given that there are uncertainties in the underlying probabilities. Keynes (1921) considered the rule of succession a.k.a. induction. For example, Bernoulli proposed that \\[\nP_{N+1} = \\dfrac{m+1}{N+2}.\n\\] Keynes (1921) (p. 371) provided a fully Bayesian model based on what we know today as Beta-Binomial model. Chapter 3 provides a full analysis. The determination of the predictive rule is equivalent to the problem of finding a sufficient statistics (a.k.a. summary statistic) and performing feature engineering in modern day artificial intelligence applications.\nde Finetti puts this in the framework of exchangeable random variables, see Kreps (1988) for further discussion. Jeffreys provides an alternative approach based on the principle of indifference. \\[\nP_{N+1} = \\dfrac{m+1/2}{N+1}.\n\\] Ramsey (1926) and de Finetti (1937) and Savage (1956) use a purely axiomatic approach in an effort to operationalize probability. In a famous quote de Finetti says “the probability does not exist”. In this framework, probability is subjective and operationalize as a willingness to bet. If a gambit \\(A\\) pays $1 if it happens and $0 otherwise, then the willingness to bet 50 cents to enter the gamble implies the subjective probability of \\(A\\) is 0.5. Contrary to the frequentist approach, the probability is not a property of the event, but a property of the person. This is the basis of the Bayesian approach to probability.\nLeonard Jimmie Savage, an American statistician, developed a decision theory framework known as the “Savage axioms” or the “Sure-Thing Principle.” This framework is a set of axioms that describe how a rational decision-maker should behave in the face of uncertainty. These axioms provide a foundation for subjective expected utility theory.\nThe Savage axioms consist of three main principles:\nSavage’s axioms provide a basis for the development of subjective expected utility theory. In this theory, decision-makers are assumed to assign subjective probabilities to different outcomes and evaluate acts based on the expected utility, which is a combination of the utility of outcomes and the subjective probabilities assigned to those outcomes.\nSavage’s framework has been influential in shaping the understanding of decision-making under uncertainty. It allows for a more flexible approach to decision theory that accommodates subjective beliefs and preferences. However, it’s worth noting that different decision theorists may have alternative frameworks, and there are ongoing debates about the appropriateness of various assumptions in modeling decision-making.\nFrequency probability is based on the idea that the probability of an event can be found by repeating the experiment many times and probability arises from from some random process on the sample space (such as random selection). For example, if we toss a coin many times, the probability of getting a head is the number of heads divided by the total number of tosses. This is the basis of the frequentist approach to probability.\nAnother way, sometimes more convenient, to talk about uncertainty and to express probabilities is odds, such as 9 to 2 or 3 to 1. We assign odds “on \\(A\\)” or “against \\(A\\)’’. For example, when we say that the odds on a Chicago Bear’s Super Bowl win are 2 to 9, it means that if they are to play 11 times (9+2), they will win 2 times. If \\(A\\) is the win event, then odds on \\(A\\) \\[\nO(A) = \\dfrac{P(A)}{P(\\mbox{not A}) }\n\\] Equivalently, probabilities can be determined from odds \\[\nP(A) = \\dfrac{1}{1+O(A)}\n\\] For example if the odds are one, then \\(O(A) = 1\\) and for every $1 bet you will payout $1. This event has probability \\(0.5\\)\nIf \\(O(A) = 2\\), then you are willing to offer \\(2:1\\). For a $1 bet you’ll payback $3. In terms of probability \\(P = 1/3\\).\nOdds are primarily used in betting markets. For example, let’s re-analyze the 2016 election in the US.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#bernoullis-problem",
    "href": "01-prob.html#bernoullis-problem",
    "title": "1  Probability and Uncertainty",
    "section": "",
    "text": "Completeness Axiom:\n\nThis axiom assumes that a decision-maker can compare and rank all possible outcomes or acts in terms of preferences. In other words, for any two acts (or lotteries), the decision-maker can express a preference for one over the other, or consider them equally preferable.\n\nTransitivity Axiom:\n\nThis axiom states that if a decision-maker prefers act A to act B and prefers act B to act C, then they must also prefer act A to act C. It ensures that the preferences are consistent and do not lead to cycles or contradictions.\n\nContinuity Axiom (or Archimedean Axiom):\n\nThe continuity axiom introduces the concept of continuity in preferences. It implies that if a decision-maker prefers act A to act B, and B to C, then there exists some probability at which the decision-maker is indifferent between A and some lottery that combines B and C. This axiom helps to ensure that preferences are not too “discontinuous” or erratic.\n\n\n\n\n\n\n\n\n\nExample 1.1 (Odds) One of the main sources of prediction markets are bookmakers who take bets on outcomes of events (mostly sporting) at agreed upon odds. Figure 1.1 shows the odds used by several bookmakers to take bets on the winner of the US presidential election in 2016. At that time the market was predicting that Hilary Clinton would win Donald Trump, the second favorite, with odds 7/3. The table is generated by the Oddschecker website.\n\n\n\n\n\n\nFigure 1.1: Presidential Odds 2016\n\n\n\nAhead of time we can assign probabilities of winning to each candidate. According to the bookmakers’ odds the candidate with highest chance to win is Hilary Clinton. The best odds on Clinton are \\(1/3\\), this means that you have to risk $3 to win $1 offered by Matchbook. Odds dynamically change as new information arrives. There is also competition between the Bookmakers and the market is adapting to provide the best possible odds. Ladbrokes is the largest UK bookie and Betfair is an online exchange. A bookmaker sets their odds trying to get equal public action on both sides , otherwise they are risking to stay out of business.\n\n\nExample 1.2 (Kentucky Derby) The Kentucky Derby happens once a year – first Saturday in May. In horse racing the odds are set by the betting public. The racetrack collects all the bets, takes a fee (18%), and then redistributes the pool to the winning tickets. The race is \\(1 \\frac{1}{4}\\) (2 kilometers) race and is the first time the three-year old horses have raced the distance.\nThere was a long period where favorites rarely won. Only six favorites have won in the 36 year period from 1979 to 2013. Recently favorites have one many times in a row. The market is getting better at predicting whose going to win. Here’s the data\n\nSpectacular Bid 1979 (with odds 0.6/1)\nFusaichi Pegasus 2000 (with odds 2.3/1)\nStreet Sense 2007 (with odds 9/2)\nBig Brown 2008 (with odds 5/2)\n\nRecently, favorites have had a lot more success\n\nCalifornia Chrome 2014 (with odds 5/2)\nAmerican Pharoah 2015 (with odds 2/1)\nNyqvist 2016 (with odds 3.3/1)\nAlways Dreaming 2017 (with odds 5.2/1)\n\nThe most famous favorite to win is Secretariat (1973) who won with odds 3/2 in a record time of 1 minute 59 and 2/5 seconds. Monarchos was the only one other horse that in 2005 has broken two minutes at odds 11.5/1.\n\n\nExample 1.3 (Boy-Girl Paradox) If a woman has two children and one is a girl, the chance that the other child is also female has to be \\(50-50\\), right? But it’s not. Let’s list the possibilities of girl-girl, girl-boy and boy-girl. So the chance that both children are girls is 33 percent. Once we are told that one child is female, this extra information constrains the odds. (Even weirder, and I’m still not sure I believe this, the author demonstrates that the odds change again if we’re told that one of the girls is named Florida.) In terms of conditional probability, the four possible combinations are \\[\nBB \\; \\; BG \\; \\; GB \\; \\; GG\n\\] Conditional on the information that one is a girl means that you know we can’t have the \\(BB\\) scenario. Hence we are left with three possibilities \\[\nBG \\; \\; GB \\; \\; GG\n\\] In one \\(1\\) of these is the other a girl. Hence \\(1/3\\).\nIt’s a different question if we say that the first child is a girl. Then the probability that the other is a girl is \\(1/2\\) as there are two possibilities \\[\nGB \\; \\; GG\n\\] This leads to the probability of \\(1/2\\).\n\n\nExample 1.4 (Galton Paradox) You flip three fair coins. What is the \\(P(\\text{all} \\; \\text{alike})\\)?\nAssuming a fair coin (i.e. \\(p(H) = p(T) = 1/2\\)), a formal approach might consist of computing the probability for all heads or all tails, which is \\[\\begin{align*}\np(HHH) &\\equiv p(H \\text{ and } H \\text{ and } H) \\\\\n&= p(H)\\times p(H)\\times p(H) \\\\\n&= \\left(\\frac{1}{2}\\right)^3\n\\end{align*}\\] and, since we’re ultimately interested in the probability of either (mutually exclusive) case, \\[\\begin{align*}\nP(\\text{all alike}) &= P(HHH \\text{ or } TTT) \\\\\n&= P(HHH) + P(TTT) \\\\\n&= 2 \\times \\frac{1}{8}\n\\end{align*}\\]\nOne could arrive at the same conclusion by enumerating the entire sample space and counting the events. Now, what about a simpler argument like the following. In a run of three coin flips, two coins will always share the same result, so the probability that the “remaining/last” coin matches the other two is 1/2; thus, \\[\np(\\text{all alike}) = 1/2\n\\] The fault lies somewhere within the terms the and/or “remaining/last” and their connotation. A faulty symmetry assumption is being made in that statement pertaining to the distribution of the “remaining/last” coin. Loosely put, you’re certain to ultimately be in the case where at least two are alike, as stated in the above argument, but within each case the probability of landing the “remaining/last” matching \\(H\\) or \\(T\\) is not \\(1/2\\), due to the variety of ways you can arrive at two matching coins.\nFor a real treatment of the subject, we highly recommend reading Galton’s essay at galton.org.\n\n\nExample 1.5 (Three Cards) Suppose that you have three cards: one red/red, one red/blue and one blue/blue. You randomly draw a card and place it face down on a table and then you reveal the top side. You see that its red. What’s the probability the other side is red? \\(1/2\\)? No, its \\(2/3\\)! By a similar logic there are six initial possibilities \\[\nB_1 B_2 \\; \\; B_2 B_1 \\; \\; B R \\; \\; R B \\; \\; R_1 R_2 \\; \\; R_2 R_1\n\\] where \\(1\\) and \\(2\\) index the sides of the same colored cards.\nIf we now condition on the top side being red we see that there are still three possibilities left \\[\nR B \\; \\; R_1 R_2 \\; \\; R_2 R_1\n\\] Hence the probability is \\(2/3\\) and not the intuitive \\(1/2\\).\n\n\nExample 1.6 (New England Patriots) Let’s consider another example and calculate the probability of winning 19 coin tosses out of 25. The New England Patriots won 19 out of 25 coin tosses in 2014-15 season. What is the probability of this happening?\nLet \\(X\\) be a random variable equal to \\(1\\) if the Patriots win and \\(0\\) otherwise. It’s reasonable to assume \\(P(X = 1) = \\frac{1}{2}\\). The probability of observing the sequence in which there is 1 on the first 19 positions and 0 afterwards is \\((1/2)^{25}\\). We can code a typical sequence as, \\[\n1,1,1,\\ldots,1,0,0,\\ldots,0.\n\\] There are \\(177,100\\) different sequences of 25 games where the Patriots win 19. There are \\(25! = 1\\cdot 2\\cdot \\ldots \\cdot 25\\) ways to re-arrange this sequence of zeroes and ones. Further, all zeroes and ones are interchangeable and there are \\(19!\\) ways to re-arrange the ones and \\(6!\\) ways to rearrange the sequence on zeroes. Thus, the total number of of different winning sequences is\n\nfactorial(25)/(factorial(19)*factorial(25-19))\n\n## [1] 177100\n\n\nEach potential sequence has probability \\(0.5^{25}\\), thus\n\\[\nP\\left(\\text{Patriots win 19 out 25 tosses}\\right) =  177,100 \\times 0.5^{25} = 0.005\n\\]\nOften, it is easier to communicate uncertainties in a form of odds. In terms of betting odds of \\(1:1\\) gives \\(P = \\frac{1}{2}\\), odds on \\(2:1\\) (I give \\(2\\) for each \\(1\\) you bet) is \\(P = \\frac{1}{3}\\).\nRemember, odds, \\(O(A)\\), is the ratio of the probability of not happening over happening, \\[\nO(A) = (1 - P(A)) / P(A),\n\\] equivalently, \\[\nP(A) = \\frac{1}{1 + O(A)}.\n\\]\nThe odds of patriot winning sequence in then 1 to 199\n\n(1-0.005)/0.005\n\n## [1] 199\n\n\n\n\nExample 1.7 (Hitting Streak) Pete Rose of the Cincinnati Reds set a National League record of hitting safely in \\(44\\) consecutive games. How likely a such a long sequence of safe hits is to be observed? If you were a bookmaker, what odds would you offer on such an event? This means that he safely reached first base after hitting the ball into fair territory, without the benefit of an error or a fielder’s choice at least once at every of those 44 games. Here are a couple of facts we know about him:\n\nRose was a \\(300\\) hitter, he hits safely 3 times out of 10 attempts\nEach at bat is assumed to be independent, i.e., the current at bat doesn’t affect the outcome of the next.\n\nAssuming he comes to bat \\(4\\) times each game, what probability might reasonably be associated with that hitting streak? First we define notation. We use \\(A_i\\) to denote an event of hitting safely at game \\(i\\), then \\[\n\\begin{aligned}\n& P( \\mathrm{Rose \\; Hits \\; Safely \\; in \\;44 \\; consecutive \\; games} ) = \\\\\n& P ( A_1 \\; \\text{and} \\;  A_2  \\ldots \\text{and} \\;  A_{44} ) = P ( A_1 ) P ( A_2 ) \\ldots P ( A_{44} )\n\\end{aligned}\n\\] We now need to find \\(P(A_i)\\text{s}\\) where \\(P (A_i ) = 1 - P ( \\text{not} \\; A_i )\\) \\[\\begin{align*}\nP ( A_1 ) & = 1 - P ( \\mathrm{ not} \\; A_1 ) \\\\\n& = 1 - P ( \\mathrm{ Rose \\; makes \\; 4 \\; outs } ) \\\\\n& = 1 - ( 0.7)^4 = 0.76\n\\end{align*}\\] For the winning streak, then we have \\((0.76)^{44} = 0.0000057\\), a very low probability. In terms of odds, there are three basic inferences\n\nThis means that the odds for a particular player as good as Pete Rose starting a hitting streak today are \\(175,470\\) to \\(1\\).\nThis doesn’t mean that the run of \\(44\\) won’t be beaten by some player at some time: the Law of Very Large Numbers\nJoe DiMaggio’s record is 56. He is a 325 better, thus we have \\((0.792)^{56} = 2.13 \\times 10^{-6}\\) or 455,962 to 1. It’s going to be hard to beat.\n\n\n\nExample 1.8 (Derek Jeter) Sample averages can have paradoxical behavior. This is related to the field of causation and the property of confounding. Let’s compare Derek Jeter and David Justice batting averages. In both 1995 and 1996, Justice had a higher batting average than Jeter did. However, when you combine the two seasons, Jeter shows a higher batting average than Justice! This is just a property of averages and a finer subset selection can change your average effects. Drug trials. Care with selection bias.\n\n\n\n\n1995\n\n1996\n\nCombined\n\n\n\n\n\nDerek Jeter\n12/48\n0.250\n183/582\n0.314\n195/650\n0.310\n\n\nDavid Justice\n104/411\n0.253\n454/140\n0.321\n149/551\n0.270\n\n\n\nThis situation is known as confounding. If occurs when two separate and different populations are aggregated to give misleading conclusions. The example shows that if \\(A,B,C\\) are events it is possible to have the three inequalities \\[\\begin{align*}\n&P( A \\mid B \\text{ and } C ) &gt; P( A \\mid B \\text{ and } \\bar C )\\\\\n&P( A \\mid \\bar  B \\text{ and } C ) &gt; P( A \\mid \\bar  B \\text{ and } \\bar  C )\\\\\n&P( A \\mid \\text{ and } C ) &gt; P( A \\text{ and } C )\n\\end{align*}\\] The three inequalities can’t hold simultaneously when \\(P(B\\mid C) = P(B\\mid \\bar  C)\\).\n\n\nExample 1.9 (Birthday Problem) The birthday problem is a classic problem in probability theory that explores the counterintuitive likelihood of shared birthdays within a group. Surprisingly, in a room of 23 people, the probability of shared birthdays is 50%. With 70 people, the probability is 99.9%.\nIn general, given \\(N\\) items (people) randomly distributed into \\(c\\) categories (birthdays), where the number of items is small compared to the number of categories \\(N \\ll c\\), the probability of no match is given by \\[\nP(\\text{no match}) \\approx \\exp\\left(-N^2/2c\\right).\n\\] Given \\(A_i\\) is the event that person \\(i\\) has a matching birthday with someone, we have \\[\nP(\\text{no match})  = \\prod_{i=1}^{N-1}(1-P(A_i)) = \\exp\\left(\\sum_{i=1}^{N-1}\\log (1-P(A_i))\\right).\n\\] Here \\(P(A_i) =\\dfrac{i}{c}\\) Then use the approximation \\(\\log(1-x) \\approx -x\\) for small \\(x\\) to get \\(P(\\text{no match})\\). \\[\n\\sum_{i=1}^{N-1}\\log (1-P(A_i)) \\approx -\\sum_{i=1}^{N-1}\\dfrac{i}{c} = -\\dfrac{N(N-1)}{2c}.\n\\]\nThe probability of at least two people sharing a birthday is then the complement of the probability above: \\[\nP(\\text{At least one shared birthday}) = 1 - P(\\text{no match}).\n\\] Solving for \\(P(\\text{match})=1/2\\), leads to a square root law \\(N=1.2\\sqrt{c}\\), if \\(c=365\\) then \\(N=23\\), and if \\(c=121\\) (near birthday mathc), then \\(N=13\\).\nThis unintuitive nature of this result is a consequence of the fact that there are many potential pairs of people in the group, and the probability of at least one pair sharing a birthday increases quickly as more people are added. The birthday problem is often used to illustrate concepts in probability, combinatorics, and statistical reasoning. It’s a great example of how our intuitions about probabilities can be quite different from the actual mathematical probabilities.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#kolmogorov-axioms",
    "href": "01-prob.html#kolmogorov-axioms",
    "title": "1  Probability and Uncertainty",
    "section": "1.2 Kolmogorov Axioms",
    "text": "1.2 Kolmogorov Axioms\nLater, in early thirties of the last century, Kolmogorov made significant contributions to the development of probability. He characterized it as a system of sets that meet specific criteria. The representation of the elements within this set is irrelevant. This is similar to how basic geometric concepts are typically introduced. For example, a circle is defined as the set of all points that are equidistant from a given point. The representation of the circle is irrelevant, as long as the set of points meets the criteria. Similarly, a probability field is defined as a set of events that meet specific criteria. This is the basis of the axiomatic approach to probability theory.\nKolmogorov’s axioms, which provided a rigorous foundation for probability theory. He showed that probability is immensely useful and adheres to only a few basic rules. These axioms provided a set of logical and mathematical rules that describe the properties of probability measures.\nLet \\(S\\) be a collection of elementary events and consider two random events \\(A\\) and \\(B\\) that are subsets of \\(S\\). The three axioms are:\n\nNon-negativity: For any random event \\(A\\), the probability of \\(A\\) is greater than or equal to zero: \\[\nP(A)\\ge 0\n\\]\nNormalization: The probability of the entire sample \\(S\\) space is equal to 1: \\[\nP(S) = 1\n\\]\nAdditivity: For mutually exclusive events, we have \\[\nP(A \\text{ or } B) = P(A) + P(B)\n\\] The probability of the union of these events is equal to the sum of their individual probabilities.\n\nMutually exclusive means that only one of the events in the sequence can occur. These axioms provided a solid and consistent foundation for probability theory, allowing mathematicians to reason rigorously about uncertainty and randomness. Kolmogorov’s work helped unify and clarify many concepts in probability, and his axioms are now widely accepted as the basis for modern probability theory. His contributions had a profound impact on various fields, including statistics, mathematical physics, and information theory.\nAssigning probabilities to events is a challenging problem. Often, the probability will be applied to analyze results of experiments (a.k.a observed data). Consider coin-tossing example. We toss coin twice and the possible outcomes are HH, HT, TH, TT. Say event \\(A\\) represents a repetition, then it will consists of the first and second outcome of the two coin-toss. Then, to empirically estimate \\(P(A)\\) we can repeat the two-toss experiment \\(n\\) times and count \\(m\\), the number of times \\(A\\) occurred. When \\(N\\) is large, \\(m/N\\) will be close to \\(P(A)\\). However, if we are to repeat this experiment under different conditions, e.g. when an unbalanced coin is used, our estimate of \\(P(A)\\) will change as well.\nThe axioms provide are a number of rules that probabilities must follow. There are several important corollaries, that can help us assigning probabilities to events. Here are some important corollaries that follow from the Kolmogorov axioms:\n\n\nComplement Rule: Let not \\(A\\) denote the complement of event A. \\[\n  P(\\text{not } A) = 1- P(A).\n\\]\nMonotonicity: If \\(A\\subset B\\), then \\(P(A)\\le P(B)\\). In other words, the probability of a larger set is greater than or equal to the probability of a subset.\nSubadditivity: This is a generalization of the addition rule, where the equality holds when events \\(A\\) and \\(B\\) are mutually exclusive. \\[\nP(A \\text{ or } B)\\le P(A)+P(B).\n\\]\nInclusion-Exclusion Principle: This principle extends subadditivity to the case where \\(A\\) and \\(B\\) are not necessarily mutually exclusive. \\[\nP(A\\text{ or } B)=P(A)+P(B)-P(A\\text{ and }B).\n\\]\nConditional Probabiliity: The conditional probability of \\(A\\) given \\(B\\) is \\[\nP(A\\mid B) = \\dfrac{P(A \\text{ and } B)}{P(B)}.\n\\]\nBayes rule is simply the calculation of conditional probables reversing the conditioning. A disciplined probability accounting so to speak. \\[\nP(A\\mid B) = \\dfrac{P(A \\text{ and } B)}{P(B)} = \\dfrac{P(B\\mid A)P(A)}{P(B)}.   \n\\]\nLaw of total probability is a direct consequence of the definition of conditional probability and the normalization axiom. It states that if \\(B_1, B_2, \\ldots, B_n\\) are mutually exclusive and exhaustive events, then \\[\nP(A) = \\sum_{i=1}^n P(A \\text{ and } B_i) = \\sum_{i=1}^n P(A \\mid B_i)P(B_i).\n\\]\n\nAll of these axioms follow simply from the principle of coherance of the avoidance of dutch book. This incudes the Bayes rule itself (de Finetti, Shimony).\nBayes rule is a fundamental rule of probability that allows us to calculate conditional probabilities. It is a direct consequence of the definition of conditional probability and the normalization axiom. This rule will become central to learning and inference in artificial intelligence.\nBayes rule simply provides a disciplined probability accounting of how this probabilities get updated in light of evidence. A rational agent requires that their subjective probabilities must obey the principle of coherence. Namely in announcing the set of probabilities he cannot undergo a sure loss. Interestingly enough, this is enough to provide a similar framework to the axiomatic approach of Kolmogorov.\nThese corollaries and principles help in deriving further results and provide additional tools for analyzing and understanding probability and random processes based on the fundamental principles laid out by Kolmogorov. Arguably the most important rule is Bayes rule for conditional probability.\nThe age of artificial intelligence (AI) has certainly proved that Bayes is a powerful tool. One of the key properties of probabilities is that they are updated as you learn new information. Conditional means given its personal characteristics of the personal situation. Personalization algorithms used by many online services rely on this concept. One can argue that all probabilities are conditional in some way. The process of Bayesian updating is central to how machines learn from observed data. Rational human behavior ought to adhere to Bayes rule, although there is much literature documenting the contrary.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#dutch-book-and-the-rules-of-probability",
    "href": "01-prob.html#dutch-book-and-the-rules-of-probability",
    "title": "1  Probability and Uncertainty",
    "section": "1.3 Dutch book and the rules of probability",
    "text": "1.3 Dutch book and the rules of probability\nIf probabilities are degrees of belief and subjective, where do they come from and what rules must they satisfy? These questions were answered to varying degrees by Ramsey, de Finetti, and Savage. Ramsey and de Finetti, working independently and at roughly the same time, developed the first primitive theories of subjective probability and expected utility, and Savage placed the theories on a more rigorous footing, combining the insights of Ramsey with the expected utility theory of von Neumann and Morgenstern.\nThe starting point for Ramsey’s and de Finetti’s theories is the measurement of one’s subjective probabilities using betting odds, which have been used for centuries to gauge the uncertainty over an event. As noted by de Finetti, “It is a question of simply making mathematically precise the trivial and obvious idea that the degree of probability attributed by an individual to a given event is revealed by the conditions under which he would be disposed to bet on that event” (p. 101). Notice the difference between the frequentist and Bayesian approach. Instead of defining the probabilities via an infinite repeated experiment, the Bayesian approach elicits probabilities from an individual’s observed behavior.\nFormally, for any event \\(A\\), the identity \\[\nP(A)  =\\frac{1}{1+\\text{odds}(A)}\\mathrm{or}\\;\\;\\text{odds}(A)=\\frac{1-P(A)}{P(A)}\\text{,}\n\\] where \\(\\bar A\\) is the complement of \\(A\\), links odds and probabilities. Throughout, we use \\(P\\) as a generic term to denote probabilities, when there is no specific reference to an underlying distribution or density. If a horse in a race has odds of 2, commonly expressed as 2:1 (read two to one), then the probability the horse wins is \\(1/3\\). The basic idea of using betting odds to elicit probabilities is simple and intuitive: ask an individual to place odds over various mutually exclusive events, and use these odds to calculate the probabilities. Odds are fair if lower odds would induce a person to take the bet and higher odds would induce the person to take the other side of the bet.\nIn constructing a collection of betting odds over various events, de Finetti and Ramsey argued that not all odds are rational (i.e., consistent or coherent). For example, the sum of the probability of each horse winning a race cannot be greater than one. If a person has inconsistent beliefs, then he “could have a book made against him by a cunning bettor and would then stand to lose in any event” (Ramsey (1931), p. 22). This situation is called a Dutch book arbitrage, and a rational theory of probability should rule out such inconsistencies. By avoiding Dutch books, Ramsey and de Finetti showed that the degrees of beliefs elicited from coherent odds satisfy the standard axioms of probability theory, such as the restriction that probabilities are between zero and one, finite additivity, and the laws of conditional. The converse also holds: probabilities satisfying the standard axioms generate odds excluding Dutch-book arbitrages. Absence of arbitrage is natural in finance and economics and is a primary assumption for many foundational results in asset pricing. In fact, the derivations given below have a similar flavor to those used to prove the existence of a state price density assuming discrete states.\nDutch-book arguments are simple to explain. To start, they require an individual to post odds over events. A bettor or bookie can then post stakes or make bets at those odds with a given payoff, \\(S\\). The choice of the stakes is up to the bettor. A Dutch book occurs when a cunning bettor makes money for sure by placing carefully chosen stakes at the given odds. Alternatively, one can view the odds as prices of lottery tickets that pay off $1 when the event occurs, and the stakes as the number of tickets bought. Thus, probabilities are essentially lottery ticket prices. In fact, de Finetti used the notation ‘Pr’ to refer to both prices and probabilities.\nTo derive the rules, consider the first axiom of probability: for any event \\(A\\), \\(0\\leq P(A) \\leq 1\\). Suppose that the odds imply probabilities \\(P(A)\\) for \\(A\\) occurring and \\(P(\\bar A)\\) for other outcomes, with associated payoffs of \\(S_{A}\\) and \\(S_{\\bar A}\\). Then, having bet \\(S_{A}\\) and \\(S_{\\bar A}\\), the gains if \\(A\\) or \\(\\bar A\\) occur, \\(G_{A}\\) and \\(G_{\\bar A}\\), respectively, are \\[\\begin{align*}\nG(A)   &  =S_{A}-\\text{$P$}(A)\nS_{A}-\\text{$P$}(\\bar A)  S_{\\bar A}\\\\\nG(\\bar A)   &  =S_{\\bar A}-\\text{$P$}(A)\nS_{A}-\\text{$P$}(\\bar A)  S_{\\bar A}\\text{.}%\n\\end{align*}\\] To see this, note that the bettor receives \\(S_{A}\\) and pays \\(P(A) S_{A}\\) for a bet on event \\(A\\). The bookie can always choose to place a zero stake on \\(\\bar A\\) occurring, which implies that \\(G(A) =S_{A}-P(A) S_{A}\\) and \\(G\\left(\\bar A\\right) =-P(A) S_{A}\\). Coherence or the absence of arbitrage implies that you cannot gain or lose in both states, thus \\(G(A) G(\\bar A) \\leq 0\\). Substituting, \\(\\left( 1-P(A) \\right) P(A) \\geq0\\) or \\(0\\leq P(A) \\leq 1\\), which is the first axiom of probability. The second axiom is that the set of all possible outcomes has probability \\(1\\) is similarly straightforward to show.\nThe third axiom is that probabilities add, that is, for two disjoint events \\(A_{1}\\) and \\(A_{2}\\), \\(P(A) =P\\left( A_{1} \\text{ or } A_{2}\\right) =P\\left( A_{1}\\right) +P\\left( A_{2}\\right)\\). Assuming stakes sizes of \\(S_{A}\\), \\(S_{A_{1}},\\) and \\(S_{A_{2}}\\) (and zero stakes on their complements) there are three possible outcomes. If neither \\(A_{1}\\) nor \\(A_{2}\\) occur, the gain is \\[\nG(\\bar A)  =-\\text{$P$}(A)  S_{A}%\n-\\text{$P$}\\left(  A_{1}\\right)  S_{A_{1}}-\\text{$P$}\\left( A_{2}\\right)  S_{A_{2}}.\n\\]\nIf \\(A_{1}\\) occurs, \\(A\\) also occurs, and the gain is \\[\nG\\left(  A_{1}\\right)  =\\left(  1-\\text{$P$}(A)\n\\right)  S_{A}+\\left(  1-\\text{$P$}\\left(  A_{1}\\right)  \\right)\nS_{A_{1}}-\\text{$P$}\\left(  A_{2}\\right)  S_{A_{2}},\n\\] and finally if \\(A_{2}\\) occurs, \\(A\\) also occurs, and \\[\nG\\left(  A_{2}\\right)  =\\left(  1-\\text{$P$}(A)\n\\right)  S_{A}-\\text{$P$}\\left(  A_{1}\\right)  S_{A_{1}}+\\left( 1-\\text{$P$}\\left(  A_{2}\\right)  \\right)  S_{A_{2}}.\n\\] Arranging these into a matrix equation, \\(G=PS\\):\n\\[\n\\left( \\begin{array}\n[c]{c}%\nG(\\bar A) \\\\\nG\\left(  A_{1}\\right) \\\\\nG\\left(  A_{2}\\right)\n\\end{array}\n\\right)  =\\left( \\begin{array}\n[c]{ccc}%\n-P(A)  & -P\\left(  A_{1}\\right)  &\n1-P(A) \\\\\n1-P(A)  & 1-P\\left(  A_{1}\\right)  &\n-P\\left(  A_{2}\\right) \\\\\n1-P(A)  & -P\\left(  A_{1}\\right)  &\n1-P\\left(  A_{2}\\right)\n\\end{array}\n\\right)  \\left( \\begin{array}\n[c]{c}%\nS_{A}\\\\\nS_{A_{1}}\\\\\nS_{A_{2}}%\n\\end{array}\n\\right)  \\text{.}%\n\\]\nThe absence of a Dutch book arbitrage implies that there is no set of stakes, \\(S_{A}\\), \\(S_{A_{1}}\\), and \\(S_{A_{2}}\\), such that the winnings in all three events are positive. If the matrix \\(P\\) is invertible, it is possible to find stakes with positive gains. To rule this, the determinant of \\(P\\) must be zero, which implies that \\(0=-P(A) +P\\left(A_{1}\\right) +P\\left( A_{2}\\right)\\), or \\(P\\left(A\\right) =P\\left( A_{1}\\right) +P\\left( A_{2}\\right)\\). The chapter, countably additivity also holds.\nThe fourth axiom is conditional probability. Consider an event \\(B\\), with \\(P\\left( B\\right) &gt;0\\), an event \\(A\\) that occurs conditional on \\(B\\), and the event that both \\(A\\) and \\(B\\) occur. The probabilities or prices of these bets are \\(P\\left( B\\right)\\), \\(P\\left( A \\mid B\\right)\\), and \\(P\\left( A \\text{ and } B\\right)\\). Consider bets with stakes \\(S_{A}\\), \\(S_{A \\mid B}\\) and \\(S_{A \\text{ and } B}\\), with the understanding that if \\(B\\) does not occur, the conditional bet on \\(A\\) is canceled. The payoffs to the events that \\(B\\) does not occur, \\(B\\) occurs but not \\(A\\), and \\(A\\) and \\(B\\) occur, are \\[\n\\left( \\begin{array}\n[c]{c}%\nG\\left(  \\bar B\\right) \\\\\nG\\left(  \\bar A \\text{ and } B\\right) \\\\\nG\\left(  A \\text{ and } B\\right)\n\\end{array}\n\\right)  =\\left( \\begin{array}\n[c]{ccc}%\n-P\\left(  B\\right)  & -P\\left(  A \\text{ and } B\\right)  & 0\\\\\n1-P\\left(  B\\right)  & -P\\left(  A \\text{ and } B\\right)  &\n-P\\left(  A \\mid B\\right) \\\\\n1-P\\left(  B\\right)  & 1-P\\left(  A \\text{ and } B\\right)  &\n1-P\\left(  A \\mid B\\right)\n\\end{array}\n\\right)  \\left( \\begin{array}\n[c]{c}%\nS_{B}\\\\\nS_{A \\text{ and } B}\\\\\nS_{A \\mid B}%\n\\end{array}\n\\right)  \\text{.}%\n\\] Similar arguments imply the determinant must be zero, which implies that \\[\nP\\left(  A \\mid B\\right)  =\\frac{P\\left(  A \\text{ and } B\\right)\n}{P\\left(  B\\right)  },\n\\] which is the law of conditional probability, given \\(P(B)&gt;0\\), of course, otherwise the conditional probability is not defined, and the \\(P\\) matrix has determinant 0.\nTo summarize, probabilities are degrees of belief and are subjective, and if these beliefs are consistent or coherent, they satisfy the rules of probability. Thus, unlike the Kolmogorov system that assumes the laws of probability, the Bayesian approach derives the laws of probability from behavior that avoids certain losses. This is why most Bayesians describe their way of thinking as rational and coherent.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#random-variables",
    "href": "01-prob.html#random-variables",
    "title": "1  Probability and Uncertainty",
    "section": "1.4 Random Variables",
    "text": "1.4 Random Variables\nA random variable is a function that maps the outcomes of a random experiment (events) to real numbers. It essentially assigns a numerical value to each outcome in the sample space of a random experiment. In other words, a random variable provides a bridge between the abstract concept of events in a sample space and the concrete calculations involving numerical values and probabilities. Similar to assigning probabilities to events, we can assign respective probabilities to random variables.\nFor example, consider a random experiment of rolling a die. Here, an event could be “the outcome is an even number”, and the random variable could be the actual number that shows up on the die. The probability of the event “the outcome is an even number” is 0.5, and the probability distribution of the random variable is a list of all numbers from 1 to 6 each with a probability of 1/6.\nSo, in summary, while events and random variables are distinct concepts, they are closely related through the framework of probability theory, with random variables serving as a key tool for calculating and working with probabilities of events.\n\n1.4.1 Discrete Random Variable\nRandom variables are quantities that we are not certain about. The simplest version of a random variable is a binary yes/no outcome. A random variable that can take a finite or a countable number of values is called discrete random variable. Otherwise, it will be a continuous random variable.\n\nA random variable will describe an uncertain quantity, denoted by \\(X\\), by attaching a numeric value to the occurrence of an event. Two examples of discrete random variable are\n\nWill a user click-through on a Google ad?\nWho will win the 2024 will elections?\n\nRandom variables are constructed by assigning specific values to events such as \\(\\{X=x\\}\\) which corresponds to the outcomes where \\(X\\) equals to a specific number \\(x\\). Associated with possible outcomes are probabilities, a number between zero and one.\nTo fix notation, we will use \\(\\prob{X=x}\\) to denote the probability that random variable \\(X\\) is equal to \\(x\\). A map from all possible values \\(x\\) of a discrete random variable \\(X\\) to probabilities is called a probability mass function \\(p(x)\\). We will interchangeably use \\(\\prob{X=x}\\) and \\(p(x)\\). An important property of the probability mass function is that (normalization Kolmogorov axiom) \\[\n\\sum_{x\\in S} p(x) = 1.\n\\] Here \\(S\\) denotes the set of all possible values of random variable \\(X\\).\nClearly, all probabilities have to be greater than or equal to zero, so that \\(p(x)\\ge 0\\).\nFor a continuous random variable, the probability distribution is represented by a probability density function (PDF), which indicates the likelihood of the variable falling within a particular range and will discuss it later. In continuous case, we will use \\(p(x)\\) to denote probability density function. Another way of describing a continuous random variable, is to use cumulative density function \\(F(x) = P(X\\le x)\\). Arguably, a more natural approach.\nThe Cumulative Distribution Function (CDF) for a discrete random variable is a function that provides the probability that the random variable is less than or equal to a particular value. The CDF is monotonically increasing function (never decreases as \\(x\\) increases). In other words, if \\(a \\leq b\\), then \\(F_X(a) \\leq F_X(b)\\). The value of the CDF always lies between 0 and 1, inclusive.\n\nExample 1.10 (Discrete CDF) Suppose \\(X\\) is a discrete random variable that represents the outcome of rolling a six-sided die. The probability mass function (PMF) of \\(X\\) is:\n\\[\nP(X = x) = \\frac{1}{6}\n\\] for \\(x = 1, 2, 3, 4, 5, 6\\)\nThe CDF of \\(X\\), \\(F(x)\\), is calculated as follows:\n\nFor \\(x &lt; 1\\), \\(F(x) = 0\\) (since it’s impossible to roll less than 1).\nFor \\(1 \\leq x &lt; 2\\), \\(F(x) = \\frac{1}{6}\\) (the probability of rolling a 1).\nFor \\(2 \\leq x &lt; 3\\), \\(F(x) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6}\\) (the probability of rolling a 1 or 2).\nThis pattern continues, adding \\(\\frac{1}{6}\\) for each integer interval up to 6.\nFor \\(x \\geq 6\\), \\(F(x) = 1\\) (since it’s certain to roll a number 6 or less).\n\nGraphically, the CDF of a discrete random variable is a step function that increases at the value of each possible outcome. It’s flat between these outcomes because a discrete random variable can only take specific, distinct values.\n\nplot(ecdf(1:6), main=\"\")\n\n\n\n\nCDF of a discrete random variable",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#bernoulli-distribution",
    "href": "01-prob.html#bernoulli-distribution",
    "title": "1  Probability and Uncertainty",
    "section": "1.5 Bernoulli Distribution",
    "text": "1.5 Bernoulli Distribution\nThe formal model of a coin toss was described by Bernoulli. He modeled the notion of probability for a coin toss, now known as the Bernoulli distribution, there \\(X \\in \\{0,1\\}\\) and \\(P(X=1)=p, P(X=0) = 1-p\\). Laplace gave us the principle of insufficient reason: where you would list out the possibilities and then place equal probability on each of the outcomes. Essentially the discrete distribution on the set of possible outcomes.\nA Bernoulli trial relates to an experiment with the following conditions\n\nThe result of each trial is either a success or failure.\nThe probability \\(p\\) of a success is the same for all trials.\nThe trials are assumed to be independent.\n\nThe Bernoulli random variable can take on one of two possible outcomes, typically labeled as “success” and “failure.” It is named after the Swiss mathematician Jacob Bernoulli, who introduced it in the 18th century. The distribution is often denoted by \\(\\text{Bernoulli}(p)\\), where \\(p\\) is the probability of success.\nThe probability mass function (PMF) of a Bernoulli distribution is defined as follows: \\[\nP(X = x) = \\begin{cases}\np & \\text{if } x = 1 \\\\\n1 - p & \\text{if } x = 0\n\\end{cases}\n\\] The expectation (mean) of a Bernoulli distributed random variable \\(X\\) is given by: \\[\\E{X} = p\n\\] Simply speaking, if you are to toss a coin many times, you expect \\(p\\) heads.\nThe variance of \\(X\\) is given by: \\[\n\\Var{X} = p(1-p)\n\\]\n\nExample 1.11 (Coin Toss) The quintessential random variable is an outcome of a coin toss. The set of all possible outcomes, known as the sample space, is \\(S = \\{H,T\\}\\), and \\(p(X = H) = p(X = T) = 1/2\\). On the other hand, a single outcome can be an element of many different events. For example, there are four possible outcomes of two coin tosses, HH, TT, HT, TH, which are equally likely with probabilities 1/4. The probability mass function over the number of heads \\(X\\) out of two coin tosses is\n\n\n\n\\(x\\)\n\\(p(x)\\)\n\n\n\n\n0\n1/4\n\n\n1\n1/2\n\n\n2\n1/4\n\n\n\nGiven the probability mass function we can, for example, calculate the probability of at least one head as \\(\\prob{X \\geq 1} = \\prob{X =0} + \\prob{X =1} = p(0)+p(1) = 3/4\\).\n\nThe Bernoulli distribution serves as the foundation for more complex distributions, such as the binomial distribution (which models the number of successes in a fixed number of independent Bernoulli trials) and the geometric distribution (which models the number of trials needed to achieve the first success). A Binomial distribution arises from a sequence of Bernoulli trials, and assigns probability to \\(X\\), which is the number of successes. It’s probability distribution is calculated via: \\[\n\\prob{X=x} = {n \\choose x} p^x(1-p)^{n-x}.\n\\] Here \\({n \\choose x}\\) is the combinatorial function, \\[\n{n \\choose x} = \\frac{n!}{x!(n-x)!},\n\\] where \\(n!=n(n-1)(n-2)\\ldots 2 \\cdot 1\\) counts the number of ways of getting \\(x\\) successes in \\(n\\) trials.\nTable below shows the expected value and variance of Binomial random variable.\n\nMean and Variance of Binomial\n\n\nBinomial Distribution\nParameters\n\n\n\n\nExpected value\n\\(\\mu = \\E{X} = n p\\)\n\n\nVariance\n\\(\\sigma^2 = \\Var{X} = n p ( 1 - p )\\)\n\n\n\nFor large sample sizes \\(n\\), this distribution is approximately normal with mean \\(np\\) and variance of \\(np(1-p)\\).\nSuppose we are about to toss two coins. Let \\(X\\) denote the number of heads. Then the following table specifies the probability distribution \\(p(x)\\) for all possible values \\(x\\) of \\(X\\). This leads to the following table\n\nOutcomes of three coin flips\n\n\n\\(x\\)\n\\(\\prob{X=x}\\)\n\n\n\n\n0\n1/4\n\n\n1\n1/2\n\n\n2\n1/4\n\n\n\nThus, most likely we will see one Head after two tosses. Now, let’s look at a more complex example and introduce our first probability distribution, namely Binomial distribution.\nLet \\(X\\) be the number of heads in three flips. Each possible outcome (“realization”) of \\(X\\) is an event. Now consider the event of getting only two heads \\[\n\\{ X= 2\\} = \\{ HHT, HTH, THH \\} ,\n\\] The probability distribution of \\(X\\) is Binomial with parameters \\(n = 3, p= 1/2\\), where \\(n\\) denotes the sample size (a.k.a. number of trials) and \\(p\\) is the probability of heads, we have a fair coin. The notation is \\(X \\sim \\mathrm{Bin} \\left ( n = 3 , p = \\frac{1}{2} \\right )\\) where the sign \\(\\sim\\) is read as distributed as.\n\nOutcomes of three coin flips\n\n\nResult\n\\(X\\)\n\\(\\prob{X=x}\\)\n\n\n\n\nHHH\n3\n\\(p^3\\)\n\n\nHHT\n2\n\\(p^2 ( 1- p)\\)\n\n\nHTH\n2\n\\(p^2 ( 1 - p)\\)\n\n\nTHH\n2\n\\((1-p)p^2\\)\n\n\nHTT\n1\n\\(p( 1-p)^2\\)\n\n\nTHT\n1\n\\(p ( 1-p)^2\\)\n\n\nTTH\n1\n\\((1-p)^2 p\\)\n\n\nTTT\n0\n\\((1-p)^3\\)\n\n\n\n\n1.5.1 Continuous Random Variables\nIf we want to build a probabilistic model of a stock price or return. We need to use a continuous random variable that can take an interval of values. Instead of frequency function we will use density function, \\(p(x)\\) to describe a continuous variable. Unlike the discrete case \\(p(x)\\) is not the probability that random variable takes value \\(x\\). Rather, we need to talk about value being inside an interval. For example probability of \\(X\\) with density \\(p(x)\\) being inside any interval \\([a,b]\\), with \\(a&lt;b\\) is given by \\[\nP(a &lt; X &lt; b) = \\int_{a}^{b}p(x)dx.\n\\] The total probability is one as \\(\\int_{-\\infty}^\\infty p(x) dx=1\\). The simplest continuous random variable is the uniform. A uniform distribution describes a variable which takes on any value as likely as any other. For example, if you are asked about what would be the temperature in Chicago on July 4 of next year, you might say anywhere between 20 and 30 C. The density function of the corresponding uniform distribution is then \\[\n  p(x) = \\begin{cases} 1, ~~~20 \\le x \\le 30\\\\0, ~~~\\mbox{otherwise}\\end{cases}\n\\]\nUnder, this model, then the probability of temperature being between 25 and 27 degrees is \\[\nP(25 \\le x \\le 27) = \\int_{25}^{27} p(x)dx = (27-25)/10 = 0.2\n\\]\n\n\n\nUniform Distribution: Probability of temperature being between 25 and 27\n\n\nThe Cumulative Distribution Function for a continuous random variable (X), denoted as (F_X(x)), is defined similarly to discrete RV CDF as \\[\nF(x) = P(X \\leq x)\n\\] Continuous RV CDF has the same properties as a discrete one (increasing and takes values in [0,1]).\n\nExample 1.12 (Continuous CDF for Uniform Distribution) \\[\n\\begin{cases}\n1 & \\text{if } 0 \\leq x \\leq 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nThe CDF, \\(F(x)\\), is obtained by integrating the PDF: - For \\(x &lt; 0\\), \\(F(x) = 0\\). - For \\(0 \\leq x \\leq 1\\), \\(F(x) = \\int_0^x 1 \\, dt = x\\). - For \\(x &gt; 1\\), \\(F(x) = 1\\). So, the CDF of this uniform distribution is a linear function that increases from 0 to 1 as \\(x\\) goes from 0 to 1.\nGraphically, the CDF of a continuous random variable is a smooth curve. It starts at 0, increases as \\(x\\) increases, and eventually reaches 1. The exact shape of the curve depends on the distribution of the variable, but the smooth, non-decreasing nature is a common feature.\n\nplot(ecdf(runif(500)), main=\"\", col=\"lightblue\", pch=21, bg=\"grey\")\n\n\n\n\nCDF of a uniform random variable\n\n\n\n\nWhat about CDF of a normal distribution?\n\nplot(ecdf(rnorm(500)), main=\"\", col=\"lightblue\", pch=21, bg=\"grey\")\n\n\n\n\nCDF of a normal random variable\n\n\n\n\n\n\n\n1.5.2 The Inverse CDF Method\nThe inverse distribution method uses samples of uniform random variables to generate draws from random variables with a continuous distribution function, \\(F\\). Since \\(F\\left(  x\\right)\\) is uniformly distributed on \\(\\left[ 0,1\\right]\\), draw a uniform random variable and invert the CDF to get a draw from \\(F\\). Thus, to sample from \\(F\\), \\[\\begin{align*}\n&  \\text{Step 1}\\text{: Draw }U\\sim U\\left[  0,1\\right]  \\ \\\\\n&  \\text{Step 2}\\text{: }\\text{Set }X=F^{-1}\\left(  U\\right)  ,\n\\end{align*}\\] where \\(F^{-1}\\left(  U\\right)  =\\inf\\left\\{  x:F\\left(  x\\right)  =U\\right\\}\\).\nThis inversion method provides i.i.d. draws from \\(F\\) provided that \\(F^{-1}\\left(  U\\right)\\) can be exactly calculated. For example, the CDF of an exponential random variable with parameter \\(\\mu\\) is \\(F\\left(  x\\right) =1-\\exp\\left(  -\\mu x\\right)\\), which can easily be inverted. When \\(F^{-1}\\) cannot be analytically calculated, approximate inversions can be used. For example, suppose that the density is a known analytical function. Then, \\(F\\left(  x\\right)\\) can be computed to an arbitrary degree of accuracy on a grid and inversions can be approximately calculated, generating an approximate draw from \\(F\\). With all approximations, there is a natural trade-off between computational speed and accuracy. One example where efficient approximations are possible are inversions involving normal distributions, which is useful for generating truncated normal random variables. Outside of these limited cases, the inverse transform method does not provide a computationally attractive approach for drawing random variables from a given distribution function. In particular, it does not work well in multiple dimensions.\n\n\n1.5.3 Functional Transformations\nThe second main method uses functional transformations to express the distribution of a random variable that is a known function of another random variable. Suppose that \\(X\\sim F\\), admitting a density \\(f\\), and that \\(y=h\\left(  x\\right)\\) is an increasing continuous function. Thus, we can define \\(x=h^{-1}\\left(  y\\right)\\) as the inverse of the function \\(h\\). The distribution of \\(y\\) is given by \\[\nF_Y\\left(y\\right)  =\\text{P}\\left(  Y\\leq y\\right)  =\\int_{-\\infty}^{h^{-1}\\left(  y\\right)  }f\\left(  x\\right)  dx=F_X\\left(  X\\leq h^{-1}\\left(y\\right)  \\right).\n\\] Differentiating with respect to \\(y\\) gives the density via Leibnitz’s rule: \\[\nf_{Y}\\left(  y\\right)  =f\\left(  h^{-1}\\left(  y\\right)  \\right)  \\left\\vert\\frac{d}{dy}\\left(  h^{-1}\\left(  y\\right)  \\right)  \\right\\vert,\n\\] where we make explicit that the density is over the random variable \\(Y\\). This result is used widely. For example, if \\(X\\sim\\mathcal{N}\\left(  0,1\\right)\\), then \\(Y=\\mu+\\sigma X\\). Since \\(x=h^{-1}\\left(  y\\right)  =\\frac{y-\\mu}{\\sigma}\\), the distribution function is \\(F\\left(  \\frac{x-\\mu}{\\sigma}\\right)\\) and density \\[\nf_{Y}\\left(  y\\right)  =\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(  -\\frac{1}{2}\\left(  \\frac{y-\\mu}{\\sigma}\\right)  ^{2}\\right).\n\\] Transformations are widely used to simulate both univariate and multivariate random variables. As examples, if \\(Y\\sim\\mathcal{X}^{2}\\left(  \\nu\\right)\\) and \\(\\nu\\) is an integer, then \\(Y=\\sum_{i=1}^{\\nu}X_{i}^{2}\\) where each \\(X_{i}\\) is independent standard normal. Exponential random variables can be used to simulate \\(\\mathcal{X}^{2}\\), Gamma, Beta, and Poisson random variables. The famous Box-Muller algorithm simulates normals from uniform and exponential random variables. In the multivariate setting, Wishart (and inverse Wishart) random variables can be via sums of squared vectors of standard normal random variables.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#conditional-marginal-and-joint-distributions",
    "href": "01-prob.html#conditional-marginal-and-joint-distributions",
    "title": "1  Probability and Uncertainty",
    "section": "1.6 Conditional, Marginal and Joint Distributions",
    "text": "1.6 Conditional, Marginal and Joint Distributions\nSuppose that we have two random variables \\(X\\) and \\(Y\\), which can be related to each other. Knowing \\(X\\) would change your news about \\(Y\\). For example, as a first pass, psychologists who study phenomenon of happiness can be interested in understanding it relation to income level. Now we need a single probability mass function (a.k.a. probabilistic model) that describes all possible values of those two variables. Joint distributions do exactly that.\nFormally, the joint distribution of two variable \\(X\\) and \\(Y\\) is a function given by \\[\np(x,y) = \\prob{X=x,Y=y}.\n\\] This maps all combinations of possible values of these two variables to a probability on the interval [0,1].\nThe conditional probability is a measure of the probability of an random variable \\(X\\), given that value of another random variable was observed \\(Y = y\\). \\[\np(x\\mid y) = \\prob{X = x \\mid Y = y}.\n\\]\nThe marginal probability of a subset of a collection of random variables is the probability distribution of the variables contained in the subset without reference to the values of the other variables. Say we have two random variables \\(X\\) and \\(Y\\), the marginal probability \\(\\prob{X}\\) is the probability distribution of \\(X\\) when the values of \\(Y\\) are not taken into consideration. This can be calculated by summing the joint probability distribution over all values of \\(Y\\). The converse is also true: the marginal distribution can be obtained for \\(Y\\) by summing over the separate values of \\(X\\).\nMarginal probability is different from conditional probability. Marginal probability is the probability of a single event occurring, independent of other events. A conditional probability, on the other hand, is the probability that an event occurs given that another specific event has already occurred.\n\nExample 1.13 (Salary-Happyness) Let’s look at an example. Suppose that to model relationship between two quantities, salary \\(Y\\) and happiness \\(X\\). After running a survey, we summarize our results using the joint distribution, that is described by the following “happiness index” table as a function of salary.\n\n\n\nResults of the Gallop survey. Rows are Salary (\\(Y\\)) and columns are happiness (\\(X\\))\n\n\n\nX = 0 (low)\nX = 1 (medium)\nX = 2 (high)\n\n\n\n\nY = low (0)\n0.03\n0.13\n0.14\n\n\nY = medium (1)\n0.12\n0.11\n0.01\n\n\nY = high (2)\n0.07\n0.01\n0.09\n\n\nY = very high (3)\n0.02\n0.13\n0.14\n\n\n\n\n\nEach cell of the table is the joint probability, e.g. 14% of people have very high income level and are very happy. Those joint probabilities are calculated by simple counting and calculating the proportions.\nNow, if we want to answer the question what is the percent of high incomers in the population. For that we need to calculate what is called a marginal probability \\(\\prob{y = 2}\\). We can calculate the proportion of high incomers \\(\\prob{y = 2}\\) by summing up the entries in the third row of the table, which is 0.17 in our case.\n\n0.07 + 0.01 + 0.09\n\n## [1] 0.17\n\n\nFormally marginal probability over \\(y\\) is calculated by summing the joint probability over the other variable, \\(x\\), \\[\np(y) = \\sum_{x \\in S}p(x,y)\n\\] Where \\(S\\) is a set of all possible values of the random variable \\(X\\).\n\n\n\n\n\n\nAnother, question of interest is whether happiness depends on income level. To answer those types of questions, we need to introduce an important concept, which is the conditional probability of \\(X\\) given that value of variable \\(Y\\) is known. This is denoted by \\(\\prob{X=x\\mid Y=y}\\) or simply \\(p(x\\mid y)\\), where \\(\\mid\\) reads as “given” or “conditional upon”.\nThe conditional probability \\(p(x\\mid y)\\) also has interpretation as updating your probability over \\(X\\) after you have learned the new information about \\(Y\\). In this sense, probability is also the language of how you change opinions in light of new evidence. Proportion of happy people among high incomers is given by the conditional probability \\(\\prob{X=2\\mid Y=2}\\) and can be calculated by dividing proportion of those who are high incomer and highly happy by the proportion of the high incomers \\[\n\\prob{X=2\\mid Y=2} = \\dfrac{\\prob{X=2,Y=2}}{\\prob{Y=2}} = \\dfrac{0.09}{0.17} = 0.5294118.\n\\]\nNow, if we compare it with the proportion of highly happy people \\(\\prob{X = 2} = 0.38\\), we see that on average you are more likely to be happy given your income is high.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#independence",
    "href": "01-prob.html#independence",
    "title": "1  Probability and Uncertainty",
    "section": "1.7 Independence",
    "text": "1.7 Independence\nHistorically, the concept of independence in experiments and random variables has been a defining mathematical characteristic that has uniquely shaped the theory of probability. This concept has been instrumental in distinguishing the theory of probability from other mathematical theories.\nUsing the notion of conditional probability, we can define independence of two variables. Two random variable \\(X\\) and \\(Y\\) are said to be independent if \\[\n\\prob{Y = y \\mid X = x} = \\prob{Y = y},\n\\] for all possible \\(x\\) and \\(y\\) values. That is, learning information \\(X=x\\) doesn’t affect.\nConditional probabilities are counter intuitive. For example, one of the most important properties is typically \\(p( x \\mid y ) \\neq p( y\\mid x )\\), our probabilistic assessment of \\(Y\\) for any value \\(y\\). This is known as Prosecutors’ Fallacy as it arises when probability is used as evidence in a court of law. In the case of independence, \\(p(x \\mid y) = p(x)\\) and \\(p(y \\mid x) = p(y)\\). Specifically, the probability of innocence given the evidence is not the same as the probability of evidence given innocence. It is very important to ask the question “what exactly are we conditioning on?” Usually, the observed evidence or data. Probability, of course, given evidence was one of the first applications of Bayes. Central to personalized probability. Clearly this is a strong condition and rarely holds in practice.\nWe just derived an important relation, that allows us to calculate conditional probability \\(p(x \\mid y)\\) when we know joint probability \\(p(x,y)\\) and marginal probability \\(p(y)\\). The total probability or evidence can be calculated as usual, via \\(p(y) = \\sum_{x}p(x,y)\\).\nWe will see that independence will lead to a different conclusion that the Bayes conditional probability decomposition: specifically, independence yields \\(p( x,y ) = p(x) p(y)\\) and Bayes says \\(p(x ,y) = p(x)p(x \\mid y)\\).\nWe need to specify distribution on each of those variables. Two random variable \\(X\\) and \\(Y\\) are independent if \\[\n\\prob{Y = y \\mid X = x} = \\prob{Y = y},\n\\] for all possible \\(x\\) and \\(y\\) values variables separately The joint distribution will be giving by \\[\np(x,y) = p(x)p(y).\n\\] If \\(X\\) and \\(Y\\) are independent then probability of the event \\(X\\) and event \\(Y\\) happening at the same time is the product of individual probabilities. From the conditional distribution formula it follows that \\[\np(x \\mid y) = \\dfrac{p(x,y)}{p(y)} = \\dfrac{p(x)p(y)}{p(y)} = p(x).\n\\] Another way to think of independence is to say that knowing the value of \\(Y\\) doesn’t tell us anything about possible values of \\(X\\). For example when tossing a coin twice, the probability of getting \\(H\\) in the second toss does not depend on the outcome of the first toss.\nThe expression of independence expresses the fact that knowing \\(X=x\\) tells you nothing about \\(Y\\). In the coin tossing example, if \\(X\\) is the outcome of the first toss and \\(Y\\) is the outcome of the second toss \\[\n\\prob{ X=H  \\mid  Y=T } = \\prob{X=H  \\mid  Y=H } = \\prob{X=H}.\n\\]\nLet’s do a similar example which illustrates this point clearly. Most people would agree with the following conditional probability assessments\n\n\n\n\nKeynes, John Maynard. 1921. A Treatise on Probability. Macmillan.\n\n\nKreps, David. 1988. Notes On The Theory Of Choice. Boulder: Westview Press.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "02-bayes.html",
    "href": "02-bayes.html",
    "title": "2  Bayes Rule",
    "section": "",
    "text": "2.1 Law of Total Probability\nOne of the key questions in the theory of learning is: How do you update your beliefs in the presence of new information? Bayes rule provides the answer. Conditional probability can be interpreted as updating your probability of event \\(A\\) after you have learned the new information that \\(B\\) has occurred. In the sense probability is also the language of how you’ll change options in the light of new evidence. For example, we need to find probability that a thrown dice shows on its upper surface an odd number and we found out that number shown is less then 4. We write \\(p(A=\\text{odd} \\mid B = \\text{less then 4})= 2/3\\).\nProbability rules allow us to change our mind if the facts change. For example, suppose that \\(B = \\{ B_1 , B_2 \\}\\) consists of two pieces of information and that we are interested in \\(P(A\\mid B_1,B_2)\\). Bayes rule simply lets you calculate this conditional probability in a sequential fashion. First, conditioning on the information contained in \\(B_1\\), let’s us calculate \\[\nP( A| B_1 ) = \\frac{ p(  B_1 \\mid A ) P( A) }{ P( B_1 ) }\n\\] Then, using the posterior probability \\(P( A| B_1 )\\) as the “new” prior for the next piece of information \\(B_2\\) lets us find \\[\nP( A| B_1 , B_2 ) = \\frac{ p(  B_2 \\mid B_1 , A ) P( A \\mid B_1 ) }{ P( B_2 \\mid B_1 ) }\n\\] Hence, we see that we need assessments of the two conditional probabilities \\(P( B_1 \\mid A )\\) and \\(P( B_2 \\mid B_1 , A )\\). In many situations, the latter will be simply \\(P( B_2 \\mid A )\\) and not involve \\(B_1\\). The events \\(( B_1, B_2 )\\) will be said to be conditionally independent given \\(A\\).\nThis concept generalizes to a sequence of events where \\(B = \\{ B_1,\\ldots B_n \\}\\). When learning from data will will use this property all the time. An illustrative example will be the Black Swan problem which we discuss later.\nBayes’ rule is a fundamental concept in probability theory and statistics. It describes how to update our beliefs about an event based on new evidence. We start with an initial belief about the probability of an event (called the prior probability). We then observe some conditional information information (e.g. evidence). We use Bayes’ rule to update our initial belief based on the evidence, resulting in a new belief called the posterior probability. Remember, the formula is \\[\nP(A\\mid B) = \\dfrac{P(B\\mid A) P(A)}{P(B)}\n\\] where:\nThe ability to use Bayes rule sequentially is a key in many applications, when we need to update our beliefs in the presence of new information. For examples, Bayesian learning was used by mathematician Alan Turing in England in Bletchley Park to break the German Enigma code - a development that helped the Allies win the Second World War (Simpson 2010). Turing called his algorithm Banburismus, it is a process he invented which used sequential conditional probability to infer information about the likely settings of the Enigma machine.\nDennis Lindley argued that we should all be trained Bayes rule and conditional probability can be simply view as disciplined probability accounting. Akin to how market odds change as evidence changes. One issue is human behavior and intuition trained in how to use Bayes rule, akin to learning the Alphabet!\nThe Law of Total Probability is a fundamental rule relating marginal probabilities to conditional probabilities. It’s particularly useful when you’re dealing with a set of mutually exclusive and collectively exhaustive events.\nSuppose you have a set of events \\(B_1, B_2, ..., B_n\\) that are mutually exclusive (i.e., no two events can occur at the same time) and collectively exhaustive (i.e., at least one of the events must occur). The Law of Total Probability states that for any other event \\(A\\), the probability of \\(A\\) occurring can be calculated as the sum of the probabilities of \\(A\\) occurring given each \\(B_i\\), multiplied by the probability of each \\(B_i\\) occurring.\nMathematically, it is expressed as:\n\\[\nP(A) = \\sum_{i=1}^{n} P(A\\mid  B_i) P(B_i)\n\\]\nThis law is particularly useful in complex probability problems where direct calculation of probability is difficult. By breaking down the problem into conditional probabilities based on relevant events, it simplifies the calculation and helps to derive a solution.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#law-of-total-probability",
    "href": "02-bayes.html#law-of-total-probability",
    "title": "2  Bayes Rule",
    "section": "",
    "text": "Example 2.2 (Total Probability) Let’s consider a simple example to illustrate this. Suppose you have two bags of balls. Bag 1 contains 3 red and 7 blue balls, while Bag 2 contains 6 red and 4 blue balls. You randomly choose one of the bags and then randomly draw a ball from that bag. What is the probability of drawing a red ball?\nHere, the events \\(B_1\\) and \\(B_2\\) can be choosing Bag 1 and Bag 2, respectively. You want to find the probability of event \\(A\\) (drawing a red ball).\nApplying the law:\n\n\\(P(A|B_1)\\) is the probability of drawing a red ball from Bag 1, which is \\(\\frac{3}{10}\\).\n\\(P(A|B_2)\\) is the probability of drawing a red ball from Bag 2, which is \\(\\frac{6}{10}\\).\nAssume the probability of choosing either bag is equal, so \\(P(B_1) = P(B_2) = \\frac{1}{2}\\).\n\nUsing the Law of Total Probability:\n\\[\nP(A) = P(A|B_1) \\times P(B_1) + P(A|B_2) \\times P(B_2)= \\frac{3}{10} \\times \\frac{1}{2} + \\frac{6}{10} \\times \\frac{1}{2} = \\frac{9}{20}\n\\]\nSo, the probability of drawing a red ball in this scenario is \\(\\frac{9}{20}\\).\n\n\n\nExample 2.3 (Craps) Craps is a fast-moving dice game with a complex betting layout. It’s highly volatile, but eventually your bankroll will drift towards zero. Lets look at the pass line bet. The expectation \\(E(X)\\) governs the long run. When 7 or 11 comes up, you win. When 2,3 or 12 comes up, this is known as “craps”, you lose. When 4,5,6,8,9 or 10 comes up, this number is called the “point”, the bettor continues to roll until a 7 (you lose) or the point comes up (you win).\nWe need to know the probability of winning. The pay-out, probability and expectation for a $1 bet\n\n\n\nWin\nProb\n\n\n\n\n1\n0.4929\n\n\n-1\n0.5071\n\n\n\nThis leads to an edge in favor of the house as \\[\nE(X) = 1 \\cdot 0.4929 + (- 1) \\cdot  0.5071 = -0.014\n\\] The house has a 1.4% edge.\nTo calculate the probability of winning: \\(P( \\text{Win} )\\) let’s use the law of total probability \\[\nP( \\text{Win} ) = \\sum_{ \\mathrm{Point} } P ( \\text{Win} \\mid \\mathrm{Point} ) P ( \\mathrm{Point} )\n\\] The set of \\(P( \\mathrm{Point} )\\) are given by\n\n\n\nValue\nProbability\nPercentage\n\n\n\n\n2\n1/36\n2.78%\n\n\n3\n2/36\n5.56%\n\n\n4\n3/36\n8.33%\n\n\n5\n4/36\n11.1%\n\n\n6\n5/36\n13.9%\n\n\n7\n6/36\n16.7%\n\n\n8\n5/36\n13.9%\n\n\n9\n4/36\n11.1%\n\n\n10\n3/36\n8.33%\n\n\n11\n2/36\n5.56%\n\n\n12\n1/36\n2.78%\n\n\n\nThe conditional probabilities \\(P( \\text{Win} \\mid \\mathrm{Point} )\\) are harder to calculate \\[\nP( \\text{Win} \\mid 7 \\; \\mathrm{or} \\; 11 ) = 1 \\; \\; \\mathrm{and} \\; \\; P( \\text{Win} \\mid 2 ,\n3 \\; \\mathrm{or} \\; 12 ) = 0\n\\] We still have to work out all the probabilities of winning given the point. Suppose the point is \\(4\\) \\[\nP( \\text{Win} \\mid 4 ) = P ( 4 \\; \\mathrm{before} \\; 7 ) = \\dfrac{P(4)}{P(7)+P(4)} = \\frac{3}{9} =\n\\frac{1}{3}\n\\] There are 6 ways of getting a 7, 3 ways of getting a 4 for a total of 9 possibilities. Now do all of them and sum them up. You get \\[\nP( \\text{Win}) = 0.4929\n\\]",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#naive-bayes",
    "href": "02-bayes.html#naive-bayes",
    "title": "2  Bayes Rule",
    "section": "2.2 Naive Bayes",
    "text": "2.2 Naive Bayes\nUse of the Bayes rule allows us to build our first predictive model, called Naive Bayes classifier. Naive Bayes is a collection of classification algorithms based on Bayes Theorem. It is not a single algorithm but a family of algorithms that all share a common principle, that every feature being classified is independent of the value of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3” in diameter. A Naive Bayes classifier considers each of these “features” (red, round, 3” in diameter) to contribute independently to the probability that the fruit is an apple, regardless of any correlations between features. Features, however, aren’t always independent which is often seen as a shortcoming of the Naive Bayes algorithm and this is why it’s labeled “naive”.\n\n\n\n\n\n\nAlthough it’s a relatively simple idea, Naive Bayes can often outperform other more sophisticated algorithms and is extremely useful in common applications like spam detection and document classification. In a nutshell, the algorithm allows us to predict a class, given a set of features using probability. So in another fruit example, we could predict whether a fruit is an apple, orange or banana (class) based on its colour, shape etc (features). In summary, the advantages are:\n\nIt’s relatively simple to understand and build\nIt’s easily trained, even with a small dataset\nIt’s fast!\nIt’s not sensitive to irrelevant features\n\nThe main disadvantage is that it assumes every feature is independent, which isn’t always the case.\nLet’s say we have data on 1000 pieces of fruit. The fruit being a Banana, Orange or some Other fruit and imagine we know 3 features of each fruit, whether it’s long or not, sweet or not and yellow or not, as displayed in the table below:\n\n\n\nFruit\nLong\nSweet\nYellow\nTotal\n\n\n\n\nBanana\n400\n350\n450\n500\n\n\nOrange\n0\n150\n300\n300\n\n\nOther\n100\n150\n50\n200\n\n\nTotal\n500\n650\n800\n1000\n\n\n\nFrom this data we can calculate marginal probabilities\n\n50% of the fruits are bananas\n30% are oranges\n20% are other fruits\n\nBased on our training set we can also say the following:\n\nFrom 500 bananas 400 (0.8) are Long, 350 (0.7) are Sweet and 450 (0.9) are Yellow\nOut of 300 oranges 0 are Long, 150 (0.5) are Sweet and 300 (1) are Yellow\nFrom the remaining 200 fruits, 100 (0.5) are Long, 150 (0.75) are Sweet and 50 (0.25) are Yellow So let’s say we’re given the features of a piece of fruit and we need to predict the class. If we’re told that the additional fruit is Long, Sweet and Yellow, we can classify it using the following formula and subbing in the values for each outcome, whether it’s a Banana, an Orange or Other Fruit. The one with the highest probability (score) being the winner.\n\nGive the evidence \\(E\\) (\\(L\\) = Long, \\(S\\) = Sweet and \\(Y\\) = Yellow) we can calculate the probability of each class \\(C\\) (\\(B\\) = Banana, \\(O\\) = Orange or \\(F\\) = Other Fruit) using Bayes’ Theorem: \\[\\begin{align*}\nP(B \\mid E) = & \\frac{P(L \\mid B)P(S \\mid B)P(Y \\mid B)P(B)}{P(L)P(S)P(Y)}\\\\\n=&\\frac{0.8\\times 0.7\\times 0.9\\times 0.5}{P(E)}=\\frac{0.252}{P(E)}\n\\end{align*}\\]\nOrange: \\[\nP(O\\mid E)=0.\n\\]\nOther Fruit: \\[\\begin{align*}\nP(F \\mid E) & = \\frac{P(L \\mid F)P(S \\mid F)P(Y \\mid F)P(F)}{P(L)P(S)P(Y)}\\\\\n=&\\frac{0.5\\times 0.75\\times 0.25\\times 0.2}{P(E)}=\\frac{0.01875}{P(E)}\n\\end{align*}\\]\nIn this case, based on the higher score, we can assume this Long, Sweet and Yellow fruit is, in fact, a Banana.\nNow that we’ve seen a basic example of Naive Bayes in action, you can easily see how it can be applied to Text Classification problems such as spam detection, sentiment analysis and categorization. By looking at documents as a set of words, which would represent features, and labels (e.g. “spam” and “ham” in case of spam detection) as classes we can start to classify documents and text automatically.\n\nExample 2.4 (Spam Filtering) Original spam filtering algorithm was based on Naive Bayes.The “naive” aspect of Naive Bayes comes from the assumption that inputs (words in the case of text classification) are conditionally independent, given the class label. Naive Bayes treats each word independently, and the model doesn’t capture the sequential or structural information inherent in the language. It does not consider grammatical relationships or syntactic structures. The algorithm doesn’t understand the grammatical rules that dictate how words should be combined to form meaningful sentences. Further, it doesn’t understand the context in which words appear. For example, it may treat the word “bank” the same whether it refers to a financial institution or the side of a river bank. Despite its simplicity and the naive assumption, Naive Bayes often performs well in practice, especially in text classification tasks.\nWe start by collecting a dataset of emails labeled as “spam” or “not spam” (ham) and calculate the prior probabilities of spam (\\(P(\\text{spam})\\)) and not spam (\\(P(\\text{ham})\\)) based on the training dataset, by simply counting the proportions of each in the data.\nThen each email gets converted into a bag-of-words representation (ignoring word order and considering only word frequencies). Then, we create a vocabulary of unique words from the entire dataset \\(w_1,w_2,\\ldots,w_N\\) and calculate conditional probabilities \\[\nP(\\mathrm{word}_i  \\mid  \\text{spam}) = \\frac{\\text{Number of spam emails containing }\\mathrm{word}_i}{\\text{Total number of spam emails}}, ~ i=1,\\ldots,n\n\\] \\[\nP(\\mathrm{word}_i  \\mid  \\text{ham}) = \\frac{\\text{Number of ham emails containing }\\mathrm{word}_i}{\\text{Total number of ham emails}}, ~ i=1,\\ldots,n\n\\]\nNow, we are ready to use our model to classify new emails. We do it by calculating the posterior probability using Bayes’ theorem. Say email has a set of \\(k\\) words \\(\\text{email} = \\{w_{e1},w_{e2},\\ldots, w_{ek}\\}\\), then \\[\nP(\\text{spam}  \\mid  \\text{email}) = \\frac{P(\\text{email}  \\mid  \\text{spam}) \\times P(\\text{spam})}{P(\\text{email})}\n\\] Here \\[\nP(\\text{email}  \\mid  \\text{spam}) = P( w_{e1}  \\mid  \\text{spam})P( w_{e2}  \\mid  \\text{spam})\\ldots P( w_{ek}  \\mid  \\text{spam})\n\\] We calculate \\(P(\\text{spam} \\mid \\text{email})\\) is a similar way.\nFinally, we classify the email as spam or ham based on the class with the highest posterior probability.\nSuppose you have a spam email with the word “discount” appearing. Using Naive Bayes, you’d calculate the probability that an email containing “discount” is spam (\\(P(\\text{spam} \\mid \\text{discount})\\)) and ham (\\(P(\\text{ham} \\mid \\text{discount})\\)), and then compare these probabilities to make a classification decision.\nWhile the naive assumption simplifies the model and makes it computationally efficient, it comes at the cost of a more nuanced understanding of language. More sophisticated models, such as transformers, have been developed to address these limitations by considering the sequential nature of language and capturing contextual relationships between words.\nIn summary, naive Bayes, due to its simplicity and the naive assumption of independence, is not capable of understanding the rules of grammar, the order of words, or the intricate context in which words are used. It is a basic algorithm suitable for certain tasks but may lack the complexity needed for tasks that require a deeper understanding of language structure and semantics.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#real-world-bayes",
    "href": "02-bayes.html#real-world-bayes",
    "title": "2  Bayes Rule",
    "section": "2.3 Real World Bayes",
    "text": "2.3 Real World Bayes\n\nExample 2.5 (Google random clicker) Google provides a service where they ask visitors to your website ato answer a single survey question before they get access to the content on the page. Among all of the users, there are two categories\n\nRandom Clicker (RC)\nTruthful Clicker (TC)\n\nThere are two possible answers to the survey: yes and no. Random clickers would click either one with equal probability. You are also giving the infor- mation that the expected fraction of random clickers is 0.3. After a trial period, you get the following survey results: 65% said Yes and 35% said No.\nThe qustion is How many people people who are truthful clickers answered yes \\(P(Y\\mid TC)\\)?\nWe are given \\(P(Y\\mid RC) = P(N\\mid RC) = 0.5\\), \\(P(RC)= 0.3\\) and \\(P(Y)\\) = 0.65\nThe totla probability is \\[\nP(Y) = P(Y\\mid RC)P(RC) + P(Y\\mid TC)P(TC) = 0.65,\n\\] Thus \\[\nP(Y\\mid TC) = (P(Y) - P(Y\\mid RC)P(RC))/P(TC) = (0.65-0.5\\cdot 0.3)/0.7 = 0.71\n\\]\n\n\nExample 2.6 (USS Scorpion sank 5 June, 1968 in the middle of the Atlantic.) Experts placed bets of each casualty and how each would affect the sinking. Undersea soundings gave a prior on location. Bayes rule: \\(L\\) is location and \\(S\\) is scenario \\[\np (L \\mid S) = \\frac{ p(S \\mid L) p(L)}{p(S)}\n\\] The Navy spent \\(5\\) months looking and found nothing. Build a probability map: within \\(5\\) days, the submarine was found within \\(220\\) yards of most likely probability!\nA similar story happened during the search of an Air France plane that flew from Rio to Paris.\n\n\n\nExample 2.7 (Wald and Airplane Safety) Many lives were saved by analysis of conditional probabilities performed by Abraham Wald during the Second World War. He was analyzing damages on the US planes that came back from bombarding missions in Germany. Somebody suggested to analyze distribution of the hits over different parts of the plane. The idea was to find a pattern in the damages and design a reinforcement strategy.\nAfter examining hundreds of damaged airplanes, researchers came up with the following table\n\n\n\nLocation\nNumber of Planes\n\n\n\n\nEngine\n53\n\n\nCockpit\n65\n\n\nFuel system\n96\n\n\nWings, fuselage, etc.\n434\n\n\n\nWe can convert those counts to probabilities\n\n\n\nLocation\nNumber of Planes\n\n\n\n\nEngine\n0.08\n\n\nCockpit\n0.1\n\n\nFuel system\n0.15\n\n\nWings, fuselage, etc.\n0.67\n\n\n\nWe can conclude the the most likely area to be damaged on the returned planes was the wings and fuselage. \\[\n\\prob{\\mbox{hit on wings or fuselage } \\mid \\mbox{returns safely}} = 0.67\n\\] Wald realized that analyzing damages only on survived planes is not the right approach. Instead, he suggested that it is essential to calculate the inverse probability \\[\n\\prob{\\mbox{returns safely} \\mid \\mbox{hit on wings or fuselage }} = ?\n\\] To calculate that, he interviewed many engineers and pilots, he performed a lot field experiments. He analyzed likely attack angles. He studied the properties of a shrapnel cloud from a flak gun. He suggested to the army that they fire thousands of dummy bullets at a plane sitting on the tarmac. Wald constructed a ‘probability model’ careful to reconstruct an estimate for the joint probabilities. Table below shows the results.\n\n\n\nHit\nReturned\nShut Down\n\n\n\n\nEngine\n53\n57\n\n\nCockpit\n65\n46\n\n\nFuel system\n96\n16\n\n\nWings, fuselage, etc.\n434\n33\n\n\n\nWhich allows us to estimate joint probabilities, for example \\[\n\\prob{\\mbox{outcome = returns safely} , \\mbox{hit  =  engine }} = 53/800 = 0.066\n\\] We also can calculate the conditional probabilities now \\[\n\\prob{\\mbox{outcome = returns safely} \\mid  \\mbox{hit  =  wings or fuselage  }} = \\dfrac{434}{434+33} = 0.93.\n\\] Should we reinforce wings or fuselage? Which part of the airplane does need ot be reinforced? \\[\n\\prob{\\mbox{outcome = returns safely} \\mid  \\mbox{hit  =  engine  }} = \\dfrac{53}{53+57} = 0.48\n\\] Here is another illustration taken from Economics literature. This insight led to George Akerlof winning the Nobel Prize for the concept of asymmetric information.\n\n\nExample 2.8 (Coin Jar) Large jar containing 1024 fair coins and one two-headed coin. You pick one at random and flip it \\(10\\) times and get all heads. What’s the probability that the coin is the two-headed coin? The probability of initially picking the two headed coin is 1/1025. There is 1/1024 chance of of getting \\(10\\) heads in a row from a fair coin. Therefore, it’s a \\(50/50\\) bet.\nLet’s do the formal Bayes rule math. Let \\(E\\) be the event that you get \\(10\\) Heads in a row, then\n\\[\nP \\left ( \\mathrm{two \\; headed}  \\mid  E \\right ) = \\frac{ P \\left ( E  \\mid  \\mathrm{ two \\; headed}  \\right )P \\left (  \\mathrm{ two \\; headed} \\right )}\n{P \\left ( E  \\mid  \\mathrm{ fair}  \\right )P \\left ( \\mathrm{ fair} \\right ) + P \\left ( E  \\mid  \\mathrm{ two \\; headed}  \\right )P \\left ( \\mathrm{ two \\; headed} \\right )}\n\\] Therefore, the posterior probability \\[\nP \\left (  \\mathrm{ two \\; headed}  \\mid  E \\right ) = \\frac{ 1 \\times \\frac{1}{1025} }{ \\frac{1}{1024} \\times \\frac{1024}{1025} + 1 \\times \\frac{1}{1025} } = 0.50\n\\] What’s the probability that the next toss is a head? Using the law of total probability gives\n\\[\\begin{align*}\n  P( H ) &= P( H  \\mid  \\mathrm{ two \\; headed} )P( \\mathrm{ two \\; headed}  \\mid E ) +  P( H  \\mid  \\mathrm{ fair} )P( \\mathrm{ fair}  \\mid E) \\\\\n  & = 1 \\times \\frac{1}{2} + \\frac{1}{2} \\times \\frac{1}{2} = \\frac{3}{4}\n\\end{align*}\\]\n\n\nExample 2.9 (Monty Hall Problem) Another example of a situation when calculating probabilities is counterintuitive. The Monte Hall problems was named after the host of the long-running TV show Let’s make a Deal. The original solution was proposed by Marilyn vos Savant, who had a column with the correct answer that many Mathematicians thought was wrong!\nThe game set-up is as follows. A contestant is given the choice of 3 doors. There is a prize (a car, say) behind one of the doors and something worthless behind the other two doors: two goats. The game is as follows:\n\nYou pick a door.\nMonty then opens one of the other two doors, revealing a goat. He can’t open your door or show you a car\nYou have the choice of switching doors.\n\nThe question is, is it advantageous to switch? The answer is yes. The probability of winning if you switch is 2/3 and if you don’t switch is 1/3.\nConditional probabilities allow us to answer this question. Assume you pick door 2 (event \\(A\\)) at random, given that the host opened Door 3 and showed a goat (event B), we need to calculate \\(P(A\\mid B)\\). The prior probability that the car is behind Door 2 is \\(P(A) =  1/3\\) and \\(P(B\\mid A) = 1\\), if the car is behind Door 2, the host has no choice but to open Door 3. The Bayes rule then gives us \\[\nP(A\\mid B) = \\frac{P(B\\mid A)P(A)}{P(B)} = \\frac{1/3}{1/2} = \\frac{2}{3}.\n\\] The overall probability of the host opening Door 3 \\[\nP(B) = (1/3 \\times 1/2) + (1/3 \\times 1) = 1/6 + 1/3 = 1/2.\n\\]\nThe posterior probability that the car is behind Door 2 after the host opens Door 3 is 2/3. It is to your advantage to switch doors.\n\n\nExample 2.10 (Prosecutors Fallacy) The Prosecutor’s Fallacy is a logical error that occurs when a prosecutor presents evidence or statistical data in a way that suggests a defendant’s guilt, even though the evidence is not as conclusive as it may seem. This fallacy arises from a misunderstanding or misrepresentation of conditional probabilities and not understanding that that \\[\nP(E\\mid G) \\ne P(G\\mid E)\n\\]\nIt involves confusion between the probability of two events: the probability of the evidence \\(E\\), given the defendant’s guilt (which is what the prosecutor may be presenting), and the probability of the defendant’s guilt \\(G\\), given the evidence (which is what is often of more interest in a trial).\nHere’s a simplified example to illustrate the Prosecutor’s Fallacy. Suppose a crime has been committed, and DNA evidence is found at the crime scene. The prosecutor claims that the probability of finding this particular DNA at the scene, given the defendant’s innocence, is very low (making the evidence seem incriminating). However, the Prosecutor’s Fallacy occurs when the prosecutor incorrectly assumes that this low probability implies a low probability of the defendant’s innocence. In reality, the probability of the DNA being found at the crime scene (given the defendant’s innocence) might also be low if the DNA is relatively rare but not exclusive to the defendant.\nThe fallacy often arises from a failure to consider the base rate or prior probability of the event being investigated. To avoid the Prosecutor’s Fallacy, it’s crucial to carefully distinguish between the probability of the evidence given the hypothesis (guilt or innocence) and the probability of the hypothesis given the evidence.\nConsider a more concrete example of base rate fallacy. Say we have a witness who is 80% certain she saw a “checker” (\\(C\\)) taxi in the accident. We need to calculate \\(P(C\\mid E)\\). Assiming the base rate of 20% \\(P(C) = 0.2\\), we get \\[\nP(C\\mid E) = \\dfrac{P(E\\mid C)P(C)}{P(E)} = \\dfrac{0.8\\cdot 0.2}{0.8\\cdot 0.2 + 0.2\\cdot 0.8} = 0.5\n\\] The witness identification accuracy \\(P(C\\mid E) = 0.8\\) is called the sensitivity.\nEven with a highly accurate witness, the probability that the identified taxi is a Checker will be less than 80%, reflecting the impact of the base rate. Ignoring the base rate can lead to a significant overestimation of the probability of the identified event.\n\n\nExample 2.11 (Law Example)  \n\nSuppose you’re serving on a jury in the city of New York, with a population of roughly 10 million people. A man stands before you accused of murder, and you are asked to judge whether he is guilty \\(G\\) or not guilty \\(\\bar G\\). In his opening remarks, the prosecutor tells you that the defendant has been arrested on the strength of a single, overwhelming piece of evidence: that his DNA matched a sample of DNA taken from the scene of the crime. Let’s denote this evidence by the letter \\(D\\). To convince you of the strength of this evidence, the prosecutor calls a forensic scientist to the stand, who testifies that the probability that an innocent person’s DNA would match the sample found at the crime scene is only one in a million. The prosecution then rests its case. Would you vote to convict this man? If you answered “yes,” you might want to reconsider! You are charged with assessing \\(P(G \\mid D)\\) - that is, the probability that the defendant is guilty, given the information that his DNA matched the sample taken from the scene. Bayes’ rule tells us that \\[\nP(G\\mid D)= P(G)P(D\\mid G)/P(D), ~ P(D) = P(D \\mid G)P(G) + P(D \\mid \\bar G)P(\\bar G)\n\\] We know the following quantities:\n\nThe prior probability of guilt, \\(P(G)\\), is about one in 10 million. New York City has 10 million people, and one of them committed the crime.\nThe probability of a false match, \\(P(D \\mid \\bar G)\\), is one in a million, because the forensic scientist tested to this fact.\n\nTo use Bayes’ rule, let’s make one additional assumption: that the likelihood, \\(P(D\\mid  G)\\), is equal to 1. This means we’re assuming that, if the accused were guilty, there is a 100% chance of seeing a positive result from the DNA test. Let’s plug these numbers into Bayes’ rule and see what we get: \\[\nP(G\\mid D) = 0.09\n\\] The probability of guilt looks to be only 9%! This result seems shocking in light of the forensic scientist’s claim that \\(P(D \\mid \\bar\nG)\\) is so small: a “one in a million chance” of a positive match for an innocent person. Yet the prior probability of guilt is very low \\(P(G)\\) is a mere one in 10 million - and so even very strong evidence still only gets us up to \\(P(G | D) = 0.09\\).\nConflating \\(P(\\bar G \\mid  D)\\) with \\(P(D \\mid \\bar G)\\) is a serious error in probabilistic reasoning. These two numbers are typically very different from one another, because conditional probabilities aren’t symmetric. As we’ve said more than once, \\(P(\\text{practices hard} \\mid \\text{plays in NBA}) \\approx 1\\), while \\(P(\\text{plays in NBA} \\mid \\text{practices hard}) \\approx 0\\). An alternate way of thinking about this result is the following. Of the 10 million innocent people in New York, ten would have DNA matches merely by chance. The one guilty person would also have a DNA match. Hence there are 11 people with a DNA match, only one of whom is guilty, and so \\(P(G \\mid D) \\approx 1/11\\). Your intuition may mislead, but Bayes’ rule never does!\n\n\nExample 2.12 (Island Problem) There are \\(N+1\\) people on the island and one is a criminal. We have probability of a trait of a criminal equal to \\(p\\), which is \\(p = P(E\\mid I)\\), the probability of evidence, given innocence. Then we have a suspect who is matching the trait and we need to find probability of being guilty, given the evidence \\(P(G \\mid E)\\). It is easier to do the Bayes rule in the odds form. There are three components to the calculations: the prior odds of innocence, \\[\nO ( I ) = P (G) / P ( I ),\n\\] the Bayes factor, \\[\n\\frac{P(E\\mid G)}{P(E\\mid I)}.\n\\] and the posterior odds of innocence. \\[\n    O(I\\mid E) = \\dfrac{P(G\\mid E)}{P(I\\mid E)} = \\dfrac{1}{Np}.\n\\]\nThe Cromwell’s rule states that the use of prior probability of 1 or 0 should be avoided except when it is known for certain that the probability is 1 or 0. It is named after Oliver Cromwell who wrote to the General Assembly of he Church of Scotland in 1650 “I beseech you, in the bowels of Christ, it is possible that you may be mistaken”. In other words, using the Bayes rule \\[\nP(G\\mid E) = \\dfrac{P(E\\mid G)}{P(E)}P(G),\n\\] if \\(P(G)\\) is zero, it does not matter what the evidence is. Symmetrically, probability of innocence is zero if the evidence is certain. In other words, if \\(P(E\\mid I) = 0\\), then \\(P(I\\mid E) = 0\\). This is a very strong statement. It is not always true, but it is a good rule of thumb, it is a good way to avoid the prosecutor’s fallacy.\n\n\nExample 2.13 (Nakamura’s Alleged Cheating) In our paper Maharaj, Polson, and Sokolov (2023), we provide a statistical analysis of the recent controversy between Vladimir Kramnik (ex-world champion) and Hikaru Nakamura . Kramnik called into question Nakamura’s 45.5 out of 46 win streak in a 3+0 online blitz contest at chess.com. In this example we reproduce this paper and assess the weight of evidence using an a priori probabilistic assessment of Viswanathan Anand and the streak evidence of Kramnik. Our analysis shows that Nakamura has 99.6 percent chance of not cheating given Anand’s prior assumptions.\nWe start by addressing the argument of Kramnik which is based on the fact that the probability of such a streak is very small. This falls into precisely the Prosecutor’s Fallacy. Let introduce the notations. We denote by \\(G\\) the event of being guilty and \\(I\\) the event of innocence. We use \\(E\\) to denote evidence. In our case the evidence is the streak of wins by Nakamura. The Kramnik’s argument is that probability of observing the streak is very low, thus we might have a case of cheating. This is the prosecutor’s fallacy \\[\nP(I \\mid E) \\neq P(E \\mid I).\n\\] Kramnik’s calculations neglects other relevant factors, such as the prior probability of the cheating. The prosecutor’s fallacy can lead to an overestimation of the strength of the evidence and may result in an unjust conviction. In the cheating problem, at the top level of chess prior probability of \\(P(G)\\) is small! According to a recent statement by Viswanathan Anand, the probability of cheating is \\(1/10000\\).\n\n\n\nAnand’s Prior\n\n\nGiven the prior ratio of cheaters to not cheaters is \\(1/N\\), meaning out of \\(N+1\\) players, there is one cheater, the Bayes calculations requires two main terms. The first one is the prior odds of guilt: \\[\nO ( G ) = P (I) / P ( G ).\n\\] Here \\(P(I)\\) and \\(P(G)\\) are the prior probabilities of innocence and guilt respectively.\nThe second term is the Bayes factor, which is the ratio of the probability of the evidence under the guilt hypothesis to the probability of the evidence under the innocence hypothesis. The Bayes factor is given by \\[\n    L(E\\mid G) = \\frac{P(E\\mid I)}{P(E\\mid G)}.\n\\]\nProduct of the Bayes factor and the prior odds is the posterior odds of guilt, given the evidence. The posterior odds of guilt is given by \\[\n    O(G\\mid E) = O(G) \\times L(E\\mid G).\n\\]\nThe odds of guilty is \\[\n    O ( G )  = \\dfrac{N/(N+1)}{1/(N+1)} = N.\n\\]\nThe Bayes factor is given by \\[\n\\frac{P(E\\mid I)}{P(E\\mid G)} = \\dfrac{p}{1} = p.\n\\] Thus, the posterior odds of guilt are \\[\n    O(G\\mid E) = Np.\n\\] There are two numbers we need to estimate to calculate the odds of cheating given the evidence, namely the prior probability of cheating given via \\(N\\) and the probability of a streak \\(p = P(E\\mid I)\\).\nThere are multiple ways to calculate the probability of a streak. We can use the binomial distribution, the negative binomial distribution, or the Poisson distribution. The binomial distribution is the most natural choice. The probability of a streak of \\(k\\) wins in a row is given by \\[\n    P(E\\mid I) = \\binom{N}{k} q^k (1-q)^{N-k}.\n\\] Here \\(q\\) is is the probability of winning a single game. Thus, for a streak of 45 wins in a row, we have \\(k = 45\\) and \\(N = 46\\). We encode the outcome of a game as \\(1\\) for a win and \\(0\\) for a loss or a draw. The probability of a win is \\(q =  0.8916\\) (Nakamura’s Estimate, he reported on his YouTube channel). The probability of a streak is then 0.029. The individual game win probability is calculated from the ELO rating difference between the players.\nThe ELO rating of Hikaru is 3300 and the average ELO rating of his opponents is 2950, according to Kramnik. The difference of 350 corresponds to the odds of winning of \\(wo = 10^{350/400} = 10^{0.875} = 7.2\\). The probability of winning a single game is \\(q = wo/(1+wo) = 0.8916\\).\nThen we use the Anand’s prior of \\(N = 10000\\) to get the posterior odds of cheating given the evidence of a streak of 45 wins in a row. The posterior odds of being innocent are 285. The probability of cheating is then \\[\nP(G\\mid E) = 1/(1+O(G\\mid E)) = 0.003491.\n\\] Therefore the probability of innocent \\[\n    P(I\\mid E) = \\frac{Np}{Np+1} = 0.9965.\n\\]\nFor completeness, we perform sensitivity analysis and also get the odds of not cheating for \\(N = 500\\), which should be high prior probability given the status of the player and the importance of the event. We get \\[\n    P(I\\mid E) = \\frac{Np}{Np+1} = 0.9445.\n\\]\nThere are several assumptions we made in this analysis.\n\nInstead of calculating game-by-game probability of winning, we used the average probability of winning of 0.8916, provided by Nakamura himself. This is a reasonable assumption given the fact that Nakamura is a much stronger player than his opponents. This assumption slightly shifts posterior odds in favor of not cheating. Due to Jensen inequality, we have \\(E(q^{50}) &gt; E(q)^{50}\\). Expected value of the probability of winning a single game is \\(E(q) = 0.8916\\) and the expected value of the probability of a streak of 50 wins is \\(E(q^{50})\\). We consider the difference between the two to be small. Further, there is some correlation between the games, which also shifts the posterior odds in favor of not cheating. For example, some players are on tilt. Given they lost first game, they are more likely to lose the second game.\nThere are many ways to win 3+0 unlike in classical chess. For example, one can win on time. We argue that probability of winning calculated from the ELO rating difference is underestimated.\n\nNext, we can use the Bayes analysis to solve an inverse problem and to find what prior you need to assume and how long of a sequence you need to observe to get 0.99 posterior? Small sample size, we have \\(p\\) close to 1. Figure 2.1 shows the combination of prior (\\(N\\)) and the probability of a streak (\\(p\\)) that gives posterior odds of 0.99.\nIndeed, the results of the Bayesian analysis contradict the results of a traditional p-value based approach. A p-value is a measure used in frequentist statistical hypothesis testing. It represents the probability of obtaining the observed results, or results more extreme, assuming that the null hypothesis is true. The null hypothesis is a default position that Nakamura is not cheating and we compare the ELO-based expected win probability of \\(q=0.8916\\) to the observed on of \\(s=45/46=0.978\\). Under the null hypothesis, Nakamura should perform at the level predicted by \\(q\\).\n\nq = 0.8916\np = dbinom(45,46,q)\nN = 10000\nodds = p*N\nprint(1-1/(1+odds))\n\n 1\n\nprint(1/(1+odds))\n\n 0.0035\n\nprint(N*p/(N*p+1))\n\n 1\n\n\n\np = seq(from=0.006, to=0.07, length.out=500)\nN = seq(500,10000, by=250)\nplot(99/N,N,xlab=\"p\", ylab=\"N\", type='l', lwd=3, col=\"blue\")\n\n\n\n\n\n\n\nFigure 2.1: The combination of prior (\\(N\\)) and the probability of a streak (\\(p\\)) that gives posterior odds of 0.99.\n\n\n\n\n\n\n\nDavid Hume discussed the problem similar to the Island problem is hiw “On Miracles” essay. Hume is making the following argument on miracles:\n\n“…no testimony is sufficient to establish a miracle, unless the testimony be of such a kind, that its falsehood would be more miraculous, than the fact, which it endeavours to establish; and even in that case there is a mutual destruction of arguments, and the superior only gives us an assurance suitable to that degree of force, which remains, after deducting the inferior.”\n\nOne can view this as an application of Island problem essentially. Assuming probability of a miricle \\(A\\) is $ p( A) = p $ and $ p( not ; A ) = 1 -p \\(. Then Bayes rule gives\\)$ p( A| a ) = \n$$ Prosecutors’ fallacy, $ p( a| not ; A) - p( a| A) $ in general.\nIn Hume’s assessment of miracles (has to be something not in the laws of nature) we have $ p(A) = 10^{-6} $. This assessment takes into account background information, \\(I\\). Rare to have a contradiction to the laws of nature. More informative to write $ p( A | I ) $. Furthermore, we take $ p( a| A) =0.99 $. Hardest bit is to assess $p(a | not ; A ) $. The “frequency” of faked miracles and mankinds propensity to be marvellous. We assess $ p(a | not ; A ) = 10^{-3} \\(.\nThis yields chance of miracle to be unlikely as\\)$ p( A| a ) = ^{-3}. $$ Feynman considers the inverse problem: can we learn the laws of nature purely from empirical observation? Uses chess as an example. Is it a miracle that we have two bishops of the same color? No! according to Hume. We just didn’t know the laws of nature (a.k.a. model).\n\nExample 2.14 (Sally Clark Case: Independence or Bayes Rule?) To show that independence can lead to dramatically different results from Bayes conditional probabilities, consider the Sally Clark case. Sally Clark was accused and convicted of killing her two children who could have both died of SIDS. One explanation is that this was a random occurrence, the other one is that they both died of sudden infant death syndrome (SIDS). How can we use conditional probability to figure out a reasonable assessment of the probability that she murdered her children. First, some known probability assessments\n\nThe chance of a family of non-smokers having a SIDS death is \\(1\\) in \\(8,500\\).\nThe chance of a second SIDS death is \\(1\\) in \\(100\\).\nThe chance of a mother killing her two children is around \\(1\\) in \\(1,000,000\\).\n\nUnder Bayes \\[\\begin{align*}\n\\prob{\\mathrm{both} \\; \\; \\mathrm{SIDS}}   &  = \\prob{\\mathrm{first} \\; \\mathrm{SIDS}} \\prob{\\mathrm{Second} \\; \\;\\mathrm{SIDS} \\mid \\mathrm{first} \\; \\mathrm{SIDS}}\\\\\n&  = \\frac{1}{8500} \\cdot \\frac{1}{100} = \\frac{1}{850,000}.\n\\end{align*}\\]\nThe \\(1/100\\) comes from taking into account the genetics properties of SIDS. Independence, as implemented by the court, gets you to a probabilistic assessment of \\[\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)  = (1/8500) (1/8500) = (1/73,000,000).\n\\] This is a low probability. It is still not the answer to our question of context. We need a conditional probability, this will come to the Bayes rule.\nFirst, some general comment on the likelihood ratio calculation used to assess the weight of evidence in favor of guilty v.s. innocent evidence. Under Bayes we’ll find that there’s reasonable evidence that she’d be acquitted. We need the relative odds ratio. Let \\(I\\) denote the event that Sally Clark is innocent and \\(G\\) denotes guilty. Let \\(E\\) denote the evidence. In most cases, \\(E\\) contains a sequence \\(E_1, E_2, \\ldots\\) of ‘facts’ and we have to use the likelihood ratios in turn. Bayes rule then tells you to combine via multiplicative fashion. If likelihood ratio \\(&gt;1\\), odds of guilty. If likelihood ratio \\(&lt;1\\), more likelihood to be \\(I\\). By Bayes rule \\[\n\\frac{p(I\\mid E)}{p(G\\mid E)} = \\frac{p( E\\text{ and } I)}{p( G, I)}.\n\\] If we further decompose \\(p(E \\text{ and } I) = p(E\\mid I )p(I)\\) then we have to discuss the prior probability of innocence, namely \\(p(I)\\). Hence this is one subtle advantage of the above decomposition.\nThe underlying intuition that Bayes gives us in this example, is that one the two possible explanations of the data, both of which are unlikely, it is the relative likelihood of comparison that should matter. Here is a case where the \\(p\\)-value would be non-sensible (\\(p(E\\mid I) \\neq p(I\\mid E)\\)). Effectively comparing two rare event probabilities from the two possible models or explanations.\nHence putting these two together gives the odds of guilt as \\[\n\\frac{p(I\\mid E)}{p(G\\mid E)} = \\frac{1/850,000}{1/1,000,000} = 1.15.\n\\] Solving for the posterior probability yields \\(46.5\\%\\) for probability of guilty given evidence. \\[\np( G\\mid E) = \\frac{1}{1 + O(G\\mid E)} = 0.465.\n\\] Basically a \\(50/50\\) bet. Not enough to definitively convict! But remember that our initial prior probability on guilt \\(p(G)\\) was \\(10^{-6}\\). So now there has been a dramatic increase to a posterior probability of \\(0.465\\). So it’s not as if Bayes rule thinks this is evidence in the suspects favor – but the magnitude is still not in the \\(0.999\\) range though, where most jurors would have to be to feel comfortable with a guilt verdict.\nIf you use the “wrong” model of independence (as the court did) you get \\[\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)  = \\frac{1}{8500}\n  \\cdot\\frac{1}{8500} = \\frac{1}{73,000,000}.\n\\] With the independence assumption, you make the assessment \\[\n\\frac{p(I\\mid E)}{p(G\\mid E)} = \\frac{1}{73} \\; \\mathrm{ and} \\; p( G\\mid E) \\approx 0.99.\n\\] Given these probability assumptions, the suspect looks guilty with probability 99%.\nExperts also mis-interpret the evidence by saying: 1 in 73 million chance that it is someone else. This is clearly false and misleading to the jury and has leads to appeals.\n\n\nExample 2.15 (O. J. Simpson Case: Dershowitz Fallacy)  \n\nThis example is based on I. J. Good’s, “When batterer turns murderer.” Nature, 15 June 1995, p. 541. Alan Dershowitz, on the O. J. Simpson defense team, stated on T.V. and in newspapers that only 1 in 2,500 of men who abuse their wives go on to murder them. He clearly wanted his audience to interpret this to mean that the evidence of abuse by Simpson would only suggest a 0.04 of his being guilty of murdering her. He used probability to argue that because so few husbands who batter their wifes actually go on to murder their wives. Thus, O.J. is highly likely to be not guilty. This leaves out the most relevant conditioning information that we also know that Nicole Brown Simpson was actually murdered. Both authors believe the jury would be more interested in the probability that the husband is guilty of the murder of his wife given that he abused his wife and his wife was murdered. They both solve this problem by using Bayes’ theorem.\nIn this example, the notation \\(B\\) represents “woman battered by her husband, boyfriend, or lover”, \\(M\\) represents the event “woman murdered”, and \\(G\\) denotes “woman murdered by her batterer”. Our goal is to show that \\[\n% P(M,B \\mid M) \\neq P(M,B \\mid B).\nP(G \\mid M,B) \\neq P(G\\mid B).\n\\]\nIt is not hard to come to a wrong conclusion if you don’t take into account all the relevant conditional information. He intended this information to exonerate O.J. In 1992 the women population of the US was 125 million and 4936 women were murdered, thus \\[\nP(M) = 4936/125,000,000 = 0.00004 = 1/25,000.\n\\] At the same year about 3.5 million women were battered \\[\nP(B) = 3.5/125 = 0.028.\n\\] That same year 1432 women were murdered by their previous batterers, so the marginal probability of that event is \\(P(G) = 1432/125,000,000 = 0.00001 = 1/87,290\\), and the conditional probability, \\(P(G | B)\\) is 1432 divided by 3.5 million, or \\(1/2444\\). These are the numbers Dershowitz used to obtain his estimate that about 1 in 2500 battered women go on to be murdered by their batterers.\nWe need to calculate \\[\nP(G \\mid M,B) = P(M | G,B) P(G) / P(M).\n\\] We know \\(P(M | G,B) = 1\\) and \\(P(G) / P(M) = 0.00001/0.00004 = 0.29\\), or about 1 in 3.5.\nAlan Dershowitz provided the jury with an accurate but irrelevant probability. The fact the women was murdered increases the probability that she was murdered by her batterer by a factor of 709 (0.29/(1/2444)). \\[\nP(G\\mid M,B)\\approx 709\\times P(G\\mid B,M).\n\\]\nThe argument used by Dershowitz relating to the Simpson case has been discussed by John Paulos in an op-ed article in the Philadelphia Inquirer (15 Oct. 1995, C7) and his book “Once Upon a Number”, by I.J. Good in an article in Nature (June 15,1995, p 541) and by Jon Merz and Jonathan Caulkins in an article in Chance Magazine, (Spring 1995, p 14).\n\nProbability measures the uncertainty of an event. But how do we measure probability? One school of thought, takes probability as subjective, namely personal to the observer. de Finetti famously concluded that “Probability does not exist.” Measuring that is personal to the observer. It’s not like mass which is a property of an object. If two different observers have differing “news” then there is an them to bet (exchange contracts). Thus leading to a assessment of probability. Ramsey (1926) takes this view.\nMuch of data science is then the art of building probability models to study phenomenon. For many events most people will agree on their probabilities, for example \\(p(H) = 0.5\\) and \\(p(T) = 0.5\\). In the subjective view of probability we can measure or elicit a personal probability as a “willingness to play”. Namely, will you be willing to bet $1 so you can get $2 if head lands Tail and $0 if Head occurs? For more details, see Chapter 4.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#sec-Sensitivity",
    "href": "02-bayes.html#sec-Sensitivity",
    "title": "2  Bayes Rule",
    "section": "2.4 Sensitivity and Specificity",
    "text": "2.4 Sensitivity and Specificity\nConditional probabilities are used to define two fundamental metrics used for many probabilistic and statistical learning models, namely sensitivity and specificity.\nSensitivity and specificity are two key metrics used to evaluate the performance of diagnostic tests, classification models, or screening tools. These metrics help assess how well a test can correctly identify individuals with a condition (true positives) and those without the condition (true negatives). Let’s break down each term:\n\nSensitivity (True Positive Rate or Recall): is the ability of a test to correctly identify individuals who have a particular condition or disease \\(P ( T=1 \\mid D=1 )\\). It is calculated as the ratio of true positives to the sum of true positives and false negatives. \\[\np(T=1\\mid D=1) = \\dfrac{p(T=1,D=1)}{p(D=1)}.\n\\] A high sensitivity indicates that the test is good at identifying individuals with the condition, minimizing false negatives.\nSpecificity (True Negative Rate): is the ability of a test to correctly identify individuals who do not have a particular condition or disease \\(P (T=0 \\mid D=0 )\\). It is calculated as the ratio of true negatives to the sum of true negatives and false positives. \\[\np(T=0\\mid D=0) = \\dfrac{p(T=0,D=0)}{p(D=0)}\n\\] A high specificity indicates that the test is good at correctly excluding individuals without the condition, minimizing false positives.\n\nSensitivity and specificity are often trade-offs. Increasing sensitivity might decrease specificity, and vice versa. Thus, depending on application, you might prefer sensitivity over specificity or vice versa, depending on the consequences of false positives and false negatives in a particular application.\nConsider a medical test designed to detect a certain disease. If the test has high sensitivity, it means that it is good at correctly identifying individuals with the disease. On the other hand, if the test has high specificity, it is good at correctly identifying individuals without the disease. The goal is often to strike a balance between sensitivity and specificity based on the specific needs and implications of the test results.\n\nSensitivity is often called the power of a procedure (a.k.a. test). There are two kinds of errors (type I and type II) as well as sensitivity and specificity are the dual concepts.\n\n\n\n\n\n\nType I error (false positive rate)\n\n\n\nis the % of healthy people who tested positive, \\(p(T=1\\mid D=0)\\), it is the mistake of thinking something is true when it is not.\n\n\n\n\n\n\n\n\nType II error (or false negative rate)\n\n\n\nis the % sick people who are tested negative, \\(p(T=0\\mid D=1)\\), it the mistake of thinking something is not true when in fact it is true.\n\n\nWe would like to control both conditional probabilities with our test. Also if someone tests positive, how likely is it that they actually have the disease. There are two ‘errors’ one can make. Falsely diagnosing someone, or not correctly finding the disease.\nIn the stock market, one can think of type I error as not not selling a loosing stock quickly enough, and a type II error as failing to buy a growing stock, e.g. Amazon or Google.\n\n\n\n\n\n\n\n\n\n\\(p(T=1\\mid D=1)\\)\nSensitivity\nTrue Positive Rate\n\\(1-\\beta\\)\n\n\n\\(p(T=0\\mid D=0 )\\)\nSpecificity\nTrue Negative Rate\n\\(1-\\alpha\\)\n\n\n\\(p(T=1\\mid D=0)\\)\n1-Specificity\nFalse Positive Rate\n\\(\\alpha\\) (type I error)\n\n\n\\(p(T=0\\mid D =1)\\)\n1-Sensitivity\nFalse Negative Rate\n\\(\\beta\\) (type II error)\n\n\n\nOften it is convenient to write those four values in a form of a two-by-to matrix, called the confusion matrix:\n\n\n\nActual/Predicted\nPositive\nNegative\n\n\n\n\nPositive\nTP\nFN\n\n\nNegative\nFP\nTN\n\n\n\nwhere: TP: True Positive. FN: False Negative, FP: False Positive, TN: True Negative\nWe will extensively use the concepts of errors, specificity and sensitivity later in the book, when describing AB testing and predictive models. These example illustrates why people can commonly miscalculate and mis-interpret probabilities. Those quantities can be calculated using the Bayes rule.\n\nExample 2.16 (Apple Watch Series 4 ECG and Bayes’ Theorem) The Apple Watch Series 4 can perform a single-lead ECG and detect atrial fibrillation. The software can correctly identify 98% of cases of atrial fibrillation (true positives) and 99% of cases of non-atrial fibrillation (true negatives).\n\n\n\n\n\n\n\n\n\nPredicted\natrial fibrillation\nno atrial fibrillation\nTotal\n\n\n\n\natrial fibrillation\n1960\n980\n2940\n\n\nno atrial fibrillation\n40\n97020\n97060\n\n\nTotal\n2000\n98000\n100000\n\n\n\nHowever, what is the probability of a person having atrial fibrillation when atrial fibrillation is identified by the Apple Watch Series 4? We use Bayes theorem to answer this question. \\[\np(\\text{atrial fibrillation}\\mid \\text{atrial fibrillation is identified }) = \\frac{0.01960}{ 0.02940} = 0.6667\n\\]\nThe conditional probability of having atrial fibrillation when the Apple Watch Series 4 detects atrial fibrillation is about 67%.\nIn people younger than 55, Apple Watch’s positive predictive value is just 19.6 percent. That means in this group – which constitutes more than 90 percent of users of wearable devices like the Apple Watch – the app incorrectly diagnoses atrial fibrillation 79.4 percent of the time. (You can try the calculation yourself using this Bayesian calculator: enter 0.001 for prevalence, 0.98 for sensitivity, and 0.996 for specificity).\nThe electrocardiogram app becomes more reliable in older individuals: The positive predictive value is 76 percent among users between the ages of 60 and 64, 91 percent among those aged 70 to 74, and 96 percent for those older than 85.\nIn the case of medical diagnostics, the sensitivity is the ratio of people who have disease and tested positive to the total number of positive cases in the population \\[\np(T=1\\mid D=1) = \\dfrac{p(T=1,D=1)}{p(D=1)} = 0.019/0.002 = 0.95\n\\] The specificity is given by \\[\np(T=0\\mid D=0) = \\dfrac{p(T=0,D=0)}{p(D=0)} = 0.9702/0.98 = 0.99.\n\\] As we see the test is highly sensitive and specific. However, only 66% of those who are tested positive will have a disease. This is due to the fact that number of sick people is much less then the number of healthy and presence of type I error.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#graphical-representation-of-probability-and-conditional-independence.",
    "href": "02-bayes.html#graphical-representation-of-probability-and-conditional-independence.",
    "title": "2  Bayes Rule",
    "section": "2.5 Graphical Representation of Probability and Conditional Independence.",
    "text": "2.5 Graphical Representation of Probability and Conditional Independence.\n\nWe can use the telescoping property of conditional probabilities to write the joint probability distribution as a product of conditional probabilities. This is the essence of the chain rule of probability. It is given by \\[\np(x_1, x_2, \\ldots, x_n) = p(x_1)p(x_2 \\mid x_1)p(x_3 \\mid x_1, x_2) \\ldots p(x_n \\mid x_1, x_2, \\ldots, x_{n-1}).\n\\] The expression on the right hand side can be simplified if some of the variables are conditionally independent. For example, if \\(x_3\\) is conditionally independent of \\(x_2\\), given \\(x_1\\), then we can write \\[\np(x_3 \\mid x_1, x_2) =p(x_3 \\mid x_1).\n\\]\nIn a high-dimensional case, when we have a joint distribution over a large number of random variables, we can often simplify the expression by using independence or conditional independence assumptions. Sometimes it is convenient to represent these assumptions in a graphical form. This is the idea behind the concept of a Bayesian network. Essentially, graph is a compact representation of a set of independencies that hold in the distribution.\nLet’s consider an example of joint distribution with three random variables, we have the following joint distribution: \\[\np(a,b,c) = p(a\\mid b,c)p(b\\mid c)p(c)\n\\]\nWhen two nodes are connected they are not independent. Consider the following three cases:\n\n\n\n\n\n\n\n\n\nLine Structure\n\n\n\n\n\\[\np(b\\mid c,a) = p(b\\mid c),~ p(a,b,c) = p(a)p(c\\mid a)p(b\\mid c)\n\\]\n\n\n\n\n\n\n\nLambda Structure\n\n\n\n\n\\[\np(a\\mid b,c) = p(a\\mid c), ~ p(a,b,c) = p(a\\mid c)p(b\\mid c)p(c)\n\\]\n\n\n\n\n\n\n\nV-structure\n\n\n\n\n\\[\np(a\\mid b) = p(a),~ p(a,b,c) = p(c\\mid a,b)p(a)p(b)\n\\]\n\n\n\nAlthough the graph shows us the conditional independence assumptions, we can also derive other independencies from the graph An interesting question if they are connected through a third node. In the first case (a), we have \\(a\\) and \\(b\\) connected through \\(c\\). Thus, \\(a\\) can influence \\(b\\). However, once \\(c\\) is known, \\(a\\) and \\(b\\) are independent. In case (b) the logic here is similar, \\(a\\) can influence \\(b\\) through \\(c\\), but once \\(c\\) is known, \\(a\\) and \\(b\\) are independent. In the third case (c), \\(a\\) and \\(b\\) are independent, but once \\(c\\) is known, \\(a\\) and \\(b\\) are not independent. You can formally derive these independencies from the graph by comparing \\(p(a,b\\mid c)\\) and \\(p(a\\mid c)p(b\\mid c)\\).\n\nExample 2.17 (Bayes Home Diagnostics) Suppose that a house alarm system sends me a text notification when some motion inside my house is detected. It detects motion when I have a person inside (burglar) or during an earthquake. Say, from prior data we know that during an earthquake alarm is triggered in 10% of the cases. Once I receive a text message, I start driving back home. While driving I hear on the radio about a small earthquake in our area. Now we want to know \\(p(b \\mid a)\\) and \\(p(b \\mid a,r)\\). Here \\(b\\) = burglary, \\(e\\) = earthquake, \\(a\\) = alarm, and \\(r\\) = radio message about small earthquake.\nThe joint distribution is then given by \\[\n  p(b,e,a,r) = p(r \\mid a,b,e)p(a \\mid b,e)p(b\\mid e)p(e).\n\\] Since we know the causal relations, we can simplify this expression \\[\np(b,e,a,r) = p(r \\mid e)p(a \\mid b,e)p(b)p(e).\n\\] The distribution is defined by\n\n\n\n\\(p(a=1 \\mid b,e)\\)\nb\ne\n\n\n\n\n0\n0\n0\n\n\n0.1\n0\n1\n\n\n1\n1\n0\n\n\n1\n1\n1\n\n\n\nGraphically, we can represent the relations between the variables known as a Directed Acyclic Graph (DAG), which is known as Bayesian network.\n\n\n\n\n\n\n\nFigure 2.2: Bayesian network for alarm.\n\n\n\nNow we can easily calculate \\(p(a=0 \\mid b,e)\\), from the property of a probability distribution \\(p(a=1 \\mid b,e) + p(a=0 \\mid b,e) = 1\\). In addition, we are given \\(p(r=1 \\mid e=1) = 0.5\\) and \\(p(r=1 \\mid e=0) = 0\\). Further, based on historic data we have \\(p(b) = 2\\cdot10^{-4}\\) and \\(p(e) = 10^{-2}\\). Note that causal relations allowed us to have a more compact representation of the joint probability distribution. The original naive representations requires specifying \\(2^4\\) parameters.\nTo answer our original question, calculate \\[\np(b \\mid a) = \\dfrac{p(a \\mid b)p(b)}{p(a)},~~p(b) = p(a=1 \\mid b=1)p(b=1) + p(a=1 \\mid b=0)p(b=0).\n\\] We have everything but \\(p(a \\mid b)\\). This is obtained by marginalizing \\(p(a=1 \\mid b,e)\\), to yield \\[\np(a \\mid b) = p(a \\mid b,e=1)p(e=1) + p(a \\mid b,e=0)p(e=0).\n\\] We can calculate \\[\np(a=1 \\mid b=1) = 1, ~p(a=1 \\mid b=0) = 0.1*10^{-2} + 0 = 10^{-3}.\n\\] This leads to \\(p(b \\mid a) = 2\\cdot10^{-4}/(2\\cdot10^{-4} + 10^{-3}(1-2\\cdot10^{-4})) = 1/6\\).\nThis result is somewhat counterintuitive. We get such a low probability of burglary because its prior is very low compared to prior probability of an earthquake. What will happen to posterior if we live in an area with higher crime rates, say \\(p(b) = 10^{-3}\\). Figure 2.3 shows the relationship between the prior and posterior. \\[\np(a \\mid b) = \\dfrac{p(b)}{p(b) + 10^{-3}(1-p(b))}\n\\]\n\nprior &lt;- seq(0, .1, length.out = 200)\npost &lt;- prior / (prior + 0.001 * (1 - prior))\nplot(prior, post, type = \"l\", lwd = 3, col = \"red\")\n\n\n\n\n\n\n\nFigure 2.3: Relationship between the prior and posterior\n\n\n\n\n\nNow, suppose that you hear on the radio about a small earthquake while driving. Then, using Bayesian conditioning, \\[\np(b=1 \\mid a=1,r=1) =  \\dfrac{p(a,r  \\mid  b)p(b)}{p(a,r)}\n\\] and \\[\np(a,r  \\mid  b)p(b) = \\dfrac{\\sum_e p(b=1,e,a=1,r=1)}{\\sum_b\\sum_ep(b,e,a=1,r=1)}\n\\] \\[\n=\\dfrac{\\sum_ep(r=1 \\mid e)p(a=1 \\mid b=1,e)p(b=1)p(e)}{\\sum_b\\sum_ep(r=1 \\mid e)p(a=1 \\mid b,e)p(b)p(e)}\n\\] which is \\(\\approx 2\\%\\) in our case. This effect is called explaining away, namely when new information explains some previously known fact.\n\n\n\n\n\nMaharaj, Shiva, Nick Polson, and Vadim Sokolov. 2023. “Kramnik Vs Nakamura or Bayes Vs p-Value.” {{SSRN Scholarly Paper}}. Rochester, NY.\n\n\nSimpson, Edward. 2010. “Edward Simpson: Bayes at Bletchley Park.” Significance 7 (2): 76–80.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "03-bl.html",
    "href": "03-bl.html",
    "title": "3  Bayesian Learning",
    "section": "",
    "text": "3.1 Exchangeability and the Bayesian view of probability models\nStatistics makes use of parametric families of distributions and make assumption that observed samples \\(y = (y_1,\\ldots,y_n)\\) are independent and identically distributed observations from a distribution with density function parametrized by \\(\\theta\\), the notation is \\(y \\sim p(y \\mid \\theta)\\). The functional form of \\(p(y \\mid \\theta)\\) is assumed to be known, but the value of \\(\\theta\\) is unknown. The goal of statistical inference is to estimate \\(\\theta\\) from the observed data \\(y_1,\\ldots,y_n\\). There are several tasks in statistical inference, including\nIn this section we present a general framework for statistical inference, known as Bayesian inference, which is based on the use of probability distributions to represent uncertainty and make inferences about unknown parameters. We will use the Bayes rule to update our beliefs about the parameters of a model based on new evidence or data. Bayesian inference provides a principled approach to statistical modeling and decision-making, and is widely used in various fields such as machine learning, econometrics, and engineering.\nIn the context of artificial intelligence and statistical modeling, Bayesian parameter learning is particularly relevant when dealing with models that have uncertain or unknown parameters. The goal is to update the probability distribution over the parameters of the model as new data becomes available. Suppose that you are interested in the values of k unknown quantities \\[\n\\theta = (\\theta_1, \\ldots, \\theta_k)\n\\]\nThe basic steps involved in Bayesian parameter learning include:\nThe key advantage of Bayesian parameter learning is its ability to incorporate prior knowledge and update beliefs based on observed data in a principled manner. It provides a framework for handling uncertainty and expressing the confidence or ambiguity associated with parameter estimates. However, it often requires computational methods, such as Markov Chain Monte Carlo (MCMC) or variational inference, to approximate or sample from the complex posterior distributions.\nThe Bayes rule allows us to combine the prior distribution and the likelihood function, sometimes we omit the total probability in the denominator on the right hand side and write Bayes rule as \\[\n\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\n\\]\nThe choice of prior distribution can significantly impact the ease of computation and the interpretation of the posterior distribution. Conjugate priors are a special type of prior distribution that, when combined with a specific likelihood function, result in a posterior distribution that belongs to the same family as the prior. This property simplifies the computation of the posterior distribution, and allows for analytical solution.\nCommon examples of conjugate priors include:\nUsing conjugate priors simplifies the Bayesian analysis, especially in cases where analytical solutions are desirable. However, the choice of a conjugate prior is often a modeling assumption, and in some cases, non-conjugate priors may be more appropriate for capturing the true underlying uncertainty in the problem. The blind use of conjugate priors can lead to misleading results. We should never ignore the absence of evidence for use of a specific model.\nAt the basis of all statistical problems is a potential sample of data, \\(y=\\left( y_{1},\\ldots,y_{T}\\right)\\), and assumptions over the data generating process such as independence, a model or models, and parameters. How should one view the relationship between models, parameters, and samples of data? How should one define a model and parameters? These questions have fundamental implications for statistical inference and can be answered from different perspectives. We will discuss the de Finetti’s representation theorem which provides a formal connection between data, models, and parameters.\nTo understand the issues, consider the simple example of an experiment consisting of tosses of a simple thumb tack in ideal “laboratory” conditions. The outcome of the experiment can be defined as a random variable \\(y_{i},\\) where \\(y_{i}=1\\) if the \\(i^{th}\\) toss was a heads (the tack lands on the spike portion) and \\(y_{i}=0\\) if the tack land tails (on its flat portion). How do we model these random variables? The frequentist or objective approach assumes tosses are independent and identically distributed. In this setting, independence implies that \\[\nP\\left(  y_{2}=1,y_{1}=1\\right)  =P\\left(  y_{2}=1\\right)\nP\\left(  y_{1}=0\\right)  \\text{.}%\n\\]\nGiven this, are thumbtack tosses independent? Surprisingly, the answer is no. Or at least absolutely not under the current assumptions. Independence implies that \\[\nP\\left(  y_{2}=1 \\mid y_{1}=1\\right)  =P\\left(  y_{2}=1\\right)\n\\text{,}%\n\\] which means that observing \\(y_{1}=1\\) does not effect the probability that \\(y_{2}=1\\). To see the implications of this simple fact, suppose that the results of 500 tosses were available. If the tosses were independent, then \\[\nP\\left(  y_{501}=1\\right)  =P\\left(  y_{501}=1\\mid {\\textstyle\\sum\\nolimits_{t=1}^{500}}y_{t}=1\\right)  =P\\left(  y_{501}=1\\mid {\\textstyle\\sum\\nolimits_{t=1}^{500}}y_{t}=499\\right)  \\text{.}\n\\] It is hard to imagine that anyone would believe this–nearly every observer would state that the second probability is near zero and the third probably is near 1 as the first 500 tosses contain a lot of information. Thus, the tosses are not independent.\nTo see the resolution of this apparent paradox, introduce a parameter, \\(\\theta\\), which is the probability that a thumb tack toss is heads. If \\(\\theta\\) were known, then it is true that, conditional on the value of this parameter, the tosses are independent and \\[\nP\\left(  y_{2}=1\\mid y_{1}=1,\\theta\\right)  =P\\left(y_{2}=1\\mid \\theta\\right)  =\\theta\\text{.}\n\\] Thus, the traditional usage of independence, and independent sampling, requires that “true” parameter values are known. With unknown probabilities, statements about future tosses are heavily influenced by previous observations, clearly violating the independence assumption. Ironically, if the data was really independent, we would not need samples in the first place to estimate parameters because the probabilities would already be known! Given this, if you were now presented with a thumb tack from a box that was to be repeatedly tossed, do you think that the tosses are independent?\nThis example highlights the tenuous foundations, an odd circularity, and the internal inconsistency of the frequentist approach that proceeds under the assumption of a fixed “true” parameter. All frequentist procedures are founded on the assumption of known parameter values:sampling distributions of estimators are computed conditional on \\(\\theta\\); confidence intervals consist of calculations of the form: \\(P\\left( f\\left( y_{1}, \\ldots ,y_{T}\\right) \\in\\left( a,b\\right) |\\theta\\right)\\); and asymptotics also all rely on the assumption of known parameter values. None of these calculations are possible without assuming the known parameters.\nIn the frequentist approach, even though the parameter is completely unknown to the researcher, \\(\\theta\\) is not a random variable, does not have a distribution, and therefore inference is not governed by the rules of probability. Given this “fixed, but unknown” definition, it is impossible to discuss concepts like “parameter uncertainty.” This strongly violates our intuition, since things that are not known are typically thought of as random.\nThe Bayesian approach avoids this internal inconsistency by shedding the strong assumption of independence and assumption of a fixed but unknown parameter. Instead it assumes that \\(\\theta\\) is a random variable and describes the uncertainty about \\(\\theta\\) using a probability distribution, \\(p\\left( \\theta\\right)\\) (the prior). The joint distribution of the data is then \\[\np(y_{1}, \\ldots ,y_{T},\\theta)  = \\int p(y_{1}, \\ldots ,y_{T} \\mid \\theta)  p(\\theta)d\\theta = \\int\\prod_{t=1}^Tp(y_t\\mid \\theta)  p( \\theta)d\\theta.\n\\] Notice, that the right-hand-side does not depend on the order of the data, and the joint distribution of the data is the same for all potential orderings. This is a natural assumption about the symmetry of the data, and is called exchangeability. The Bayesian approach makes no assumptions about the order in which the data may arrive, and each observation has the same marginal distribution, \\(P\\left( y_{i}=1\\right) =P\\left(y_{j}=1\\right)\\) for any \\(i\\) and \\(j\\).\nThus, we replace the independence assumption with a weaker and more natural assumption of exchangeability: collection of random variables, \\(y_{1}, \\ldots ,y_{T}\\), is exchangeable if the distribution of \\(y_{1}, \\ldots ,y_{T}\\) is the same as the distribution of any permutation \\(y_{\\pi_{1}}, \\ldots ,y_{\\pi_{T}}\\), where \\(\\pi=\\left( \\pi_{1}, \\ldots ,\\pi_{T}\\right)\\) is a permutation of the integers \\(1\\) to \\(T\\). Independent events are always exchangeable, but the converse is not true. Notice the differences between the assumptions in the Bayesian and frequentist approach: the Bayesian makes assumptions over potentially realized data, and there is no need to invent the construct of a fixed but unknown parameter, since exchangeability makes no reference to parameters.\nIn the case of the tack throwing experiment, exchangeability states that the ordering of heads and tails does not matter. Thus, if the experiment of 8 tosses generated 4 heads, it does not matter if the ordering was \\(\\left(1,0,1,0,1,0,1,0\\right)\\) or \\(\\left( 0,1,1,0,1,0,0,1\\right)\\). This is a natural assumption about the symmetry of the tack tosses, capturing the idea that the information in any toss or sequence of tosses is the same as any other–the idea of a truly random sample. It is important to note that exchangeability is property that applies prior to viewing the data. After observation, data is no longer a random variable, but a realization of a random variable.\nBruno de Finetti introduced the notion of exchangeability, and then asked a simple question: “What do exchangeable sequences of random variables look like?” The answer to this question is given in the famous de Finetti’s theorem, which also defines models, parameters, and provides important linkages between frequentist and classical statistics.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#exchangeability-and-the-bayesian-view-of-probability-models",
    "href": "03-bl.html#exchangeability-and-the-bayesian-view-of-probability-models",
    "title": "3  Bayesian Learning",
    "section": "",
    "text": "3.1.1 de Finetti’s Representation theorem\nde Finetti’s representation theorem provides the theoretical connection between data, models, and parameters. It is stated first in the simplest setting, where the observed data takes two values, either zero or one, and then extended below.\n\nTheorem 3.1 (de Finetti’s Representation theorem) Let \\(\\left( y_{1},y_{2},\\ldots\\right)\\) be an infinite sequence of 0-1 exchangeable random variables with joint density \\(p\\left(y_{1}, \\ldots ,y_{T}\\right)\\). Then there exists a distribution function \\(P\\) such that \\[\np(y_{1},\\ldots,y_{T})=\\int\\prod_{t=1}^{T}\\theta^{y_{t}}(1-\\theta)^{1-y_{t}%\n}dP(\\theta)=\\int\\prod_{t=1}^{T}p\\left(  y_{t} \\mid \\theta\\right)  dP(\\theta)\n\\tag{3.1}\\] where \\[\nP(\\theta)=\\underset{T\\rightarrow\\infty}{\\lim}\\text{Prob}\\left[  \\frac{1}{T}\\sum_{t=1}^{T}y_{t}\\leq\\theta\\right]  \\text{ and }\\theta=\\underset {T\\rightarrow\\infty}{\\lim}\\frac{1}{T}\\sum_{t=1}^{T}y_{t}\\text{.}%\n\\] If the distribution function or measure admits a density with respect to Lebesgue measure, then \\(dP(\\theta)=p\\left( \\theta\\right) d\\theta\\).\n\nde Finetti’s representation theorem has profound implications for understanding models from a subjectivist perspective and in relating subjectivist to frequentist theories of inference. The theorem is interpreted as follows:\n\nUnder exchangeability, parameters exist, and one can act as if the \\(y_{t}\\)’\\(s\\) are drawn independently from a Bernoulli distribution with parameter \\(\\theta\\). That is, they are draws from the model \\(p\\left(y_{t} \\mid \\theta\\right) =\\theta^{y_{t}}(1-\\theta)^{1-y_{t}},\\) generating a likelihood function \\(p\\left( y \\mid \\theta\\right) =\\prod_{t=1}^{T}p\\left(y_{t} \\mid \\theta\\right)\\). Formally, the likelihood function is defined via the density \\(p\\left( y \\mid \\theta\\right)\\), viewed as a function of \\(\\theta\\) for a fixed sample \\(y=\\left( y_{1}, \\ldots ,y_{T}\\right)\\). More “likely” parameter values generate higher likelihood values, thus the name. The maximum likelihood estimate or MLE is \\[\n\\widehat{\\theta}=\\arg\\underset{\\theta\\in\\Theta}{\\max}\\text{ }p\\left(y \\mid \\theta\\right)  =\\arg\\underset{\\theta\\in\\Theta}{\\max}\\ln p\\left(y \\mid \\theta\\right)  \\text{,}%   \n\\] where \\(\\Theta\\) is the parameter space.\nParameters are random variables. The limit \\(\\theta=\\underset {T\\rightarrow\\infty}{\\lim}T^{-1}\\sum_{t=1}^{T}y_{t}\\) exists but is a random variable. This can be contrasted with the strong law of large numbers that requires independence and implies that \\(T^{-1}\\sum_{t=1}^{T}y_{t}\\) converges almost surely to a fixed value, \\(\\theta_{0}\\). From this, one can interpret a parameter as a limit of observables and justifies the frequentist interpretation of \\(\\theta\\) as a limiting frequency of 1’s.\nThe distribution \\(P\\left( \\theta\\right)\\) or density \\(p\\left(\\theta\\right)\\) can be interpreted as beliefs about the limiting frequency \\(\\theta\\) prior to viewing the data. After viewing the data, beliefs are updated via Bayes rule resulting in the posterior distribution, \\[\np\\left(  \\theta \\mid y\\right)  \\propto p\\left(  y \\mid \\theta\\right)  p(\\theta).\n\\] Since the likelihood function is fixed in this case, any distribution of observed data can be generated by varying the prior distribution.\n\nThe main implication of de Finetti’s theorem is a complete justification for Bayesian practice of treating the parameters as random variables and specifying a likelihood and parameter distribution. Stated differently, a “model” consists of both a likelihood and a prior distribution over the parameters. Thus, parameters as random variables and priors are a necessity for statistical inference, and not some extraneous component motivated by philosophical concerns.\nMore general versions of de Finetti’s theorem are available. A general version is as follows. If \\(\\left\\{ y_{t}\\right\\} _{t\\geq1}\\), \\(y_{t}\\in\\mathbb{R}\\), is a sequence of infinitely exchangeable random variables, then there exists a probability measure \\(P\\) on the space of all distribution functions, such that \\[\nP(y_{1},\\ldots,y_{T})=\\int\\prod_{t=1}^{T}F\\left(  y_{t}\\right)\nP(dF)\n\\] with mixing measure \\[\nP\\left(  F\\right)  =\\underset{T\\rightarrow\\infty}{\\lim}P(F_{T}),\n\\] where \\(F_{T}\\) is the empirical distribution of the data. At this level of generality, the distribution function is infinite-dimensional. In practice, additional subjective assumptions are needed that usually restrict the distribution function to finite dimensional spaces, which implies that distribution function is indexed by a parameter vector \\(\\theta\\): \\[\np(y_{1},\\ldots,y_{T})=\\int\\prod_{t=1}^{T}p\\left(  y_{t} \\mid \\theta\\right)\ndP\\left(  \\theta\\right)  \\text{.}%\n\\] To operationalize this result, the researcher needs to choose the likelihood function and the prior distribution of the parameters.\nAt first glance, de Finetti’s theorem may seem to suggest that there is a single model or likelihood function. This is not the case however, as models can be viewed in the same manner as parameters. Denoting a model specification by \\(\\mathcal{M}\\), then de Finetti’s theorem would imply that \\[\\begin{align*}\np(y_{1},\\ldots,y_{T})  &  =\\int\\prod_{t=1}^{T}p\\left(  y_{t} \\mid \\theta ,\\mathcal{M}\\right)  p\\left(  \\theta \\mid \\mathcal{M}\\right)  p\\left(\\mathcal{M}\\right)  d\\theta d\\mathcal{M}\\\\\n&  =\\int p(y_{1},\\ldots,y_{T} \\mid \\mathcal{M})p\\left(  \\mathcal{M}\\right)\nd\\mathcal{M}\\text{,}%\n\\end{align*}\\] in the case of a continuum of models. Thus, under the mild assumption of exchangeability, it is as if the \\(y_{t}\\)’\\(s\\) are generated from \\(p\\left( y_{t} \\mid \\theta,\\mathcal{M}\\right)\\), conditional on the random variables \\(\\theta\\) and \\(\\mathcal{M}\\), where \\(p\\left( \\theta \\mid \\mathcal{M}\\right)\\) are the beliefs over \\(\\theta\\) in model \\(\\mathcal{M}\\), and \\(p\\left(\\mathcal{M}_{j}\\right)\\) are the beliefs over model specifications.\nThe objective approach has been a prevailing one in scientific applications. However, it only applies to events that can be repeated under the same conditions a very large number of times. This is rarely the case in many important applied problems. For example, it is hard to repeat an economic event, such as a Federal Reserve meeting or the economic conditions in 2008 infinitely often. This implies that at best, the frequentist approach is limited to laboratory situations. Even in scientific applications, when we attempt to repeat an experiment multiple times, an objective approach is not guaranteed to work. For example, the failure rate of phase 3 clinical trials in oncology is 60% (Shen et al. (2021),Sun et al. (2022)). Prior to phase 3, the drug is usually tested on several hundred patients.\nSubjective probability is a more general definition of probability than the frequentist definition, as it can be used for all types of events, both repeatable and unrepeatable events. A subjectivist has no problem discussing the probability a republican president will be re-elected in 2024, even though that event has never occurred before and cannot occur again. The main difficulty in operationalizing subjective probability is the process of actually quantifying subjective beliefs into numeric probabilities.\nInstead of using repetitive experiments, subjective probabilities can be measured using betting odds, which have been used for centuries to gauge the uncertainty over an event. The probability attributed to winning a coin toss is revealed by the type of of odds one would accept to bet. Notice the difference between the frequentist and Bayesian approach. Instead of defining the probabilities via an infinite repeated experiment, the Bayesian approach elicits probabilities from an individual’s observed behavior.\n\n\n3.1.2 Posterior Empirical CDF\nLet \\(\\mathcal{M} = \\{ f_\\theta ( y ) : y \\in \\mathcal{Y} \\}\\) be a model. When necessary we index the parameters in model \\(m\\), as \\(\\theta_m\\). Let \\(y = ( y_1 , \\ldots , y_n )\\) be a vector of signals. The conditional likelihood, under \\(m\\), is given by \\(f_\\theta(y) =  \\prod_{i=1}^n f_\\theta ( y_i )\\). We also allow for the possibility that the data is generated from a model \\(f\\) that does not belong to the family of models \\(f_\\theta\\).\nGiven a prior measure, \\(\\Pi ( d F )\\), over \\(\\mathcal{F}\\) the set of distributions, we can calculate the predictive density \\[\nf_n  ( y_{n+1} | y_1 , \\ldots , y_n ) = \\int f (y) \\Pi_n ( d F ) \\; {\\rm where} \\; \\Pi_n ( d f ) = \\frac{ \\prod_{i=1}^n f( y_i ) \\Pi( d f ) }{  \\int  \\prod_{i=1}^n f( y_i ) \\Pi( d f ) }\n\\] Under the family, $ f_\\(, we can calculate the parameter posterior as\\)$ p( | y ) = ; {} ; m(y) = f_(y) p( ) d $$ Here \\(p(\\theta)\\) is a prior distribution over parameters and \\(m(y)\\) is the marginal distribution of the data implied by the model. There are many applications in Bayesian non-parametrics statistics.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#sufficient-statistic-summary-statistic",
    "href": "03-bl.html#sufficient-statistic-summary-statistic",
    "title": "3  Bayesian Learning",
    "section": "3.2 Sufficient Statistic (Summary Statistic)",
    "text": "3.2 Sufficient Statistic (Summary Statistic)\nA statistic \\(S(y)\\) is sufficient for \\(\\theta\\), if the conditional distribution of \\(y\\) given \\(S(y)\\) is independent of \\(\\theta\\), namely \\[\np(y\\mid S(y),\\theta) = p(y\\mid S(y)).\n\\] Then, we can view \\(S(y)\\) as a dimension reducing map, as inference for \\(\\theta\\) can be solely determined by \\(S\\). This follows as \\(S(y)\\) as a deterministic map of \\(y\\), so with \\(P(S(y))&gt;0\\), we have \\[\np(y,S(y)\\mid \\theta) = p(y\\mid S(y),\\theta)p(S(y)\\mid \\theta) = p(y\\mid S(y))p(S(y)\\mid \\theta) \\propto p(S(y)\\mid\\theta)\n\\]\nIn Bayesian inference, we need to compute the posterior over unknown model parameters \\(\\theta\\), given data \\(y\\). The posterior density is denoted by \\(p(\\theta \\mid y)\\). Here \\(y = ( y_1 , \\ldots , y_n )\\) is high dimensional. A map from data to a real number or to a low-dimension vector \\[\nS = S(y) = ( S_1(y) , \\ldots , S_k(y) )\n\\] is called a statistic. Since statistic is a deterministic function of the data, then \\[\np(y\\mid \\theta) = p(y,S\\mid \\theta) = p(S\\mid \\theta)p(y\\mid S,\\theta).\n\\] If it happens that the likelihood is conditionally independent on \\(\\theta\\), given \\(S\\) then \\[\np(y\\mid \\theta) = p(S\\mid \\theta)p(y\\mid S).\n\\] In this case the statistic \\(S\\) is called the sufficient statistic for parameter \\(\\theta\\) given data \\(y\\). In other words all the information needed for estimating \\(\\theta\\) is given by \\(S\\).\nThere is a nice connection between the posterior mean and the sufficient statistics, especially minimal sufficient statistics in the exponential family. If there exists a sufficient statistic \\(S^*\\) for \\(\\theta\\), then Kolmogorov (1942) shows that for almost every \\(y\\), \\(p(\\theta\\mid y) = p(\\theta\\mid S^*(y))\\) , and further \\(S(y) = E_{p}(\\theta \\mid y) = E_{p}(\\theta \\mid S^*(y))\\) is a function of \\(S^*(y)\\). In the special case of an exponential family with minimal sufficient statistic \\(S^*\\) and parameter \\(\\theta\\), the posterior mean \\(S(y) = E_{p}(\\theta \\mid y)\\) is a one-to-one function of \\(S^*(y)\\), and thus is a minimal sufficient statistic.\nSummary Statistic: Let \\(S(y)\\) is sufficient summary statistic in the Bayes sense (Kolmogorov (1942)), if for every prior \\(p\\) \\[\nf_B (y) :=   p_{\\theta \\mid y}(\\theta \\in B\\mid y) = p_{\\theta \\mid s(y)}(\\theta \\in B\\mid s(y)).\n\\] Then we need to use our pattern matching dataset \\((y^{(i)} , \\theta^{(i)})\\) which is simulated from the prior and forward model to “train” the set of functions \\(f_B (y)\\), where we pick the sets \\(B = ( - \\infty , q ]\\) for a quantile \\(q\\). Hence, we can then interpolate in between.\nEstimating the full sequence of functions is then done by interpolating for all Borel sets \\(B\\) and all new data points \\(y\\) using a NN architecture and conditional density NN estimation.\nThe notion of a summary statistic is prevalent in the ABC literature and is tightly related to the notion of a Bayesian sufficient statistic \\(S^*\\) for \\(\\theta\\), then (Kolmogorov 1942), for almost every \\(y\\), \\[\np(\\theta \\mid  Y=y) = p(\\theta \\mid S^*(Y) = S^*(y))\n\\] Furthermore, \\(S(y) = \\mathrm{E}\\left(\\theta \\mid Y = y\\right) = \\mathrm{E}_{p}\\left(\\theta \\mid S^*(Y) = S^*(y)\\right)\\) is a function of \\(S^*(y)\\). In the case of exponential family, we have \\(S(Y) = \\mathrm{E}_{p}\\left(\\theta | Y \\right)\\) is a one-to-one function of \\(S^*(Y)\\), and thus is a minimal sufficient statistic.\nSufficient statistics are generally kept for parametric exponential families, where \\(S(\\cdot)\\) is given by the specification of the probabilistic model. However, many forward models have an implicit likelihood and no such structures. The generalization of sufficiency is a summary statistics (a.k.a. feature extraction/selection in a neural network). Hence, we make the assumption that there exists a set of features such that the dimensionality of the problem is reduced.\n\nExample 3.1 (Posterior Distribution for Coin Toss) What if we gamble against unfair coin flips or the person who performs the flips is trained to get the side he wants? In this case, we need to estimate the probability of heads \\(\\theta\\) from the data. Suppose we have observed 10 flips \\[\n\\{H, T, H, H, H, T, H, T, H, H\\},\n\\] and only three of them were tails. What is the probability that the next flip will be tail? The frequency-based answer would be \\(3/10 = 0.3\\). However, Bayes approach gives us more flexibility. Suppose we have a prior belief that the coin is fair, but we are not sure. We can model this belief by a prior distribution. Let discretize the variable \\(\\theta\\) and assign prior probabilities to each value of \\(\\theta\\) as follows\n\ntheta &lt;- seq(0, 1, by = 0.1)\nprior = c(0, 0.024, 0.077, 0.132, 0.173, 0.188, 0.173, 0.132, 0.077, 0.024, 0)\nbarplot(prior, names.arg = theta, xlab = \"theta\", ylab = \"prior\")\n\n\n\n\nPrior distribution\n\n\n\n\nWe put most of the mass to the fair assumption (\\(\\theta = 0.5\\)) and zero mass to the extreme values \\(\\theta = 0\\) and \\(\\theta = 1\\). Our mass is exponentially decaying as we move away from 0.5. This is a reasonable assumption, since we are not sure about the fairness of the coin. Now, we can use Bayes rule to update our prior belief. The posterior distribution is given by \\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}.\n\\] The denominator is the marginal likelihood, which is given by \\[\np(y) = \\sum_{\\theta} p(y \\mid \\theta) p(\\theta).\n\\] The likelihood is given by the Binomial distribution \\[\np(y \\mid \\theta) \\propto \\theta^3 (1 - \\theta)^7.\n\\] Notice, that the posterior distribution depends only on the number of positive and negative cases. Those numbers are sufficient for the inference about \\(\\theta\\). The posterior distribution is given by\n\nlikelihood &lt;- function(theta, n, Y) {\n  theta^Y * (1 - theta)^(n - Y)\n}\nposterior &lt;- likelihood(theta, 10,3) * prior\nposterior &lt;- posterior / sum(posterior) # normalize\nbarplot(posterior, names.arg = theta, xlab = \"theta\", ylab = \"posterior\")\n\n\n\n\nPosterior distribution\n\n\n\n\nIf you are to keep collecting more observations and say observe a sequence of 100 flips, then the posterior distribution will be more concentrated around the value of \\(\\theta = 0.3\\).\n\nposterior &lt;- likelihood(theta, 100,30) * prior\nposterior &lt;- posterior / sum(posterior) # normalize\nbarplot(posterior, names.arg = theta, xlab = \"theta\", ylab = \"posterior\")\n\n\n\n\nPosterior distribution for n=100\n\n\n\n\nThis demonstrates that for large sample sizes, the frequentist approach and Bayes approach agree.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#sec-betabinomial",
    "href": "03-bl.html#sec-betabinomial",
    "title": "3  Bayesian Learning",
    "section": "3.3 Beta-Binomial Model",
    "text": "3.3 Beta-Binomial Model\nThe Beta-Binomial Bayesian model is a statistical model that is used when we are interested in learning about a proportion or probability of success, denoted by \\(p\\). This model is particularly useful when dealing with binary data such as conversions or clicks in A/B testing.\nIn the Beta-Binomial model, we assume that the probability of success \\(\\theta\\) in each of \\(n\\) Bernoulli trials is not fixed but randomly drawn from a Beta distribution. The Beta distribution is defined by two shape parameters, \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\).\nThe model combines the prior information about \\(\\theta\\) (represented by the Beta distribution) and the observed data (represented by the Binomial distribution) to update our beliefs about \\(p\\). This is done using Bayes’ Rule, which in this context can be written as: \\[\np(\\theta \\mid Y) = \\dfrac{p(Y \\mid \\theta)p(\\theta)}{p(Y)}\n\\] where \\(p(\\theta)\\) is the prior distribution (Beta), \\(p(Y \\mid \\theta)\\) is the likelihood function (Binomial), and \\(p(\\theta\\mid Y)\\) is the posterior distribution.\nThe Beta distribution is a family of continuous probability distributions defined on the interval [0,1] in terms of two positive parameters, denoted by alpha (\\(\\alpha\\)) and beta (\\(\\beta\\)), that appear as exponents of the variable and its complement to 1, respectively, and control the shape of the distribution. The Beta distribution is frequently used in Bayesian statistics, empirical Bayes methods, and classical statistics to model random variables with values falling inside a finite interval.\nThe probability density function (PDF) of the Beta distribution is given by: \\[\nBeta(x; \\alpha, \\beta) = \\frac{x^{\\alpha - 1}(1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\] where \\(x \\in [0, 1]\\), \\(\\alpha &gt; 0\\), \\(\\beta &gt; 0\\), and \\(B(\\alpha, \\beta)\\) is the beta function. It is simply a normalizing constant \\[\nB\\left( a,A\\right)  =\\int_{0}^{1}\\theta^{a-1}\\left(  1-\\theta\\right)\n^{A-1}d\\theta .\n\\]\nThe mean and variance of the Beta distribution are given by: \\[\n\\begin{aligned}\n\\mu &= \\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\sigma^2 &= \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n\\end{aligned}\n\\] where \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance.\nThe Beta-Binomial model is one of the simplest Bayesian models and is widely used in various fields including epidemiology, intelligence testing, and marketing. It provides the tools we need to study the proportion of interest, \\(p\\), in a variety of settings.\nThe nice property of the Beta-Binomial model is that the posterior \\(p(p\\mid Y)\\) is yet another Beta distribution. Beta is called a conjugate prior for the Binomial likelihood and is a very useful property. Given that we observed \\(x\\) successful outcome \\[\nY = \\sum_{i=1}^n Y_i\n\\] the posterior distribution is given by \\[\np(\\theta\\mid Y) =Beta(Y+\\alpha, n-Y+\\beta)\n\\] Here the count of successful outcome \\(Y\\) acts as a sufficient statistic for the parameter \\(p\\). This means that the posterior distribution depends on the data only through the sufficient statistic \\(Y\\). This is a very useful property and is a consequence of the conjugacy of the Beta prior and Binomial likelihood.\n\nExample 3.2 (Black Swans) A related problem is the Black Swan inference problem. Suppose that after \\(n\\) trials where \\(n\\) is large you have only seen successes and that you assess the probability of the next trial being a success as \\((T+1)/(T+2)\\) that is, almost certain. This is a model of observing White Swans and having never seen a Black Swan. Taleb (2007) makes it sound as if the rules of probability are not rich enough to be able to handle Black Swan events. There is a related class of problems in finance known as Peso problems where countries decide to devalue their currencies and there is little a prior evidence from recent history that such an event is going to happen.\nTo obtain such a probability assessment we use a Binomial/Beta conjugate Bayes updating model. The key point is that it can also explain that there is still a large probability of a Black Swan event to happen sometime in the future. Independence model has difficulty doing this.\nThe Bayes Learning Beta-Binomial model will have no problem. We model with where \\(Y_{t}=0\\) or \\(1\\), with probability \\(P\\left( Y_{t}=1\\mid \\theta\\right) =\\theta\\). This is the classic Bernoulli “coin-flipping” model and is a component of more general specifications such as regime switching or outlier-type models.\nLet \\(Y = \\sum_{t=1}^{T}y_{t}\\) be the number of observed successful outcomes. The likelihood for a sequence of Bernoulli observations is then \\[\np\\left(  y\\mid \\theta\\right)  =\\prod_{t=1}^{T}p\\left(  y_{t}\\mid \\theta\\right)\n=\\theta^{Y}\\left(  1-\\theta\\right)^{T-Y}.\n\\] The maximum likelihood estimator is the sample mean, \\(\\widehat{\\theta} = T^{-1}Y\\). This makes little sense when you just observe white swans. It predicts \\(\\hat{\\theta} = 1\\) and gets shocked when it sees a black swan (zero probability event). Bayes, on the other hand, allows for “learning”.\nBayes rule then tell us how to combine the likelihood and prior to obtain a posterior distribution, namely \\(\\theta \\mid Y=y\\). What do we believe about \\(\\theta\\) given a sequence of. Our predictor rule is then \\(P(Y_{t=1} =1 \\mid Y=y ) = \\mathbb{E}(\\theta \\mid y)\\) it is straightforward to show that the posterior distribution is again a Beta distribution with \\[\np\\left( \\theta\\mid y\\right)  \\sim Beta\\left(  a_{T},A_{T}\\right)  \\; \\mathrm{ and} \\;  a_{T}=a+k , A_{T}=A+T-k.\n\\]\nThere is a “conjugate” form of the posterior: it is also a Beta distribution and the hyper-parameters \\(a_{T}\\) and \\(A_{T}\\) depend on the data only via the sufficient statistics, \\(T\\) and \\(k\\). The posterior mean and variance are \\[\n\\mathbb{E}\\left[ \\theta\\mid y\\right]  =\\frac{a_{T}}{a_{T}+A_{T}} \\;\\text{ and }\\; \\Var{\n\\theta\\mid y}  =\\frac{a_{T}A_{T}}{\\left(  a_{T}+A_{T}\\right)  ^{2}\\left(   a_{T}+A_{T}+1\\right)  }\\text{,}\n\\] respectively. This implies that for large samples, \\(\\E{\\theta\\mid y} \\approx \\bar{y} = \\widehat{\\theta}\\), the MLE.\nSuppose that after \\(n\\) trials where \\(n\\) is large you have only seen successes and that you assess the probability of the next trial being a success as \\((T+1)/(T+2)\\) that is, almost certain. This is a model of observing White Swans and having never seen a Black Swan. (Taleb, 2008, The Black Swan: the Impact of the Highly Improbable).\nTo obtain such a probability assessment a natural model is Binomial/Beta conjugate Bayesian updating model. We can access the probability that a black Swan event to happen sometime in the future.\nFor the purpose of illustration, start with a uniform prior specification, \\(\\theta \\sim \\mathcal{U}(0,1)\\), then we have the following probability assessment. After \\(T\\) trials, suppose that we have only seen \\(T\\) successes, namely, \\(( y_1 , \\ldots , y_T ) = ( 1 , \\ldots , 1 )\\). Then you assess the probability of the next trial being a success as \\[\np( Y_{T+1} =1 \\mid y_1=1 , \\ldots , y_T=1 ) = \\frac{T+1}{T+2}\n\\] This follows from the mean of the Beta posterior, \\[\n\\theta \\mid y \\sim \\text{Beta}(T+1, T+1), ~ P(Y_{T+1} = 1 \\mid y) = \\mathbb{E}_{\\theta \\mid y}\\left[P(Y_{T=1} \\mid \\theta) \\right] = \\mathbb{E}[\\theta \\mid y].\n\\] For large \\(T\\) this is almost certain.\nNow consider a future set of \\(n\\) trials, where \\(n\\) is also large. The probability of never seeing a Black Swan is then given by \\[\np( y_{T+1} =1 , \\ldots ,  y_{T+n} = 1 \\mid y_1=1 , \\ldots , y_T=1 ) = \\frac{ T+1 }{ T+n+1 }\n\\] For a fixed \\(T\\), and large \\(n\\), we have \\(\\frac{ T+1 }{ T+n+1 } \\rightarrow 0\\). Hence, we will see a Black Swan event with large probability — we just don’t know when! The exchangeable Beta-Binomial model then implies that a Black Swan event will eventually appear. One shouldn’t be that surprised when it actually happens.\n\n\nExample 3.3 (Clinical trials) Consider a problem of designing clinical trials in which \\(K\\) possible drugs \\(a\\in 1,\\dots,K\\) need to be tested. The outcome of the treatment with drug \\(a\\) is binary \\(y(a) \\in \\{0,1\\}\\). We use Bernoulli distribution with mean \\(f(a)\\) to model the outcome. Thus, the full probabilistic model is described by \\(w = f(1),\\dots,f(K)\\). Say we have observed a sample \\(D = \\{y_1,\\dots,y_n\\}\\). We would like to compute posterior distribution over \\(w\\). We start with \\(Beta\\) prior \\[\np(w\\mid \\alpha,\\beta) = \\prod_{a=1}^K Beta(w_a\\mid \\alpha,\\beta)    \n\\] Then posterior distribution is given by \\[\np(w\\mid D) = \\prod_{a=1}^K Beta(w_a\\mid \\alpha + n_{a,1},\\beta + n_{a,0})   \n\\]\nThis setup allows us to perform sequential design of experiment. The simplest version of it is called the Thompson sampling. After observing \\(n\\) patients, we draw a single sample \\(\\tilde w\\) from the posterior and then maximize the resulting surrogate \\[\na_{n+1} = \\argmax_{a} f_{\\tilde w}(a), ~~~ \\tilde{w} \\sim p(w\\mid D)\n\\]\n\n\nExample 3.4 (Shrinkage and baseball batting averages) The batter-pitcher match-up is a fundamental element of a baseball game. There are detailed baseball records that are examined regularly by fans and professionals. This data provides a good illustration of Bayesian hierarchical methods. There is a great deal of prior information concerning the overall ability of a player. However, we only see a small amount of data about a particular batter-pitcher match-up. Given the relative small sample size, to determine our optimal estimator we build a hierarchical model taking into account the within pitcher variation.\nLet’s analyze the variability in Jeter’s \\(2006\\) season. Let \\(p_{i}\\) denote Jeter’s ability against pitcher \\(i\\) and assume that \\(p_{i}\\) varies across the population of pitchers according to a particular probability distribution \\((p_{i} \\mid \\alpha,\\beta)\\sim Be(\\alpha,\\beta)\\). To account for extra-binomial variation we use a hierarchical model for the observed number of hits \\(y_{i}\\) of the form \\[\n(y_{i} \\mid p_{i})\\sim Bin(T_{i},p_{i})\\;\\;\\mathrm{with}\\;\\;p_{i}\\sim\nBe(\\alpha,\\beta)\n\\] where \\(T_{i}\\) is the number of at-bats against pitcher \\(i\\). A priori we have a prior mean given by \\(E(p_{i})=\\alpha/(\\alpha+\\beta)=\\bar{p}\\). The extra heterogeneity leads to a prior variance \\(Var(p_{i})=\\bar{p}(1-\\bar{p})\\phi\\) where \\(\\phi=(\\alpha+\\beta+1)^{-1}\\). Hence \\(\\phi\\) measures how concentrated the beta distribution is around its mean, \\(\\phi=0\\) means highly concentrated and \\(\\phi=1\\) means widely dispersed. \nThis model assumes that each player \\(i\\) has a true ability \\(p_{i}\\) that is drawn from a common distribution. The model is hierarchical in the sense that the parameters \\(\\alpha\\) and \\(\\beta\\) are estimated from the data. The model is also a shrinkage model in the sense that the estimates of \\(p_{i}\\) are shrunk towards the overall mean \\(\\bar{p}_{i}\\). In reality, we don’t know that each \\(p_i\\) exists. We also don’t know if it follows a Binomial distribution with the Beta prior. We are making a model assumption. However, the model is a good approximation to the data and is a good way to estimate the parameters.\nStern et al. (2007) estimates the parameter \\(\\hat{\\phi} = 0.002\\) for Derek Jeter, showing that his ability varies a bit but not very much across the population of pitchers. The effect of the shrinkage is not surprising. The extremes are shrunk the most with the highest degree of shrinkage occurring for the match-ups that have the smallest sample sizes. The amount of shrinkage is related to the large amount of prior information concerning Jeter’s overall batting average. Overall Jeter’s performance is extremely consistent across pitchers as seen from his estimates. Jeter had a season \\(.308\\) average. We see that his Bayes estimates vary from\\(.311\\) to\\(.327\\) and that he is very consistent. If all players had a similar record then the assumption of a constant batting average would make sense.\n\n\n\nPitcher\nAt-bats\nHits\nObsAvg\nEstAvg\n95% Int\n\n\n\n\nR. Mendoza\n6\n5\n.833\n.322\n(.282, .394)\n\n\nH. Nomo\n20\n12\n.600\n.326\n(.289, .407)\n\n\nA.J.Burnett\n5\n3\n.600\n.320\n(.275, .381)\n\n\nE. Milton\n28\n14\n.500\n.324\n(.291, .397)\n\n\nD. Cone\n8\n4\n.500\n.320\n(.218, .381)\n\n\nR. Lopez\n45\n21\n.467\n.326\n(.291, .401)\n\n\nK. Escobar\n39\n16\n.410\n.322\n(.281, .386)\n\n\nJ. Wettland\n5\n2\n.400\n.318\n(.275, .375)\n\n\nT. Wakefield\n81\n26\n.321\n.318\n(.279, .364)\n\n\nP. Martinez\n83\n21\n.253\n.312\n(.254, .347)\n\n\nK. Benson\n8\n2\n.250\n.317\n(.264, .368)\n\n\nT. Hudson\n24\n6\n.250\n.315\n(.260, .362)\n\n\nJ. Smoltz\n5\n1\n.200\n.314\n(.253, .355)\n\n\nF. Garcia\n25\n5\n.200\n.314\n(.253, .355)\n\n\nB. Radke\n41\n8\n.195\n.311\n(.247, .347)\n\n\nD. Kolb\n5\n0\n.000\n.316\n(.258, .363)\n\n\nJ. Julio\n13\n0\n.000\n.312\n(.243, .350 )\n\n\nTotal\n6530\n2061\n.316\n\n\n\n\n\nSome major league managers believe strongly in the importance of such data (Tony La Russa, Three days in August). One interesting example is the following. On Aug 29, 2006, Kenny Lofton (career \\(.299\\) average, and current \\(.308\\) average for \\(2006\\) season) was facing the pitcher Milton (current record \\(1\\) for \\(19\\)). He was rested and replaced by a \\(.273\\) hitter. Is putting in a weaker player rally a better bet? Was this just an over-reaction to bad luck in the Lofton-Milton match-up? Statistically, from Lofton’s record against Milton we have \\(P\\left( \\leq 1\\;\\mathrm{hit\\;in}\\ 19\\;\\mathrm{attempts} \\mid p=0.3\\right) =0.01\\) an unlikely \\(1\\)-in-\\(100\\) event. However, we have not taken into account the multiplicity of different batter-pitcher match-ups. We know that Loftin’s batting percentage will vary across different pitchers, it’s just a question of how much? A hierarchical analysis of Lofton’s variability gave a \\(\\phi=0.008\\) – four times larger than Jeter’s \\(\\phi=0.002\\). Lofton has batting estimates that vary from\\(.265\\) to \\(.340\\) with the lowest being against Milton. Hence, the optimal estimate for a pitch against Milton is \\(.265&lt;.275\\) and resting Lofton against Milton is justified by this analysis.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#poisson-model-for-count-data",
    "href": "03-bl.html#poisson-model-for-count-data",
    "title": "3  Bayesian Learning",
    "section": "3.4 Poisson Model for Count Data",
    "text": "3.4 Poisson Model for Count Data\nThe Poisson distribution is obtained as a result of the Binomial when \\(p\\) is small and \\(n\\) is large. In applications, the Poisson models count data. Suppose we want to model arrival rate of users to one of our stores. Let \\(\\lambda = np\\), which is fixed and take the limit as \\(n \\rightarrow \\infty\\). There is a relationship between , \\(p(x)\\) ans \\(p(x+1)\\) given by \\[\n\\dfrac{p(x+1)}{p(x)}= \\dfrac{\\left(\\dfrac{n}{x+1}\\right)p^{x+1}(1-p)^{n-x-1}}{\\left(\\dfrac{n}{x}\\right)p^{x}(1-p)^{n-x}} \\approx \\dfrac{np}{x+1}\n\\] If we approximate \\(p(x+1)\\approx \\lambda p(x)/(x+1)\\) with \\(\\lambda=np\\), then we obtain the Poisson pdf given by \\(p(x) = p(0)\\lambda^x/x!\\). To ensure that \\(\\sum_{x=0}^\\infty p(x) = 1\\), we set \\[\nf(0) = \\dfrac{1}{\\sum_{x=0}^{\\infty}\\lambda^x/x!} = e^{-\\lambda}.\n\\] The above equality follows from the power series property of the exponent function \\[\ne^{\\lambda} = \\sum_{x=0}^{\\infty}\\dfrac{\\lambda^x}{x!}\n\\] The Poisson distribution counts the occurrence of events. Given a rate parameter, denoted by \\(\\lambda\\), we calculate probabilities as follows \\[\np( X = x ) = \\frac{ e^{-\\lambda} \\lambda^x }{x!} \\; \\mathrm{ where} \\; x=0,1,2,3, \\ldots\n\\] The mean and variance of the Poisson are given by:\n\n\n\nPoisson Distribution\nParameters\n\n\n\n\nExpected value\n\\(\\mu = \\E{X} = \\lambda\\)\n\n\nVariance\n\\(\\sigma^2 = \\Var{X} = \\lambda\\)\n\n\n\nHere \\(\\lambda\\) denotes the rate of occurrence of an event.\nConsider the problem of modeling soccer scores in the English Premier League (EPL) games. We use data from Betfair, a website, which posts odds on many football games. The goal is to calculate odds for the possible scores in a match. \\[\n0-0, \\; 1-0, \\; 0-1, \\; 1-1, \\; 2-0, \\ldots\n\\] Another question we might ask, is what’s the odds of a team winning?\nThis is given by \\(P\\left ( X&gt; Y \\right )\\). The odds’s of a draw are given by \\(P \\left ( X = Y \\right )\\)?\nProfessional sports betters rely on sophisticated statistical models to predict the outcomes. Instead, we present a simple, but useful model for predicting outcomes of EPL games. We follow the methodology given in Spiegelhalter and Ng (2009).\nFirst, load the data and then model the number of goals scored using Poisson distribution.\nsdcdsc\n\ndf = read.csv(\"../data/epl.csv\")\nknitr::kable(head(df[,c(\"home_team_name\",\"away_team_name\",\"home_score\",\"guest_score\")]))\n\n\n\n\nhome_team_name\naway_team_name\nhome_score\nguest_score\n\n\n\n\nArsenal\nLiverpool\n3\n4\n\n\nBournemouth\nManchester United\n1\n3\n\n\nBurnley\nSwansea\n0\n1\n\n\nChelsea\nWest Ham\n2\n1\n\n\nCrystal Palace\nWest Bromwich Albion\n0\n1\n\n\nEverton\nTottenham\n1\n1\n\n\n\n\n\nLet’s look at the empirical distribution across the number of goals scored by Manchester United\n\nteam_name=\"Manchester United\" \nteam_for  = c(df[df$home_team_name==team_name,\"home_score\"],df[df$away_team_name==team_name,\"guest_score\"]) \nn = length(team_for) \nfor_byscore = table(team_for)/n \nbarplot(for_byscore, col=\"coral\", main=\"\")\n\n\n\n\nHistogram of Goals Scored by MU\n\n\n\n\nHence the historical data fits closely to a Poisson distribution, the parameter \\(\\lambda\\) describes the average number of goals scored and we calculate it by calculating the sample mean, the maximum likelihood estimate. A Bayesian method where we assume that \\(\\lambda\\) has a Gamma prior is also available. This lets you incorporate outside information into the predictive model.\n\nlambda_for = mean(team_for) \nbarplot(rbind(dpois(0:4, lambda = lambda_for),for_byscore),beside = T, col=c(\"aquamarine3\",\"coral\"), xlab=\"Goals\", ylab=\"probability\", main=\"\") \nlegend(\"topright\", c(\"Poisson\",\"MU\"), pch=15, col=c(\"aquamarine3\", \"coral\"), bty=\"n\")\n\n\n\n\nHistogram vs Poisson Model Prediction of Goals Scored by MU\n\n\n\n\nNow we will use Poisson model and Monte Carlo simulations to predict possible outcomes of the MU vs Hull games. First we estimate the rate parameter for goals by MU lmb_mu and goals by Hull lmb_h. Each team played a home and away game with every other team, thus 38 total games was played by all teams. We calculate the average by dividing total number of goals scored by the number of games\n\nsumdf = df %&gt;% \n  group_by(home_team_name) %&gt;% \n  summarise(Goals_For_Home = sum(home_score)) %&gt;%\n  full_join(df %&gt;% \n              group_by(away_team_name) %&gt;% \n              summarise(Goals_For_Away = sum(guest_score)), by = c(\"home_team_name\" = \"away_team_name\")\n            ) %&gt;%\n  full_join(df %&gt;% \n              group_by(home_team_name) %&gt;% \n              summarise(Goals_Against_Home = sum(guest_score))\n            ) %&gt;%\n  full_join(df %&gt;% \n              group_by(away_team_name) %&gt;%\n              summarise(Goals_Against_Away = sum(home_score)), by = c(\"home_team_name\" = \"away_team_name\")\n            ) %&gt;%\n  rename(Team=home_team_name)\nknitr::kable(sumdf[sumdf$Team %in% c(\"Manchester United\", \"Hull\"),])\n\n\n\n\n\n\n\n\n\n\n\nTeam\nGoals_For_Home\nGoals_For_Away\nGoals_Against_Home\nGoals_Against_Away\n\n\n\n\nHull\n28\n9\n35\n45\n\n\nManchester United\n26\n28\n12\n17\n\n\n\n\n\n\nlmb_mu = (26+28)/38; print(lmb_mu)\n\n 1.4\n\nlmb_h = (28+9)/38; print(lmb_h)\n\n 0.97\n\n\nNow we simulate 100 games between the teams\n\nx = rpois(100,lmb_mu)\ny = rpois(100,lmb_h)\nsum(x&gt;y)\n\n 57\n\nsum(x==y)\n\n 18\n\nknitr::kable(table(x,y))\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\n8\n4\n6\n0\n0\n\n\n1\n22\n5\n10\n1\n1\n\n\n2\n7\n11\n4\n3\n0\n\n\n3\n5\n5\n3\n1\n0\n\n\n4\n2\n1\n1\n0\n0\n\n\n\n\n\nFrom our simulation that 57 number of times MU wins and 18 there is a draw. The actual outcome was 0-0 (Hull at MU) and 0-1 (Mu at Hull). Thus our model fives a reasonable prediction.\nThe model can be improved by calculating different averages for home and away games. For example, Hull does much better at home games compared to away games. Further, we can include the characteristics of the opponent team to account for interactions between attack strength (number of scored) and defense weakness of the opponent. Now we modify our value of expected goals for each of the teams by calculating \\[\n\\hat \\lambda = \\lambda \\times  \\text{Defense weakness}\n\\]\nLet’s model the MU at Hull game. The average away goals for MU \\(28/19 = 1.47\\) and the defense weakness of Hull is \\(36/19 = 1.84\\), thus the adjusted expected number of goals to be scored by MU is 2.79. Similarly, the adjusted number of the goals Hull is expected to score is \\(28/19 \\times 17/19 = 1.32\\)\nAs a result of the simulation, we obtain\n\nset.seed(1)\nx &lt;- rpois(100, 28 / 19 * 35 / 19)\ny &lt;- rpois(100, 28 / 19 * 17 / 19)\nknitr::kable(table(x, y))\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n1\n3\n0\n0\n0\n0\n\n\n1\n3\n5\n6\n1\n1\n0\n\n\n2\n4\n16\n7\n3\n0\n0\n\n\n3\n6\n7\n2\n3\n0\n0\n\n\n4\n4\n7\n5\n2\n1\n0\n\n\n5\n2\n3\n1\n2\n0\n2\n\n\n6\n1\n0\n0\n1\n0\n0\n\n\n7\n1\n0\n0\n0\n0\n0\n\n\n\n\nimage(z = table(x, y), x = 0:7, y = 0:5, xlab = \"MU Score\", ylab = \"Hull Score\")\n\n\n\n\n\n\n\n\nNow we can calculate the number of times MU wins:\n\nsum(x &gt; y)\n\n 67\n\n\n\n\n\n\nA model is only as good as its predictions. Let’s see how our model did in out-of-sample prediction,\n\nMan U wins 67 games out of 100, we should bet when odds ratio is below 67 to 100.\nMost likely outcome is 1-2 (16 games out of 100)\nThe actual outcome was 0-1 (they played on August 27, 2016)\nIn out simulation 0-1 was the fourth most probable outcome (8 games out of 100).\n\n\nExample 3.5 (EPL Betting) Fen et al. (2016) employ a Skellam process (a difference of Poisson random variables) to model real-time betting odds for English Premier League (EPL) soccer games. Given a matrix of market odds on all possible score outcomes, we estimate the expected scoring rates for each team. The expected scoring rates then define the implied volatility of an EPL game. As events in the game evolve, they re-estimate the expected scoring rates and our implied volatility measure to provide a dynamic representation of the market’s expectation of the game outcome. They use real-time market odds data for a game between Everton and West Ham in the 2015-2016 season. We show how the implied volatility for the outcome evolves as goals, red cards, and corner kicks occur.\nGambling on soccer is a global industry with revenues of over $1 trillion a year (see “Football Betting - the Global Gambling Industry worth Billions,” BBC Sport). Betting on the result of a soccer match is a rapidly growing market, and online real-time odds exist (Betfair, Bet365, Ladbrokes). Market odds for all possible score outcomes (\\(0-0, 1-0, 0-1, 2-0, \\ldots\\)) as well as outright win, lose, and draw are available in real time. In this paper, we employ a two-parameter probability model based on a Skellam process and a non-linear objective function to extract the expected scoring rates for each team from the odds matrix. The expected scoring rates then define the implied volatility of the game.\nSkellam Process\nTo model the outcome of a soccer game between team A and team B, we let the difference in scores, \\(N(t) = N_A(t) - N_B(t)\\), where \\(N_A(t)\\) and \\(N_B(t)\\) are the team scores at time point \\(t\\). Negative values of \\(N(t)\\) indicate that team A is behind. We begin at \\(N(0) = 0\\) and end at time one with \\(N(1)\\) representing the final score difference. The probability \\(\\mathbb{P}(N(1) &gt; 0)\\) represents the ex-ante odds of team A winning. Half-time score betting, which is common in Europe, is available for the distribution of \\(N(\\frac{1}{2})\\).\nThen we find a probabilistic model for the distribution of \\(N(1)\\) given \\(N(t) = \\ell\\), where \\(\\ell\\) is the current lead. This model, together with the current market odds, can be used to infer the expected scoring rates of the two teams and then to define the implied volatility of the outcome of the match. We let \\(\\lambda^A\\) and \\(\\lambda^B\\) denote the expected scoring rates for the whole game. We allow for the possibility that the scoring abilities (and their market expectations) are time-varying, in which case we denote the expected scoring rates after time \\(t\\) by \\(\\lambda^A_t\\) and \\(\\lambda^B_t\\), respectively, instead of \\(\\lambda^A(1-t)\\) and \\(\\lambda^B(1-t)\\).\nThe Skellam distribution is defined as the difference between two independent Poisson variables given by:\n\\[\n\\begin{aligned}\nN_A(t) &= W_A(t) + W(t) \\\\\nN_B(t) &= W_B(t) + W(t)\n\\end{aligned}\n\\]\nwhere \\(W_A(t)\\), \\(W_B(t)\\), and \\(W(t)\\) are independent processes with:\n\\[\nW_A(t) \\sim \\text{Poisson}(\\lambda^A t), \\quad W_B(t) \\sim \\text{Poisson}(\\lambda^B t).\n\\]\nHere \\(W(t)\\) is a non-negative integer-valued process to induce a correlation between the numbers of goals scored. By modeling the score difference, \\(N(t)\\), we avoid having to specify the distribution of \\(W(t)\\) as the difference in goals scored is independent of \\(W(t)\\). Specifically, we have a Skellam distribution:\n\\[\nN(t) = N_A(t) - N_B(t) \\sim \\text{Skellam}(\\lambda^A t, \\lambda^B t).\n\\tag{3.2}\\]\nAt time \\(t\\), we have the conditional distributions:\n\\[\n\\begin{aligned}\nW_A(1) - W_A(t) &\\sim \\text{Poisson}(\\lambda^A(1-t)) \\\\\nW_B(1) - W_B(t) &\\sim \\text{Poisson}(\\lambda^B(1-t)).\n\\end{aligned}\n\\]\nNow letting \\(N^*(1-t)\\), the score difference of the sub-game which starts at time \\(t\\) and ends at time 1 and the duration is \\((1-t)\\). By construction, \\(N(1) = N(t) + N^*(1-t)\\). Since \\(N^*(1-t)\\) and \\(N(t)\\) are differences of two Poisson process on two disjoint time periods, by the property of Poisson process, \\(N^*(1-t)\\) and \\(N(t)\\) are independent. Hence, we can re-express equation (Equation 3.2) in terms of \\(N^*(1-t)\\), and deduce\n\\[\n%N^*(1-t) = W^*_A(1-t) - W^*_B(1-t) \\sim Skellam(\\lambda^A (1-t),\\lambda^B (1-t) )\nN^*(1-t) = W^*_A(1-t) - W^*_B(1-t) \\sim \\text{Skellam}(\\lambda^A_t,\\lambda^B_t)\n\\]\nwhere \\(W^*_A(1-t) = W_A(1) - W_A(t)\\), \\(\\lambda^A = \\lambda^A_0\\) and \\(\\lambda^A_t=\\lambda^A(1-t)\\). A natural interpretation of the expected scoring rates, \\(\\lambda^A_t\\) and \\(\\lambda^B_t\\), is that they reflect the “net” scoring ability of each team from time \\(t\\) to the end of the game. The term \\(W(t)\\) models a common strength due to external factors, such as weather. The “net” scoring abilities of the two teams are assumed to be independent of each other as well as the common strength factor. We can calculate the probability of any particular score difference, given by \\(\\mathbb{P}(N(1)=x|\\lambda^A,\\lambda^B)\\), at the end of the game where the \\(\\lambda\\)’s are estimated from the matrix of market odds. Team strength and “net” scoring ability can be influenced by various underlying factors, such as the offensive and defensive abilities of the two teams. The goal of our analysis is to only represent these parameters at every instant as a function of the market odds matrix for all scores.\nAnother quantity of interest is the conditional probability of winning as the game progresses. If the current lead at time \\(t\\) is \\(\\ell\\), and \\(N(t)=\\ell=N_A(t)-N_B(t)\\), the Poisson property implied that the final score difference \\((N(1)|N(t)=\\ell)\\) can be calculated by using the fact that \\(N(1)=N(t)+N^*(1-t)\\) and \\(N(t)\\) and \\(N^*(1-t)\\) are independent. Specifically, conditioning on \\(N(t)=\\ell\\), we have the identity\n\\[ N(1)=N(t)+N^*(1-t)=\\ell+\\text{Skellam}(\\lambda^A_t,\\lambda^B_t). \\]\nWe are now in a position to find the conditional distribution (\\(N(1)=x|N(t)=\\ell\\)) for every time point \\(t\\) of the game given the current score. Simply put, we have the time homogeneous condition\n\\[\n\\begin{aligned}\n\\mathbb{P}(N(1)=x|\\lambda^A_t,\\lambda^B_t,N(t)=\\ell) &= \\mathbb{P}(N(1)-N(t)=x-\\ell |\\lambda^A_t,\\lambda^B_t,N(t)=\\ell) \\\\\n&= \\mathbb{P}(N^* (1-t)=x-\\ell |\\lambda^A_t,\\lambda^B_t)\n\\end{aligned}\n\\]\nwhere \\(\\lambda^A_t\\), \\(\\lambda^B_t\\), \\(\\ell\\) are given by market expectations at time \\(t\\). See Feng et al. for details.\nMarket Calibration\nOur information set at time \\(t\\) includes the current lead \\(N(t) = \\ell\\) and the market odds for \\(\\{Win, Lose, Draw, Score\\}_t\\), where \\(Score_t = \\{ ( i - j ) : i, j = 0, 1, 2, ....\\}\\). These market odds can be used to calibrate a Skellam distribution which has only two parameters \\(\\lambda^A_t\\) and \\(\\lambda^B_t\\). The best fitting Skellam model with parameters \\(\\{\\hat\\lambda^A_t,\\hat\\lambda^B_t\\}\\) will then provide a better estimate of the market’s information concerning the outcome of the game than any individual market (such as win odds) as they are subject to a “vig” and liquidity. Suppose that the fractional odds for all possible final score outcomes are given by a bookmaker. In this case, the bookmaker pays out three times the amount staked by the bettor if the outcome is indeed 2-1. Fractional odds are used in the UK, while money-line odds are favored by American bookmakers with \\(2:1\\) (“two-to-one”) implying that the bettor stands to make a $200 profit on a $100 stake. The market implied probability makes the expected winning amount of a bet equal to 0. In this case, the implied probability \\(p=1/(1+3)=1/4\\) and the expected winning amount is \\(\\mu=-1*(1-1/4)+3*(1/4)=0\\). We denote this odds as \\(odds(2,1)=3\\). To convert all the available odds to implied probabilities, we use the identity\n\\[ \\mathbb{P}(N_A(1) = i, N_B(1) = j)=\\frac{1}{1+odds(i,j)}. \\]\nThe market odds matrix, \\(O\\), with elements \\(o_{ij}=odds(i-1,j-1)\\), \\(i,j=1,2,3...\\) provides all possible combinations of final scores. Odds on extreme outcomes are not offered by the bookmakers. Since the probabilities are tiny, we set them equal to 0. The sum of the possible probabilities is still larger than 1 (see Dixon and Coles (1997) and Dixon and Coles (1997)). This “excess” probability corresponds to a quantity known as the “market vig.” For example, if the sum of all the implied probabilities is 1.1, then the expected profit of the bookmaker is 10%. To account for this phenomenon, we scale the probabilities to sum to 1 before estimation.\nTo estimate the expected scoring rates, \\(\\lambda^A_t\\) and \\(\\lambda^B_t\\), for the sub-game \\(N^*(1-t)\\), the odds from a bookmaker should be adjusted by \\(N_A(t)\\) and \\(N_B(t)\\). For example, if \\(N_A(0.5)=1\\), \\(N_B(0.5)=0\\) and \\(odds(2,1)=3\\) at half time, these observations actually says that the odds for the second half score being 1-1 is 3 (the outcomes for the whole game and the first half are 2-1 and 1-0 respectively, thus the outcome for the second half is 1-1). The adjusted \\({odds}^*\\) for \\(N^*(1-t)\\) is calculated using the original odds as well as the current scores and given by\n\\[\n{odds}^*(x,y)=odds(x+N_A(t),y+N_B(t)).\n\\]\nAt time \\(t\\) \\((0\\leq t\\leq 1)\\), we calculate the implied conditional probabilities of score differences using odds information\n\\[\n\\mathbb{P}(N(1)=k|N(t)=\\ell)=\\mathbb{P}(N^*(1-t)=k-\\ell)=\\frac{1}{c}\\sum_{i-j=k-\\ell}\\frac{1}{1+{odds}^*(i,j)}\\]\nwhere \\(c=\\sum_{i,j} \\frac{1}{1+{odds}^*(i,j)}\\) is a scale factor, \\(\\ell=N_A(t)-N_B(t)\\), \\(i,j\\geq 0\\) and \\(k=0,\\pm 1,\\pm 2\\ldots\\).\nExample: Everton vs West Ham (3/5/2016)\nTable below shows the implied Skellam probabilities.\n\n\n\nTable 3.1: Table: Original odds data from Ladbrokes before the game started.\n\n\n\n\n\nEverton  West Ham\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n11/1\n12/1\n28/1\n66/1\n200/1\n450/1\n\n\n1\n13/2\n6/1\n14/1\n40/1\n100/1\n350/1\n\n\n2\n7/1\n7/1\n14/1\n40/1\n125/1\n225/1\n\n\n3\n11/1\n11/1\n20/1\n50/1\n125/1\n275/1\n\n\n4\n22/1\n22/1\n40/1\n100/1\n250/1\n500/1\n\n\n5\n50/1\n50/1\n90/1\n150/1\n400/1\n\n\n\n6\n100/1\n100/1\n200/1\n250/1\n\n\n\n\n7\n250/1\n275/1\n375/1\n\n\n\n\n\n8\n325/1\n475/1\n\n\n\n\n\n\n\n\n\n\nTable Table 3.1 shows the raw data of odds right the game. We need to transform odds data into probabilities. For example, for the outcome 0-0, 11/1 is equivalent to a probability of 1/12. Then we can calculate the marginal probability of every score difference from -4 to 5. We neglect those extreme scores with small probabilities and rescale the sum of event probabilities to one.\n\n\n\nTable 3.2: Market implied probabilities for the score differences versus Skellam implied probabilities at different time points. The estimated parameters \\(\\hat\\lambda^A=2.33\\), \\(\\hat\\lambda^B=1.44\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScore difference\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\n\n\n\n\nMarket Prob. (%)\n1.70\n2.03\n4.88\n12.33\n21.93\n22.06\n16.58\n9.82\n4.72\n2.23\n\n\nSkellam Prob. (%)\n0.78\n2.50\n6.47\n13.02\n19.50\n21.08\n16.96\n10.61\n5.37\n2.27\n\n\n\n\n\n\nTable Table 3.2 shows the model implied probability for the outcome of score differences before the game, compared with the market implied probability. As we see, the Skellam model appears to have longer tails. Different from independent Poisson modeling in Dixon and Coles (1997), our model is more flexible with the correlation between two teams. However, the trade-off of flexibility is that we only know the probability of score difference instead of the exact scores.\n\n\n\n\n\n\nFigure 3.1: The betting market data for Everton and West Ham is from ladbrokes.com. Market implied probabilities (expressed as percentages) for three different results (Everton wins, West Ham wins and draw) are marked by three distinct colors, which vary dynamically as the game proceeds. The solid black line shows the evolution of the implied volatility. The dashed line shows significant events in the game, such as goals and red cards. Five goals in this game are 13’ Everton, 56’ Everton, 78’ West Ham, 81’ West Ham and 90’ West Ham.\n\n\n\nFigure Figure 3.1 examines the behavior of the two teams and represent the market predictions on the final result. Notably, we see the probability change of win/draw/loss for important events during the game: goals scoring and a red card penalty. In such a dramatic game, the winning probability of Everton gets raised to 90% before the first goal of West Ham in 78th minutes. The first two goals scored by West Ham in the space of 3 minutes completely reverses the probability of winning. The probability of draw gets raised to 90% until we see the last-gasp goal of West Ham that decides the game.\nFigure Figure 3.1 plots the path of implied volatility throughout the course of the game. Instead of a downward sloping line, we see changes in the implied volatility as critical moments occur in the game. The implied volatility path provides a visualization of the conditional variation of the market prediction for the score difference. For example, when Everton lost a player by a red card penalty at 34th minute, our estimates \\(\\hat\\lambda^A_t\\) and \\(\\hat\\lambda^B_t\\) change accordingly. There is a jump in implied volatility and our model captures the market expectation adjustment about the game prediction. The change in \\(\\hat\\lambda_A\\) and \\(\\hat\\lambda_B\\) are consistent with the findings of Vecer, Kopriva, and Ichiba (2009) where the scoring intensity of the penalized team drops while the scoring intensity of the opposing team increases. When a goal is scored in the 13th minute, we see the increase of \\(\\hat\\lambda^B_t\\) and the market expects that the underdog team is pressing to come back into the game, an effect that has been well-documented in the literature. Another important effect that we observe at the end of the game is that as goals are scored (in the 78th and 81st minutes), the markets expectation is that the implied volatility increases again as one might expect.\n\n\n\n\n\n\nFigure 3.2: Red line: the path of implied volatility throughout the game, i.e., \\(\\sigma_{t}^{red} = \\sqrt{\\hat\\lambda^A_t+\\hat\\lambda^B_t}\\). Blue lines: the path of implied volatility with constant \\(\\lambda^A+\\lambda^B\\), i.e., \\(\\sigma_{t}^{blue} = \\sqrt{(\\lambda^A+\\lambda^B)*(1-t)}\\). Here \\((\\lambda^A+\\lambda^B) = 1, 2, ..., 8\\).\n\n\n\n\n\n\nTable 3.3: The calibrated \\(\\{\\hat\\lambda^A_t, \\hat\\lambda^B_t\\}\\) divided by \\((1-t)\\) and the implied volatility during the game. \\(\\{\\lambda^A_t, \\lambda^B_t\\}\\) are expected goals scored for rest of the game. The less the remaining time, the less likely to score goals. Thus \\(\\{\\hat\\lambda^A_t, \\hat\\lambda^B_t\\}\\) decrease as \\(t\\) increases to 1. Diving them by \\((1-t)\\) produces an updated version of \\(\\hat\\lambda_{0}\\)’s for the whole game, which are in general time-varying (but not decreasing necessarily).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nt\n0\n0.11\n0.22\n0.33\n0.44\n0.50\n0.61\n0.72\n0.83\n0.94\n1\n\n\n\n\n\\(\\hat\\lambda^A_t/(1-t)\\)\n2.33\n2.51\n2.53\n2.46\n1.89\n1.85\n2.12\n2.12\n2.61\n4.61\n0\n\n\n\\(\\hat\\lambda^B_t/(1-t)\\)\n1.44\n1.47\n1.59\n1.85\n2.17\n2.17\n2.56\n2.90\n3.67\n5.92\n0\n\n\n\\((\\hat\\lambda^A_t+\\hat\\lambda^B_t)/(1-t)\\)\n3.78\n3.98\n4.12\n4.31\n4.06\n4.02\n4.68\n5.03\n6.28\n10.52\n0\n\n\n\\(\\sigma_{IV,t}\\)\n1.94\n1.88\n1.79\n1.70\n1.50\n1.42\n1.35\n1.18\n1.02\n0.76\n0\n\n\n\n\n\n\nFigure Figure 3.2 compares the updating implied volatility of the game with implied volatilities of fixed \\((\\lambda^A+\\lambda^B)\\). At the beginning of the game, the red line (updating implied volatility) is under the “(\\(\\lambda^A+\\lambda^B=4)\\)”-blue line; while at the end of the game, it’s above the “(\\(\\lambda^A+\\lambda^B=8)\\)”-blue line. As we expect, the value of \\((\\hat\\lambda^A_t + \\hat\\lambda^B_t)/(1-t)\\) in Table Table 3.3 increases throughout the game, implying that the game became more and more intense and the market continuously updates its belief in the odds.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#poisson-gamma-learning-about-a-poisson-intensity",
    "href": "03-bl.html#poisson-gamma-learning-about-a-poisson-intensity",
    "title": "3  Bayesian Learning",
    "section": "3.5 Poisson-Gamma: Learning about a Poisson Intensity",
    "text": "3.5 Poisson-Gamma: Learning about a Poisson Intensity\nConsider a continuous-time stochastic process, \\(\\left\\{ N_{t}\\right\\} _{t\\geq0}\\), with \\(N_{0}=0\\), counting the number of events that have occurred up to time \\(t\\). The process is constant between event times, and jumps by one at event times: \\(\\Delta N_{t}=N_{t}-N_{t-}=1,\\) where \\(N_{t-}\\) is the limit from the left. The probability of an event over the next short time interval, \\(\\Delta t\\) is \\(\\lambda\\Delta t\\), and \\(N_{t}\\) is called a Poisson process because \\[\nP\\left[  N_{t}=k\\right]  =\\frac{e^{-\\lambda t}\\left(  \\lambda\nt\\right)  ^{k}}{k!}\\text{ for }k=1,\\ldots\n\\] which is the Poisson distribution, thus \\(N_{t}\\sim Poi\\left(\\lambda t\\right)\\). A more general version of the Poisson process is a Cox process, or doubly stochastic point process.\nHere, there is additional conditioning information in the form of state variables, \\(\\left\\{X_{t}\\right\\}_{t&gt;0}\\). The process now has two sources of randomness, one associated with the discontinuous jumps and another in the form of random state variables, \\(\\left\\{X_{t}\\right\\}_{t&gt;0}\\), that drive the intensity of the process. The intensity of the Cox process is \\(\\lambda_{t}=\\int_{0}^{t}\\lambda\\left( X_{s}\\right) ds\\), which is formally defined as \\[\nP\\left[  N_{t}-N_{s}=k \\mid \\left\\{  X_{u}\\right\\}  _{s\\leq u\\leq\nt}\\right]  =\\frac{\\left(  \\int_{s}^{t}\\lambda\\left(  X_{s}\\right)  ds\\right)\n^{k}\\exp\\left(  -\\int_{s}^{t}\\lambda\\left(  X_{s}\\right)  ds\\right)}{k!}, ~ k=0,1,\\ldots\n\\] Cox processes are very useful extensions to Poisson processes and are the basic building blocks of reduced form models of defaultable bonds.\nThe inference problem is to learn about \\(\\lambda\\) from a continuous-record of observation up to time \\(t\\). The likelihood function is given by \\[\np\\left(  N_{t}=k \\mid \\lambda\\right)  =\\frac{\\left(  \\lambda t\\right)  ^{k}%\n\\exp\\left(  -\\lambda t\\right)  }{k!},\n\\] and the MLE is \\(\\widehat{\\lambda}=N_{t}/t\\). The MLE has the unattractive property that prior to the first event \\(\\left\\{ t:N_{t}=0\\right\\}\\), the MLE is 0, despite the fact that the model explicitly assumes that events are possible. This problem often arises in credit risk contexts, where it would seem odd to assume that the probability of default is zero just because a default has not yet occurred.\nA natural prior for this model is the Gamma distribution, which has the following pdf \\[\np\\left(  \\lambda \\mid a,A\\right)  =\\frac{A^{a}}{\\Gamma(a)  }\\lambda^{a-1}\\exp\\left(  -A\\lambda\\right)  \\text{.}\n\\tag{3.3}\\] Like the beta distribution, a Gamma prior distribution allows for a variety of prior shapes and is parameterized by two hyperparameters. Combining the prior and likelihood, the posterior is also Gamma: \\[\np\\left(  \\lambda \\mid N_{t}\\right)  \\propto\\frac{\\left(  \\lambda\\right)\n^{N_{t}+a-1}\\exp\\left(  -\\lambda\\left(  t+A\\right)  \\right)  }{N_{t}!}%\n\\sim\\mathcal{G}\\left(  a_{t},A_{t}\\right)  ,\n\\] where \\(a_{t}=N_{t}+a\\) and \\(A_{t}=t+A\\). The expected intensity, based on information up to time \\(t\\), is \\[\n\\mathbb{E}\\left[  \\lambda \\mid N_{t}\\right]  =\\frac{a_{t}}{A_{t}}=\\frac{N_{t}%\n+a}{t+A}=w_{t}\\frac{N_{t}}{t}+\\left(  1-w_{t}\\right)  \\frac{a}{A},\n\\] where the second line expresses the posterior mean in shrinkage form as a weighted average of the MLE and the prior mean where \\(w_{t}=t/(t+A)\\). In large samples, \\(w_{t}\\rightarrow1\\) and \\(E\\left( \\lambda \\mid N_{t}\\right) \\approx N_{t}/t=\\widehat{\\lambda}\\).\nTo understand the updating mechanics, Figure 3.3 (right column) displays a simulated sample path, posterior means, and (5%,95%) posterior quantiles for various prior configurations. In this case, time is measured in years and the intensity used to simulate the data is \\(\\lambda=1\\), implying on average one event per year. The four prior configurations embody different beliefs. In the first case, in the middle left panel, \\(a=4\\) and \\(A=1\\), captures a high-activity prior, that posits that jumps occur, on average, four times per year, and there is substantial prior uncertainty over the arrival rate as the (5%,95%) prior quantiles are (1.75,6.7). In the second case, captures a prior that is centered over the true value with modest prior uncertainty. The third case captures a low-activity prior, with a prior mean of 0.2 jumps/year. The fourth case captures a dogmatic prior, that posits that jumps occur three times per year, with high confidence in these beliefs.\nThe priors were chosen to highlight different potential paths for Bayesian learning. The first thing to note from the priors is the discontinuity upward at event times, and the exponential decrease during periods of no events, both of which are generic properties of Bayesian learning in this model. If one thinks of the events as rare, this implies rapid revisions in beliefs at event times and a constant drop in estimates of the intensity in periods of no events. For the high-activity prior and the sample path observed, the posterior begins well above \\(\\lambda=1\\), and slowly decreases, getting close to \\(\\lambda=1\\) at the end of the sample. This can be somewhat contrasted with the low-activity prior, which has drastic revisions upward at jump times. In the dogmatic case, there is little updating at event times. The prior parameters control how rapidly beliefs change, with noticeable differences across the priors.\nset.seed(8) # Ovi\nt = 1:5\nlmb = 1\nN = rpois(5,t*lmb)\n\n# A: rate (beta), a: shape (alpha)\nplotgamma = function(a,A,N) {\n    x = seq(0,10,0.01)\n    plot(x,dgamma(x,a,A),type=\"l\",xlab=\"t\",ylab=\"Gamma(t)\")\n    at = N+a\n    At = t+A\n    mean = at/At\n    plot(N, type='l', col=\"orange\", ylim=c(0,5), xlab=\"t\", ylab=\"N(t)\")\n    lines(mean, col=\"blue\", lwd=3, lty=2)\n    lines(qgamma(0.05,at,At), col=\"grey\", lwd=1, lty=2)\n    lines(qgamma(0.95,at,At), col=\"grey\", lwd=1, lty=2)\n}\nplotgamma(a=4,A=1, N)\nplotgamma(a=1,A=1, N)\nplotgamma(a=1,A=5, N)\nplotgamma(a=30,A=10, N)\n\n\n\n\n\n\n\n\n\n\n\n(a) a = 4, A = 1\n\n\n\n\n\n\n\n\n\n\n\n(b) Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) a = 1, A = 1\n\n\n\n\n\n\n\n\n\n\n\n(d) Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) a = 1, A = 5\n\n\n\n\n\n\n\n\n\n\n\n(f) Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) a = 30, A = 10\n\n\n\n\n\n\n\n\n\n\n\n(h) Posterior\n\n\n\n\n\n\n\nFigure 3.3: Sensitivity of Gamma Prior for Poisson Process\n\n\n\nPoisson event models are often embedded as portion of more complicated model to capture rare events such as stock market crashes, volatility surges, currency revaluations, or defaults. In these cases, prior distributions are often important–even essential–since it is common to build models with events that could, but have not yet occurred. These events are often called ‘Peso’ events. For example, in the case of modeling corporate defaults a researcher wants to allow for a jump to default. This requires positing a prior distribution that places non-zero probability on an event occurring. Classical statistical methods have difficulties dealing with these situations since the MLE of the jump probability is zero, until the first event occurs.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#normal-normal-model-for-continuous-data",
    "href": "03-bl.html#normal-normal-model-for-continuous-data",
    "title": "3  Bayesian Learning",
    "section": "3.6 Normal-Normal Model for Continuous Data",
    "text": "3.6 Normal-Normal Model for Continuous Data\nThe Normal or Gaussian distribution is central to probability and statistical inference. Suppose that we are trying to predict tomorrow’s return on the S&P500. There’s a number of questions that come to mind\n\nWhat is the random variable of interest?\nHow can we describe our uncertainty about tomorrow’s outcome?\nInstead of listing all possible values we’ll work with intervals instead. The probability of an interval is defined by the area under the probability density function.\n\nReturns are are continuous (as opposed to discrete) random variables. Hence a normal distribution would be appropriate - but on what scale? We will see that on the log-scale a Normal distribution provides a good approximation.\nThe most widely used model for a continuous random variable is the normal distribution. Standard normal random variable \\(Z\\) has the following properties\nThe standard Normal has mean \\(0\\) and has a variance \\(1\\), and is written as \\[\nZ \\sim N(0,1)\n\\] Then, we have the probability statements of interest \\[\\begin{align*}  \nP(-1 &lt;Z&lt; 1) &=0.68\\\\\nP(-1.96 &lt;Z&lt; 1.96) &=0.95\\\\\n\\end{align*}\\]\nIn R, we can find probabilities\n\npnorm(1.96)\n\n 0.98\n\n\nand quantiles\n\nqnorm(0.9750)\n\n 2\n\n\nThe quantile function qnorm is the inverse of pnorm.\nA random variable that follows normal distribution with general mean and variance \\(X \\sim \\mbox{N}(\\mu, \\sigma^2)\\), has the following properties \\[\\begin{align*}\n  p(\\mu - 2.58 \\sigma &lt; X &lt; \\mu + 2.58 \\sigma) &=& 0.99 \\\\\n  p(\\mu - 1.96 \\sigma &lt; X &lt; \\mu + 1.96 \\sigma) &=& 0.95 \\, .\n\\end{align*}\\] The chance that \\(X\\) will be within \\(2.58 \\sigma\\) of its mean is \\(99\\%\\), and the chance that it will be within \\(2\\sigma\\) of its mean is about \\(95\\%\\).\nThe probability model is written \\(X \\sim N(\\mu,\\sigma^2)\\), where \\(\\mu\\) is the mean, \\(\\sigma^2\\) is the variance. This can be transformed to a standardized normal via \\[\nZ =\\frac{X-\\mu}{\\sigma} \\sim N(0,1).\n\\] For a normal distribution, we know that \\(X \\in [\\mu-1.96\\sigma,\\mu+1.96\\sigma]\\) with probability 95%. We can make similar claims for any other distribution using the Chebyshev’s empirical rule, which is valid for any population:\n\nAt least 75% probability lies within 2\\(\\sigma\\) of the mean \\(\\mu\\)\nAt least 89% lies within 3\\(\\sigma\\) of the mean \\(\\mu\\)\nAt least \\(100(1-1/m^2)\\)% lies within \\(m\\times \\sigma\\) of the mean \\(\\mu\\).\n\nThis also holds true for the Normal distribution. The percentages are \\(95\\)%, \\(99\\)% and \\(99.99\\)%.\n\nExample 3.6 (Google Stock 2019) Consider observations of daily log-returns of a Google stock for 2019 Daily log-return on day \\(t\\) is calculated by taking a logarithm of the ratio of price at close of day \\(t\\) and at close of day \\(t-1\\) \\[\n  y_t = \\log\\left(\\dfrac{P_t}{P_{t-1}}\\right)\n\\] For example on January 3 of 2017, the open price is 778.81 and close price was 786.140, then the log-return is \\(\\log(786.140/778.81) =  -0.0094\\). It was empirically observed that log-returns follow a Normal distribution. This observation is a basis for Black-Scholes model with is used to evaluate future returns of a stock.\n\np = read.csv(\"../data/GOOG2019.csv\")$Adj.Close; n = length(p) \nr = log(p[2:n]/p[1:(n-1)]) \nhist(r, breaks=30, col=\"lightblue\", main=\"\")\n\n\n\n\n\n\n\n\nObservations on the far right correspond to the days when positive news was released and on the far left correspond to bad news. Typically, those are days when the quarterly earnings reports are released.\nTo estimate the expected value \\(\\mu\\) (return) and standard deviation \\(\\sigma\\) (a measure of risk), we simply calculate their sample counterparts \\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i, ~\\mathrm{ and }~    s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x} )^2\n\\] The empirical (or sample) values \\(\\bar x\\) and \\(s^2\\) are called sample mean and sample variance. Here simply vie them as our best guess about the mean and variance of the normal distribution model then our probabilistic model for next day’s return is then given by \\[\nR \\sim N(\\bar x, s^2).\n\\]\nSay we are interested in investing into Google and would like to calculated the expected return of our investment as well as risk associated with this investment We assume that behavior of the returns in the future will be the same as in 2019.\n\nn = length(r) \nrbar = sum(r)/n; print(rbar) \n\n 0.00098\n\ns2 = sum((r-rbar)^2)/(n-1); print(s2) \n\n 0.00023\n\nx = seq(-0.08,0.08, length.out = 200) \nhist(r, breaks=30, col=\"lightblue\", freq = F, main=\"\", xlab=\"\") \nlines(x,dnorm(x,rbar,sqrt(s2)), col=\"red\", lwd=2)\n\n\n\n\nHistogram (blue) and fitted normal curve (red) of for the Google stock daily return data.\n\n\n\n\nNow, assume, I invest all my portfolio into Google. I can predict my annual return to be \\(251 \\times 9.8\\times 10^{-4}\\) = 0.25 and risk (volatility) of my investment is \\(\\sqrt{s^2}\\) = 0.02% a year.\nI can predict the risk of loosing 3% or more in one day using my model is 1.93%.\n\npnorm(log(1-0.03), rbar, sqrt(s2))*100\n\n 1.9\n\n\n\nsp = read.csv(\"../data/SPMonthly.csv\")$Adj.Close; n = length(sp) \nspret = sp[602:n]/sp[601:(n-1)]-1 # Calculate  1977-1987 returns \n\n\nmean(spret) \n\n 0.012\n\nsd(spret)\n\n 0.043",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#normal-with-unknown-mean",
    "href": "03-bl.html#normal-with-unknown-mean",
    "title": "3  Bayesian Learning",
    "section": "3.7 Normal With Unknown Mean",
    "text": "3.7 Normal With Unknown Mean\nLet \\(Y\\) be a random variable with a normal distribution, \\(Y \\sim N(\\mu, \\sigma^2)\\). The mean \\(\\mu\\) is unknown, but the variance \\(\\sigma^2\\) is known. The likelihood function is given by \\[\np(y \\mid \\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(y-\\mu)^2\\right)\n\\] The MLE of \\(\\mu\\) is \\(\\hat{\\mu} = \\bar{y}\\), the sample mean. Normal prior for the mean parameter \\(\\mu\\) is conjugate to the normal likelihood. \\[\n\\mu \\sim N(\\mu_0, \\sigma_0^2)\n\\] The posterior distribution is also normal. \\[\np(\\mu \\mid y) \\sim N(\\mu_n, \\sigma_n^2)\n\\]\nwhere \\[\n\\mu_n = \\frac{\\sigma^2}{n\\sigma_0^2 + \\sigma^2}\\mu_0 + \\frac{n\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\\bar{y}\n\\] and \\[\n\\sigma_n^2 = \\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\n\\] The posterior mean is a weighted average of the prior mean and the sample mean, with the weights being proportional to the precision of the prior and the likelihood. The posterior variance is smaller than the prior variance, and the sample size \\(n\\) appears in the denominator. The posterior mean is a shrinkage estimator of the sample mean, and the amount of shrinkage is controlled by the prior variance \\(\\sigma_0^2\\). A couple of observations \\[\n\\frac{\\sigma^2}{n\\sigma_0^2 + \\sigma^2} \\rightarrow 0 \\text{ and } \\frac{n\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\\rightarrow 1, \\text{ as } n \\rightarrow \\infty.\n\\] Further, \\[\n\\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2} \\rightarrow 0 \\text{ as } n \\rightarrow \\infty.\n\\]\n\nExample 3.7 (Stylezed Example) Assuming the prior distribution \\(\\mu \\sim N(-1,1)\\), say we observed \\(y=2\\) and we want to update our beliefs about \\(\\mu\\). The likelihood function is \\(p(y \\mid \\mu) = N(\\mu,2)\\), and the posterior distribution is \\[\np(\\mu \\mid y) \\propto p(y \\mid \\mu) p(\\mu) = N(y\\mid \\mu,2) N(\\mu\\mid -1,1) = N(-0.4,0.9).\n\\]\n\nmu0 = -1\nsigma0 = 1\nybar = 2\nsigma = 2\nmu1 = (mu0/sigma0^2 + ybar/sigma^2)/(1/sigma0^2 + 1/sigma^2)\nsigma1 = sqrt(1/(1/sigma0^2 + 1/sigma^2))\nsprintf(\"Posterior mean: %f, Posterior variance: %f\", mu1, sigma1)\n\n \"Posterior mean: -0.400000, Posterior variance: 0.894427\"\n\n\nGraphically we can represent this as follows\n\n# The prior distribution \nmu = seq(-4,10,0.01)\ny = seq(-4,10,0.01)\n# Prior\nplot(mu,dnorm(mu,mu0,sigma0),type=\"l\",xlab=\"x\",ylab=\"p(x)\",lwd=2,col=\"blue\",ylim=c(0,0.5))\n# The likelihood function\nlines(y,dnorm(y,ybar,sigma),type=\"l\",lwd=2,col=\"red\")\n# The posterior distribution\nlines(y,dnorm(y,mu1,sigma1),type=\"l\",lwd=2,col=\"green\")\n# legend\nlegend(\"topright\", c(\"Prior\",\" Data (Likelihood)\",\"Posterior\"), pch=15, col=c(\"blue\", \"red\", \"green\"), bty=\"n\")\n\n\n\n\n\n\n\nFigure 3.4: Norm-Norm Updating\n\n\n\n\n\nNote, the posterior mean is in between those of prior and likelihood and posterior variance is lower than variance of both prior and likelihood, this is effect of combining information from data and prior!\n\nMore generally, when we observe \\(n\\) independent and identically distributed (i.i.d.) data points \\(y_1,\\ldots,y_n\\) from a normal distribution with known variance \\(\\sigma^2\\), the likelihood function is given by \\[\np(y \\mid \\mu) = N(\\bar y\\mid \\mu,\\sigma^2/n),~ \\text{where}~ \\bar y = \\frac{1}{n}\\sum_{i=1}^n y_i.\n\\] Note, that average over the observed data \\(\\bar y = \\mathrm{Ave}(y_1,\\ldots,y_n)\\) is the sufficient statistics for the mean \\(\\mu\\). The prior distribution is given by \\[\np(\\mu) = N(\\mu\\mid \\mu_0,\\sigma_0^2)\n\\] The posterior distribution is given by \\[\n\\begin{split}\np(\\mu\\mid y)\n& \\propto  \\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\mu_0-\\mu_0^2}{2\\sigma_0^2}}\\Bigg]\\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\bar{y}-\\bar{y}^2}{2\\sigma^2/n}}\\Bigg] \\\\\n& \\propto  \\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\mu_0}{2\\sigma_0^2}}\\Bigg]\\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\bar{y}}{2\\sigma^2/n}}\\Bigg]. \\\\\n\\end{split}\n\\] Now we combine the terms \\[\n\\begin{split}\np(\\mu\\mid y)\n& \\propto  \\exp\\Bigg[{\\frac{(-\\mu^2+2\\mu\\mu_0)\\sigma^2 +(-\\mu^2+2\\mu\\bar{y})n\\sigma_0^2}{2\\sigma_0^2\\sigma^2}}\\Bigg]. \\\\\n\\end{split}\n\\] Now re-arrange and combine \\(\\mu^2\\) and \\(\\mu\\) terms \\[\n\\begin{split}\np(\\mu\\mid y)\n& \\propto  \\exp\\Bigg[{\\frac{-\\mu^2(n\\sigma_0^2+\\sigma^2)+2\\mu(\\mu_0\\sigma^2+ \\bar{y}n\\sigma_0^2) }{2\\sigma_0^2\\sigma^2}}\\Bigg] \\\\\n& \\propto  \\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\left(\\frac{\\mu_0\\sigma^2 + \\bar{y}n\\sigma_0^2}{n\\sigma_0^2+\\sigma^2}\\right) }{2(\\sigma_0^2\\sigma^2) /(n\\sigma_0^2+\\sigma^2)}}\\Bigg]. \\\\\n\\end{split}\n\\] Now we add constants which do not depend upon \\(\\mu\\) to complete the square in the numerator: \\[\n\\begin{split}\np(\\mu\\mid y)\n& \\propto  \\exp\\Bigg[{\\frac{-\\bigg(\\mu - \\frac{\\mu_0\\sigma^2 + \\bar{y}n\\sigma_0^2}{n\\sigma_0^2+\\sigma^2}\\bigg)^2 }{2(\\sigma_0^2\\sigma^2) /(n\\sigma_0^2+\\sigma^2)}}\\Bigg]. \\\\\n\\end{split}\n\\] Finally we get the posterior mean \\[\n\\mu_n = \\frac{\\mu_0\\sigma^2+ \\bar{y}n\\sigma_0^2}{n\\sigma_0^2+\\sigma^2} = \\mu_0\\frac{\\sigma^2}{n\\sigma_0^2+\\sigma^2} + \\bar{y}\\frac{n\\sigma_0^2}{n\\sigma_0^2+\\sigma^2}\n\\] and the posterior variance \\[\n\\sigma_n^2 = \\frac{\\sigma_0^2\\sigma^2}{n\\sigma_0^2+\\sigma^2}.\n\\]\n\nExample 3.8 (Chicago Bears 2014-2015 Season) The Chicago Bears are a professional American football team based in Chicago, Illinois. The Bears were a young team in 2014-2015, an were last in the their division. This season the Chicago Bears suffered back-to-back \\(50\\)-points defeats and lost to Patriots and Packers.\n\nPatriots-Bears \\(51-23\\)\nPackers-Bears \\(55-14\\)\n\nTheir next game was at home against the Minnesota Vikings. Current line against the Vikings was \\(-3.5\\) points. Slightly over a field goal. What’s the Bayes approach to learning the line? We use hierarchical data and Bayes learning to update our beliefs in light of new information. The current average win/lose this year can be modeled as a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). We assume that \\(\\mu\\) is normally distributed with mean \\(\\mu_0\\) and standard deviation \\(\\tau\\). \\[\\begin{align*}\n\\bar{y} \\mid \\mu & \\sim N \\left ( \\mu , \\frac{\\sigma^2}{n} \\right ) \\sim N \\left ( \\mu , \\frac{18.34^2}{9} \\right )\\\\\n\\mu & \\sim N( 0 , \\tau^2 )\n\\end{align*}\\] Here \\(n =9\\) games so far. With \\(s = 18.34\\) points. We assume the pre-season prior mean \\(\\mu_0 = 0\\), standard deviation \\(\\tau = 4\\). Base on the observed data so-far: \\(\\bar{y} = -9.22\\).\nThe Bayes Shrinkage estimator is then \\[\n\\mathbb{E} \\left( \\mu \\mid \\tau, \\bar y  \\right) = \\frac{ \\tau^2 }{ \\tau^2 + \\frac{\\sigma^2}{n} }\\bar{y} .\n\\]\nThe shrinkage factor is \\(0.3\\)! That’s quite a bit of shrinkage. Why? Our updated estimator is \\[\n\\mathbb{E} \\left ( \\mu | \\bar{y} , \\tau \\right ) = - 2.75 &gt; -.3.5\n\\] where current line is \\(-3.5\\).\n\nBased on our hierarchical model this is an over-reaction. One point change on the line is about \\(3\\)% on a probability scale.\nAlternatively, calculate a market-based \\(\\tau\\) given line \\(=-3.5\\). \\[\n\\tau^2 = \\frac{\\sigma^2}{n} \\frac{1}{0.3^2} = 18.34^2 \\frac{1}{0.3^2} = 180.\n\\]\nThe market-based \\(\\tau\\) is \\(13.4\\) points.\n\n\nbears=c(-3,8,8,-21,-7,14,-13,-28,-41)\nprint(mean(bears))\n\n -9.2\n\nprint(sd(bears))\n\n 18\n\ntau=4\nsig2=sd(bears)*sd(bears)/9\nprint(tau^2/(sig2+tau^2))\n\n 0.3\n\nprint(0.29997*-9.22)\n\n -2.8\n\nprint(pnorm(-2.76/18))\n\n 0.44\n\n\nHome advantage is worth \\(3\\) points. The actual result of the game is Bears 21, Vikings 13.\n\nPosterior Predictive\nThe posterior predictive distribution is the distribution of a new observation \\(y_{n+1}\\) given the observed data \\(y_1,\\ldots,y_n\\). The posterior predictive distribution is given by \\[\np(y_{n+1} \\mid y_1,\\ldots,y_n) = \\int p(y_{n+1} \\mid \\mu) p(\\mu \\mid y_1,\\ldots,y_n) d\\mu = \\int N(y_{n+1} \\mid \\mu, \\sigma^2) N(\\mu \\mid \\mu_n, \\sigma_n^2) d\\mu = N(y_{n+1} \\mid \\mu_n, \\sigma_n^2 + \\sigma^2).\n\\] This follows from the general properties of the Gaussian distribution",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#normal-with-unknown-variance",
    "href": "03-bl.html#normal-with-unknown-variance",
    "title": "3  Bayesian Learning",
    "section": "3.8 Normal With Unknown Variance",
    "text": "3.8 Normal With Unknown Variance\nConsider, another example, when mean \\(\\mu\\) is fixed and variance is a random variable which follows some distribution \\(\\sigma^2 \\sim p(\\sigma^2)\\). Given an observed sample \\(y\\), we can update the distribution over variance using the Bayes rule \\[\np(\\sigma^2 \\mid  y) = \\dfrac{p(y\\mid \\sigma^2 )p(\\sigma^2)}{p(y)}.\n\\] Now, the total probability in the denominator can be calculated as \\[\np(y) = \\int p(y\\mid \\sigma^2 )p(\\sigma^2) d\\sigma^2.\n\\]\nA conjugate prior that leads to analytically calculable integral for variance under the normal likelihood is the inverse Gamma. Thus, if \\[\n\\sigma^2 \\mid  \\alpha,\\beta \\sim IG(\\alpha,\\beta) = \\dfrac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\sigma^{2(-\\alpha-1)}\\exp\\left(-\\dfrac{\\beta}{\\sigma^2}\\right)\n\\] and \\[\ny \\mid \\mu,\\sigma^2 \\sim N(\\mu,\\sigma^2)\n\\] Then the posterior distribution is another inverse Gamma \\(IG(\\alpha_{\\mathrm{posterior}},\\beta_{\\mathrm{posterior}})\\), with \\[\n\\alpha_{\\mathrm{posterior}} = \\alpha + 1/2, ~~\\beta_{\\mathrm{posterior}} = \\beta + \\dfrac{y-\\mu}{2}.\n\\]\nNow, the predictive distribution over \\(y\\) can be calculated by \\[\np(y_{new}\\mid y) = \\int p(y_{new},\\sigma^2\\mid y)p(\\sigma^2\\mid y)d\\sigma^2.\n\\] Which happens to be a \\(t\\)-distribution with \\(2\\alpha_{\\mathrm{posterior}}\\) degrees of freedom, mean \\(\\mu\\) and variance \\(\\alpha_{\\mathrm{posterior}}/\\beta_{\\mathrm{posterior}}\\).\n\n3.8.1 The Normal-Gamma Model\nNow, consider the case when both mean and variance are unknow. To simplify the formulas, we will use precision \\(\\rho = 1/\\sigma^2\\) instead of variance \\(\\sigma^2\\). The normal-Gamma distribution is a conjugate prior for the normal distribution, when we do not know the precision and the mean. Given the observed data \\(Y  = \\{y_1,\\ldots,y_n\\}\\), we assume normal likelihood \\[\ny_i \\mid \\theta, \\rho \\sim N(\\theta, 1/\\rho)\n\\]\nThe normal-gamma prior distribution is defined as \\[\n\\theta\\mid \\mu,\\rho,\\nu \\sim N(\\mu, 1/(\\rho \\nu)), \\quad \\rho \\mid \\alpha, \\beta \\sim \\text{Gamma}(\\alpha, \\beta).\n\\] Thus, \\(1/\\rho\\) has inverse-Gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\). Conditional on \\(\\rho\\), the mean \\(\\theta\\) has normal distribution with mean \\(\\mu\\) and precision \\(\\nu\\rho\\). Notice that in this model the mean \\(\\theta\\) and precision \\(\\rho\\) are not independent. When the precision of observations \\(\\rho\\) is low, we are also less certain about the mean. However, when \\(\\nu=0\\), we have an improper uniform distribution over \\(\\theta\\), that is independent of \\(\\rho\\). There is no conjugate distribution for \\(\\theta,\\rho\\) in which \\(\\theta\\) is independent of \\(\\rho\\). Given the normal likelihood \\[\np(y\\mid \\theta, \\rho) = \\left(\\dfrac{\\rho}{2\\pi}\\right)^{1/2}\\exp\\left(-\\dfrac{\\rho}{2}\\sum_{i=1}^n(y_i-\\theta)^2\\right)\n\\] and the normal-gamma prior \\[\np(\\theta, \\rho \\mid \\mu,\\nu,\\alpha,\\beta) = \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\nu\\rho^{\\alpha-1}\\exp(-\\beta\\rho)\\left(\\dfrac{\\nu\\rho}{2\\pi}\\right)^{1/2}\\exp\\left(-\\dfrac{\\nu\\rho}{2}(\\theta-\\mu)^2\\right)\n\\] the posterior distribution is given by \\[\np(\\theta, \\rho\\mid y) \\propto p(y\\mid \\theta, \\rho)p(\\theta, \\rho).\n\\] The posterior distribution is a normal-Gamma distribution with parameters \\[\n\\begin{aligned}\n\\mu_n &= \\dfrac{\\nu\\mu + n\\bar{y}}{\\nu+n},\\\\\n\\nu_n &= \\nu+n,\\\\\n\\alpha_n &= \\alpha + \\dfrac{n}{2},\\\\\n\\beta_n &= \\beta + \\dfrac{1}{2}\\sum_{i=1}^n(y_i-\\bar{y})^2 + \\dfrac{n\\nu}{2(\\nu+n)}(\\bar{y}-\\mu)^2.\n\\end{aligned}\n\\] where \\(\\bar{y} = n^{-1}\\sum_{i=1}^n y_i\\) is the sample mean and \\(n\\) is the sample size. The posterior distribution is a normal-Gamma distribution with parameters \\(\\mu_n, \\nu_n, \\alpha_n, \\beta_n\\).\n\n\n3.8.2 Credible Intervals for Normal-Gamma Model Posterior Parameters\nThe precission posterior follows a Gamma distribution with parameters \\(\\alpha_n, \\beta_n\\), thus we can use quantiles of the Gamma distribution to calculate credible intervals. A symmetric \\(100(1-c)%\\) credible interval \\([g_{c/2},g_{1-c/2}]\\) is given by \\(c/2\\) and \\(1-c/2\\) quantiles of the gamma distrinution. To find credible intterval for the variance \\(v = 1/\\rho\\), we simply use \\[\n[1/g_{1-c/2},1/g_{c/2}].\n\\] and for standard deviation \\(s = \\sqrt{v}\\) we use \\[\n[\\sqrt{1/g_{1-c/2}},\\sqrt{1/g_{c/2}}].\n\\] To find credible interval over the mean \\(\\theta\\), we need to integrate out the precision \\(\\rho\\) from the posterior distribution. The marginal distribution of \\(\\theta\\) is a Student’s t-distribution with parameters center at \\(\\mu_n\\), variance \\(\\beta_n/(\\nu_n\\alpha_n)\\) and degrees of freedom \\(2\\alpha_n\\).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#multivariate-normal",
    "href": "03-bl.html#multivariate-normal",
    "title": "3  Bayesian Learning",
    "section": "3.9 Multivariate Normal",
    "text": "3.9 Multivariate Normal\nIn the multivariate case, the normal-normal model is \\[\n\\theta \\sim N(\\mu_0,\\Sigma_0), \\quad y \\mid \\theta \\sim N(\\theta,\\Sigma).\n\\] The posterior distribution is \\[\n\\theta \\mid y \\sim N(\\mu_1,\\Sigma_1),\n\\] where \\[\n\\Sigma_1 = (\\Sigma_0^{-1} + \\Sigma^{-1})^{-1}, \\quad \\mu_1 = \\Sigma_1(\\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}y).\n\\] The predictive distribution is \\[\ny_{new} \\mid y \\sim N(\\mu_1,\\Sigma_1 + \\Sigma).\n\\]\n\nExample 3.9 (Satya Nadella: CEO of Microsoft) In 2014, Satya Nadella became the CEO of Microsoft. The stock price of Microsoft has been on a steady rise since then. Suppose that you are a portfolio manager and you are interested in analyzing the returns of Microsoft stock compared to the market.\nSuppose you are managing a portfolio with two positions stock of Microsoft (MSFT) and an index fund that follows S&P500 index and tracks overall market performance. We are interested in estimating the mean returns of the positions in our portfolio. You believe that the returns are normally distributed and are related to each other. You have prior beliefs about these returns, which are also normally distributed. We will use what is called the empirical prior for the mean returns. This is a prior that is based on historical data. The empirical prior is a good choice when you have a lot of historical data and you believe that the future mean returns will be similar to the historical mean returns. We assume the prior for the mean returns is a bivariate normal distribution, let \\(\\mu_0 = (\\mu_{M}, \\mu_{S})\\) represent the prior mean returns for the stocks. The covariance matrix \\(\\Sigma_0\\) captures your beliefs about the variability and the relationship between these stocks’ returns in the prior. We will use the sample mean and covariance matrix of the historical returns as the prior mean and covariance matrix. The prior covariance matrix is given by \\[\n\\Sigma_0 = \\begin{bmatrix} \\sigma_{M}^2 & \\sigma_{MS} \\\\ \\sigma_{MS} & \\sigma_{S}^2 \\end{bmatrix},\n\\] where \\(\\sigma_{M}^2\\) and \\(\\sigma_{S}^2\\) are the sample variances of the historical returns of MSFT and SPY, respectively, and \\(\\sigma_{MS}\\) is the sample covariance of the historical returns of MSFT and SPY. The prior mean is given by \\[\n\\mu_0 = \\begin{bmatrix} \\mu_{M} \\\\ \\mu_{S} \\end{bmatrix},\n\\] where \\(\\mu_{M}\\) and \\(\\mu_{S}\\) are the sample means of the historical returns of MSFT and SPY, respectively. The likelihood of observing the data, given the mean returns, is also a bivariate normal distribution. The mean of this distribution is the true (but unknown) mean returns \\(\\mu = [\\mu_A, \\mu_B]\\). The covariance matrix \\(\\Sigma\\) of the likelihood represents the uncertainty in your data. We will use the sample mean and covariance matrix of the observed returns as the likelihood mean and covariance matrix. The likelihood covariance matrix is given by \\[\n\\Sigma = \\begin{bmatrix} \\sigma_{M}^2 & \\sigma_{MS} \\\\ \\sigma_{MS} & \\sigma_{S}^2 \\end{bmatrix},\n\\] where \\(\\sigma_{M}^2\\) and \\(\\sigma_{S}^2\\) are the sample variances of the observed returns of MSFT and SPY, respectively, and \\(\\sigma_{MS}\\) is the sample covariance of the observed returns of MSFT and SPY. The likelihood mean is given by \\[\n\\mu = \\begin{bmatrix} \\mu_{M} \\\\ \\mu_{S} \\end{bmatrix},\n\\] where \\(\\mu_{M}\\) and \\(\\mu_{S}\\) are the sample means of the observed returns of MSFT and SPY, respectively. In a Bayesian framework, you update your beliefs (prior) about the mean returns using the observed data (likelihood). The posterior distribution, which combines your prior beliefs and the new information from the data, is also a bivariate normal distribution. The mean \\(\\mu_{\\text{post}}\\) and covariance \\(\\Sigma_{\\text{post}}\\) of the posterior are calculated using Bayesian updating formulas, which involve \\(\\mu_0\\), \\(\\Sigma_0\\), \\(\\mu\\), and \\(\\Sigma\\).\nWe use observed returns prior to Nadella’s becoming CEO as our prior and analyze the returns post 2014. Thus, our observed data includes July 2015 - Dec 2023 period. We assume the likelihood of observing this data, given the mean returns, is also a bivariate normal distribution. The mean of this distribution is the true (but unknown) mean returns. The covariance matrix \\(Sigma\\) of the likelihood represents the uncertainty in your data and is calculated from the overall observed returns data 2001-2023.\n\ngetSymbols(c(\"MSFT\", \"SPY\"), from = \"2001-01-01\", to = \"2023-12-31\")\n\n \"MSFT\" \"SPY\" \n\ns = 3666 # 2015-07-30\nprior = 1:s\nobs = s:nrow(MSFT) # post covid\n# obs = 5476:nrow(MSFT) # 2022-10-06 bull run if 22-23\na = as.numeric(dailyReturn(MSFT))\nc = as.numeric(dailyReturn(SPY))\n# Prior\nmu0 = c(mean(a[prior]), mean(c[prior]))\nSigma0 = cov(data.frame(a=a[prior],c=c[prior]))\n# Data\nmu = c(mean(a[obs]), mean(c[obs]))\nSigma = cov(data.frame(a=a,c=c))\n# Posterior\nSigmaPost = solve(solve(Sigma0) + solve(Sigma))\nmuPost = SigmaPost %*% (solve(Sigma0) %*% mu0 + solve(Sigma) %*% mu)\n# Plot\nplot(a[obs], c[obs], xlab=\"MSFT\", ylab=\"SPY\", xlim=c(-0.005,0.005), ylim=c(-0.005,0.005), pch=16, cex=0.5)\nabline(v=0, h=0, col=\"grey\")\nabline(v=mu0[1], h=mu0[2], col=\"blue\",lwd=3) #prior\nabline(v=mu[1], h=mu[2], col=\"red\",lwd=3) #data\nabline(v=muPost[1], h=muPost[2], col=\"green\",lwd=3) #posterior\nlegend(\"bottomright\", c(\"Prior\", \"Likelihood\", \"Posterior\"), pch=15, col=c(\"blue\", \"red\", \"green\"), bty=\"n\")\n\n\n\n\n\n\n\nFigure 3.5: Bayesian Portfolio Updating\n\n\n\n\n\nWe can see the posterior mean for SPY is close to the prior mean, while the posterior mean for MSFT is further away. The performance of MSFT was significantly better past 2015 compared to SPY. The posterior mean (green) represents mean reversion value. We can think of it a expected mean return if the performance of MSFT starts reverting to its historical averages.\nThis model is particularly powerful because it can be extended to more dimensions (more stocks) and can include more complex relationships between the variables. It’s often used in finance, econometrics, and other fields where understanding the joint behavior of multiple normally-distributed variables is important.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#mixtures-of-conjugate-priors",
    "href": "03-bl.html#mixtures-of-conjugate-priors",
    "title": "3  Bayesian Learning",
    "section": "3.10 Mixtures of Conjugate Priors",
    "text": "3.10 Mixtures of Conjugate Priors\nThe mixture of conjugate priors is a powerful tool for modeling complex data. It allows us to combine multiple conjugate priors to create a more flexible model that can capture a wider range of data patterns. The mixture of conjugate priors is particularly useful when the data is generated from a mixture of distributions, where each component of the mixture is generated from a different distribution. \nIf \\(p_1(x),\\ldots,p_k(x)\\) are proper density functions and \\(\\pi_1,\\ldots,\\pi_k\\) are non-negative weights that sum to 1, then the mixture distribution is given by \\[\np(x) = \\sum_{i=1}^k \\pi_i p_i(x).\n\\] It is easy to show that \\(p(x)\\) is a proper density. Indeed, given domain \\(x\\in A\\subset \\mathbb{R}\\) we have \\[\n\\int_A p(x)dx = \\sum_{i=1}^k \\pi_i \\int_A p_i(x)dx  = \\sum_{i=1}^k \\pi_i = 1.\n\\]\nAssume our prior is a mixture of distributions, that is \\[\n\\theta \\sim p(\\theta) = \\sum_{k=1}^K \\pi_k p_k(\\theta).\n\\] Then the posterior is also a mixture of normal distributions, that is \\[\np(\\theta\\mid y) = p(y\\mid \\theta)\\sum_{k=1}^K \\pi_k p_k(\\theta)/C.\n\\] We introduce a normalizing constant for each component \\[\nC_k = \\int p(y\\mid \\theta)p_k(\\theta)d\\theta.\n\\] then \\[\np_k(\\theta\\mid y)  = p_k(\\theta)p(y\\mid \\theta)/C_k\n\\] is a proper distribution and our posterior is a mixture of these distributions \\[\np(\\theta\\mid y) = \\sum_{k=1}^K \\pi_k C_k p_k(\\theta\\mid y)/C.\n\\] Meaning that we need to require \\[\n\\dfrac{\\sum_{k=1}^K \\pi_k C_k}{C} = 1.\n\\] or \\[\nC = \\sum_{k=1}^K \\pi_k C_k\n\\] Then the posterior density is a mixture \\[\np(\\theta\\mid y) = \\sum_{k=1}^K \\hat \\pi_k p_k(\\theta \\mid y),\n\\] where \\[\n\\hat \\pi_k = \\dfrac{\\pi_k C_k}{\\sum_{i=1}^{K}\\pi_i C_i}\n\\] are the posterior weights.\nConsider an example of a mixture of two normal distributions. The prior distribution is a mixture of two normal distributions, that is \\[\n\\mu \\sim 0.5 N(0,1) + 0.5 N(5,1).\n\\] The likelihood is a normal distribution with mean \\(\\mu\\) and variance 1, that is \\[\ny \\mid \\mu \\sim N(\\mu,1).\n\\] The posterior distribution is a mixture of two normal distributions, that is \\[\np(\\mu \\mid y) \\propto \\phi(y\\mid \\mu,1) \\left(0.5 \\phi(\\mu\\mid 0,1) + 0.5 \\phi(\\mu\\mid 5,1)\\right),\n\\] where \\(\\phi(x\\mid \\mu,\\sigma^2)\\) is the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). We can calculate it using property of a normal distribution \\[\n\\phi(x\\mid \\mu_1,\\sigma_1^2)\\phi(x\\mid \\mu_2,\\sigma_2^2) = \\phi(x\\mid \\mu_3,\\sigma_3^2)\\phi(\\mu_1-\\mu_2\\mid 0,\\sigma_1^2+\\sigma_2^2)\n\\] where \\[\n\\mu_3 = \\dfrac{\\mu_1/\\sigma_2^2 + \\mu_2/\\sigma_1^2}{1/\\sigma_1^2 + 1/\\sigma_2^2}, \\quad \\sigma_3^2 = \\dfrac{1}{1/\\sigma_1^2 + 1/\\sigma_2^2}.\n\\]\nGiven, we observed \\(y = 2\\), we can calculate the posterior distribution for \\(\\mu\\)\n\nmu0 = c(0,5)\nsigma02 = c(1,1)\npi = c(0.5,0.5)\ny = 2\nmu3 = (mu0/sigma02 + y) / (1/sigma02 + 1)\nsigma3 = 1/(1/sigma02 + 1)\nC = dnorm(y-mu0,0,1+sigma02)*pi\nw = C/sum(C)\nsprintf(\"Component parameters:\\nMean = (%1.1f,%2.1f)\\nVar = (%1.1f,%1.1f)\\nweights = (%1.2f,%1.2f)\", mu3[1],mu3[2], sigma3[1],sigma3[2],w[1],w[2])\n\n \"Component parameters:\\nMean = (1.0,3.5)\\nVar = (0.5,0.5)\\nweights = (0.65,0.35)\"",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#exponential-gamma-model",
    "href": "03-bl.html#exponential-gamma-model",
    "title": "3  Bayesian Learning",
    "section": "3.11 Exponential-Gamma Model",
    "text": "3.11 Exponential-Gamma Model\nExponential distribution is a continuous distribution that is often used to model waiting times between events. For example, the time between two consecutive arrivals of a Poisson process is exponentially distributed. If the number of events in 1 unit of time has the Poisson distribution with rate parameter \\(\\lambda\\), then the time between events has the exponential distribution with mean \\(1/\\lambda\\). The probability density function (PDF) of an exponential distribution is defined as: \\[\nf(x;\\lambda) =  \\lambda e^{-\\lambda x}, ~ x \\geq 0\n\\] The exponential distribution is defined for \\(x \\geq 0\\), and \\(\\lambda\\) is the rate parameter, which is the inverse of the mean (or expected value) of the distribution. It must be greater than 0. The exponential distribution is a special case of the Gamma distribution with shape 1 and scale \\(1/\\lambda\\).\nThe mean and variance are give in terms of the rate parameter \\(\\lambda\\) as\n\n\n\nExponential Distribution\nParameters\n\n\n\n\nExpected value\n\\(\\mu = \\E{X} = 1/\\lambda\\)\n\n\nVariance\n\\(\\sigma^2 = \\Var{X} = 1/\\lambda^2\\)\n\n\n\nHere are some examples of when exponential model provides a good fit\n\nLifespan of Electronic Components: The exponential distribution can model the time until a component fails in systems where the failure rate is constant over time.\nTime Between Arrivals: In a process where events (like customers arriving at a store or calls arriving at a call center) occur continuously and independently, the time between these events can often be modeled with an exponential distribution.\nRadioactive Decay: The time until a radioactive atom decays is often modeled with an exponential distribution.\n\nIn these examples, the key assumption is that events happen independently and at a constant average rate, which makes the exponential distribution a suitable model.\nThe Exponential-Gamma model, often used in Bayesian statistics, is a hierarchical model where the exponential distribution’s parameter is itself modeled as following a Gamma distribution. This approach is particularly useful in situations where there is uncertainty or variability in the rate parameter of the exponential distribution.\nThe Exponential-Gamma model assumes that the data follows an exponential distribution (likelihood). As mentioned earlier, the exponential distribution is suitable for modeling the time between events in processes where these events occur independently and at a constant rate. At the next level, the rate parameter \\(\\lambda\\) of the exponential distribution is assumed to follow a Gamma distribution. The Gamma distribution is a flexible two-parameter family of distributions and can model a wide range of shapes. \\[\\begin{align*}\n    \\lambda &\\sim \\text{Gamma}(\\alpha, \\beta) \\\\\n    x_i &\\sim \\text{Exponential}(\\lambda)\n\\end{align*}\\]\nThe probability density function of the Gamma distribution is given by Equation 3.3 and has two parameters, shape \\(\\alpha\\) and rate \\(\\beta\\). The posterior distribution of the rate parameter \\(\\lambda\\) is given by: \\[\np(\\lambda\\mid x_1, \\ldots, x_n) \\propto \\lambda^{\\alpha - 1} e^{-\\beta\\lambda} \\prod_{i=1}^n \\lambda e^{-\\lambda x_i} = \\lambda^{\\alpha + n - 1} e^{-(\\beta + \\sum_{i=1}^n x_i)\\lambda}\n\\] which is a Gamma distribution with shape parameter \\(\\alpha + n\\) and rate parameter \\(\\beta + \\sum_{i=1}^n x_i\\). The posterior mean and variance are given by: \\[\n\\mathbb{E}[\\lambda|x_1, \\ldots, x_n] = \\frac{\\alpha + n}{\\beta + \\sum_{i=1}^n x_i}, \\quad \\mathrm{Var}[\\lambda|x_1, \\ldots, x_n] = \\frac{\\alpha + n}{(\\beta + \\sum_{i=1}^n x_i)^2}.\n\\] Notice, that \\(\\sum x_i\\) is the sufficient statistic for inference about parameter \\(\\lambda\\)!\nSome applications of this model include the following:\n\nReliability Engineering: In situations where the failure rate of components or systems may not be constant and can vary, the Exponential-Gamma model can be used to estimate the time until failure, incorporating uncertainty in the failure rate.\nMedical Research: For modeling survival times of patients where the rate of mortality or disease progression is not constant and varies across a population. The variability in rates can be due to different factors like age, genetics, or environmental influences.\nEcology: In studying phenomena like the time between rare environmental events (e.g., extreme weather events), where the frequency of occurrence can vary due to changing climate conditions or other factors.\n\nIn these applications, the Exponential-Gamma model offers a more nuanced approach than using a simple exponential model, as it accounts for the variability in the rate parameter.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#summary-of-conjugate-priors-for-common-likelihoods",
    "href": "03-bl.html#summary-of-conjugate-priors-for-common-likelihoods",
    "title": "3  Bayesian Learning",
    "section": "3.12 Summary of Conjugate Priors for Common Likelihoods",
    "text": "3.12 Summary of Conjugate Priors for Common Likelihoods\nSummary table of random variables\n\n\n\nTable 3.4: Summary table of commonly used random variables\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nParameters\nPDF\nMean\nVariance\nSupport\n\n\n\n\nNormal\n\\(\\mu, \\sigma^2\\)\n\\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(x \\in \\mathbb{R}\\)\n\n\nExponential\n\\(\\lambda\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\\(\\frac{1}{\\lambda}\\)\n\\(\\frac{1}{\\lambda^2}\\)\n\\(x \\geq 0\\)\n\n\nGamma\n\\(\\alpha, \\beta\\)\n\\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\\)\n\\(\\frac{\\alpha}{\\beta}\\)\n\\(\\frac{\\alpha}{\\beta^2}\\)\n\\(x \\geq 0\\)\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\frac{e^{-\\lambda}\\lambda^x}{x!}\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\\(x \\in \\mathbb{N}\\)\n\n\nBinomial\n\\(n, p\\)\n\\(\\binom{n}{x}p^x(1-p)^{n-x}\\)\n\\(np\\)\n\\(np(1-p)\\)\n\\(x \\in \\{0, 1, \\ldots, n\\}\\)\n\n\nBernoulli\n\\(p\\)\n\\(p^x(1-p)^{1-x}\\)\n\\(p\\)\n\\(p(1-p)\\)\n\\(x \\in \\{0, 1\\}\\)\n\n\nMultinomial\n\\(n, \\boldsymbol{p}\\)\n\\(\\frac{n!}{x_1!x_2!\\cdots x_k!}p_1^{x_1}p_2^{x_2}\\cdots p_k^{x_k}\\)\n\\(np_i\\)\n\\(np_i(1-p_i)\\)\n\\(\\sum x_i = n, x_i \\in \\mathbb{R}^+\\)\n\n\nBeta\n\\(\\alpha, \\beta\\)\n\\(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\)\n\\(\\frac{\\alpha}{\\alpha + \\beta}\\)\n\\(\\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\)\n\\(x \\in [0, 1]\\)\n\n\nDirichlet\n\\(\\boldsymbol{\\alpha}\\)\n\\(\\frac{\\Gamma(\\sum \\alpha_i)}{\\prod \\Gamma(\\alpha_i)}\\prod x_i^{\\alpha_i-1}\\)\n\\(\\frac{\\alpha_i}{\\sum \\alpha_i}\\)\n\\(\\frac{\\alpha_i(\\sum \\alpha_i - \\alpha_i)}{\\sum \\alpha_i^2(\\sum \\alpha_i + 1)}\\)\n\\(\\sum x_i = 1, x_i \\in [0, 1]\\)\n\n\nInverse Gamma\n\\(\\alpha, \\beta\\)\n\\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{-\\alpha-1}e^{-\\frac{\\beta}{x}}\\)\n\\(\\frac{\\beta}{\\alpha-1}\\)\n\\(\\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)}\\)\n\\(x &gt; 0\\)\n\n\nGamma\n\\(\\alpha, \\beta\\)\n\\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\\)\n\\(\\frac{\\alpha}{\\beta}\\)\n\\(\\frac{\\alpha}{\\beta^2}\\)\n\\(x \\geq 0\\)\n\n\n\n\n\n\nTable 3.5 summarizes the conjugate prior distributions for common likelihoods. Thus far, we’ve considered the Normal-Normal model with both known and unknown variance as well as Poisson-Gamma and Beta Binomial. The other pairs are left as an exercise. Given observed data \\(x = (x_1,\\ldots,x_n)\\) and \\(s = \\sum_{i=1}^nx_i\\), \\(\\bar x = s/n\\).\n\n\n\nTable 3.5: Conjugate prior table for common likelihoods\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood\nPrior\nPrior Parameters\nModel Parameters\nPosterior Parameters\n\n\n\n\nNormal (known \\(\\sigma^2\\))\nNormal\n\\(\\mu_0, \\sigma^2_0\\)\n\\(\\mu\\)\n\\(\\frac{n\\sigma^2_0 \\bar x + \\sigma^2 \\mu_0}{\\sigma^2 + n\\sigma^2_0},~\\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2+\\sigma^2}\\)\n\n\nNormal (known \\(\\mu\\))\nInverse Gamma\n\\(\\alpha,\\beta\\)\n\\(\\sigma^2\\)\n\\(\\alpha+n/2, \\beta + \\frac{1}{2}\\sum(x_i-\\mu)^2\\)\n\n\nBinomial (\\(m\\) trials)\nBeta\n\\(\\alpha, \\beta\\)\n\\(p\\)\n\\(\\alpha + s, \\beta + nm - s\\)\n\n\nPoisson\nGamma\n\\(\\alpha, \\beta\\)\n\\(\\lambda\\)\n\\(\\alpha + s, \\beta + n\\)\n\n\nExponential\nGamma\n\\(\\alpha, \\beta\\)\n\\(\\lambda\\)\n\\(\\alpha + n, \\beta + s\\)\n\n\nMultinomial\nDirichlet\n\\(\\alpha \\in \\mathbb{R}^k\\)\n\\(p \\in \\mathbb{R}^k\\)\n\\(\\alpha+s\\)\n\n\nNormal\nNormal-inverse gamma\n\\(\\mu_0, \\nu, \\alpha, \\beta\\)\n\\(\\mu, \\sigma\\)\n\\(\\frac{\\nu\\mu_0+n\\bar{x}}{\\nu+n} ,\\, \\nu+n,\\, \\alpha+\\frac{n}{2} ,\\,\\)  \\(\\beta + \\tfrac{1}{2} \\sum_{i=1}^n (x_i - \\bar{x})^2 + \\frac{n\\nu}{\\nu+n}\\frac{(\\bar{x}-\\mu_0)^2}{2}\\)\n\n\n\n\n\n\nThese conjugate relationships simplify Bayesian calculations by ensuring that the posterior distributions are in the same family as the priors.\n\n\n\n\nDixon, Mark J., and Stuart G. Coles. 1997. “Modelling Association Football Scores and Inefficiencies in the Football Betting Market.” Journal of the Royal Statistical Society Series C: Applied Statistics 46 (2): 265–80.\n\n\nKolmogorov, AN. 1942. “Definition of Center of Dispersion and Measure of Accuracy from a Finite Number of Observations (in Russian).” Izv. Akad. Nauk SSSR Ser. Mat. 6: 3–32.\n\n\nShen, Changyu, Enrico G Ferro, Huiping Xu, Daniel B Kramer, Rushad Patell, and Dhruv S Kazi. 2021. “Underperformance of Contemporary Phase III Oncology Trials and Strategies for Improvement.” Journal of the National Comprehensive Cancer Network 19 (9): 1072–78.\n\n\nSpiegelhalter, David, and Yin-Lam Ng. 2009. “One Match to Go!” Significance 6 (4): 151–53.\n\n\nStern, H, Adam Sugano, J Albert, and R Koning. 2007. “Inference about Batter-Pitcher Matchups in Baseball from Small Samples.” Statistical Thinking in Sports, 153–65.\n\n\nSun, Duxin, Wei Gao, Hongxiang Hu, and Simon Zhou. 2022. “Why 90% of Clinical Drug Development Fails and How to Improve It?” Acta Pharmaceutica Sinica B 12 (7): 3049–62.\n\n\nTaleb, Nassim Nicholas. 2007. The Black Swan: The Impact of the Highly Improbable. Annotated edition. New York. N.Y: Random House.\n\n\nVecer, Jan, Frantisek Kopriva, and Tomoyuki Ichiba. 2009. “Estimating the Effect of the Red Card in Soccer: When to Commit an Offense in Exchange for Preventing a Goal Opportunity.” Journal of Quantitative Analysis in Sports 5 (1).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "04-dec.html",
    "href": "04-dec.html",
    "title": "4  Utility, Risk and Decisions",
    "section": "",
    "text": "4.1 Expectation and Variance (Reward and Risk)\nLife is about making decisions under uncertainty. We always prefer informed decisions Statistical decision theory studies the process of finding a reasonable course of action when faced with statistical uncertainty–uncertainty that can in part be quantified from observed data. In most cases, the problem can be separated into two problems: a learning problem or parameter estimation problem and then a decision problem that uses the output of the learning problem. In finance, a classic example of this is finding optimal portfolios with a mean-variance utility/criterion function assuming the underlying means, variances and covariances are unknown based on a historical sample of data. In statistics, the classic problem is using decision theory to evaluate the relative merits of different parameter estimators hypothesis tests.\nAn expected value of a random variable, denoted by \\(\\E{X}\\) is a weighted average. Each possible value of a random variable is weighted by its probability. For example, Google map uses expected value when calculating travel times. We might compute two different routes by their expected travel time. Typically, a forecast or expected value is all that is required — there expected values can be updated in real time as we travel. Say I am interested in travel time from Washington National airport to Fairfax in Virginia. The histogram below shows the travel times observed for a work day evening and were obtained from Uber.\nThe expected value \\(\\E{X}\\) of discrete random variable \\(X\\) which takes possible values \\(\\{x_1,\\ldots x_n\\}\\) is calculated using\n\\[\n\\E{X} =\\sum_{i=1}^{n}x_i\\prob{X = x_i}\n\\]\nFor example, in a binary scenario, if \\(X\\in \\{0,1\\}\\) and \\(P(X=1)=p\\), then \\(\\E{X} = 0\\times(1-p)+1\\times p = p\\). Expected value of a Bernoulli random variable is simply the probability of success. In many binary scenarios, probabilistic forecast is sufficient.\nIf \\(X\\) is continuous with probability distribution \\(p(x)\\), then we have to calculate the expectation as an integral \\[\n\\E{X} = \\int xp(x)d x  = \\ \\text{ and } \\int p(x)dx = 1.\n\\] When you have a random variable \\(x\\) that has a support that is non-negative (that is, the variable has nonzero density/probability for only positive values), you can use the following property: \\[\nE(X) = \\int_0^\\infty \\left( 1 - F(x) \\right) \\,\\mathrm{d}x,\n\\] where \\(F(x)\\) is the cumulative distribution function (CDF) of \\(X\\). The proof is as follows: \\[\nE(X) = \\int_0^\\infty \\left( 1 - F(x) \\right) \\,\\mathrm{d}x = \\int_0^\\infty \\int_x^\\infty f(y) \\,\\mathrm{d}y \\,\\mathrm{d}x = \\int_0^\\infty \\int_0^y \\,\\mathrm{d}x f(y) \\,\\mathrm{d}y = \\int_0^\\infty y f(y) \\,\\mathrm{d}y,\n\\] where \\(f(x)\\) is the probability density function (PDF) of \\(X\\). Moreover \\[\nE(X) = \\int_{0}^1 F^{-1}(p) \\,\\mathrm{d}p,\n\\] where \\(F^{-1}(p) = \\inf\\{y \\in \\Re \\mid F(y)\\ge p\\}\\) is the inverse CDF of \\(X\\). The proof is as follows: \\[\nE(X) = \\int_{0}^1 F^{-1}(p) \\,\\mathrm{d}p = \\int_{0}^1 \\int_{-\\infty}^{F^{-1}(p)} f(x) \\,\\mathrm{d}x \\,\\mathrm{d}p = \\int_{-\\infty}^{\\infty} \\int_{0}^{F(x)} \\,\\mathrm{d}p f(x) \\,\\mathrm{d}x = \\int_{-\\infty}^{\\infty} x f(x) \\,\\mathrm{d}x.\n\\]",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#expectation-and-variance-reward-and-risk",
    "href": "04-dec.html#expectation-and-variance-reward-and-risk",
    "title": "4  Utility, Risk and Decisions",
    "section": "",
    "text": "Example 4.1 (Uber) Let’s look at the histogram of travel times from Fairfax, VA to Washington, DC\n\nd = read.csv(\"../data/dc_travel_time.csv\") \n# use eventing travel times (column 18) and convert from seconds to minutes \nevening_tt = d[,18]/60; day_tt = d[,15]/60; \nevening_tt = evening_tt[!is.na(evening_tt)] # remove missing observations \nhist(evening_tt, freq = F,main=\"\", xlab=\"Travel Time [min]\", nclass=20, col=\"blue\")\n\n\n\n\nTravel times in the evening\n\n\n\n\nFrom this dataset, we can empirically estimate the probabilities of observing different values of travel times\n\nbins = hist(evening_tt, breaks = 3, plot = F) \nknitr::kable(data.frame(\"tt\" = bins$mids, \"Probability\" = bins$counts/length(evening_tt)),col.names = c(\"Travel Time\",\"Probability\"),digits=2)\n\n\n\n\nTravel Time\nProbability\n\n\n\n\n18\n0.05\n\n\n22\n0.77\n\n\n28\n0.18\n\n\n\n\n\nThere is a small chance (5%) I can get to Fairfax in 18 minutes, which probably happens on a holiday and a non-trivial chance (18%) to travel for 28 minutes, possibly due to a sports game or bad weather. Most of the times (77%) our travel time is 22 minutes. However, when Uber shows you the travel time, it uses the expected value as a forecast rather than the full distribution. Specifically, you will be given an expected travel of 23 minutes.\n\n0.05*18 + 0.77*22 + 0.18*28\n\n 23\n\n\nIt is a simple summary takes into account travel accidents and other events that can effect travel time as best as it can.\n\n\n\n\n\n\n4.1.1 Standard Deviation and Covariance\nVariance measures the spread of a random variable around its expected value \\[\n\\Var{X} = \\E{(X-\\E{X})^2} =  \\sum_{i=1}^n (x_i-\\mu)^2 \\prob{X=x_i}.\n\\] In the continious case, we have \\[\n\\Var{X} = \\int_{-\\infty}^\\infty (x-\\mu) ^2 p(x)dx,\n\\] where \\(\\mu = \\mathbb{E}(X)=\\int_{-\\infty}^{\\infty}p_X(x)dx\\). The standard deviation is more convenient and is a square root of variance \\(\\sd{X} = \\sqrt{\\Var{X}}\\). Standard deviation has the desirable property that it is measured in the same units as the random variable \\(X\\) itself and is a more useful measure.\nSuppose that we have two random variables \\(X\\) and \\(Y\\). We need to measure whether they move together or in opposite directions. The covariance is defined by \\[\n\\Cov{X,Y} = \\E{\\left[ X- \\E{X})(Y- \\E{Y}\\right]}.\n\\]\nWhen \\(X\\) and \\(Y\\) are discrete and we are given the joint probability distribution, we need to calculate \\[\n\\Cov{X,Y} = \\sum_{x,y}  ( x - \\E{X} )(y - \\E{Y})p(x,y).\n\\] Covariance is measured in unit of \\(X^2\\times\\)unit of \\(Y^2\\). This can be inconvenient and makes it hard to compare covariances of different pairs of variables. A more convenient metric is the correlation, which is defined by \\[\n\\Cor{X,Y}= \\frac{ \\Cov{X,Y} }{ \\sd{X} \\sd{Y} }.\n\\] Correlation, \\(\\Cor{X,Y}\\), is unites and takes values between 0 and 1.\nIn the case of joint continuous distribution it is convenient to use the covariance matrix \\(\\Sigma\\) which is defined as \\[\n\\Sigma = \\begin{bmatrix}\n\\Var{X} & \\Cov{X,Y} \\\\\n\\Cov{X,Y} & \\Var{Y}\n\\end{bmatrix}.\n\\] If \\(X\\) and \\(Y\\) are independent, then \\(\\Cov{X,Y} = 0\\) and \\(\\Sigma\\) is diagonal. The correlation matrix is defined as \\[\n\\rho = \\begin{bmatrix}\n1 & \\Cor{X,Y} \\\\\n\\Cor{X,Y} & 1\n\\end{bmatrix}.\n\\] If \\(X\\) and \\(Y\\) have an exact linear relationship, then \\(\\Cor{X,Y} = 1\\) and \\(\\Cov{X,Y}\\) is the product of standard deviations. In matrix notations, the relation between the covariance matrix and correlation matrix is given by \\[\n\\rho = \\mathrm{diag}\\left(\\Sigma\\right)^{-1/2} \\Sigma\\mathrm{diag}\\left(\\Sigma\\right)^{-1/2},\n\\] where \\(\\sigma\\) is a diagonal matrix with standard deviations on the diagonal.\n\n\n4.1.2 Portfolios: Linear combinations\nCalculating means and standard deviations of combination of random variables is central tool in probability. It is known as the portfolio problem. Let \\(P\\) be your portfolio, which comprises a mix of two assets \\(X\\) and \\(Y\\), typically stocks and bonds, \\[\nP = aX + bY,\n\\] where \\(a\\) and \\(b\\) are the portfolio weights, typically \\(a+b=1\\), as we are allocating our total capital. Imagine, that you have placed \\(a\\) dollars on the random outcome \\(X\\), and \\(b\\) dollars on \\(Y\\). The portfolio \\(P\\) measures your total weighted outcome.\nKey portfolio rules: The expected value and variance follow the relations \\[\\begin{align*}\n\\E{aX + bY} = &      a\\E{X}+b\\E{Y}\\\\\n\\Var{ aX + bY }  = & a^2 \\Var{X} + b^2 \\Var{Y} + 2 ab \\Cov{X,Y },\n\\end{align*}\\] with covariance defined by \\[\n\\Cov{X,Y} = \\E{ ( X- \\E{X} )(Y- \\E{Y})}.\n\\] Expectation and variance help us to understand the long-run behavior. When we make long-term decisions, we need to use the expectations to avoid biases.\nThe covariance is related to the correlation by \\(\\Cov{X,Y} = \\text{Corr}(X, Y) \\cdot \\sqrt{\\text{Var}{X} \\cdot \\text{Var}{Y}}\\).\n\nExample 4.2 (Tortoise and Hare) Tortoise and Hare who are selling cars. Say \\(X\\) is the number of cars sold and probability distributions, means and variances is given by the following table\n\n\n\n\n\\(X\\)\n\n\n\nMean\nVariance\nsd\n\n\n\n\n\n0\n1\n2\n3\n\\(\\E{X}\\)\n\\(\\Var{X}\\)\n\\(\\sqrt{\\Var{X}}\\)\n\n\nTortoise\n0\n0.5\n0.5\n0\n1.5\n0.25\n0.5\n\n\nHare\n0.5\n0\n0\n0.5\n1.5\n2.25\n1.5\n\n\n\nLet’s do Tortoise expectations and variances \\[\\begin{align*}\n\\E{T} & = (1/2) (1) + (1/2)(2) = 1.5 \\\\\n\\Var{T} & = \\E{T^2} - \\E{T}^2 \\\\\n& =  (1/2)(1)^2 + (1/2)(2)^2 - (1.5)^2 = 0.25\n\\end{align*}\\]\nNow the Hare’s \\[\\begin{align*}\n\\E{H} & = (1/2)(0) + (1/2)(3) = 1.5 \\\\\n\\Var{H} & =  (1/2)(0)^2 + (1/2)(3)^2- (1.5)^2 = 2.25\n\\end{align*}\\]\nWhat do these tell us about the long run behavior?\n\nTortoise and Hare have the same expected number of cars sold.\nTortoise is more predictable than Hare. He has a smaller variance.\n\nThe standard deviations \\(\\sqrt{\\Var{X}}\\) are \\(0.5\\) and \\(1.5\\), respectively. Given two equal means, you always want to pick the lower variance. If we are to invest into one of those, we prefer Tortoise.\nWhat about a portfolio of Tortoise and Hare? Suppose I want to evenly split my investment between Tortoise and Hare. What is the expected number of cars sold and the variance of the number of cars sold? \\[\n\\E{\\frac{1}{2}T + \\frac{1}{2}H} = \\frac{1}{2} \\E{T} + \\frac{1}{2} \\E{H} = 1.5\n\\] For variance, we need to know \\(Cov ( Tortoise, Hare )\\). Let’s take \\(Cov (T,H) = -1\\) and see what happens. \\[\n\\Var{\\frac{1}{2}T + \\frac{1}{2}H} = \\frac{1}{4} \\Var{T} + \\frac{1}{4} \\Var{H} + \\frac{1}{2} \\Cov{T,H} = 0.0625 + 0.5625 -0.5 = 0.125\n\\]",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#expected-utility",
    "href": "04-dec.html#expected-utility",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.2 Expected Utility",
    "text": "4.2 Expected Utility\nLet \\(P,Q\\) be two possible risky gambles or probability bets. An agents preferences can then be specified as an ordering on probability bets where we write \\(P\\) is preferred to \\(Q\\) as \\(P \\succeq Q\\) and indifference as \\(P \\sim Q\\). A compound or mixture bet is defined by the probability assignment \\(p P + (1 - p ) Q\\) for a prospected weight \\(0 \\leq p \\leq 1\\).\nRamsey-de Finetti-Savage show that if an agents’ preferences satisfy a number of plausible axioms – completeness, transitivity, continuity and independence – then they can be represented by the expectation of a utility function. The theory is a normative one and not necessarily descriptive. It suggests how a rational agent should formulate beliefs and preferences and not how they actually behave.\nThis representation of preferences in terms of expected utility \\(U(P)\\) of a risky gamble is then equivalent to \\[\nP \\succeq Q \\; \\; \\iff \\; \\; U (P) \\geq U (Q )\n\\] Therefore, the higher the value taken by the utility function the more the gamble is preferred. Specifically, the axioms lead to existence of expected utility and uniqueness of probability.\nThe two key facts then are uniqueness of probability and existence of expected utility. Formally,\n\nIf \\(P \\succeq R \\succeq Q\\) and \\(w P + (1 - w ) Q \\sim R\\) then \\(w\\) is unique.\nThere exists an expected utility \\(U(\\cdot )\\) such that \\(P \\succeq Q \\; \\; \\iff \\; \\; U (P) \\geq U (Q)\\). Furthermore \\[\nU \\left (w P + (1 - w ) Q \\right ) = wU (P) +(1 - w ) U(Q)\n\\] for any \\(P, Q\\) and \\(0 \\leq w \\leq 1\\).\n\nThis implies that \\(U\\) is additive and it is also unique up to affine transformation.\nProof: If \\(w\\) is not unique then \\(\\exists w_1\\) such that \\(w_1 P + (1 - w_1 ) Q \\sim R\\). Without loss of generality assume that \\(w_1 &lt; w\\) and so \\(0 &lt; w - w_1 &lt; 1 - w_1\\). However, we can write the bet \\(Q\\) as \\[\nQ = \\left ( \\frac{w-w_1}{1-w_1} \\right ) Q + \\left ( \\frac{1-w}{1-w_1} \\right ) Q\n\\] By transitivity, as \\(P \\succeq Q\\) we have \\[\n\\left ( \\frac{w-w_1}{1-w_1} \\right ) P + \\left ( \\frac{1-w}{1-w_1} \\right ) Q \\succeq Q\n\\] However, \\[\nw P + ( 1 - w) Q = w_1 P + (1 - w_1 ) \\left (  \\left ( \\frac{w-w_1}{1-w_1} \\right ) P + \\left ( \\frac{1-w}{1-w_1} \\right ) Q\n\\right )\n\\] implying by transitivity that \\[\nw P + (1 - w ) Q \\succeq w_1 P + (1 - w_1 ) Q\n\\] which is a contradiction.\nThis can be used together with the axioms to then prove the existence and uniqueness of a utility function.\n\nTheorem 4.1 If \\(V\\) is any other function satisfying these results then \\(V\\) is an affine function of \\(U\\).\n\n\nProof. If \\(\\forall P , Q\\) we have \\(P \\sim Q\\), then define \\(u(P) \\equiv 0\\). Hence suppose that there exists \\(S \\succ T\\). Define \\(U(S) =1\\) and \\(U(T)=0\\). For any \\(P \\in \\mathcal{P}\\) there are five possibilities: \\(P \\succ T\\) or \\(P \\sim S\\) or \\(S \\succ P \\succ T\\) or \\(P \\sim T\\) or \\(T \\succ P\\).\nIn the first case define \\(1/U(P)\\) to be the unique \\(p\\) (see previous theorem) defined by \\(p P + ( 1 -p )T \\sim S\\). In the second case, define \\(U(P) =1\\). In the third, there exists a unique \\(q\\) with \\(q S + ( 1 -q )T \\sim P\\) and then define \\(U(P)=q\\). In the fourth case, define \\(U(P)=0\\) and finally when \\(T \\succ P\\) there exists a unique \\(r\\) with \\(r S + ( 1-r )P \\sim T\\) and then we define \\(U(P) = - r / (1 - r)\\).\nThen check that \\(U(P)\\) satisfies the conditions. See Savage (1954), Ramsey (1927) and de Finetti (1931)\n\nOther interesting extensions: how do people come to a consensus (DeGroot, 1974, Morris, 1994, 1996). Ramsey (1926) observation that if someone is willing to offer you a bet then that’s conditioning information for you. All probabilities are conditional probabilities.\nIf the bet outcome \\(P\\) is a monetary value, then the utility functions \\(P, P^2, \\sqrt{P}, \\ln P\\) are all monotonically increasing (the more the better). However, the utility function \\(P^2\\) is concave and the utility function \\(\\ln P\\) is convex. The concavity of the utility function implies that the agent is risk averse and the convexity implies that the agent is risk seeking.\n\nExample 4.3 (Saint Petersburg Paradox) The Saint Petersburg paradox is a concept in probability and decision theory that was first introduced by Daniel Bernoulli in 1738. It revolves around the idea of how individuals value risky propositions and how those valuations may not align with classical expected utility theory.\nThe paradox is named after the city of Saint Petersburg, where the problem was formulated. Here’s a simplified version of the paradox:\nImagine a gambling game where a fair coin is flipped repeatedly until it lands on heads. The payoff for the game is \\(2^n\\), where n is the number of tosses needed for the coin to land on heads. The expected value of this game, calculated by multiplying each possible payoff by its probability and summing the results, is infinite:\n\\[\nE(X) = \\frac{1}{2} \\cdot 2 + \\frac{1}{4} \\cdot 4 + \\frac{1}{8} \\cdot 8 + \\ldots = \\infty\n\\]\nThis means that, in theory, a rational person should be willing to pay any finite amount to play this game, as the expected value is infinite. However, in reality, most people would be unwilling to pay a large amount to play such a game.\nThe paradox arises because traditional expected utility theory assumes that individuals make decisions based on maximizing their expected gain. Bernoulli argued that people do not maximize expected monetary value but rather expected utility, where utility is a subjective measure of satisfaction or happiness. He proposed that individuals exhibit diminishing marginal utility for wealth, meaning that the additional satisfaction gained from an extra unit of wealth decreases as total wealth increases.\nIn the case of the Saint Petersburg paradox, although the expected monetary value is infinite, the utility gained from each additional dollar diminishes rapidly, leading to a reluctance to pay large amounts to play the game.\nIn modern decision theory and economics, concepts like diminishing marginal utility and expected utility are fundamental in understanding how individuals make choices under uncertainty and risk. The Saint Petersburg paradox highlights the limitations of relying solely on expected monetary value in explaining human behavior in such situations.\nOne common approach is to consider aspects of potential players, such as their possible risk aversion, available funds, etc., through a utility function \\(U(x)\\). Applying a utility function in this situation means changing our focus to the quantity \\[\nE[U(X)] = \\sum^\\infty_{k=1} 2^{-k} U(2^k).   \n\\]\nSome examples of utility functions are,\n\n\\(U(x) = V_0 (1-x^{-\\alpha})\\), \\(\\alpha &gt; 0\\), which gives an expected utility of \\(V_0 \\left(1-\\frac{1}{2^{\\alpha+1}-1}\\right)\\)\nLog utility, \\(U(x) = \\log(x)\\), with expected value \\(2 \\log(2)\\).\n\nNotice that after obtaining an expected utility value, you’ll have to find the corresponding reward/dollar amount.\n\nNow, consider a more general situation, when you have two gambles 1: get \\(P_1\\) for sure, 2: get \\(P_2 = P_1+k\\) and \\(P_3 = P_1-k\\) with probability 1/2. Then we will compare the utility of those gambles \\[\n\\dfrac{1}{2}U(P_2) + \\dfrac{1}{2}U(P_3) \\text{ and } U(P_1).\n\\] If the utility function is linear then we should be indifferent between the two gambles. However, if the utility function is concave then we should prefer the sure thing. This is known as the certainty effect. \\[\n\\dfrac{1}{2}U(P_2) + \\dfrac{1}{2}U(P_3) &lt; U(P_1).\n\\]\nThe usual situation can be described as follows. Let \\(\\Omega\\) be a finite set of possible outcomes with \\(\\Omega = \\{ \\omega_1 , \\ldots , \\omega_n \\}\\). Let \\(P_i\\) be the consequence that assigns one to outcome \\(\\omega_i\\) and zero otherwise and let \\(P = ( p_1 , \\ldots , p_n )\\) assign probability \\(p_i\\) to outcome \\(\\omega_i\\). Then we can write the expected utility, \\(U(P)\\), of the gamble \\(P\\) as \\[\nU(P) = \\sum_{i=1}^n p_i U( P_i ).\n\\] That is, the utility of \\(P\\) is the expected value of a random variable \\(W\\) (wealth) that takes the value \\(U(P_i)\\) if the outcome is \\(\\omega_i\\). Therefore, we can write \\(U(P) = \\mathbb{E}_P \\left ( U( W ) \\right)\\).\nThis leads us to the notion of risk aversion and a categorization of agents according to their risk tolerance: the agent is said to be\n\nRisk Averse if \\(\\mathbb{E}_P \\left ( u(W) \\right ) \\leq u \\left (  \\mathbb{E}_P  (W)  \\right )\\)\nRisk Neutral if \\(\\mathbb{E}_P \\left ( u(W) \\right ) =    u \\left (  \\mathbb{E}_P  (W)  \\right )\\)\nRisk Seeking if \\(\\mathbb{E}_P \\left ( u(W) \\right ) \\geq u \\left (  \\mathbb{E}_P  (W)  \\right )\\)\n\nHere we assum that that these hold for all probabilities and random variables. Risk aversion is equivalent to the agent having concave utility and risk seeking convex.\n\nExample 4.4 (Risk Aversion) Consider two concave utility functions log-utility (Kelly) \\(U(W)=\\log(W)\\) and fractional Kelly \\(U(W)=\\dfrac{W^{1-\\gamma} - 1}{1-\\gamma}\\), when \\(\\gamma = 1\\), we get the log-utility. Both correspond to different risk aversion levels. The log-utility is a special case of the power utility function, which is defined as \\(U(W) = \\frac{W^{1-\\gamma} - 1}{1-\\gamma}\\) for \\(\\gamma &gt; 0\\). The fractional Kelly criterion is a specific case of the power utility function where \\(\\gamma = 2\\).\n\nw &lt;- seq(0.2, 2, length.out = 500)\ngamma=2\n # Plot this data frame\ndf &lt;- data.frame(w = w, \n                 logu = log(w), \n                 kelly = (w^(1-gamma) - 1) / (1-gamma))\n# 3. Plot use different y-scales for \nggplot(df, aes(x = w)) +\n  geom_line(aes(y = logu, color = \"log utility\"), size = 1.5) +\n  geom_line(aes(y = kelly, color = \"Kelly criterion\"), size = 1.5) +\n  geom_vline(xintercept = c(0.5, 1, 1.5), linetype = \"dashed\", color = \"black\") +\n  scale_color_manual(values = c(\"log utility\" = \"blue\", \"Kelly criterion\" = \"red\")) +\n  labs(title = \"Concave vs Convex Utility Functions\",\n       x = \"Wealth (w)\",\n       y = \"Utility\",\n       color = \"Utility Type\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nw1 &lt;- 0.5\nw2 &lt;- 1.5\np &lt;- 0.5\ngamme = 2\n# Utility function (example: log utility)\nutility1 &lt;- function(w) log(w)\nutility2 &lt;- function(w) (w^(1 - gamma) - 1) / (1 - gamma)\n\n# Calculate expected utility\nexpected_utility &lt;- p * utility1(w1) + p * utility1(w2)\nexpected_utility2 &lt;- p * utility2(w1) + p * utility2(w2)\n# Print expected utility\ncat(\"Expected utility for log utility:\", expected_utility, \"\\n\")\n\nExpected utility for log utility: -0.14 \n\ncat(\"Expected utility for Kelly criterion:\", expected_utility2, \"\\n\")\n\nExpected utility for Kelly criterion: -0.33 \n\n\n\nGeometrically, Jensen’s inequality explains the pattern: for a concave function the chord lies below the curve (risk aversion).\n\n\nExample 4.5 (Kelly Criterion) The Kelly criterion has been used effectively by many practitioners. Ed Thorp, in his book Beat the Dealer, pioneered its use in blackjack and later applied it to investing in financial markets. Since then, many market participants, such as Jim Simons, have stressed the importance of this money management approach. The criterion’s application extends to other domains: Phil Laak described its use for bet sizing in a game-theoretic approach to poker, and Bill Benter applied it to horse racing. Stewart Ethier provided a mathematical framework for multiple outcomes and analyzed a “play the winner” rule in roulette. Claude Shannon also developed a system to detect and exploit unintentionally biased roulette wheels, an endeavor chronicled in the book The Eudaemonic Pie.\nSuppose you have $1000 to invest. With probability \\(0.55\\) you will win whatever you wager and with probability \\(0.45\\) you lose whatever you wager. What’s the proportion of capital that leads to the fastest compounded growth rate?\nQuoting Kelly (1956), the exponential rate of growth, \\(G\\), of a gambler’s capital is \\[\nG = \\lim_{N\\to \\infty} \\frac{1}{N} \\log_2 \\frac{W_N}{W_0}\n\\] for initial capital \\(W_0\\) and capital after \\(N\\) bets \\(W_N\\).\nUnder the assumption that a gambler bets a fraction of his capital, \\(\\omega\\), each time, we use \\[\nW_N = (1+\\omega)^W (1-\\omega)^L W_0\n\\] where \\(W\\) and \\(L\\) are the number of wins and losses in \\(N\\) bets. We get \\[\nG = p \\log_2(1+\\omega)+ q \\log_2(1-\\omega)\n\\] in which the limit(s) of \\(\\frac{W}{N}\\) and \\(\\frac{L}{N}\\) are the probabilities \\(p\\) and \\(q\\), respectively.\nThis also comes about by considering the sequence of i.i.d. bets with \\[\np ( X_t = 1 ) = p \\; \\; \\text{ and} \\; \\; p ( X_t = -1 ) = q=1-p\n\\] We want to find an optimal allocation \\(\\omega^*\\) that maximizes the expected long-run growth rate: \\[\\begin{align*}\n\\max_\\omega \\mathbb{E} \\left ( \\ln ( 1 + \\omega W_T ) \\right )\n& = p \\ln ( 1 + \\omega ) + (1 -p) \\ln (1 - \\omega ) \\\\\n& \\leq p \\ln p + q \\ln q + \\ln 2 \\; \\text{ and} \\; \\omega^\\star = p - q\n\\end{align*}\\]\nThe solution is \\(w^* = 0.55 - 0.45 = 0.1\\).\nBoth approaches give the same optimization problem, which, when solved, give the optimal fraction rate \\(\\omega^* = p-q\\), thus, with \\(p=0.55\\), the optimal allocation is 10% of capital.\nWe can generalize the rule to the case of asymmetric payouts \\((a,b)\\). Then the expected utility function is \\[\np \\ln ( 1 + b \\omega ) + (1 -p) \\ln (1 - a \\omega )\n\\] The optimal solution is \\[\n\\omega^\\star = \\frac{bp - a q}{ab}\n\\]\nIf \\(a=b=1\\) this reduces to the pure Kelly criterion.\nA common case occurs when \\(a=1\\) and market odds \\(b=O\\). The rule becomes \\[\n\\omega^* = \\frac{p \\cdot O  -q }{O}.\n\\]\nLet’s consider another scenario. You have two possible market opportunities: one where it offers you \\(4/1\\) when you have personal odds of \\(3/1\\) and a second one when it offers you \\(12/1\\) while you think the odds are \\(9/1\\).\nIn expected return these two scenarios are identical both offering a 33% gain. In terms of maximizing long-run growth, however, they are not identical.\nTable 4.1 shows the Kelly criteria advises an allocation that is twice as much capital to the lower odds proposition: \\(1/16\\) weight versus \\(1/40\\).\n\n\n\nTable 4.1: Kelly rule\n\n\n\n\n\nMarket\nYou\n\\(p\\)\n\\(\\omega^\\star\\)\n\n\n\n\n\\(4/1\\)\n\\(3/1\\)\n\\(1/4\\)\n\\(1/16\\)\n\n\n\\(12/1\\)\n\\(9/1\\)\n\\(1/10\\)\n\\(1/40\\)\n\n\n\n\n\n\nThe optimal allocation \\(\\omega^\\star = ( p O - q ) / O\\) is \\[\n\\frac{ (1/4) \\times 4 - (3/4) }{4} = \\frac{1}{16} \\; \\text{ and} \\;\n\\frac{ (1/10) \\times 12 - (9/10) }{12} = \\frac{1}{40}.\n\\]\n\nPower utility and log-utilities allow to model constant relative risk aversion (CRRA). The main advantage that the optimal rule is unaffected by wealth effects. The CRRA utility of wealth takes the form \\[\nU_\\gamma (W) = \\frac{ W^{1-\\gamma} -1 }{1-\\gamma}\n\\]\nThe special case \\(U(W) = \\log (W )\\) for \\(\\gamma = 1\\).\nThis leads to a myopic Kelly criterion rule.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#unintuitive-nature-of-decision-making",
    "href": "04-dec.html#unintuitive-nature-of-decision-making",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.3 Unintuitive Nature of Decision Making",
    "text": "4.3 Unintuitive Nature of Decision Making\n\nExample 4.6 (Ellsberg Paradox: Ambiguity Aversion) The Ellsberg paradox is a thought experiment that was first proposed by Daniel Ellsberg in 1961. It is a classic example of a situation where individuals exhibit ambiguity aversion, meaning that they prefer known risks over unknown risks. The paradox highlights the importance of considering ambiguity when making decisions under uncertainty.\nThere are two urns each containing 100 balls. It is known that urn A contains 50 red and 50 black, but urn B contains an unknown mix of red and black balls. The following bets are offered to a participant:\n\nBet 1A: get $1 if red is drawn from urn A, $0 otherwise\nBet 2A: get $1 if black is drawn from urn A, $0 otherwise\nBet 1B: get $1 if red is drawn from urn B, $0 otherwise\nBet 2B: get $1 if black is drawn from urn B, $0 otherwise\n\n\n\nExample 4.7 (Allais Paradox: Independence Axiom) The Allais paradox is a choice problem designed by Maurice Allais to show an inconsistency of actual observed choices with the predictions of expected utility theory. The paradox is that the choices made in the second problem seem irrational, although they can be explained by the fact that the independence axiom of expected utility theory is violated.\nWe run two experiments. In each experiment a participant has to make a choice between two gambles.\n\n\n\n\n\n\n\n\n\nExperiment 1\n\n\n\n\n\n\n\nGamble \\({\\cal G}_1\\)\n\nGamble \\({\\cal G}_2\\)\n\n\n\nWin\nChance\nWin\nChance\n\n\n$25m\n0\n$25m\n0.1\n\n\n$5m\n1\n$5m\n0.89\n\n\n$0m\n0\n$0m\n0.01\n\n\n\n\n\n\n\n\nExperiment 2\n\n\n\n\n\n\n\nGamble \\({\\cal G}_3\\)\n\nGamble \\({\\cal G}_4\\)\n\n\n\nWin\nChance\nWin\nChance\n\n\n$25\n0\n$25m\n0.1\n\n\n$5\n0.11\n$5m\n0\n\n\n$0m\n0.89\n$0m\n0.9\n\n\n\n\n\n\nThe difference in expected gains is identical in two experiments\n\nE1 = 5*1 \nE2 = 25*0.1 + 5*0.89 + 0*0.01\nE3 = 5*0.11 + 0*0.89\nE4 = 25*0.1 + 0*0.9\nprint(c(E1-E2,E3-E4))\n\n -2 -2\n\n\nHowever, typically a person prefers \\({\\cal G}_1\\) to \\({\\cal G}_2\\) and \\({\\cal G}_4\\) to \\({\\cal G}_3\\), we can conclude that the expected utilities of the preferred is greater than the expected utilities of the second choices. The fact is that if \\({\\cal G}_1 \\geq {\\cal G}_2\\) then \\({\\cal G}_3 \\geq {\\cal G}_4\\) and vice-versa.\nAssuming the subjective probabilities \\(P = ( p_1 , p_2 , p_3)\\). The expected utility \\(E ( U | P )\\) is \\(u ( 0 ) = 0\\) and for the high prize set \\(u ( \\$ 25 \\; \\text{million} ) = 1\\). Which leaves one free parameter \\(u = u ( \\$ 5 \\; \\text{million} )\\).\nHence to compare gambles with probabilities \\(P\\) and \\(Q\\) we look at the difference \\[\nE ( u | P ) - E ( u | Q ) = ( p_2 - q_2 ) u + ( p_3 - q_3 )\n\\]\nFor comparing \\({\\cal G}_1\\) and \\({\\cal G}_2\\) we get \\[\\begin{align*}\nE ( u | {\\cal G}_1 ) - E ( u | {\\cal G}_2 ) &= 0.11 u - 0.1 \\\\\nE ( u | {\\cal G}_3 ) - E ( u | {\\cal G}_4 ) &= 0.11 u - 0.1\n\\end{align*}\\] The order is the same, given your \\(u\\). If your utility satisfies \\(u &lt; 0.1/0.11 = 0.909\\) you take the “riskier” gamble.\n\n\nExample 4.8 (Winner’s Curse) One of the interesting facts about expectation is that when you are in a competitive auctioning game then you shouldn’t value things based on pure expected value. You should take into consideration the event that you win \\(W\\). Really you should be calculating \\(E(X\\mid W)\\) rather than \\(E(X)\\).\nThe winner’s curse: given that you win, you should feel regret: \\(E(X\\mid W) &lt; E(X)\\).\nA good example is claiming racehorse whose value is uncertain.\n\n\n\nValue\nOutcome\n\n\n\n\n0\nhorse never wins\n\n\n50,000\nhorse improves\n\n\n\nSimple expected value tells you \\[\nE(X) = \\frac{1}{2} \\cdot 0 + \\frac{1}{2} \\cdot 50,000 = \\$25,000.\n\\] In a $20,000 claiming race (you can buy the horse for this fixed fee ahead of time from the owner) it looks like a simple decision to claim the horse.\nIts not so simple! We need to calculate a conditional expectation. What’s \\(E( X\\mid W )\\), given you win event (\\(W\\))? This is the expected value of the horse given that you win that is relevant to assessing your bid. In most situations \\(E(X\\mid W) &lt; 20,000\\).\nAnother related feature is this problem is asymmetric information. The owner or trainer of the horse maybe know something that you don’t know. There’s a reason why they are entering the horse into a claiming race in the first place.\nWinner’s curse implies that immediately after you have win, you should feel a little regret, as the object is less valuable to you after you have won! Or put another way, in an auction nobody else in the room is willing to offer more than you at that time.\n\n\nExample 4.9 (The Hat Problem) There are \\(N\\) prisoners in a forward facing line. Each guy is wearing a blue or red hat. Everyone can see all the hats in front of him, but cannot see his own hat. The hats can be in any combination of red and blue, from all red to all blue and every combination in between. The first guy doesn’t know his own hat.\nA guard is going to walk down the line, starting in the back, and ask each prisoner what color hat they have on. They can only answer “blue” or “red.” If they answer incorrectly, or say anything else, they will be shot dead on the spot. If they answer correctly, they will be set free. Each prisoner can hear all of the other prisoners’ responses, as well as any gunshots that indicate an incorrect response. They can remember all of this information.\nThere is a rule that all can agree to follow such that the first guy makes a choice (“My hat is …”) and everyone after that, including the last guy, will get their color right with probability \\(1\\). You have a \\(100\\)% chance of saving all but the last prisoner, and a \\(50\\)% chance of saving that one. Here’s the strategy the prisoners have agreed on. The last prisoner counts the number of blue hats worn; if the number is even, the last prisoner yells “blue”, if odd, yells “red”. If the \\(99\\)th prisoner hears “blue”, but counts an odd number of blue hats, then his hat must be blue so that the total number of blue hats is even. If he counts an even number of blue hats, then his hat must be red. If the last prisoner yells red, then 99 knows that there are an odd number of blue hats. So 99 counts the number of blue hats he can see. Again, if they are even, his hat is blue, if odd, his hat is red. The 99th prisoner then yells out the color of his hat and is spared. The next prisoner now knows whether the remaining number of blue hats, including his own, is odd or even, by taking into account whether 99 had a blue hat or not. Then by counting the number of blue hats he sees, he knows the color of his hat. So he yells out the color of his hat and is spared. This saves all but the last prisoner, and there is a \\(50\\)% chance that his hat is the color he shouted out.\nOne hundred prisoners are too many to work with. Suppose there are two. The last person can save the guy in front of him by shouting out the color of his hat. OK, how about if there are three? The third prisoner can see 0,1, or 2 blue hats. There seem to be three possibilities but only two choices of things to say. But, two of the possibilities have something in common namely the number of blue hats is even. So if the last prisoner yells “blue” then he can tell 1 and 2 that he sees an even number of blue hats. Then the second prisoner, by looking ahead and counting the number of blue hats, knows his must be blue if he sees one blue hat, and red if he sees no blue hats. The last prisoner agrees to yell “red” if the number of blue hats seen is odd. Then if 2 sees a blue hat on 1, his must be red, and if 1 has a red hat, his must be blue. By shouting out the color of his hat, 1 also knows his hat color. Two “blues” or two “reds” in a row mean he wears blue, while one blue and one red means he wears red. OK. This looks like this always works, because there are always only two possibilities as far as the number of blue hats worn they are either even or odd. So, check as in the three-person case that using this strategy (“blue” for an even number of blue hats “red” for an odd number) tells 99 the color of his hat, and then each prisoner in turn can learn the color of his hat by taking into account the parity of the number of blue hats he can see, the parity of the number of blue hats 100 saw and the number of prisoners behind him wearing blue hats.\n\n\nExample 4.10 (Lemon’s Problem) The lemon problem is an interesting conditional probability puzzle and is a classic example of asymmetric information in economics. It was first proposed by George Akerlof in his 1970 paper “The Market for Lemons: Quality Uncertainty and the Market Mechanism.” The problem highlights the importance of information in markets and how it can lead to adverse selection, where the quality of goods or services is lower than expected.\nThe basic tenet of the lemons principle is that low-value cars force high-value cars out of the market because of the asymmetrical information available to the buyer and seller of a used car. This is primarily due to the fact that a seller does not know what the true value of a used car is and, therefore, is not willing to pay a premium on the chance that the car might be a lemon. Premium-car sellers are not willing to sell below the premium price so this results in only lemons being sold.\nSuppose that a dealer pays $20K for a car and wants to sell for $25K. Some cars on the market are Lemons. The dealer knows whether a car is a lemon. A lemon is only worth $5K. There is asymmetric information as the customer doesn’t know if the particular new car is a lemon. S/he estimates the probability of lemons on the road by using the observed frequency of lemons. We will consider two separate cases:\n\nLet’s first suppose only 10% of cars are lemons.\nWe’ll then see what happens if 50% are lemons.\n\nThe question is how does the market clear (i.e. at what price do car’s sell). Or put another way does the customer buy the car and if so what price is agreed on? This is very similar to winner’s curse: when computing an expected value what conditioning information should I be taking into account?\nIn the case where the customer thinks that \\(p=0.10\\) of the car’s are lemons, they are willing to pay \\[\nE (X)= \\frac{9}{10} \\cdot 25 + \\frac{1}{10} \\cdot 5 = \\$ 23 K\n\\] This is greater than the initial $20 that the dealer paid. The car then sells at $23K \\(&lt;\\) $25K.\nOf course, the dealer is disappointed that there are lemons on the road as he is not achieving the full value – missing $2000. Therefore, they should try and persuade the customer its not a lemon by offering a warranty for example.\nThe more interesting case is when \\(p=0.5\\). The customer now values the car at \\[\nE (X)  = \\frac{1}{2} \\cdot 25 + \\frac{1}{2} \\cdot 5 = \\$ 15K\n\\] This is lower than the $20K – the reservation price that the dealer would have for a good car. Now what type of car and at what price do they sell?\nThe key point in asymmetric information is that the customer must condition on the fact that if the dealer still wants to sell the car, the customer must update his probability of the type of the car. We already know that if the car is not a lemon, the dealer won’t sell under his initial cost of $20K. So at $15K he is only willing to sell a lemon. But then if the customer computes a conditional expectation \\(E( X \\mid \\mathrm{Lemon})\\) – conditioning on new information that the car is a lemon \\(L\\) we get the valuation \\[\nE ( X \\mid L ) = 1 \\cdot  5 = \\$ 5K\n\\] Therefore only lemons sell, at $ 5K, even if the dealer has a perfectly good car the customer is not willing to buy!\nAgain what should the dealer do? Try to raise the quality and decrease the frequency of lemons in the observable market. This type of modeling has all been used to understand credit markets and rationing in periods of loss of confidence.\n\n\nExample 4.11 (Envelope Paradox) The envelope paradox is a thought experiment or puzzle related to decision-making under uncertainty. It is also known as the “exchange paradox” or the “two-envelope paradox.” The paradox highlights the importance of carefully considering the information available when making decisions under uncertainty and the potential pitfalls of making assumptions about unknown quantities.\nA swami puts \\(m\\) dollars in one envelope and \\(2 m\\) in another. He hands on envelope to you and one to your opponent. The amounts are placed randomly and so there is a probability of \\(\\frac{1}{2}\\) that you get either envelope.\nYou open your envelope and find \\(x\\) dollars. Let \\(y\\) be the amount in your opponent’s envelope. You know that \\(y = \\frac{1}{2} x\\) or \\(y = 2 x\\). You are thinking about whether you should switch your opened envelope for the unopened envelope of your friend. It is tempting to do an expected value calculation as follows \\[\nE( y) = \\frac{1}{2} \\cdot  \\frac{1}{2} x + \\frac{1}{2} \\cdot 2 x = \\frac{5}{4} x &gt; x\n\\] Therefore, it looks as if you should switch no matter what value of \\(x\\) you see. A consequence of this, following the logic of backwards induction, that even if you didn’t open your envelope that you would want to switch! Where’s the flaw in this argument?\nThis is an open-ended problem, but it will not be very confusing if we well understand both the frequentist and bayesian approaches. Actually, this is a very good example to show how these two approaches are different and to check if we understand them correctly. There many conditions in this problem, so we cannot argue everything in this example; instead, we are going to focus on some interesting cases. First, assume we’re risk-neutral (although, we can simply change “money” with “utility” in this paradox, so it doesn’t matter). We will compare frequentist/bayesian, open/not open, and discrete/continuous. The finite, or bounded space, case will not be considered here since they are not very interesting.\nIf I DO NOT look in my envelope, in this case, even from a frequentist viewpoint, we can find a fallacy in this naive expectation reasoning \\(E[trade] = 5X/4\\) . First, the right answer from a frequentist view is, loosely, as follows. If we switch the envelope, we can obtain \\(m\\) (when \\(X = m\\)) or lose \\(m\\) (when \\(X = 2m\\)) with the same probability \\(1/2\\). Thus, the value of a trade is zero, so that trading matters not for my expected wealth.\nInstead, naive reasoning is confusing the property of variables \\(x\\) and \\(m\\), \\(x\\) is a random variable and \\(m\\) is a fixed parameter which is constant (again, from a frequentist viewpoint). By trading, we can obtain \\(x\\) or lose \\(x/2\\) with the same probability. Here, the former \\(x=m\\) is different from the latter \\(X= 2m\\). Thus, \\(X \\frac{1}{2} - \\frac{X}{2} \\frac{1}{2} = \\frac{X}{4}\\) is the wrong expected value of trading. On the other hand, from a bayesian view, since we have no information, we are indifferent to either trading or not.\nThe second scenario is if I DO look in my envelope. As the Christensen & Utts (1992) article said, the classical view cannot provide a completely reasonable resolution to this case. It is just ignoring the information revealed. Also, the arbitrary decision rule introduced at the end of the paper or the extension of it commented by Ross (1996) are not the results of reasoning from a classical approach. However, the bayesian approach provides a systematic way of finding an optimal decision rule using the given information.\nWe can use the Bayes rule to update the probabilities of which envelope your opponent has! Assume \\(p(m)\\) of dollars to be placed in the envelope by the swami. Such an assumption then allows us to calculate an odds ratio \\[\n\\frac{ p \\left ( y = \\frac{1}{2} x | x \\right ) }{ p \\left ( y = 2 x | x \\right ) }\n\\] concerning the likelihood of which envelope your opponent has.\nThen, the expected value is given by \\[\nE(y\\mid x) =  p \\left ( y = \\frac{1}{2} x \\mid  x \\right ) \\cdot  \\frac{1}{2} x +\n  p \\left ( y = 2 x | x \\right ) \\cdot 2 x\n\\] and the condition \\(E(y) &gt; x\\) becomes a decision rule.\nLet \\(g(m)\\) be the prior distribution of \\(m\\). Applying Bayes’ theorem, we have \\[\np(m = x \\mid X = x) = \\frac{p(X = x \\mid m = x) g(x)}{p(X = x)} = \\frac{g(x)}{g(x)+g(x/2)}.\n\\] Similarly, we have \\[\np(m = x/2 \\mid X = x) = \\frac{p(X = x \\mid m = x/2) g(x/2)}{p(X = x/2)} = \\frac{g(x/2)}{g(x)+g(x/2)}.\n\\] The Bayesian can now compute his expected winnings from the two actions. If he keeps the envelope he has, he wins \\(x\\) dollars. If he trades envelopes, he wins \\(x/2\\) if he currently has the envelope with \\(2m\\) dollars, i.e., if \\(m = x/2\\) and he wins \\(2\\)x if he currently has the envelope with \\(m\\) dollars, i.e., \\(m = x\\). His expected winnings from a trade are \\[\nE(W\\mid Trade) = E(Y\\mid X = x) = \\frac{g(x/2)}{g(x)+g(x/2)} \\frac{x}{2} + \\frac{g(x)}{g(x)+g(x/2)} 2x.\n\\] It is easily seen that when \\(g(x/2) = 2g(x)\\), \\(E(W\\mid Trade) = x\\). Therefore, if \\(g(x/2) &gt; 2g(x)\\) it is optimal to keep the envelope and if \\(g(x/2) &lt; 2g(x)\\) it is optimal to trade envelopes. For example, if your prior distribution on \\(m\\) is exponential \\(\\lambda\\), so that \\(g(m) = \\lambda e^{-\\lambda m}\\), then it is easily seen that it is optimal to keep your envelope if \\(x &gt; 2\\log(2)/\\lambda\\).\nThe intuitive value of the expected winnings when trading envelopes was shown to be \\(5x/4\\). This value can be obtained by assuming that \\(g(x)/[g(x) + g(x/2)] =\n1/2\\) for all \\(x\\). In particular, this implies that \\(g(x) = g(x/2)\\) for all x, i.e., \\(g(x)\\) is a constant function. In other words, the intuitive expected winnings assumes an improper “noninformative” uniform density on \\([0, \\infty)\\). It is of interest to note that the improper noninformative prior for this problem gives a truly noninformative (maximum entropy) posterior distribution.\nMost of the arguments in the Christensen & Utts (1992) paper are right, but there is one serious error in the article which is corrected in Bachman-Christensen-Utts (1996) and discussed in Brams & Kilgour (1995). The paper calculated the marginal density of \\(X\\) like below. \\[\\begin{align*}\np(X = x) &= p(m = x)g(x) + p(2m = x)g(x/2) \\\\\n&= \\frac{1}{2} g(x) + \\frac{1}{2} g(x/2)\n\\end{align*}\\] where \\(g(x)\\) is the prior distribution of \\(m\\). However, integrating \\(p(X = x)\\) with respect to \\(x\\) from \\(0\\) to \\(\\infty\\) gives \\(3/2\\) instead of \\(1\\). In fact, their calculation of \\(p(X = x)\\) can hold only when the prior distribution \\(g(x)\\) is discrete and \\(p(X = x)\\), \\(g(m)\\), \\(g(m/2)\\) represent the probabilities that \\(X = x\\), \\(m = m\\), \\(m = m/2\\), respectively.\nFor the correct calculation of the continuous \\(X\\) case, one needs to properly transform the distribution. That can be done by remembering to include the Jacobian term alongside the transformed PDF, or by working with the CDF of \\(X\\) instead. The latter forces one to properly consider the transform, and we proceed with that method.\nLet \\(G(x)\\) be the CDF of the prior distribution of \\(m\\) corresponding to \\(g(x)\\). \\[\\begin{align*}\np(x &lt; X \\leq x+dx) &= p(m = x)dG(x)+ p(2m = x)dG(x/2) \\\\\n&= \\frac{1}{2} \\left( dG(x)+ dG(x/2) \\right)\n\\end{align*}\\] where \\(g(x) = dG(x)/dx\\). Now, the PDF of \\(X\\) is \\[\\begin{align*}\nf_X(x) &= \\frac{d}{dx} p(x &lt; X \\leq x + dx) \\\\\n&= \\frac{1}{2} \\left(g(x) + \\frac{1}{2} g(x/2) \\right)\n\\end{align*}\\] We have an additional \\(1/2\\) in the last term due to the chain rule, or the Jacobian in the change-in-variable formula. Therefore, the expected amount of a trade is \\[\\begin{align*}\nE(Y\\mid X = x) &= \\frac{x}{2} p(2m = x\\mid X = x) + 2 x \\, p(m = x\\mid X = x) \\\\\n&= \\frac{x}{2} \\frac{g(x)}{g(x) + g(x/2)/2} + 2 x \\frac{g(x/2)/2}{g(x) + g(x/2)/2} \\\\\n&=  \\frac{\\frac{x}{2}g(x) + x g(x/2)}{g(x) + g(x/2)/2}\n\\end{align*}\\]\nThus, for the continuous case, trading is advantageous whenever \\(g(x/2) &lt; 4g(x)\\), instead of the decision rule for the discrete case \\(g(x/2) &lt; 2g(x)\\).\nNow, think about which prior will give you the same decision rule as the frequentist result. In the discrete case, \\(g(x)\\) such that \\(g(x/2) = 2g(x)\\), and in the continuous case \\(g(x)\\) such that \\(g(x/2) = 4g(x)\\). However, both do not look like useful, non-informative priors. Therefore, the frequentist approach does not always equal the Bayes approach with a non-informative prior. At the moment you start to treat \\(x\\) as a given number, and consider \\(p(m \\mid X = x)\\) (or \\(p(Y \\mid X = x)\\)), you are thinking in a bayesian way, and need to understand the implications and assumptions in that context.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#decision-trees",
    "href": "04-dec.html#decision-trees",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.4 Decision Trees",
    "text": "4.4 Decision Trees\nDecision trees can effectively model and visualize conditional probabilities. They provide a structured way to break down complex scenarios into smaller, more manageable steps, allowing for clear calculations and interpretations of conditional probabilities.\nEach node in a decision tree, including thr root represents an event or condition. The branches represent the possible outcomes of that condition. Along each branch, you’ll often see a probability. This is the chance of that outcome happening, given the condition at the node. As you move down the tree, you’re looking at more specific conditions and their probabilities. The leaves of the tree show the final probabilities of various outcomes, considering all the conditions along the path to that leaf. Thus, the probabilities of the leaves need to sum to 1.\n\nExample 4.12 (Medical Testing) A patient goes to see a doctor. The doctor performs a test which is 95% sensitive – that is 95 percent of people who are sick test positive and 99% specific – that is 99 percent of the healthy people test negative. The doctor also knows that only 1 percent of the people in the country are sick. Now the question is: if the patient tests positive, what are the chances the patient is sick? The intuitive answer is 99 percent, but the correct answer is 66 percent.\nFormally, we have two binary variables, \\(D=1\\) that indicates you have a disease and \\(T=1\\) that indicates that you test positive for it. The estimates we know already are given by \\(P(D) = 0.02\\), \\(P(T\\mid D) = 0.95\\), and \\(P(\\bar T \\mid \\bar D) = 0.99\\). Here we used shortcut notations, instead of writing \\(P(D=1)\\) we used \\(P(D)\\) and instead of \\(P(D=0)\\) we wrote \\(P(\\bar D)\\).\nSometimes it is more intuitive to describe probabilities using a tree rather than tables. The tree below shows the conditional distribution of \\(D\\) and \\(T\\).\n\n\n\n\n\n\n\nFigure 4.1: Medical Diagnostics Decision Tree\n\n\n\nThe result is not as intuitive as in the NBA example. Let’s think about this intuitively. Rather than relying on Bayes’s math to help us with this, let us consider another illustration. Imagine that the above story takes place in a small town, with \\(1,000\\) people. From the prior \\(P(D)=0.02\\), we know that 2 percent, or 20 people, are sick, and \\(980\\) are healthy. If we administer the test to everyone, the most probable result is that 19 of the 20 sick people test positive. Since the test has a 1 percent error rate, however, it is also probable that 9.8 of the healthy people test positive, we round it to 10.\nNow if the doctor sends everyone who tests positive to the national hospital, there will be 10 healthy and 19 sick patients. If you meet one, even though you are armed with the information that the patient tested positive, there is only a 66 percent chance this person is sick.\nLet’s extend the example and add the utility of the test and the utility of the treatment. Then the decision problem is to treat \\(a_T\\) or not to treat \\(a_N\\). The Q-function is the function of the state \\(S \\in \\{D_0,D_1\\}\\) and the action \\(A \\in \\{a_T,a_N\\}\\)\n\nUtility of the test and the treatment.\n\n\nA/S\n\\(a_T\\)\n\\(a_N\\)\n\n\n\n\n\\(D_0\\)\n90\n100\n\n\n\\(D_1\\)\n90\n0\n\n\n\nThen expected utility of the treatment is 90 and no treatment is 98. A huge difference. Given our prior knowledge, we should not treat everyone.\n\n0.02*90 + 0.98*90  # treat\n\n 90\n\n0.02*0 + (1-0.02)*100 # do not treat\n\n 98\n\n\nHowever, the expected utility will change when our probability of disease changes. Let’s say that we are in a country where the probability of disease is 0.1 or we performed a test and updated our prior probability of disease to some number \\(p\\). Then the expected utility of the treatment is \\(E\\left[U(a_T)\\right]\\) is 90 and no treatment is \\[\nE\\left[U(a_N)\\right] = 0\\cdot p + 100 \\cdot (1-p) = 100(1-p)\n\\] When we are unsure about the value of \\(p\\) we may want to explore how the optimal decision changes as we vary \\(p\\)\n\np = seq(0,1,0.01)\nplot(p, 100*(1-p), type = \"l\", xlab = \"p\", ylab = TeX(\"$E[U(a)]$\"))\nabline(h=90, col=\"red\")\nlegend(\"bottomleft\", legend = c(TeX(\"$E[U(a_N)]$\"), TeX(\"$E[U(a_T)]$\")), col = c(\"black\", \"red\"), lty = 1, bty='n')\n\n\n\n\nExpected utility of the treatment and no treatment as a function of the prior probability of disease.\n\n\n\n\nIf our estimate at the crossover point, then we should be indifferent between treatment and no treatment, if on the left of the crossover point, we should treat, and if on the right, we should not treat. The crossover point is. \\[\n100(1-p) = 90, ~p = 0.1\n\\]\nThe gap of of \\(0.9-100(1-p)\\) is the expected gain from treatment.\n\nplot(p, 90-100*(1-p), type = \"l\", xlab = \"p\", ylab = TeX(\"Utility gain from treatment\"))\n\n\n\n\n\n\n\n\nNow, let us calculate the value of test, e.g. the change in expected utility from the test. We will need to calculate the posterior probabilities\n\n# P(D | T = 0) = P(T = 0 | D) P(D) / P(T = 0)\npdt0 = 0.05*0.02/(0.05*0.02 + 0.99*0.98) \nprint(pdt0)\n\n 0.001\n\n# Expected utility given the test is negative \n# E[U(a_N | T=0)]\nUN0 = pdt0*0 + (1-pdt0)*100\nprint(UN0)\n\n 100\n\n# E[U(a_T | T=0)]\nUT0 = pdt0*90 + (1-pdt0)*90\nprint(UT0)\n\n 90\n\n\nGiven test is negative, our best action is not to treat. Our utility is 100. What if the test is positive?\n\n# P(D | T = 1) = P(T = 1 | D) P(D) / P(T = 1)\npdt = 0.95*0.02/(0.95*0.02 + 0.01*0.98)\nprint(pdt)\n\n 0.66\n\n# E[U(a_N | T=1)]\nUN1 = pdt*0 + (1-pdt)*100\nprint(UN1)\n\n 34\n\n# E[U(a_T | T=1)]\nUT1 = pdt*90 + (1-pdt)*90\nprint(UT1)\n\n 90\n\n\nThe best option is to treat now! Given the test our strategy is to treat if the test is positive and not treat if the test is negative. Let’s calculate the expected utility of this strategy.\n\n# P(T=1) = P(T=1 | D) P(D) + P(T=1 | D=0) P(D=0)\npt = 0.95*0.02 + 0.01*0.98\nprint(pt)\n\n 0.029\n\n# P(T=0) = P(T=0 | D) P(D) + P(T=0 | D=0) P(D=0)\npt0 = 0.05*0.02 + 0.99*0.98\nprint(pt0)\n\n 0.97\n\n# Expected utility of the strategy\npt*UT1 + pt0*UN0\n\n 100\n\n\nThe utility of out strategy of 100 is above of the strategy prior to testing (98), this difference of 2 is called the value of information.\n\n\nExample 4.13 (Mudslide) I live in in a house that is at risk of being damaged by a mudslide. I can build a wall to protect it. The wall costs $10,000. If there is a mudslide, the wall will protect the house with probability \\(0.95\\). If there is no mudslide, the wall will not cause any damage. The prior probability of a mudslide is \\(0.01\\). If there is a mudslide and the wall does not protect the house, the damage will cost $100,0000. Should I build the wall?\nLet’s formally solve this as follows:\n\nBuild a decision tree.\nThe tree will list the probabilities at each node. It will also list any costs there are you going down a particular branch.\nFinally, it will list the expected cost of going down each branch, so we can see which one has the better risk/reward characteristics.\n\n\n\nThe first dollar value is the cost of the edge, e.g. the cost of building the wall is $40,000. The second dollar value is the expected cost of going down that branch. For example, if you build the wall and there is a mudslide, the expected cost is $90,000. If you build the wall and there is no mudslide, the expected cost is $40,000. The expected cost of building the wall is $40,500. The expected cost of not building the wall is $10. The expected cost of building the wall is greater than the expected cost of not building the wall, so you should not build the wall. The dollar value at the leaf nodes is the expected cost of going down that branch. For example, if you build the wall and there is a mudslide and the wall does not hold, the expected cost is $1004000.\nThere’s also the possibility of a further test to see if the wall will hold. Let’s include the geological testing option. The test costs $3000 and has the following accuracies. \\[\nP( T  \\mid  \\mathrm{Slide} ) = 0.90 \\; \\; \\mathrm{and } \\; \\; P( \\mathrm{not~}T  \\mid\n\\mathrm{No \\; Slide} ) = 0.85\n\\] If you choose the test, then should you build the wall?\nLet’s use the Bayes rule. The initial prior probabilities are \\[\nP( Slide ) = 0.01  \\; \\; \\mathrm{and} \\; \\; P ( \\mathrm{No \\; Slide} ) = 0.99\n\\]\n\\[\\begin{align*}\nP( T) & = P( T  \\mid  \\mathrm{Slide} ) P( \\mathrm{Slide} ) +\nP( T  \\mid  \\mathrm{No \\;  Slide} ) P( \\mathrm{No \\; Slide} ) \\\\\nP(T)& = 0.90 \\times 0.01 + 0.15 \\times 0.99 = 0.1575\n\\end{align*}\\] We’ll use this to find our optimal course of action.\nThe posterior probability given a positive test is \\[\\begin{align*}\nP ( Slide  \\mid  T ) & = \\frac{ P ( T  \\mid  Slide ) P ( Slide )}{P(T)} \\\\\n& = \\frac{ 0.90 \\times 0.01}{ 0.1575} = 0.0571\n\\end{align*}\\]\nThe posterior probability given a negative test is \\[\\begin{align*}\nP \\left ( \\mathrm{Slide}  \\mid  \\mathrm{not~}T \\right ) & = \\frac{ P ( \\mathrm{not~}T  \\mid  \\mathrm{Slide} ) P ( \\mathrm{Slide} )}{P(\\mathrm{not~}T)} \\\\\n& = \\frac{0.1 \\times 0.01 }{0.8425} \\\\\n& =0.001187\n\\end{align*}\\]\nCompare this to the initial base rate of a \\(1\\)% chance of having a mud slide.\nGiven that you build the wall without testing, what is the probability that you’ll lose everything? With the given situation, there is one path (or sequence of events and decisions) that leads to losing everything:\n\nBuild without testing (given) Slide (\\(0.01\\))\nDoesn’t hold (\\(0.05\\)) \\[\nP ( \\mathrm{losing} \\; \\mathrm{everything}  \\mid  \\mathrm{build} \\; \\mathrm{w/o} \\;\n\\mathrm{testing} ) = 0.01 \\times 0.05 = 0.0005\n\\]\n\nGiven that you choose the test, what is the probability that you’ll lose everything? There are two paths that lead to losing everything:\n\nThere are three things that have to happen to lose everything. Test +ve (\\(P=0.1575\\)), Build, Slide (\\(P= 0.0571\\)), Doesn’t Hold (\\(P=0.05\\))\nNow you lose everything if Test -ve (\\(P=0.8425\\)), Don’t Build, Slide given negative (\\(P=0.001187\\)).\n\nThe conditional probabilities for the first path \\[\nP ( \\mathrm{first} \\; \\mathrm{path} ) = 0.1575 \\times 0.0571 \\times 0.05\n= 0.00045\n\\]\nFor the second path \\[\nP ( \\mathrm{second} \\; \\mathrm{path} ) = 0.8425 \\times 0.001187 = 0.00101\n\\]\nHence putting it all together \\[\nP ( \\mathrm{losing} \\; \\mathrm{everything}  \\mid  \\mathrm{testing} ) = 0.00045 + 0.00101 = 0.00146\n\\]\nPutting these three cases together we can build a risk/reward table\n\n\n\nChoice\nExpected Cost\nRisk\nP\n\n\n\n\nDon’t Build\n$10,000\n0.01\n1 in 100\n\n\nBuild w/o testing\n$40,500\n0.0005\n1 in 2000\n\n\nTest\n$10,760\n0.00146\n1 in 700\n\n\n\nThe expected cost with the test is \\(3+40\\times 0.1575+1000\\times 0.001187 = 10,760\\)\nWhat do you choose?",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#nash-equilibrium",
    "href": "04-dec.html#nash-equilibrium",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.5 Nash Equilibrium",
    "text": "4.5 Nash Equilibrium\nWhen multiple decision makers interact with each other, meaning the decision of one player changes the state of the “world” and thus affects the decision of another player, then we need to consider the notion of equilibrium. It is a central concept in economics and game theory. The most widely used type of equilibrium is the Nash equilibrium, named after John Nash, who introduced it in his 1950 paper “Equilibrium Points in N-Person Games.” It was popularized by the 1994 film “A Beautiful Mind,” which depicted Nash’s life and work.\nIt is defined as a set of strategies where no player can improve their payoff by unilaterally changing their strategy, assuming others keep their strategies constant. In other words, a Nash equilibrium is a set of strategies where no player has an incentive to deviate from their current strategy, given the strategies of the other players.\nHere are a few examples of Nash equilibria:\n\nPrisoner’s Dilemma: Two prisoners must decide whether to cooperate with each other or defect. The Nash equilibrium is for both to defect, even though they would be better off if they both cooperated.\nPricing Strategies: Firms in a market choose prices to maximize profits, taking into account their competitors’ pricing decisions. The equilibrium is the set of prices where no firm can increase profits by changing its price unilaterally.\nTraffic Flow: Drivers choose routes to minimize travel time, based on their expectations of other drivers’ choices. The equilibrium is the pattern of traffic flow where no driver can reduce their travel time by choosing a different route.\n\n\nExample 4.14 (Marble Game) Here is a subtle marble game where players have to call out (or present) either red or blue with different payoffs according to how things match. Two players \\(A\\) and \\(B\\) have both a red and a blue marble. They present one marble to each other. The payoff table is as follows:\n\nIf both present red, \\(A\\) wins $3.\nIf both present blue, \\(A\\) wins $1.\nIf the colors do not match, \\(B\\) wins $2\n\nThe question is whether it is better to be \\(A\\) or \\(B\\) or does it matter? Moreover, what kind of strategy should you play? A lot depends on how much credit you give your opponent. A lot of empirical research has studying the tit-for-tat strategy, where you cooperate until your opponent defects. Then you match his last response.\nNash equilibrium will also allow us to study the concept of a randomized strategy (ie. picking a choice with a certain probability) which turns out to be optimal in many game theory problems.\nFirst, assume that the players have a \\(\\frac{1}{2}\\) probability of playing Red or Blue. Thus each player has the same expected payoff \\(E(A) = \\$1\\) \\[\\begin{align*}\n    E(A) &= \\frac{1}{4} \\cdot 3 + \\frac{1}{4} \\cdot 1 =1 \\\\\n    E(B) &= \\frac{1}{4} \\cdot 2 + \\frac{1}{4} \\cdot 2 =1\n\\end{align*}\\] We might go one step further and look at the risk (and measured by a standard deviation) and calculate the variances of each players payout \\[\\begin{align*}\n    Var (A) & = (1-1)^2 \\cdot \\frac{1}{4} +(3-1)^2 \\cdot \\frac{1}{4} + (0-1)^2 \\cdot \\frac{1}{2} = 1.5 \\\\\n    Var(B) & = 1^2 \\cdot \\frac{1}{2} + (2-1)^2 \\cdot \\frac{1}{2} = 1\n\\end{align*}\\] Therefore, under this scenario, if you are risk averse, player \\(B\\) position is favored.\nThe matrix of probabilities with equally likely choices is given by\n\n\n\n\\(A,B\\)\nProbability\n\n\n\n\n\\(P( red, red )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( red, blue )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( blue, red )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( blue, blue )\\)\n(1/2)(1/2)=1/4\n\n\n\nNow they is no reason to assume ahead of time that the players will decide to play \\(50/50\\). We will show that there’s a mixed strategy (randomized) that is a Nash equilibrium that is, both players won’t deviate from the strategy. We’ll prove that the following equilibrium happens:\n\n\\(A\\) plays Red with probability 1/2 and blue 1/2\n\\(B\\) plays Red with probability 1/4 and blue 3/4\n\nIn this case the expected payoff to playing Red equals that of playing Blue for each player. We can simply calculate: \\(A\\)’s expected payoff is 3/4 and \\(B\\)’s is $1 \\[\nE(A) = \\frac{1}{8} \\cdot 3 + \\frac{3}{8} \\cdot 1 = \\frac{3}{4}\n\\] Moreover, \\(E(B) =1\\), thus \\(E(B) &gt; E(A)\\). We see that \\(B\\) is the favored position. It is simple that if I know that you are going to play this strategy and vice-versa, neither of us will deviate from this strategy – hence the Nash equilibrium concept.\nNash equilibrium probabilities are: \\(p=P( A \\; red )= 1/2, p_1 = P( B \\; red ) = 1/4\\) with payout matrix\n\n\n\n\\(A,B\\)\nProbability\n\n\n\n\n\\(P( red, red )\\)\n(1/2)(1/4)=1/8\n\n\n\\(P( red, blue )\\)\n(1/2)(3/4)=3/8\n\n\n\\(P( blue, red )\\)\n(1/2)(1/4)=1/8\n\n\n\\(P( blue, blue )\\)\n(1/2)(3/4)=3/8\n\n\n\nWe have general payoff probabilities: \\(p=P( A \\; red ), p_1 = P( B \\; red )\\)\n\\[\\begin{align*}\n    f_A ( p , p_1 ) =& 3 p p_1 + ( 1 -p ) ( 1 - p_1 ) \\\\\n    f_B ( p , p_1 ) =& 2 \\{ p(1 - p_1) + ( 1 -p ) p_1 \\}\n\\end{align*}\\]\nTo find the equilibrium point \\[\\begin{align*}\n    ( \\partial / \\partial p ) f_A ( p , p_1 ) =& 3 p_1 - ( 1 - p_1 ) = 4 p_1 -1 \\; \\; \\mathrm{so} \\; \\; p_1= 1/4 \\\\\n    ( \\partial / \\partial p_1 ) f_B ( p , p_1 ) =& 2 ( 1 - 2p ) \\; \\; \\mathrm{so} \\; \\; p= 1/2\n\\end{align*}\\]\nMuch research has been directed to repeated games versus the one-shot game and is too large a topic to discuss further.\n\nEquilibrium analysis helps predict the likely outcomes of strategic interactions, even when individuals are acting in their own self-interest. Further, we can use it to understand how markets function and how firms make pricing and production decisions or to design mechanisms (e.g., auctions, voting systems) that incentivize desired behavior and achieve efficient outcomes.\nOne major drawback is that equilibrium analysis relies on assumptions about rationality and common knowledge of preferences and strategies, which may not always hold in real-world situations. Furthermore, some games may have multiple equilibria, making it difficult to predict which one will be reached. The problem of dynamic strategies, when individuals may learn and adjust their strategies as they gain experience is hard.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#statistical-decisions-and-risk",
    "href": "04-dec.html#statistical-decisions-and-risk",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.6 Statistical Decisions and Risk",
    "text": "4.6 Statistical Decisions and Risk\nThe statistical decision making problem can be posed as follows. A decision maker (you) has to chose from a set of decisions or acts. The consequences of these decisions depend on an unknown state of the world. Let \\(d\\in\\mathcal{D}\\) denote the decision and \\(\\theta\\in\\Theta\\) the state of the world. As an example, think of \\(\\theta\\) as the unknown parameter and the decision as choosing a parameter estimation or hypothesis testing procedure. To provide information about the parameter, the decision maker obtains a sample \\(y\\in\\mathcal{Y}\\) that is generated from the likelihood function \\(p\\left(y|\\theta\\right)\\). The resulting decision depends on the observed data, is denoted as \\(d\\left(  y\\right)\\), and is commonly called the decision rule.\nTo make the decision, the decision maker uses a “loss” function as a quantitative metric to assesses the consequences or performance of different decisions. For each state of the world \\(\\theta\\), and decision \\(d\\), \\(L\\left(  \\theta,d\\right)\\) quantifies the “loss” made by choosing \\(d\\) when the state of the world is \\(\\theta.\\) Common loss functions include a quadratic loss, \\(L(\\theta,d)=(\\theta-d)^{2},\\) an absolute loss, \\(L(\\theta,d)=|\\theta-d|\\), and a \\(0-1\\) loss, \\[\nL(\\theta,d)=L_{0}1_{\\left[  \\theta\\in\\Theta_{0}\\right]  }+L_{1}1_{\\left[  \\theta\\in\\Theta_{1}\\right]  }.\n\\] For Bayesians, the utility function provides a natural loss function. Historically, decision theory was developed by classical statisticians, thus the development in terms of “objective” loss functions instead of “subjective” utility.\nClassical decision theory takes a frequentist approach, treating parameters as “fixed but unknown” and evaluating decisions based on their population properties. Intuitively, this thought experiment entails drawing a dataset \\(y\\) of given length and applying the same decision rule in a large number of repeated trials and averaging the resulting loss across those hypothetical samples. Formally, the classical risk function is defined as \\[\nR(\\theta,d)=\\int_{\\mathcal{Y}}L\\left[  \\theta,d(y)\\right]  p(y|\\theta )dy=\\mathbb{E}\\left[  L\\left[  \\theta,d(y)\\right]  |\\theta\\right]  .\n\\] Since the risk function integrates over the data, it does not depend on a given observed sample and is therefore an ex-ante or a-priori metric. In the case of quadratic loss, the risk function is the mean-squared error (MSE) and is \\[\\begin{align*}\nR(\\theta,d)  &  =\\int_{\\mathcal{Y}}\\left[  \\theta-d\\left(  y\\right)  \\right]\n^{2}p(y|\\theta)dy\\\\\n&  =\\mathbb{E}\\left[  \\left(  d\\left(  y\\right)  -E\\left[  d\\left(  y\\right)\n|\\theta\\right]  \\right)  ^{2}|\\theta\\right]  +\\mathbb{E}\\left[  \\left(\nE\\left[  d\\left(  y\\right)  |\\theta\\right]  -\\theta\\right)  ^{2}|\\theta\\right]\n\\\\\n&  =Var\\left(  d\\left(  y\\right)  |\\theta\\right)  +\\left[  bias\\left(\nd\\left(  y\\right)  -\\theta\\right)  \\right]  ^{2}%\n\\end{align*}\\] which can be interpreted as the bias of the decision/estimator plus the variance of the decision/estimator. Common frequentist estimators choose unbiased estimators so that the bias term is zero, which in most settings leads to unique estimators.\nThe goal of the decision maker is to minimize risk. Unfortunately, rarely is there a decision that minimizes risk uniformly for all parameter values. To see this, consider a simple example of \\(y\\sim N\\left(  \\theta,1\\right)\\), a quadratic loss, and two decision rules, \\(d_{1}\\left(  y\\right)  =0\\) or \\(d_{2}\\left(  y\\right)  =y\\). Then, \\(R\\left(  \\theta,d_{1}\\right)  =\\theta^{2}\\) and \\(R\\left(  \\theta,d_{2}\\right)  =1\\). If \\(\\left\\vert \\theta\\right\\vert &lt;1\\), then \\(R\\left(  \\theta,d_{1}\\right)  &lt;R\\left(  \\theta,d_{2}\\right)\\), with the ordering reversed for \\(\\left\\vert \\theta\\right\\vert &gt;1\\). Thus, neither rule uniformly dominates the other.\nOne way to deal with the lack of uniform domination is to use the minimax principle: first maximize risk as function of \\(\\theta\\), \\[\n\\theta^{\\ast}=\\underset{\\theta\\in\\Theta}{\\arg\\max}R(\\theta,d)\\text{,}%\n\\] and then minimize the resulting risk by choosing a decision:\n\\[\nd_{m}^{\\ast}=\\underset{d\\in\\mathcal{D}}{\\arg\\min}\\left[  R(\\theta^{\\ast },d)\\right]  \\text{.}%\n\\] The resulting decision is known as a minimax decision rule. The motivation for minimax is game theory, with the idea that the statistician chooses the best decision rule against the other player, mother nature, who chooses the worst parameter.\nThe Bayesian approach treats parameters as random and specifies both a likelihood and prior distribution, denoted here by \\(\\pi\\left(  \\theta\\right)\\). The Bayesian decision maker recognizes that both the data and parameters are random, and accounts for both sources of uncertainty when calculating risk. The Bayes risk is defined as\n\\[\\begin{align*}\nr(\\pi,d)  &  =\\int_{\\mathcal{\\Theta}}\\int_{\\mathcal{Y}}L\\left[  \\theta ,d(y)\\right]  p(y|\\theta)\\pi\\left(  \\theta\\right)  dyd\\theta\\\\\n&  =\\int_{\\mathcal{\\Theta}}R(\\theta,d)\\pi\\left(  \\theta\\right)  d\\theta =\\mathbb{E}_{\\pi}\\left[  R(\\theta,d)\\right]  ,\n\\end{align*}\\] and thus the Bayes risk is an average of the classical risk, with the expectation taken under the prior distribution. The Bayes decision rule minimizes expected risk:\n\\[\nd_{\\pi}^{\\ast}=\\underset{d\\in\\mathcal{D}}{\\arg\\min}\\text{ }r(\\pi,d)\\text{.}%\n\\] The classical risk of a Bayes decision rule is defined as \\(R\\left(\n\\theta,d_{\\pi}^{\\ast}\\right)\\), where \\(d_{\\pi}^{\\ast}\\) does not depend on \\(\\theta\\) or \\(y\\). Minimizing expected risk is consistent with maximizing posterior expected utility or, in this case, minimizing expected loss. Expected posterior risk is \\[\nr(\\pi,d)=\\int_{\\mathcal{Y}}\\left[  \\int_{\\mathcal{\\Theta}}L\\left[\n\\theta,d(y)\\right]  p(y|\\theta)\\pi\\left(  \\theta\\right)  d\\theta\\right]  dy,\n\\] where the term in the brackets is posterior expected loss. Minimizing posterior expected loss for every \\(y\\in\\mathcal{Y},\\) is clearly equivalent to minimizing posterior expected risk, provided it is possibility to interchange the order of integration.\nThe previous definitions did not explicitly state that the prior distribution was proper, that is, that \\(\\int_{\\mathcal{\\Theta}}\\pi\\left(  \\theta\\right)d\\theta=1\\). In some applications and for some parameters, researchers may use priors that do not integrate, \\(\\int_{\\Theta}\\pi\\left(  \\theta\\right)d\\theta=\\infty\\), commonly called improper priors. A generalized Bayes rule is one that minimizes \\(r(\\pi,d),\\) where \\(\\pi\\) is not necessarily a distribution, if such a rule exists. If \\(r(\\pi,d)&lt;\\infty\\), then the mechanics of this rule is clear, although its meaning is less clear.\n\n4.6.1 Risk Decomposition\nHow does one find an optimal decision rule? It could be a test region, an estimation procedure or the selection of a model. Bayesian decision theory addresses this issue.\nThe a posteriori Bayes risk approach is as follows. Let \\(\\delta(x)\\) denote the decision rule. Given the prior \\(\\pi(\\theta)\\), we can simply calculate \\[\nR_n  ( \\pi , \\delta ) = \\int_x m( x )  \\left \\{ \\int_\\Theta L( \\theta , \\delta(x) )  p( \\theta | x ) d \\theta \\right \\} d x .\n\\] Then the optimal Bayes rule is to pointwise minmise the inner integral (a.k,a. the posterior Bayes risk), namely \\[\n\\delta^\\star ( x ) = \\arg \\max_\\delta \\int_\\Theta L( \\theta , \\delta(x) )  p( \\theta | x ) d \\theta .\n\\] The caveat is that this gives use no intuition into the characteristics of the prior which are important. Moreover, we do not need the marginal beliefs \\(m(x)\\).\nFor example, under squared error estimation loss, the optimal estimator is simply the posterior mean, \\(\\delta^\\star (x)  = E( \\theta | x )\\).\nThe optimal Bayes rule \\(\\delta_\\pi( x ) = E( \\theta \\mid x )\\) is the posterior mean under squared error loss. An interesting feature of the Bayes rule is that it is biased (except in degenerate cases like improper priors which can lose optimality properties). This can be seen using the following decomposition. If the rule was unbiased then the Bayes risk would be zero. This follows, via contraction, assume f.a.c. that \\(E_{ x| \\theta } ( \\delta_\\pi(x) ) = \\theta\\), then \\[\\begin{align*}\nr ( \\pi , \\delta_\\pi ( x ) ) & = E_{\\pi} \\left ( E_{ x| \\theta } ( \\delta_\\pi ( x ) - \\theta )^2 \\right ) \\\\\n& = E_\\pi ( \\theta^2 ) + E_x \\left ( \\delta_\\pi ( x )^2 \\right ) - 2 E_\\pi \\left ( \\theta E_{x | \\theta } ( \\delta_\\pi( x ) ) \\right ) \\\\\n& = 0\n\\end{align*}\\] which is a contradiction.\nThe key feature of Bayes rule then is the bias-variance trade-off inherent in their nature. You achieve a large reduction in variance for a small amount of bias. This is the under-pinning of James-Stein estimation which we discuss later.\nAnother interesting feature, is that the Bayes rule \\(E(\\theta \\mid x )\\) is always Bayes sufficient in the sense that \\[\nE_\\pi \\left ( \\theta \\; | \\; E_\\pi(\\theta |x ) \\right ) = E_\\pi( \\theta |x )\n\\] So conditioning on \\(E_\\pi( \\theta |x )\\) is equivalent to conditioning on \\(x\\) when estimating $ $. This is used in the quantile neural network approach to generative methods.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "05-ab.html",
    "href": "05-ab.html",
    "title": "5  AB Testing",
    "section": "",
    "text": "5.1 Confidence Intervals\nThe Internet age opened the door to enamours data collection from images to videos as well as personalized information. The data collected are observational rather than ones collected from a designed experiment - where hopefully we can control the environment in order to find the effects of intervening in the situation.\nAny competitive business always strives to improve its efficiency. Requirements to improve can be driven by changing market conditions, customer behavior and their demands. We would like to test our ideas on how to improve things and to pick the best course of action. AB testing provides a statistical framework for addressing such issues. It is the underpinning for testing new ideas and measuring the effects of strategy A versus B. It is widely used for testing marketing complains, product designs or even effects of new drugs. It relies on a statistical procedure sometimes known as hypothesis testing, hence AB testing allows us to draw conclusions from controlled designed experiments. We would also like to use these methods on observational study and thus brings up questions of causation. Instrumental variables provide ways of trying to tease out main effects. Propensity scores and matching are also popular techniques. We often interested in a question whether the observed effect is there due to noise or a true one. We can use hypothesis testing to answer this question in a rigorous way. Think, of coin tossing experiment. If you tossed coin twice and it came heads both of the times, does it mean that the coin is biased? Common sense tells us that two tosses is not enough to make a definitive conclusion and we should toss this coin a few more times to gain confidence. Hypothesis, testing is just that, the procedure that tells us if we have enough evidence to make a conclusion or more data is required to collect. It uses probability distributions to quantify uncertainty about a experiment outcomes. Let’s do a more practical example.\nThere is a whole field on bandit problems - how to optimally sequential allocate our resources as a trade-off between exploitation (gain more information in an environment you understand to solely gain efficiency) or exploration (to learn about new environments which might be less optimal than the current one).\nYou work as a quant for a trading firm and you have developed a new algorithm to trade stocks. You tested you algorithm on the historical data and it outperformed the state of the art algorithm used in your company. Now, the important question is whether your trading strategy can truly outperform the market or it is just got lucky. We need to analyze the performance of the algorithm after it was created and need to decide whether we have truly discovered a dominant strategy. Sequential analysis - a natural framework for Bayesian methods - allows us to decide on how long we need to wait before we have enough evidence that our algorithm has an edge.\nThe effect we try to measure is usually present in some statistics that we calculate from data, for example sample mean, proportion, or difference in means.\nCentral Limit Theorem\nCLT states that, given a sufficiently large sample size, the distribution of the sample means will be approximately normally distributed, regardless of the shape of the population distribution. This normal distribution is also known as the Gaussian distribution. The theorem applies to a wide range of population distributions, including distributions that are not normal. This universality makes it one of the most powerful and widely-used theorems in statistics.\nThe first and simplest case of the CLT was published in 1738 by the De Moivre and is called the De Moivre-Laplace theorem. According to this theorem the standard normal distribution arises as the limit of scaled and centered Binomial distributions, in the following sense. Let \\(x_1,\\ldots,x_n\\) be independent, identically distributed Rademacher random variables, that is, independent random variables with distribution \\[\nP(x_i = 1) = P(x_i = -1) = \\frac{1}{2}.\n\\] Then, the distribution of the sum of these random variables converges to the standard normal distribution as \\(n\\) tends to infinity. That is, for any \\(a&lt;b\\), we have \\[\n\\lim_{n\\to\\infty} P\\left(a \\le \\frac{x_1+\\cdots+x_n}{\\sqrt{n}} \\le b\\right) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx.\n\\] In this case, the sum \\(x_1+\\cdots+x_n\\) has mean \\(n\\mu\\) and variance \\(n\\sigma^2\\), so that the standardized sum \\((x_1+\\cdots+x_n - n\\mu)/\\sqrt{n\\sigma^2}\\) has mean \\(0\\) and variance \\(1\\). The theorem then states that the distribution of this standardized sum converges to the standard normal distribution as \\(n\\) tends to infinity.\nIn 1889 Francis Galton published a paper where he described what we now call the Galton Board. The Galton Board is a vertical board with interleaved rows of pins. Balls are dropped from the top, and bounce left and right as they hit the pins. Eventually, they are collected into one of several bins at the bottom. The distribution of balls in the bins approximates the normal distribution. Each pin is a physical realization of the binomial draw and each row is a summand. The location at the bottom is a sum of the binomial draws. The galton-ball.r script simulates the Galton board experiment. The script is available in the R folder of the book repository. Figure 5.1 shows the result of the simulation. The distribution of the balls in the bins approximates the normal distribution.\n\\(Z\\)-Score\nThe fact that the distribution of the simulated means from the Pyx example can be described well by a normal bell curve, in fact has a theoretical justification. It is called the Central Limit Theorem. The Central Limit Theorem states that, given a sufficiently large sample size, the distribution of the sample means will be approximately normally distributed, regardless of the shape of the population distribution. This normal distribution is also known as the Gaussian distribution.\nThere are a few conditions. The sampled observations must be independent. In practice, this means that the sampling should be random, and one observation should not influence another.Further, the sample size should be sufficiently large. While there is no strict rule for what constitutes ‘large enough,’ a common guideline is a sample size of 30 or more. However, if the population distribution is far from normal, a larger sample size may be required.\nWe can estimate the mean of this bell curve using \\(\\bar x\\) and the standard deviation (standard error) using \\(s/\\sqrt{n}\\).\nThe square-root nature of this relation is somewhat unfortunate. To double your certainty about the population mean, you need to quadruple the sample size.\nOne of the main applications of this results is the construction of confidence intervals. A confidence interval is a range of values that is likely to contain the true value of the population mean. It is a plausible range for the quantity we are trying to estimate. The confidence interval is calculated using the sample mean \\(\\bar x\\) and the standard error \\(s/\\sqrt{n}\\). The confidence interval is centered around the sample mean and has a width of \\(2 \\times s_{\\bar x}\\). The confidence interval is calculated as follows \\[\n\\bar x \\pm 2 \\times s_{\\bar x} = \\bar x \\pm 2 \\times \\frac{s}{\\sqrt{n}}.\n\\]\nThe theorem applies to a wide range of population distributions, including distributions that are not normal. This universality makes it one of the most powerful and widely-used theorems in statistics.\nHere are a few conclusions we can make thus far 1. Mean estimates are based on random samples and therefore random (uncertain) themselves\nWe need to account for this uncertainty!\nComing back to the Patriots coin toss example, we know that they won 19 out of 25 tosses during the 2014-2015 season. In this example, our observations are values 0 (lost toss) and 1 (won toss) and the average over those 0-1 observations is called the proportion and is denoted by \\(\\hat p\\) instead of \\(\\bar x\\). When we deal with proportions, we can calculate the sample variance from its mean \\(\\hat p\\) as follows \\[\ns^2_{\\hat p} = \\frac{\\hat p(1-\\hat p)}{n}.\n\\] Thus, we know that given our observations and CLT, the true vale of the probability of winning a toss is normally distributed. Our best guess at the mean \\(\\hat p\\) is \\(19/25 = 0.76\\) and variance \\(s^2 = 0.76(1-0.76)/25 = 0.091\\) \\[\n\\hat p \\sim N(0.76, 0.091).\n\\] Then a \\(95\\%\\) Confidence Interval is calculated by\n0.76 + c(-1.96,1.96)*0.091\n\n 0.58 0.94\nSince 0.5 is outside the confidence interval, we say that we do not have enough evidence to say that the coin tosses were fair.\nThen we formulate a hypothesis that we are to test. Our status-quo assumption (there is no effect) is called the null hypothesis and is typically denoted by \\(H_0\\).\nTo translate the question from this experiment into language of hypothesis testing, we say that our null hypothesis is that proportion of yawning participants in control (\\(\\hat p_c\\)) and experimental group (\\(\\hat p_e\\)) is the same \\(H_0: \\hat p_c - \\hat p_e = 0\\), and the alternative hypothesis is \\(H_a: \\hat p_c &gt; \\hat p_e\\). The goal is to use the data tell us if the hypothesis is correct or not.\nA key statistical fact behind the hypothesis testing is the Central Limit Theorem. It states that if we have a sample \\(\\{x_1,\\ldots,x_n\\}\\) with \\(n\\) observations from any distribution \\(x_i \\sim p(x)\\), then average of the sample follows a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) \\[\n\\bar X = \\frac{1}{n}\\sum_{i=1}^{n}X_i \\sim N(\\mu, \\sigma^2/n)\n\\]\nLet’s us a simple simulated data set to demonstrate the central limit theorem. We generate 100 outcomes of a Bernoulli trial with \\(p=0.3\\) and calculate the mean of this sample \\(\\hat p\\). We repeat it 2000 times and compare the empirical distribution of \\(\\hat p\\) with \\(N(0.3, 0.046)\\).\nset.seed(1)\na = replicate(2000,mean(rbinom(100,1,0.3)))\nplot(density(a), main=\"\")\nse = sqrt(0.3*(1-0.3)/100)  #  0.046\nx = seq(0,0.5,length.out = 300)\nlines(x,dnorm(x,mean = 0.3,sd = se), col=\"red\", lwd=3)\nThere are three ways to quantify uncertainty in hypothesis testing. The first approach relies on calculating confidence intervals, as we did for the yawn example. There are two complimentary approaches. One is to calculate what is called a \\(p\\)-value, that is the probability of getting the result observed in the data, assuming null-hypothesis is true. If \\(p\\)-value is low, then we reject the null-hypothesis. For the yawn example, the conditional probability that the observed difference in proportions is greater then 0.044, given null hypothesis is true is given by\n\\[\np-\\mathrm{value} = P(\\hat p_e - \\hat p_c \\ge 0.044 \\mid H_0),\n\\] which can be calculated using pnorm function\n1 - pnorm(0.044,0,sqrt(0.017))     \n\n 0.37\nThe \\(p\\)-value of 0.37 means that there is 37% chance to observe the difference to be greater then 0.044 assuming the null-hypothesis. It is quite high! We want the \\(p\\)-value to be low, only then we can claim that we have discovered a new fact, i.e. that yawning is contentious. In many applications we require this number to be at most 0.005. The smallest acceptable \\(p\\)-value is called the significance level and is typically denoted as \\(\\alpha\\). We can test the hypothesis at different levels of significance \\(\\alpha\\). Further we assume that the statistic we are analyzing follows the sampling distribution. The probability distribution of the statistics values is either Normal, or \\(t\\)-distribution for continuous variable.\nIn a nutshell a hypothesis is a statement about a population developed for the purpose of testing with data.To summarize the process of testing a significance of our discovery for proportions, we perform the hypothesis testing following the 5-step process.\nIn Steps 1-2 we formulate the hypothesis. In steps 3-5 we make a decision.\nIn the context of hypothesis testing, we come back to the type I and type II errors we already discussed. They can be used to describe two types of errors you can make when testing\nAnd the significance level is then \\[\nPr(\\mbox{reject } H_0 \\mid H_0 \\; \\mbox{ true}) =\nP(\\mbox{type I error}).\n\\]\nHypothesis testing is often used in scientific reporting. For example, the discovery of Higgs Boson was announced as a result of hypothesis testing. Scientists used the five-sigma concept to test the Higgs-Boson hypothesis. This concept, however, is somewhat counter-intuitive. It has to do with a . That is not the probability that the Higgs boson doesn’t exist. It is, rather, the inverse: If the particle doesn’t exist, one in 3.5 million is the chance an experiment just like the one announced would nevertheless come up with a result appearing to confirm it does exist. In other words, one in 3.5 million is the likelihood of finding a false positive a fluke produced by random statistical fluctuation that seems as definitive as the findings released by two teams of researchers at the CERN laboratory in Geneva. So we can talk about the significance level as \\(p\\)-value to be one-in-3.5-million andt hen the \\(Z\\)-score is five.\nThe test statistic (\\(T\\) or \\(Z\\)) to quantifies uncertainty between the null-hypothesis value and the observed one is equal the number many standard deviations they are apart from each other. This value is called the \\(Z\\)-score, and is calculated as \\[\nZ = \\frac{ \\bar{x} - \\mu_0 }{\\sqrt{\\Var{\\bar x}} },\n\\] where \\(\\mu_0\\) is the mean assumed under null-hypothesis. The square root of the statistic’s variance \\(\\sqrt{\\Var{\\bar x}}\\) is called standard error and is denoted by \\(se(\\bar x)\\).\nLet’s calculate the \\(Z\\)- score for the yawning example. When we plug-in \\(\\mu_0 = 0\\), \\(\\bar{x}  = \\hat p_e - \\hat p_c = 0.044\\), \\(\\Var{\\bar x} =\\Var{\\hat p_e - \\hat p_c} =  0.0177\\), we get \\(Z\\) statistic to be 0.33. Thus, our observed difference is very close to 0.\nTo summarize the duality of confidence interval, \\(p\\)-value and \\(Z\\)-score, the following statements are equivalent\nLet us proceed with another example.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AB Testing</span>"
    ]
  },
  {
    "objectID": "05-ab.html#confidence-intervals",
    "href": "05-ab.html#confidence-intervals",
    "title": "5  AB Testing",
    "section": "",
    "text": "Standard Error measures the uncertainty of an estimate\nUsing properties of the Normal distribution, we can construct \\(95\\)% Confidence Intervals\nThis provides us with a plausible range for the quantity we are trying to estimate.\n\n\n\n\n\nExample 5.4 (Mythbusters) In 2006 the creators of Mythbusters TV show on Discovery channel wanted to test weather yawning is contagious in humans. They have recruited 50 participants and each of those went through an interview. At the end of 34 randomly selected interviews the interviewer did yawn. Then participants were asked to wait in a next door room. Out of 34 participants form the experimental group, 10 did yawn (29.4%) and only 4 out 16 (25%) in the control group did yawn. The difference in the proportion of those who did yawn was 4.4%. The show hosts Kari Byron, Tory Belleci and Scottie Chapman concluded that yawn is indeed contagious.\nThe question is what happens if we are to re-run this experiment several times with different groups of participants, will wee see the same difference of 4.4%? The fact is that from one experiment to another calculated proportions of yawners in both groups will be different.\nIn our example, the proportion of yawners in the experiment group is \\(\\hat p_e = 0.294\\) and in the control group is \\(\\hat p_c = 0.25\\). Thus, \\[\n\\hat \\sigma^2_e = 0.294(1-0.294) = 0.21,~~~\\hat \\sigma^2_c = 0.25(1-0.25) = 0.19\n\\]\nWe can apply CLT and calculate the uncertainty about \\(\\hat p_{e}\\) and \\(\\hat p_{c}\\) \\[\n\\hat p_e\\sim N(0.294, 0.21/34),~~~ \\hat p_c\\sim N(0.25, 0.19/34).\n\\] Now, instead of comparing proportions (numbers), we can compare their distributions and thus quantify uncertainties. If we plot density functions of those two Normal variables, we can see that although means are different, there is a large overlap of the two density functions.\n\np = seq(0.0,0.6, length.out = 200)\nplot(p,dnorm(p,0.25, sqrt(0.19/34)), col=2, type='l', lwd=3, ylab=\"Density\")\nlines(p,dnorm(p,0.294, sqrt(0.21/34)), col=3, lwd=3)\nlegend(\"topright\", c(\"control\", \"experiment\"), col=c(3,2), lwd=3, bty='n')\n\n\n\n\n\n\n\n\nThe amount of overlap is the measure of how certain we are that \\(p_e\\) and \\(p_c\\) are different. Large overlap means we are not very certain if proportions are truly different. For example, both \\(p_e\\) and \\(p_c\\) have a high probability of being between 0.2 and 0.4. We can use properties of normal distribution to say specifically what is the amount of this overlap by calculating the corresponding 95% confidence interval of the difference between the proportions. Know that difference of two Normal random variables is another Normal \\[\n\\hat p_e - \\hat p_c \\sim N(0.294 - 0.25, 0.208/34 + 0.185/16) = N(0.044, 0.0177)\n\\] Now we can calculate 95% confidence interval for \\(\\hat p_e - \\hat p_c\\), again using properties of Normal\n\n0.044 + c(-1.96,1.96)*sqrt(0.0177)\n\n -0.22  0.30\n\n\nThe interval is wide and most importantly, it does contain 0. Thus, we cannot say for sure that the proportions are different. They might just appear to be different due to a chance (sampling error). Meaning, that if we are to re-run the experiment we should expect the difference to be anywhere between -0.22 and 0.31 in 95% if of the cases.\nThus, statistical analysis does not confirm the conclusion made by the show hosts and indicates that there no evidence that the proportion of yawners is difference between the control and experiment groups.\n\n\nExample 5.5 (Search algorithm) Let’s look at another example and test effectiveness of Google’s new search algorithm. We measure effectiveness by the number of users who clicked on one of the search results. As users send the search requests, they will be randomly processed with Algo 1 or Algo 2. We wait until 2500 search requests were processed by each of the algorithms and calculate the following table based on how often people clicked through\n\nGoogle Search Algorithm\n\n\n\nAlgo1\nAlgo2\n\n\n\n\nsuccess\n1755\n1818\n\n\nfailure\n745\n682\n\n\ntotal\n2500\n2500\n\n\n\nThe probability of success is estimated to be \\(\\hat{p}_1 = 0.702\\) for the current algorithm and \\(\\hat{p}_2 = 0.727\\) for the new algorithm. We can calculate the 95% confidence interval or 95% Bayesian credible region for both estimated proportions Is the new algorithm\nFor Algo 1:\n\np1 = 0.702; p2 = 0.727\np1 + c(-1.96,1.96)*sqrt(p1*(1-p1)/2500)\n\n 0.68 0.72\n\np2 + c(-1.96,1.96)*sqrt(p2*(1-p2)/2500)\n\n 0.71 0.74\n\n\nGiven that the intervals do not overlap, there is enough evidence that algorithms are different, and the new Algo 1 is indeed more efficient.\nWe will get a slightly more precise estimation of uncertainty if we calculate confidence interval for the difference of the proportions. Since \\(p_1\\) and \\(p_2\\) both follow Normal distribution, thus their difference is also normally distributed \\[\np_1 - p_2 \\sim N(\\hat p_1 - \\hat p_2, s_1^2/n + s_2^2/n).\n\\] Applying this formula for the Google search algorithm experiment, we calculate the 95% confidence interval for the difference\n\np1 - p2  + c(-1.96,1.96)*sqrt(p1*(1-p1)/2500 + p2*(1-p2)/2500)\n\n -5.0e-02  2.9e-05\n\n\nThe confidence interval for the difference does contain 0, and thus we cannot say that we are confident that algorithms are different!\nMore generally, if the number of observations in two groups are different, say \\(n_1\\) and \\(n_2\\) then the \\[\ns_{ \\bar{X}_1 - \\bar{X}_2 } = \\sqrt{ \\frac{ s^2_{ \\bar{X}_1 }}{n_1} + \\frac{ s^2_{ \\bar{X}_2 }}{n_2} }\n\\] or for proportions, we compute \\[\ns_{ \\hat{p}_1 - \\hat{p}_2 } = \\sqrt{ \\frac{ \\hat{p}_1 (1- \\hat{p}_1)}{n_1} + \\frac{ \\hat{p}_2 (1- \\hat{p}_2)}{n_2} }.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Formulate the Null Hypothesis (\\(H_0\\)), which we assume to be true unless there is sufficient evidence to the contrary. Then, alternative Hypothesis (\\(H_1\\)): test against the null, e.g. \\(H_0: p_e - p_c = 0\\), and \\(H_a: p_e - p_c &gt; 0\\). If there is evidence that \\(H_0\\) is false, we accept \\(H_1\\).\nStep 2:Select the significance level \\(\\alpha\\). While \\(\\alpha = 0.05\\) (the 5% level) is the most commonly used., \\(\\alpha = 0.01\\) (the 1% level) is prevalent in medical and quality assurance examples.\nStep 3: Compute the Test Statistic (\\(Z\\) or \\(T\\))\nStep 4: Formulate the Decision Rule. For example, reject the Null hypothesis if \\(|Z| &gt; 1.96\\)\nStep 5: Make a Decision, Compute the p-value. p-value is the smallest significance level at which a null hypothesis can be rejected. If \\(p\\)-value \\(&lt;\\alpha\\), we have evidence that \\(H_1\\) is is true, we accept \\(H_1\\) and claim we have a discovery. If \\(p\\)-value is \\(\\ge \\alpha\\), then we cannot reject the null-hypothesis.\n\n\n\n\nType I Error: Rejecting a true \\(H_0\\).\nType II Error: Not rejecting a false \\(H_0\\).\n\n\n\n\n\n\n\n0 is inside the 95% confidence interval\n\\(p\\)-value is greater then 0.05\n\\(Z\\)-statistic is less then 1.96\n\n\n\nExample 5.6 (Coke vs Pepsi) The most famous hypothesis test in history in whether people can decide the difference between Coke and Pepsi. We run a double blind experiment, neither the experimenter or subject know the allocation. Pepsi claimed that more than half of Diet Coke drinkers said they preferred to drink Diet Pepsi. That is our null hypothesis. The data comes from a random sample of \\(100\\) drinkers. We find that \\(56\\) favor Pepsi.\nThis is a hypothesis test about the proportion of drinkers who prefer Pepsi \\[H_0 : p = \\frac{1}{2} \\; \\; \\mathrm{ and} \\; \\; H_1 : p &gt; \\frac{1}{2}\\] Let’s estimate our statistics form data: \\[\\hat{p} = X/n = 56/100 = 0.56\\]\nThis is my best estimate of the true \\(p\\). The standard error of my statistic \\[se(\\hat{p}) =  \\sqrt{\\hat{p}(1-\\hat{p})/n} = 0.0496 .\\]\nThe \\(95\\)% is then \\[0.56 \\pm 1.96 (0.0496) = 0.56 \\pm 0.098 = ( 0.463, 0.657 )\\] \\(p=0.5\\) lies inside the confidence interval. Pepsi was lying!\nThe \\(Z\\)-score now with \\(s_{ \\hat{p} }= \\sqrt{ p_0(1-p_0)/n} = 0.05\\) \\[Z = \\frac{ \\hat{p} - p_0 }{ s_{\\hat{p} }} = \\frac{ 0.56-0.5}{0.05} = 1.2 &lt; 1.64\\] Let’s take the usual \\(\\alpha = 0.05\\). Don’t reject \\(H_0\\) for a one-sided test at \\(5\\)% level. We need a larger \\(n\\) to come to a more definitive conclusion. We might come to a different conclusion with a larger sample size. One of the downsides of hypothesis testing is that it generates a yes/no answer without having any uncertainty associated with it.\n\nprop.test(56,100,alternative='greater', conf.level = 0.95)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  56 out of 100, null probability 0.5\nX-squared = 1, df = 1, p-value = 0.1\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.47 1.00\nsample estimates:\n   p \n0.56 \n\n\n\n\nExample 5.7 (Avonex) Now we consider a few more examples of Hypothesis testing. We consider the dispute about the Biogen’s Avonex. Biogen made the following assertion:\n“Avonex delivers the highest rate of satisfaction: 95% among patients”In response to that statement, the U.S. Food and Drug Administration(FDA) on October 30th, 2002 informed the biotech company Biogen to stop publishing misleading promotions for its multiple sclerosis drug Avonex. To clarify the issue, FDA did run an experiment. The FDA found that in arandom sample of \\(75\\) patients surveyed, only 60% said they were withAvonex. Who, the question is: WHo is Right?\nLet’s use hypothesis testing to get an answer. Following our five-stepprocess to set up a Hypothesis Test:\nThe null hypothesis: \\(H_0 : p = 0.95 = p_0\\).\nThe alternative hypothesis: \\(H_1 : p &lt; 0.95\\).\nA 1-sided alternative.\nWe’ll use a small significance level, 1%.\nThe appropriate test statistic is \\[Z = \\frac{ \\hat{p} - p_0 }{ \\sqrt{ \\frac{ p_0 ( 1 - p_0 ) }{ n} } }\\] where \\(\\hat{p} = 0.60 , p_0 = 0.95\\) and \\(n=75\\).\nHence \\(Z = \\frac{ 0.6 - 0.95 }{ \\sqrt{ \\frac{ 0.95 \\times 0.05  }{ 75} } } = - 14\\).\nNow lets find the critical region and \\(p\\)-value\nThe critical region is \\(Z &lt; -2.32\\).\nAs the observed test statistic \\(Z\\) falls well within the rejection region.\nThe p-value of the test is \\(P ( Z &lt; - 14 ) = 0.0000\\). Again the statistical evidence is that the FDA is right and Biogen is not.\nAvonex: Testing Proportions in R\nNull Hypothesis: Biogen is innocent\n\nprop.test(45,75,0.95)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  45 out of 75, null probability 0.95\nX-squared = 186, df = 1, p-value &lt;2e-16\nalternative hypothesis: true p is not equal to 0.95\n95 percent confidence interval:\n 0.48 0.71\nsample estimates:\n  p \n0.6 \n\n\nThe p-value is \\(2.2 \\times 10^{-16}\\)!\n\n\nExample 5.8 (Pfizer) We consider another example that involves pharmaceutical company Pfizer. Pfizer introduced Viagra in early 1998. During \\(1998\\) of the \\(6\\) million Viagra users \\(77\\) died from coronary problems such as heart attacks.Pfizer claimed that this rate is no more than the general population.A clinical study found \\(11\\) out of \\(1,500,000\\) men who were not on Viagra died of coronary problems during the same length of time as the\\(77\\) Viagra users who died in \\(1998\\).The question is,Let’s calculate the significance Interval. A 95% confidence interval fora difference in proportions \\(p_1 - p_2\\) is \\[\n( \\hat{p}_1 - \\hat{p}_2 ) \\pm 1.96\n\\sqrt{ \\frac{ \\hat{p}_1 ( 1 - \\hat{p}_1 ) }{ n_1 }  +\n    \\frac{ \\hat{p}_2 ( 1 - \\hat{p}_2 ) }{ n_2 } }\n\\]\n\nCan do a confidence interval or a \\(Z\\)-score test.\nWith Viagra, \\(\\hat{p}_1 = 77/6000000 = 0.00001283\\) and without Viagra \\(\\hat{p}_2 = 11/1500000 = 0.00000733\\).\nNeed to test whether these are equal.\n\nWith a \\(95\\)% confidence interval for \\(( p_1 - p_2 )\\) you get an interval \\[\n( 0.00000549 , 0.0000055).\n\\]\nThis interval doesn’t contain zero.\nThe evidence is that the proportion is higher.\n\nMeasured very accurately as \\(n\\) is large even though \\(p\\) is small.\nWith testing might use a one-sided test and an \\(\\alpha\\) of \\(0.01\\).\n\nDifference of proportions:\n\nprop.test(x=c(11,77), n=c(1500000,6000000), alternative='greater',conf.level=.95)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(11, 77) out of c(1500000, 6e+06)\nX-squared = 3, df = 1, p-value = 0.9\nalternative hypothesis: greater\n95 percent confidence interval:\n -1e-05  1e+00\nsample estimates:\n prop 1  prop 2 \n7.3e-06 1.3e-05 \n\n\nThe p-value for the Null is \\(1-0.948 =0.052\\).\n\n\nExample 5.9 (Lord Rayleigh’s Argon Discovery) Lord Rayleigh won the Nobel Prize for discovery of Argon. This discovery occurred when he noticed a small discrepancy between two sets of measurements on nitrogen gas that he had extracted from the air and one he had made in the lab.\n\nFirst, he removed all oxygen from a sample of air. He measured the density of the remaining gas in a fixed volume at constant temperature and pressure.\nSecond, he prepared the same volume of pure nitrogen by the chemical decomposition of nitrous oxide (\\(N_2 O\\)) and nitric oxide \\(NO\\).\n\nHere’s the results\n\nair = c(2.31017, 2.30986, 2.31010, 2.31001, 2.31024, 2.31010, 2.31028, NA)\ndecomp = c(2.30143, 2.29890, 2.29816, 2.30182, 2.29869, 2.29940, 2.29849, 2.29889)\nd = data.frame(\"Air\"=air,\"Chemical Decomposition\"=decomp)\nknitr::kable(d, booktabs = TRUE,caption = 'Lord Rayleigh Argon Discovery')\n\n\nLord Rayleigh Argon Discovery\n\n\nAir\nChemical.Decomposition\n\n\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\nNA\n2.3\n\n\n\n\n\n\nboxplot(d)\n\n\n\n\nLord Rayleigh’s results\n\n\n\n\n\n\n\n\n\n\nmean\nsd\n\n\n\n\nAir\n2.3\n0.00014\n\n\nDecomposition\n2.3\n0.00138\n\n\n\n\nt.test(air,decomp,var.equal=T)\n\n\n    Two Sample t-test\n\ndata:  air and decomp\nt = 20, df = 13, p-value = 3e-11\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.0095 0.0118\nsample estimates:\nmean of x mean of y \n      2.3       2.3 \n\n\nThe Z-score is 20. It is a 20-sigma event and we’ve found Argon!",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AB Testing</span>"
    ]
  },
  {
    "objectID": "05-ab.html#multiple-testing",
    "href": "05-ab.html#multiple-testing",
    "title": "5  AB Testing",
    "section": "5.2 Multiple Testing",
    "text": "5.2 Multiple Testing\nIf we want to test 1000 hypothesis and we test each hypothesis one-by-one. Say, the ground truth is that only 10% (100) of those hypothesis are true. Using \\(\\alpha=0.05\\) rule, we assume that out of 900 false hypothesis \\(0.05 \\cdot 900 = 45\\) will show up as positive (false positives). Now we run our one-by-one hypothesis tests and our procedure correctly identified 80 out of 100 true positives and incorrectly identified 45 false positives and 20 false negatives. Now, among 125 hypothesis identified as positives 45 in fact are not! Another way to look at it is to calculate the probability of at least one false positive \\(p(\\mbox{at least one false positive}) = 1 - (1-0.05)^{1000} = 1\\). We are almost guaranteed to see at least one false positive.\n\nplot(1:100,1 - (1-0.05)^{1:100}, type='l', ylab=\"False Positive Rate\", xlab=\"Number of Tests\")\n\n\n\n\nProbability of At Least 1 False Positive\n\n\n\n\nOne way to deal with the problem is to lower the cut-off to \\(\\alpha/n\\). This approach is called the Bonferroni correction. For case of 1000 hypothesis we set \\(\\alpha = 0.00005\\). However this conservative approach will lead to many false negatives. Probability of identifying at least one significant result is then \\(1 - (1-0.00005)^{1000} = 0.049\\)\n\nClassification of results for a testing procedure. T/F = True/False, D/N = Discovery/Non-discovery. We observe \\(m\\), \\(D\\) and \\(N\\).\n\n\n\n\\(H_0\\) Accepted\n\\(H_0\\) Rejected\nTotal\n\n\n\n\n\\(H_0\\) True\nTN\nFD\n\\(T_0\\)\n\n\n\\(H_0\\) False\nFN\nTD\n\\(T_1\\)\n\n\nTotal\n\\(N\\)\n\\(D\\)\n\\(m\\)\n\n\n\nA more practical approach is to use the False Discovery Rate \\[\n\\text{FDR} = \\E{\\frac{FD}{D}}\n\\] which is the proportion of false positives among all significant results. We aim to set a cutoff so that FDR \\(&lt; Q\\). The FDR approach allows to increase the power while maintaining some principled bound on error.\nBenjamini and Hochberg developed a procedure based on FDR to perform multiple testing. Under their procedure, we put individual \\(p\\)-values in order from smallest to largest. The we choose the largest \\(p_k\\) value that is smaller than \\((k/m)/Q\\). where \\(Q\\) is the false discovery rate you choose. Then all hypothesis with index \\(i&lt;k\\) are significant. Benjamini and Hochberg showed that under this procedure the FDR \\(&lt;Q\\).\nAs an example, García-Arenzana et al. (2014) tested associations of 25 dietary variables with mammographic density, an important risk factor for breast cancer, in Spanish women. They found the following results:\n\nd = read.csv(\"../data/cancer-diet.csv\")\nknitr::kable(d, booktabs = TRUE,caption = 'Dietary Risk Factors of Cancer')\n\n\nDietary Risk Factors of Cancer\n\n\nLabel\np.value\nRank\nBH\n\n\n\n\nTotal calories\n0.00\n1\n0.01\n\n\nOlive oil\n0.01\n2\n0.02\n\n\nWhole milk\n0.04\n3\n0.03\n\n\nWhite meat\n0.04\n4\n0.04\n\n\nProteins\n0.04\n5\n0.05\n\n\nNuts\n0.06\n6\n0.06\n\n\nCereals and pasta\n0.07\n7\n0.07\n\n\nWhite fish\n0.20\n8\n0.08\n\n\nButter\n0.21\n9\n0.09\n\n\nVegetables\n0.22\n10\n0.10\n\n\nSkimmed milk\n0.22\n11\n0.11\n\n\nRed meat\n0.25\n12\n0.12\n\n\nFruit\n0.27\n13\n0.13\n\n\nEggs\n0.28\n14\n0.14\n\n\nBlue fish\n0.34\n15\n0.15\n\n\nLegumes\n0.34\n16\n0.16\n\n\nCarbohydrates\n0.38\n17\n0.17\n\n\nPotatoes\n0.57\n18\n0.18\n\n\nBread\n0.59\n19\n0.19\n\n\nFats\n0.70\n20\n0.20\n\n\nSweets\n0.76\n21\n0.21\n\n\nDairy products\n0.94\n22\n0.22\n\n\nSemi-skimmed milk\n0.94\n23\n0.23\n\n\nTotal meat\n0.98\n24\n0.24\n\n\nProcessed meat\n0.99\n25\n0.25\n\n\n\n\n\nIf we choose \\(Q = 0.25\\), then \\(k=5\\) (Proteins) is our cut-off rank. Thus we accept \\(H_0\\) for the first five tests. Note, that traditional hypothesis testing procedure only controls for Type 1 error and FDR-based procedure controls for both error types.\n\n\n\n\nGarcía-Arenzana, Nicolás, Eva María Navarrete-Muñoz, Virginia Lope, Pilar Moreo, Carmen Vidal, Soledad Laso-Pablos, Nieves Ascunce, et al. 2014. “Calorie Intake, Olive Oil Consumption and Mammographic Density Among Spanish Women.” International Journal of Cancer 134 (8): 1916–25.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AB Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html",
    "href": "06-hyp.html",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "",
    "text": "6.1 Likelihood Principle\nThe hypothesis testing problem is as follows. Based on a sample of data, \\(y\\), generated from \\(p\\left( y \\mid \\theta\\right)\\) for \\(\\theta\\in\\Theta\\), the goal is to determine if \\(\\theta\\) lies in \\(\\Theta_{0}\\) or in \\(\\Theta_{1}\\), two disjoint subsets of \\(\\Theta\\). In general, the hypothesis testing problem involves an action: accepting or rejecting a hypothesis. The problem is described in terms of a null, \\(H_{0}\\), and alternative hypothesis, \\(H_{1}\\), which are defined as \\[\nH_{0}:\\theta\\in\\Theta_{0}\\;\\;\\mathrm{and}\\;\\;H_{1}%\n:\\theta\\in\\Theta_{1}\\text{.}%\n\\]\nDifferent types of regions generate different types of hypothesis tests. If the null hypothesis assumes that \\(\\Theta_{0}\\) is a single point, \\(\\Theta _{0}=\\theta_{0}\\), this is known as a simple or “sharp” null hypothesis. If the region consists of multiple points than the hypothesis is called a composite, which occurs if the space is unconstrained or an interval of the real line. In the case of a single parameter, typical one-sided tests are of the form \\(H_{0}:\\theta&lt;\\theta_{0}\\) and \\(H_{1}:\\theta&gt;\\theta_{0}\\).\nThere are correct decisions and two types of possible errors. The correct decisions are accepting a null or alternative that is true. A Type I error incorrectly rejects a true null, and a Type II error incorrectly accepts a false null.\nFormally, the probabilities of Type I (\\(\\alpha\\)) and Type II (\\(\\beta\\)) errors are defined as: \\[\n\\alpha=P \\left[  \\text{reject }H_{0} \\mid H_{0}\\text{\nis true }\\right]  \\text{ and }\\beta=P \\left[  \\text{accept\n}H_{0} \\mid H_{1}\\text{ is true }\\right]  \\text{.}%\n\\]\nIt is useful to think of the decision to accept or reject as a decision rule, \\(d\\left( y\\right)\\). In many cases, the decision rules form a critical region \\(R\\), such that \\(d\\left( y\\right) =d_{1}\\) if \\(y\\in R\\). These regions are often take the form of simple inequalities. Next, defining the decision to accept the null is \\(d\\left( y\\right) =d_{0}\\), and the decision to accept the alternative is \\(d_{1},\\) the error types are \\[\\begin{align*}\n\\alpha_{\\theta}\\left(  d\\right)   &  =P \\left[  d\\left(  y\\right)\n=d_{1} \\mid \\theta\\right]  \\text{ if }\\theta\\in\\Theta_{0}\\text{ }(H_{0}\\text{ is true})\\\\\n\\beta_{\\theta}\\left(  d\\right)   &  =P \\left[  d\\left(  y\\right)\n=d_{0} \\mid \\theta\\right]  \\text{ if }\\theta\\in\\Theta_{1}\\text{ }(H_{1}\\text{ is true})\\text{.}%\n\\end{align*}\\] where both types of errors explicitly depend on the decision and the true parameter value. Notice that both of these quantities are determined by the population properties of the data. In the case of a composite null hypothesis, the size of the test (the probability of making a type I error) is defined as \\[\n\\alpha = \\underset{\\theta\\in\\Theta_{0}}{\\sup}~\\alpha_{\\theta}\\left( d\\right)\n\\] and the power is defined as \\(1-\\beta_{\\theta}\\left( d\\right)\\). It is always possible to set either \\(\\alpha_{\\theta}\\left( d\\right)\\) or \\(\\beta_{\\theta }\\left( d\\right)\\) equal to zero, by finding a test that always rejects alternative or null, respectively.\nThe total probability of making an error is \\(\\alpha_{\\theta}\\left(d\\right) +\\beta_{\\theta}\\left(d\\right)\\), and ideally one would seek to minimize the total error probability, absent additional information. The optimal action \\(d^*\\) minimizes the posterior expected loss, is \\(d^* = d_0 = 0\\) if the posterior probability of hypothesis \\(H_0\\) exceeds 1/2, and \\(d^* = d_1=1\\) else \\[\nd^* = 1\\left(  P \\left(  \\theta \\in \\Theta_0 \\mid y\\right) &lt; P \\left(  \\theta \\in \\Theta_1 \\mid y\\right)\\right)  = 1\\left(P \\left(  \\theta \\in \\Theta_0 \\mid y\\right)&lt;1/2\\right).\n\\] Simply speaking, the hypothesis with higher posterior probability is selected.\nThe easiest way to reduce the error probability is to gather more data, as the additional evidence should lead to more accurate decisions. In some cases, it is easy to characterize optimal tests, those that minimize the sum of the errors. Simple hypothesis tests of the form \\(H_{0}:\\theta=\\theta_{0}\\) versus \\(H_{1}:\\theta=\\theta_{1}\\), are one such case admiting optimal tests. Defining \\(d^{\\ast}\\) as a test accepting \\(H_{0}\\) if \\(a_{0}f\\left( y \\mid \\theta_{0}\\right) &gt;a_{1}f\\left( y \\mid \\theta_{1}\\right)\\) and \\(H_{1}\\) if \\(a_{0}f\\left( y \\mid \\theta_{0}\\right) &lt;a_{1}f\\left( y \\mid \\theta _{1}\\right)\\), for some \\(a_{0}\\) and \\(a_{1}\\). Either \\(H_{0}\\) or \\(H_{1}\\) can be accepted if \\(a_{0}f\\left(y \\mid \\theta_{0}\\right) =a_{1}f\\left( y \\mid \\theta_{1}\\right)\\). Then, for any other test \\(d\\), it is not hard to show that \\[\na_{0}\\alpha\\left(  d^{\\ast}\\right)  +a_{1}\\beta\\left(  d^{\\ast}\\right)  \\leq\na_{0}\\alpha\\left(  d\\right)  +a_{1}\\beta\\left(  d\\right),\n\\] where \\(\\alpha_{d}=\\alpha_{d}\\left( \\theta\\right)\\) and \\(\\beta_{d}=\\beta_{d}\\left( \\theta\\right)\\). This result highlights the optimality of tests defining rejection regions in terms of the likelihood ratio statistic, \\(f\\left( y \\mid \\theta_{0}\\right)/f\\left( y \\mid \\theta_{1}\\right)\\). It turns out that the results are in fact stronger. In terms of decision theoretic properties, tests that define rejection regions based on likelihood ratios are not only admissible decisions, but form a minimal complete class, the strongest property possible.\nOne of the main problems in hypothesis testing is that there is often a tradeoff between the two goals of reducing type I and type II errors: decreasing \\(\\alpha\\) leads to an increase in \\(\\beta\\), and vice-versa. Because of this, it is common to fix \\(\\alpha_{\\theta}\\left( d\\right)\\), or \\(\\sup~\\alpha_{\\theta}\\left( d\\right)\\), and then find a test to minimize \\(\\beta_{d}\\left( \\theta\\right)\\). This leads to “most powerful” tests. There is an important result from decision theory: test procedures that use the same size level of \\(\\alpha\\) in problems with different sample sizes are inadmissible. This is commonly done where significance is indicated by a fixed size, say 5%. The implications of this will be clearer below in examples.\nGiven observed data \\(y\\) and likelihood function \\(l(\\theta) = p(y\\mid \\theta)\\), the likelihood principle states that all relevant experimental information is contained in the likelihood function for the observed \\(y\\). Furthermore, two likelihood functions contain the same information about \\(\\theta\\) if they are proportional to each other. For example, the widely used maximum-likelihood estimation does satisfy the likelihood principle. However, this principle is sometimes violated by non-Bayesian hypothesis testing procedures. The likelihood principle is a fundamental principle in statistical inference, and it is a key reason why Bayesian procedures are often preferred.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#likelihood-principle",
    "href": "06-hyp.html#likelihood-principle",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "",
    "text": "Example 6.1 (Testing fairness) Suppose we are interested in testing \\(\\theta\\), the unknown probability of heads for possibly biased coin. Suppose, \\[\nH_0 :~\\theta=1/2 \\quad\\text{v.s.} \\quad  H_1 :~\\theta&gt;1/2.\n\\] An experiment is conducted and 9 heads and 3 tails are observed. This information is not sufficient to fully specify the model \\(p(y\\mid \\theta)\\). There are two approaches.\nScenario 1: Number of flips, \\(n = 12\\) is predetermined. Then number of heads \\(Y \\mid \\theta\\) is binomial \\(B(n, \\theta)\\), with probability mass function \\[\np(y\\mid \\theta)= {n \\choose x} \\theta^x(1-\\theta)^{n-x} = 220 \\cdot \\theta^9(1-\\theta)^3\n\\] For a frequentist, the p-value of the test is \\[\nP(Y \\geq 9\\mid H_0)=\\sum_{y=9}^{12} {12 \\choose y} (1/2)^y(1-1/2)^{12-y} = (1+12+66+220)/2^{12} =0.073,\n\\] and if you recall the classical testing, the \\(H_0\\) is not rejected at level \\(\\alpha = 0.05\\).\nScenario 2: Number of tails (successes) \\(\\alpha = 3\\) is predetermined, i.e, the flipping is continued until 3 tails are observed. Then, \\(Y\\) - number of heads (failures) until 3 tails appear is Negative Binomial \\(NB(3, 1- \\theta)\\), \\[\np(y\\mid \\theta)= {\\alpha+y-1 \\choose \\alpha-1} \\theta^{y}(1-\\theta)^{\\alpha} = {3+9-1 \\choose 3-1} \\theta^9(1-\\theta)^3 = 55\\cdot \\theta^9(1-\\theta)^3.\n\\] For a frequentist, large values of \\(Y\\) are critical and the p-value of the test is \\[\nP(Y \\geq 9\\mid H_0)=\\sum_{y=9}^{\\infty} {3+y-1 \\choose 2} (1/2)^{x}(1/2)^{3} = 0.0327.\n\\] We used the following identity here \\[\n\\sum_{x=k}^{\\infty} {2+x \\choose 2}\\dfrac{1}{2^x} = \\dfrac{8+5k+k^2}{2^k}.\n\\] The hypothesis \\(H_0\\) is rejected, and this change in decision is not caused by observations.\nAccording to Likelihood Principle, all relevant information is in the likelihood \\(l(\\theta) \\propto \\theta^9(1 - \\theta)^3\\), and Bayesians could not agree more!\nEdwards, Lindman, and Savage (1963, 193) note: The likelihood principle emphasized in Bayesian statistics implies, among other things, that the rules governing when data collection stops are irrelevant to data interpretation. It is entirely appropriate to collect data until a point has been proven or disproven, or until the data collector runs out of time, money, or patience.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#the-bayesian-approach",
    "href": "06-hyp.html#the-bayesian-approach",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.2 The Bayesian Approach",
    "text": "6.2 The Bayesian Approach\nFormally, the Bayesian approach to hypothesis testing is a special case of the model comparison results to be discussed later. The Bayesian approach just computes the posterior distribution of each hypothesis. By Bayes rule, for \\(i=0,1\\) \\[\nP \\left(  H_{i} \\mid y\\right)  =\\frac{p\\left(  y \\mid H_{i}\\right)  P \\left(  H_{i}\\right)  }{p\\left(  y\\right)\n}\\text{,}%\n\\] where \\(P \\left( H_{i}\\right)\\) is the prior probability of \\(H_{i}\\), \\[\np\\left( y \\mid H_{i}\\right) =\\int_{\\theta \\in \\Theta_i} p\\left( y \\mid \\    \\theta\\right) p\\left( \\theta \\mid H_{i}\\right) d\\theta\n\\] is the marginal likelihood under \\(H_{i}\\), \\(p\\left( \\theta \\mid H_{i}\\right)\\) is the parameter prior under \\(H_{i}\\), and \\[\np\\left(  y\\right)  = \\sum_{i=1,2} p\\left(  y \\mid H_{i}\\right)  P \\left( H_{i}\\right).\n\\]\nIf the hypothesis are mutually exclusive, \\(P \\left( H_{0}\\right) =1-P \\left( H_{1}\\right)\\).\nThe posterior odds of the null to the alternative is \\[\n\\text{Odds}_{0,1}=\\frac{P \\left(  H_{0} \\mid y\\right)  }{P %\n\\left(  H_{1} \\mid y\\right)  }=\\frac{p\\left(  y \\mid H_{0}\\right)\n}{p\\left(  y \\mid H_{1}\\right)  }\\frac{P \\left(  H_{0}\\right)  }{P \\left(  H_{1}\\right)  }\\text{.}%\n\\]\nThe odds ratio updates the prior odds, \\(P \\left( H_{0}\\right) /P \\left( H_{1}\\right)\\), using the Bayes Factor, \\[\n\\mathcal{BF}_{0,1}=\\dfrac{p\\left(y \\mid H_{0}\\right)}{p\\left( y \\mid H_{1}\\right)}.\n\\] With exhaustive competing hypotheses\\(,\\) \\(P \\left( H_{0} \\mid y\\right)\\) simplifies to \\[\nP \\left(  H_{0} \\mid y\\right)  =\\left(  1+\\left(  \\mathcal{BF}_{0,1}\\right)  ^{-1}\\frac{\\left(  1-P \\left(  H_{0}\\right)\n\\right)  }{P \\left(  H_{0}\\right)  }\\right)  ^{-1}\\text{,}%\n\\] and with equal prior probability, \\(p\\left( H_{0} \\mid y\\right) =\\left( 1+\\left( \\mathcal{BF}_{0,1}\\right) ^{-1}\\right) ^{-1}\\). Both Bayes factors and posterior probabilities can be used for comparing hypotheses. Jeffreys (1961) advocated using Bayes factors, and provided a scale for measuring the strength of evidence that was given earlier. Bayes factors merely indicate that the null hypothesis is more likely if \\(\\mathcal{BF}_{0,1}&gt;1\\), \\(p\\left( y \\mid H_{0}\\right) &gt;p\\left( y \\mid H_{1}\\right)\\). The Bayesian approach merely compares density ordinates of \\(p\\left( y \\mid H_{0}\\right)\\) and \\(p\\left( y \\mid H_{1}\\right)\\), which mechanically involves plugging in the observed data into the functional form of the marginal likelihood.\nFor a point null, \\(H_{0}:\\theta=\\theta_{0}\\), the parameter prior is \\(p\\left( \\theta \\mid H_{0}\\right) =\\delta_{\\theta_{0}}\\left( \\theta\\right)\\) (a Dirac mass at \\(\\theta_{0}\\)), which implies that \\[\np\\left( y \\mid H_{0}\\right) =\\int p\\left( y \\mid \\theta_{0}\\right) p\\left( \\theta \\mid H_{0}\\right) d\\theta=p\\left( y \\mid \\theta_{0}\\right).\n\\] With a general alternative, \\(H_{1}:\\theta\\neq\\theta_{0}\\), the probability of the null is \\[\nP \\left(  \\theta=\\theta_{0} \\mid y\\right)  =\\frac{p\\left(  y \\mid \\theta\n_{0}\\right)  P \\left(  H_{0}\\right)  }{p\\left(  y \\mid \\theta\n_{0}\\right)  P \\left(  H_{0}\\right)  +\\left(  1-p\\left( H_{0}\\right)  \\right)  \\int_{\\Theta}p\\left(  y \\mid \\theta,H_{1}\\right)  p\\left(  \\theta \\mid H_{1}\\right)  d\\theta},\n\\] where \\(p\\left( \\theta \\mid H_{1}\\right)\\) is the parameter prior under the alternative. This formula will be used below.\nBayes factors and posterior null probabilities measure the relative weight of evidence of the hypotheses. Traditional hypothesis involves an additional decision or action: to accept or reject the null hypothesis. For Bayesian, this typically requires some statement of the utility/loss codifies the benefits/costs of making a correct or incorrect decisision. The simplest situation occurs if one assumes a zero loss of making a correct decision. The loss incurred when accepting the null (alternative) when the alternative is true (false) is \\(L\\left( d_{0} \\mid H_{1}\\right)\\) and \\(L\\left( d_{1} \\mid H_{0}\\right)\\), respectively.\nThe Bayesian will accept or reject based on the posterior expected loss. If the expected loss of accepting the null is less than the alternative, the rational decision maker will accept the null. The posterior loss of accepting the null is \\[\n\\mathbb{E}\\left[  \\text{Loss}\\mid d_{0},y\\right]  =L\\left(  d_{0} \\mid H_{0}\\right)\nP \\left(  H_{0} \\mid y\\right)  +L\\left(  d_{0} \\mid H_{1}\\right)  P \\left(  H_{1} \\mid y\\right)  =L\\left( d_{0} \\mid H_{1}\\right)  P \\left(  H_{1} \\mid y\\right)  ,\n\\] since the loss of making a correct decision, \\(L\\left( d_{0} \\mid H_{0}\\right)\\), is zero. Similarly, \\[\n\\mathbb{E}\\left[  \\text{Loss} \\mid d_{1},y\\right]  =L\\left(  d_{1} \\mid H_{0}\\right)\nP \\left(  H_{0} \\mid y\\right)  +L\\left(  d_{1} \\mid H_{1}\\right)  P \\left(  H_{1} \\mid y\\right)  =L\\left( d_{1} \\mid H_{0}\\right)  P \\left(  H_{0} \\mid y\\right)  .\n\\] Thus, the null is accepted if \\[\n\\mathbb{E}\\left[  \\text{Loss} \\mid d_{0},y\\right]  &lt;\\mathbb{E}\\left[  \\text{Loss} \\mid d_{1},y\\right]\n\\Longleftrightarrow L\\left(  d_{0} \\mid H_{1}\\right)  P \\left( H_{1} \\mid y\\right)  &lt;L\\left(  d_{1} \\mid H_{0}\\right)\nP \\left(  H_{0} \\mid y\\right)  ,\n\\] which further simplifies to \\[\n\\frac{L\\left(  d_{0} \\mid H_{1}\\right)  }{L\\left(  d_{1} \\mid H_{0}\\right)  }&lt;\\frac{P \\left(  H_{0} \\mid y\\right)  }{P \\left(  H_{1} \\mid y\\right)  }.\n\\] In the case of equal losses, this simplifies to accept the null if \\(P \\left( H_{1} \\mid y\\right) &lt;P \\left( H_{0} \\mid y\\right)\\). One advantage of Bayes procedures is that the resulting estimators and decisions are always admissible.\n\nExample 6.2 (Enigma machine: Code-breaking) Consider an alphabet of \\(26\\) letters. Let \\(x\\) and \\(y\\) be two codes of length \\(T\\). We will look to see how many letters match (\\(M\\)) and don’t match \\(N\\). In these sequences. Even though the codes are describing different sentences, when letters are the same, if the same code is being used then the sequence will have a match. To compute the bayes factor we need the joint probabilities \\[\nP( x,y\\mid  H_0 ) \\; \\; \\mathrm{ and} \\; \\; P( x,y\\mid  H_1 ),\n\\] where under \\(H_0\\) they are different codes, in which case the joint prob is \\(( 1 / A )^{2T}\\). For \\(H_1\\) we first need to know the chance of the same letter matching. If \\(p_t\\) denotes the frequencies of the use of English letters, then we have this match probability \\(m = \\sum_{i} p_i^2\\) which is about \\(2/26\\). Hence for a particular set of letters \\[\nP( x_i , y_i \\mid H_1 ) = \\frac{m}{A} \\; \\mathrm{ if} \\; x_i =y_i \\; \\; \\mathrm{ and} \\; \\;  P( x_i , y_i \\mid H_1 ) = \\frac{1-m}{A(A-1)} \\; \\mathrm{ if} \\; x_i \\neq y_i.\n\\] Hence the log Bayes factor is \\[\\begin{align*}\n\\ln \\frac{P( x,y\\mid  H_1 )}{P( x,y\\mid  H_0 )} & = M \\ln \\frac{ m/A}{1/A^2} +N \\ln \\frac{ ( 1-m ) / A(A-1) }{ 1/ A^2} \\\\\n& = M \\ln mA  + N \\ln \\frac{ ( 1-m )A }{A-1 }\n\\end{align*}\\] The first term comes when you get a match and the increase in the Bayes factor is large, \\(3.1\\) (on a \\(log_{10}\\))-scale, otherwise you get a no-match and the Bayes factor decreases by \\(- 0.18\\).\nExample, \\(N=4\\), \\(M=47\\) out of \\(T=51\\), then gives evidence of 2.5 to 1 in favor of \\(H_1\\)\nHow long a sequence do you need to look at? Calculate the expected log odds. Turing and Good figured you needed sequences of about length \\(400\\). Can also look at doubles and triples.\n\n\nExample 6.3 (Signal Transmission) Suppose that the random variable \\(X\\) is transmitted over a noisy communication channel. Assume that the received signal is given by \\[\nY=X+W,\n\\] where \\(W\\sim N(0,\\sigma^2)\\) is independent of \\(X\\). Suppose that \\(X=1\\) with probability \\(p\\), and \\(X=-1\\) with probability \\(1-p\\). The goal is to decide between \\(X=1\\) and \\(X=-1\\) by observing the random variable \\(Y\\). We will assume symmetric loss and will accept the hypothesis with the higher posterior probability. This is also sometimes called the maximum a posteriori (MAP) test.\nWe assume that \\(H_0: ~ X = 1\\), thus \\(Y\\mid X_0 \\sim N(1,\\sigma^2)\\), and \\(Y\\mid X_1 \\sim N(-1,\\sigma^2)\\). The Bayes factor is simply the likelihood ratio \\[\n\\dfrac{p(y\\mid H_0)}{p(y \\mid H_1)} =  \\exp\\left( \\frac{2y}{\\sigma^2}\\right).\n\\] The propr odds are \\(p/(1-p)\\), thus the posterior odds are \\[\n\\exp\\left( \\frac{2y}{\\sigma^2}\\right)\\dfrac{p}{1-p}.\n\\] We choose \\(H_0\\) (true \\(X\\) is 1), if the posterior odds are greater than 1, i.e., \\[\ny &gt; \\frac{\\sigma^2}{2} \\log\\left( \\frac{p}{1-p}\\right) = c.\n\\]\nFurther, we can calculate the error probabilities of our test. \\[\np(d_1\\mid H_0) = P(Y&lt;c\\mid X=1) = \\Phi\\left( \\frac{c-1}{\\sigma}\\right),\n\\] and \\[\np(d_0\\mid H_1) = P(Y&gt;c\\mid X=-1) = 1- \\Phi\\left( \\frac{c+1}{\\sigma}\\right).\n\\] Let’s plot the total error rate as a function of \\(p\\) and assuming \\(\\sigma=0.2\\) \\[\nP_e = p(d_1\\mid H_0) (1-p) + p(d_0\\mid H_1) p\n\\]\n\nsigma &lt;- 0.2\np &lt;- seq(0.01,0.99,0.01)\nc &lt;- sigma^2/2*log(p/(1-p))\nPe &lt;- pnorm((c-1)/sigma)*(1-p) + (1-pnorm((c+1)/sigma))*p\nplot(p,Pe,type=\"l\",xlab=\"p\",ylab=\"Total Error Rate\")\n\n\n\n\n\n\n\n\n\n\nExample 6.4 (Hockey: Hypothesis Testing for Normal Mean) General manager of Washington Capitals (an NHL hockey team) thinks that their star center player Evgeny Kuznetsov is underperformed and is thinking of trading him to a different team. He uses the number of goals per season as a metric of performance. He knows that historically, a top forward scores on average 30 goals per season with a standard deviation of 5, \\(\\theta \\sim N(30,25)\\). In the 2022-2023 season Kuznetsov scored 12 goals. For the number of goals \\(X\\mid \\theta\\) he uses normal likelihood \\(N(\\theta, 36)\\). Kuznetsov’s performance was not stable over the years, thus high variance in the likelihood. Thus, the posterior is \\(N(23,15)\\).\n\nsigma2 = 36\nsigma02 = 25\nmu=30\ny=12\nk = sigma02 + sigma2\nmu1 = sigma2/k*mu + sigma02/k*y\nsigma21 = sigma2*sigma02/k\nmu1\n\n 23\n\nsigma21\n\n 15\n\n\nThe manager thinks, that Kuznetsov simply had a bad year and his true performance is at least 24 goals per season \\(H_0: \\theta &gt; 24\\), \\(H_1: \\theta&lt;24\\). The posterior probability of \\(H_0\\) hypothesis is\n\na = 1-pnorm(24,mu1,sqrt(sigma21))\na\n\n 0.36\n\n\nIt is less than 1/2, only 36%. Thus, we should reject the null hypothesis. The posterior odds in favor of the null hypothesis is\n\na/(1-a)\n\n 0.56\n\n\nIf underestimating (and trading) Kuznetsov is two times more costly than overestimating him (fans will be upset and team spirit might be affected), that is \\(L(d_1\\mid H_0) = 2L(d_0\\mid H_1)\\), then we should accept the null when posterior odds are greater than 1/2. This is the case here, 0.55 is greater than 1/2. The posterior odds are in favor of the null hypothesis. Thus, the manager should not trade Kuznetsov.\nKuznetsov was traded to Carolina Hurricanes towards the end of the 2023-2024 season.\nNotice, when we try to evaluate a new-comer to the league, we use prior probability of \\(\\theta &gt; 24\\)\n\na = 1-pnorm(24,mu,sqrt(sigma02))\nprint(a)\n\n 0.88\n\na/(1-a)\n\n 7.7\n\n\nThus, the prior odds in favor of \\(H_0\\) are 7.7.\n\n\nExample 6.5 (Hypothesis Testing for Normal Mean: Two-Sided Test) In the case of two sided test, we are interested in testing\n\n\\(H_0: \\theta = m_0\\), \\(p\\left( \\theta \\mid H_{0}\\right) =\\delta_{m_{0}}\\left( \\theta\\right)\\)\n\\(H_1: \\theta \\neq m_0\\), \\(p\\left( \\theta \\mid H_{1}\\right) = N\\left( m_{0},\\sigma^{2}/n_0\\right)\\)\n\nWhere \\(n\\) is the sample size and \\(\\sigma^2\\) is the variance (known) of the population. Observed samples are \\(Y = (y_1, y_2, \\ldots, y_n)\\) with \\[\ny_i \\sim N(\\theta, \\sigma^2).\n\\]\nThe Bayes factor can be calculated analytically \\[\nBF_{0,1} = \\frac{p(Y\\mid \\theta = m_0, \\sigma^2 )}\n{\\int p(Y\\mid \\theta, \\sigma^2) p(\\theta \\mid m_0, n_0, \\sigma^2)\\, d \\theta}\n\\] \\[\n\\int p(Y\\mid \\theta, \\sigma^2) p(\\theta \\mid m_0, n_0, \\sigma^2)\\, d \\theta = \\frac{\\sqrt{n_0}\\exp\\left\\{-\\frac{n_0(m_0-\\bar y)^2}{2\\left(n_0+n\\right)\\sigma^2}\\right\\}}{\\sqrt{2\\pi}\\sigma^2\\sqrt{\\frac{n_0+n}{\\sigma^2}}}\n\\] \\[\np(Y\\mid \\theta = m_0, \\sigma^2 ) = \\frac{\\exp\\left\\{-\\frac{(\\bar y-m_0)^2}{2 \\sigma ^2}\\right\\}}{\\sqrt{2 \\pi } \\sigma }\n\\] Thus, the Bayes factor is \\[\nBF_{0,1} = \\frac{\\sigma\\sqrt{\\frac{n_0+n}{\\sigma^2}}e^{-\\frac{(m_0-\\bar y)^2}{2\\left(n_0+n\\right)\\sigma^2}}}{\\sqrt{n_0}}\n\\]\n\\[\nBF_{0,1} =\\left(\\frac{n + n_0}{n_0} \\right)^{1/2} \\exp\\left\\{-\\frac 1 2 \\frac{n }{n + n_0} Z^2 \\right\\}\n\\]\n\\[\nZ =  \\frac{(\\bar{Y} - m_0)}{\\sigma/\\sqrt{n}}\n\\]\nOne way to interpret the scaling factor \\(n_0\\) is ro look at the standard effect size \\[\n\\delta = \\frac{\\theta - m_0}{\\sigma}.\n\\] The prior of the standard effect size is \\[\n\\delta \\mid H_1 \\sim N(0, 1/n_0).\n\\] This allows us to think about a standardized effect independent of the units of the problem.\nLet’s consider now example of Argon discovery.\n\nair =    c(2.31017, 2.30986, 2.31010, 2.31001, 2.31024, 2.31010, 2.31028, 2.31028)\ndecomp = c(2.30143, 2.29890, 2.29816, 2.30182, 2.29869, 2.29940, 2.29849, 2.29889)\n\nOur null hypothesis is that the mean of the difference equals to zero. We assume that measurements made in the lab have normal errors, this the normal likelihood. We empirically calculate the standard deviation of our likelihood. The Bayes factor is\n\ny = air - decomp\nn = length(y)\nm0 = 0\nsigma = sqrt(var(air) + var(decomp))\nn0 = 1\nZ = (mean(y) - m0)/(sigma/sqrt(n))\nBF = sqrt((n + n0)/n0)*exp(-0.5*n/(n + n0)*Z^2)\nBF\n\n 1.9e-91\n\n\nWe have extremely strong evidence in favor \\(H_1: \\theta \\ne 0\\) hypothesis. The posterior probability of the alternative hypothesis is numerically 1!\n\na = 1/(1+BF)\na\n\n 1\n\n\n\n\nExample 6.6 (Hypothesis Testing for Proportions) Let’s look at again at the effectiveness of Google’s new search algorithm. We measure effectiveness by the number of users who clicked on one of the search results. As users send the search requests, they will be randomly processed with Algo 1 or Algo 2. We wait until 2500 search requests were processed by each of the algorithms and calculate the following table based on how often people clicked through\n\n\n\n\nAlgo1\nAlgo2\n\n\n\n\nsuccess\n1755\n1818\n\n\nfailure\n745\n682\n\n\ntotal\n2500\n2500\n\n\n\nHere we assume binomial likelihood and use conjugate beta prior, for mathematical convenience. We are putting independent beta priors on the click-through rates of the two algorithms, \\(p_1\\sim Beta(\\alpha_1,\\beta_1)\\) and \\(p_2\\sim Beta(\\alpha_2,\\beta_2)\\). The posterior for \\(p_1\\) and and \\(p_2\\) are independent betas \\[\np(p_1, p_1 \\mid y) \\propto p_1^{\\alpha_1 + 1755 - 1} (1-p_1)^{\\beta_1 + 745 - 1}\\times p_2^{\\alpha_2 + 1818 - 1} (1-p_2)^{\\beta_2 + 682 - 1}.\n\\]\nThe easiest way to explore this posterior is via Monte Carlo simulation of the posterior.\n\nset.seed(92) #Kuzy\ny1 &lt;- 1755; n1 &lt;- 2500; alpha1 &lt;- 1; beta1 &lt;- 1\ny2 &lt;- 1818; n2 &lt;- 2500; alpha2 &lt;- 1; beta2 &lt;- 1\nm = 10000\np1 &lt;- rbeta(m, y1 + alpha1, n1 - y1 + beta1)\np2 &lt;- rbeta(m, y2 + alpha2, n2 - y2 + beta2)\nrd &lt;- p2 - p1\nplot(density(rd), main=\"Posterior Difference in Click-Through Rates\", xlab=\"p2 - p1\", ylab=\"Density\")\nq = quantile(rd, c(.05, .95))\nprint(q)\n\n    5%    95% \n0.0037 0.0465 \n\nabline(v=q,col=\"red\")",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#interval-estimation-credible-sets",
    "href": "06-hyp.html#interval-estimation-credible-sets",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.3 Interval Estimation: Credible Sets",
    "text": "6.3 Interval Estimation: Credible Sets\nThe interval estimators of model parameters are called credible sets. If we use the posterior measure to assess the credibility, the credible set is a set of parameter values that are consistent with the data and gives us is a natural way to measure the uncertainty of the parameter estimate.\nThose who are familiar with the concept of classical confidence intervals (CI’s) often make an error by stating that the probability that the CI interval \\([L, U ]\\) contains parameter \\(\\theta\\) is \\(1 - \\alpha\\). The right statement seems convoluted, one needs to generate data from such model many times and for each data set to exhibit the CI. Now, the proportion of CI’s covering the unknown parameter is “tends to” \\(1 - \\alpha\\). Bayesian interpretation of a credible set \\(C\\) is natural: The probability of a parameter belonging to the set \\(C\\) is \\(1 - \\alpha\\). A formal definition follows. Assume the set \\(C\\) is a subset of domain of the parameter \\(\\Theta\\). Then, \\(C\\) is credible set with credibility \\((1 - \\alpha)\\cdot 100\\%\\) if \\[\np(\\theta \\in C \\mid y) = \\int_{C}p(\\theta\\mid y)d\\theta \\ge 1 - \\alpha.\n\\] If the posterior is discrete, then the integral becomes sum (counting measure) and \\[\np(\\theta \\in C \\mid y) = \\sum_{\\theta_i\\in C}p(\\theta_i\\mid y)d\\theta \\ge 1 - \\alpha.\n\\] This is the definition of a \\((1 - \\alpha)100\\%\\) credible set, and of course for a given posterior function such set is not unique.\nFor a given credibility level \\((1 - \\alpha)100\\%\\), the shortest credible set is of interest. To minimize size the sets should correspond to highest posterior probability (density) areas. Thus the acronym HPD.\n\nDefinition 6.1 (Highest Posterior Density (HPD) Credible Set) The \\((1 - \\alpha)100\\%\\) HDP credible set for parameter \\(\\theta\\) is a set \\(C \\subset \\Theta\\) of the form \\[\nC = \\{ \\theta \\in \\Theta : p(\\theta \\mid y) \\ge k(\\alpha) \\},\n\\] where \\(k(\\alpha)\\) is the smallest value such that \\[\nP(\\theta\\in C \\mid y) = \\int_{C}p(\\theta\\mid y)d\\theta \\ge 1 - \\alpha.\n\\] Geometrically, if the posterior density is cut by a horizontal line at the hight \\(k(\\alpha)\\), the set \\(C\\) is projection on the \\(\\theta\\) axis of the part of line inside the density, i.e., the part that lies below the density.\n\n\n\n\n\n\n\n\n\n\n\nLemma 6.1 The HPD set \\(C\\) minimizes the size among all sets \\(D \\subset \\Theta\\) for which \\[\nP(\\theta \\in D) = 1 - \\alpha.\n\\]\n\n\nProof. The proof is essentially a special case of Neyman-Pearson lemma. If \\(I_C(\\theta) = 1(\\theta \\in C)\\) and \\(I_D(\\theta) = 1(\\theta \\in D)\\), then the key observation is \\[\n\\left(p(\\theta\\mid y) - k(\\alpha)\\right)(I_C(\\theta) - I_D(\\theta)) \\ge 0.\n\\] Indeed, for \\(\\theta\\)’s in \\(C\\cap D\\) and \\((C\\cup D)^c\\), the factor \\(I_C(\\theta)-I_D(\\theta) = 0\\). If \\(\\theta \\in C\\cap D^c\\), then \\(I_C(\\theta)-I_D(\\theta) = 1\\) and \\(p(\\theta\\mid y)-k(\\alpha) \\ge 0\\). If, on the other hand, \\(\\theta \\in D\\cap C^c\\), then \\(I_C(\\theta)-I_D(\\theta) = -1\\) and \\(p(\\theta\\mid y)-k(\\alpha) \\le 0\\). Thus, \\[\n\\int_{\\Theta}(p(\\theta\\mid y) - k(\\alpha))(I_C(\\theta) - I_D(\\theta))d\\theta \\ge 0.\n\\] The statement of the theorem now follows from the chain of inequalities, \\[\n\\int_{C}(p(\\theta\\mid y) - k(\\alpha))d\\theta \\ge \\int_{D}(p(\\theta\\mid y) - k(\\alpha))d\\theta\n\\] \\[\n(1-\\alpha) - k(\\alpha)\\text{size}(C) \\ge (1-\\alpha) - k(\\alpha)\\text{size}(D)\n\\] \\[\nsize(C) \\le size(D).\n\\] The size of a set is simply its total length if the parameter space \\(\\theta\\) is one dimensional, total area, if \\(\\theta\\) is two dimensional, and so on.\n\nNote, when the distribution \\(p(\\theta \\mid y)\\) is unimodal and symmetric using quantiles of the posterior distribution is a good way to obtain the HPD set.\nAn equal-tailed interval (also called a central interval) of confidence level\n\\[\nI_{\\alpha} = [q_{\\alpha/2}, q_{1-\\alpha/2}],\n\\] here \\(q\\)’s are the quantiles of the posterior distribution. This is an interval on whose both right and left side lies \\((1-\\alpha/2)100\\%\\) of the probability mass of the posterior distribution; hence the name equal-tailed interval.\nUsually, when a credible interval is mentioned without specifying which type of the credible interval it is, an equal-tailed interval is meant.\nHowever, unless the posterior distribution is unimodal and symmetric, there are point outsed of the equal-tailed credible interval having a higher posterior density than some points of the interval. If we want to choose the credible interval so that this not happen, we can do it by using the highest posterior density criterion for choosing it.\n\nExample 6.7 (Cauchy.) Assume that the observed samples\n\ny = c(2,-7,4,-6)\n\ncome from Cauchy distribution. The likelihood is \\[\np(y\\mid \\theta, \\gamma) = \\frac{1}{\\pi\\gamma} \\prod_{i=1}^{4} \\frac{1}{1+\\left(\\dfrac{y_i-\\theta}{\\gamma}\\right)^2}.\n\\] We assume unknown location parameter \\(\\theta\\) and scale parameter \\(\\gamma=1\\). For the flat prior \\(\\pi(\\theta) = 1\\), the posterior is proportional to the likelihood.\n\nlhood = function(theta) 1/prod(1+(y-theta)^2)\ntheta &lt;- seq(-10,10,0.1)\npost &lt;- sapply(theta,lhood)\npost = 10*post/sum(post)\nplot(theta,post,type=\"l\",xlab=expression(theta),ylab=\"Posterior Density\")\nabline(h=c(0.008475, 0.0159, 0.1, 0.2),col=\"red\")\n\n\n\n\n\n\n\n\nThe four horizontal lines correspond to four credible sets\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(C\\)\n\\(P(\\theta \\in C \\mid y)\\)\n\n\n\n\n0.008475\n[-8.498, 5.077]\n99%\n\n\n0.0159\n[-8.189, -3.022] \\(\\cup\\) [-0.615, 4.755]\n95%\n\n\n0.1\n[-7.328, -5.124] \\(\\cup\\) [1.591, 3.120]\n64.2%\n\n\n0.2\n[-6.893, -5.667]\n31.2%\n\n\n\nNotice that for \\(k = 0.0159\\) and \\(k = 0.1\\) the credible set is not a compact. This shows that two separate intervals “clash” for the ownership of \\(\\theta\\) and this is a useful information. This non-compactness can also point out that the prior is not agreeing with the data. There is no frequentist counterpart for the CI for \\(\\theta\\) in the above model.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#alternative-approaches",
    "href": "06-hyp.html#alternative-approaches",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.4 Alternative Approaches",
    "text": "6.4 Alternative Approaches\nThe two main alternatives to the Bayesian approach are significance testing using \\(p-\\)values, developed by Ronald Fisher, and the Neyman-Pearson approach.\n\n6.4.1 Significance testing using p-values\nFisher’s approach posits a test statistic, \\(T\\left( y\\right)\\), based on the observed data. In Fisher’s mind, if the value of the statistic was highly unlikely to have occured under \\(H_{0}\\), then the \\(H_{0}\\) should be rejected. Formally, the \\(p-\\)value is defined as \\[\np=P \\left[  T\\left(  Y\\right)  &gt;T\\left(  y\\right)   \\mid H_{0}\\right]  ,\n\\] where \\(y\\) is the observed sample and \\(Y=\\left( Y_{1}, \\ldots ,Y_{T}\\right)\\) is a random sample generated from model \\(p\\left( Y \\mid H_{0}\\right)\\), that is, the null distribution of the test-statistic in repeated samples. Thus, the \\(p-\\)value is the probability that a data set would generate a more extreme statistic under the null hypothesis, and not the probability of the null, conditional on the data.\nThe testing procedure is simple. Fisher (1946, p. 80) argues that: *“If P (the p-value) is between* \\(0.1\\) and \\(0.9\\), there is is certainly no reason to suspect the hypothesis tested. If it is below \\(0.02\\), it is strongly indicated that the hypothesis fails to account for the whole of the facts. We shall not be astray if we draw a line at 0.05 and consider that higher values of \\(\\mathcal{X}^{2}\\) indicate a real discrepancy.” Defining \\(\\alpha\\) as the significance level, the tests rejects \\(H_{0}\\) if \\(p&lt;\\alpha\\). Fisher advocated a fixed significance level of \\(5\\%\\), based largely that \\(5\\%\\) is roughly the tail area of a mean zero normal distribution more than two standard deviations from \\(0\\), indicating a statistically significant departure. In practice, testing with \\(p-\\)values involves identifying a critical value, \\(t_{\\alpha}\\), and rejecting the null if the observed statistic \\(t\\left( y\\right)\\) is more extreme than \\(t_{\\alpha}\\). For example, for a significance test of the sample mean, \\(t\\left( y\\right) =\\left( \\overline{y}-\\theta_{0}\\right) /se\\left( \\overline{y}\\right)\\), where \\(se\\left( \\overline{y}\\right)\\) is the standard error of \\(\\overline{y}\\); the \\(5\\%\\) critical value is 1.96; and Fisher would reject the null if \\(t\\left( y\\right) &gt;t_{\\alpha}\\).\nFisher interpreted the \\(p-value\\) as the weight or measure of evidence of the null hypothesis. The alternative hypothesis is noticeable in its absence in Fisher’s approach. Fisher largely rejected the consideration of alternatives, believing that researchers should weigh the evidence or draw conclusions about the observed data rather than making decisions such as accepting or rejecting hypotheses based on it.\nThere are a number of issues with Fisher’s approach. The first and most obvious criticism is that it is possible to reject the null, when the alternative hypothesis is less likely. This is an inherent problem in using population tail probabilities–essentially rare events. Just because a rare event has occurred does not mean the null is incorrect, unless there is a more likely alternative. This situation often arises in court cases, where a rare event like a murder has occurred. Decisions based on p-values generates a problem called prosecutor’s Fallacy, which is discussed below. Second, Fisher’s approach relies on population properties (the distribution of the statistic under the null) that would only be revealed in repeated samples or asymptotically. Thus, the testing procedure relies on data that is not yet seen, a violation of what is known as the likelihood principle. As noted by Jeffreys’ (1939, pp. 315-316): “What the use of P implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable data that have not occurred. This seems a remarkable procedure” \nThird, Fisher is agnostic regarding the source of the test statistics, providing no discussion of how the researcher decides to focus on one test statistic over another. In some simple models, the distribution of properly scaled sufficient statistics provides natural test statistics (e.g., the \\(t-\\)test). In more complicated models, Fisher is silent on the sources. In many cases, there are numerous test statistics (e.g., testing for normality), and test choice is clearly subjective. For example, in GMM tests, the choice of test moments is clearly a subjective choice. Finally, from a practical perspective, \\(p-\\)values have a serious deficiency: tests using \\(p\\)-values often appear to give the wrong answer, in the sense that they provide a highly misleading impression of the weight of evidence in many samples. A number of examples of this will be given below, but in all cases, Fisher’s approach tends to over-reject the null hypotheses.\n\n\n6.4.2 Neyman-Pearson\nThe motivation for the Neyman-Pearson (NP) approach was W.S. Gosset, the famous `Student’ who invented the \\(t-\\)test. In analyzing a hypothesis, Student argued that a hypothesis is not rejected unless an alternative is available that provides a more plausible explanation of the data, in which case. Mathematically, this suggests analyzing the likelihood ratio, \\[\n\\mathcal{LR}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)  }{p\\left( y \\mid H_{1}\\right)  }\\text{,}%\n\\] and rejecting the null in favor of the alternative when the likelihood ratio is small enough, \\(\\mathcal{LR}_{0,1}&lt;k\\). This procedures conforms in spirit with the Bayesian approach.\nThe main problem was one of finding a value of the cut off parameter \\(k.\\) From the discussion above, by varying \\(k\\), one varies the probabilities of type one and type two errors in the testing procedure. Originally, NP argued this tradeoff should be subjectively specified: “how the balance (between the type I and II errors) should be struck must be left to the investigator” (Neyman and Pearson (1933a, p. 296) and “we attempt to adjust the balance between the risks \\(P_{1}\\)\\(P_{2}\\) to meet the type of problem before us” (1933b, p. 497). This approach, however, was not “objective *, and they then advocated fixing \\(\\alpha\\), the probability of a type I error, in order to determine \\(k\\). This led to their famous lemma:\n\nLemma 6.2 (Neyman-Pearson Lemma) Consider the simple hypothesis test of \\(H_{0}:\\theta=\\theta_{0}\\) versus \\(H_{1}:\\theta =\\theta_{1}\\) and suppose that the null is rejected if \\(\\mathcal{LR}_{0,1}&lt;k_{\\alpha}\\), where \\(k_{\\alpha}\\) is chosen to fix the probability of a type I error at \\(\\alpha:\\)% \\[\n\\alpha=P \\left[  y:\\mathcal{LR}_{0,1}&lt;k_{\\alpha} \\mid H_{0}\\right]  \\text{.}%\n\\] Then, this test is the most powerful test of size \\(\\alpha\\) in the sense that any other test with greater power, must have a higher size.\n\nIn the case of composite hypothesis tests, parameter estimation is required under the alternative, which can be done via maximum likelihood, leading to the likelihood ratio \\[\n\\mathcal{LR}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)  }{\\underset\n{\\theta\\in\\Theta}{\\sup}p\\left(  y \\mid \\Theta\\right)  }=\\frac{p\\left( y \\mid H_{0}\\right)  }{p\\left(  y \\mid \\widehat{\\theta}\\right)  }\\text{,}%\n\\] where \\(\\widehat{\\theta}\\) is the MLE. Because of this, \\(0\\leq\\mathcal{LR}_{0,1}\\leq 1\\) for composite hypotheses. In multi-parameter cases, finding the distribution of the likelihood ratio is more difficult, requiring asymptotic approximations to calibrate \\(k_{\\alpha}.\\)\nAt first glance, the NP approach appears similar to the Bayesian approach, as it takes into account the likelihood ratio. However, like the \\(p-\\)value, the NP approach has a critical flaw. Neyman and Pearson fix the Type I error, and then minimizes the type II error. In many practical cases, \\(\\alpha\\) is set at \\(5\\%\\) and the resulting \\(\\beta\\) is often very small, close to 0. Why is this a reasonable procedure? Given the previous discussion, this is essentially a very strong prior over the relative benefits/costs of different types of errors. While these assumptions may be warranted in certain settings, it is difficult to a priori understand why this procedure would generically make sense. The next section highlights how the \\(p-\\)value and NP approaches can generate counterintuitive and even absurd results in standard settings.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#examples-and-paradoxes",
    "href": "06-hyp.html#examples-and-paradoxes",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.5 Examples and Paradoxes",
    "text": "6.5 Examples and Paradoxes\nThis section provides a number of paradoxes arising when using different hypothesis testing procedures. The common strands of the examples will be discussed at the end of the section.\n\nExample 6.8 (Neyman-Pearson tests) Consider testing \\(H_{0}:\\mu=\\mu_{0}\\) versus \\(H_{1}:\\mu=\\mu_{1}\\), \\(y_{t}\\sim\\mathcal{N}\\left( \\mu,\\sigma^{2}\\right)\\) and \\(\\mu_{1}&gt;\\mu_{0}\\). For this simple test, the likelihood ratio is given by \\[\n\\mathcal{LR}_{0,1}=\\frac{\\exp\\left(  -\\frac{1}{2\\sigma^{2}}%\n%TCIMACRO{\\tsum \\nolimits_{t=1}^{T}}%\n%BeginExpansion\n{\\textstyle\\sum\\nolimits_{t=1}^{T}}\n%EndExpansion\n\\left(  y_{t}-\\mu_{0}\\right)  ^{2}\\right)  }{\\exp\\left(  -\\frac{1}{2\\sigma\n^{2}}%\n%TCIMACRO{\\tsum \\nolimits_{t=1}^{T}}%\n%BeginExpansion\n{\\textstyle\\sum\\nolimits_{t=1}^{T}}\n%EndExpansion\n\\left(  y_{t}-\\mu_{1}\\right)  ^{2}\\right)  }=\\exp\\left(  -\\frac{T}{\\sigma^{2}%\n}\\left(  \\mu_{1}-\\mu_{0}\\right)  \\left(  \\overline{y}-\\frac{1}{2}\\left( \\mu_{0}+\\mu_{1}\\right)  \\right)  \\right)  \\text{.}%\n\\] Since \\(\\mathcal{BF}_{0,1}=\\mathcal{LR}_{0,1}\\), assuming equal prior probabilities and symmetric losses, the Bayesian accepts \\(H_{0}\\) if \\(\\mathcal{BF}_{0,1}&gt;1\\). Thus, the Bayes procedure rejects \\(H_{0}\\) if \\(\\overline{y}&gt;\\frac{1}{2}\\left( \\mu_{0}+\\mu_{1}\\right)\\) for any \\(T\\) and \\(\\sigma^{2}\\), with \\(\\mu_{0}\\),\\(\\mu_{1}\\), \\(T,\\)and \\(\\sigma^{2}\\) determining the strength of the rejction. If \\(\\mathcal{BF}_{0,1}=1\\), there is equal evidence for the two hypotheses.\nThe NP procedure proceeds by first setting \\(\\alpha=0.05,\\) and rejects when \\(\\mathcal{LR}_{0,1}\\) is large. This is equivalent to rejecting when \\(\\overline{y}\\) is large, generating an `optimal’ rejection region of the form \\(\\overline{y}&gt;c\\). The cutoff value \\(c\\) is calibrated via the size of the test, \\[\nP \\left[  reject\\text{ }H_{0} \\mid H_{0}\\right]\n=P \\left[  \\overline{y}&gt;c \\mid \\mu_{0}\\right]  =P \\left[\n\\frac{\\left(  \\overline{y}-\\mu_{0}\\right)  }{\\sigma/\\sqrt{T}}&gt;\\frac{\\left( c-\\mu_{0}\\right)  }{\\sigma/\\sqrt{T}} \\mid H_{0}\\right] .\n\\] The size equals \\(\\alpha\\) if \\(\\sqrt{T}\\left( c-\\mu_{0}\\right) /\\sigma =z_{\\alpha}\\). Thus, the NP test rejects if then if \\(\\overline{y}&gt;\\mu _{0}+\\sigma z_{\\alpha}/\\sqrt{T}\\). Notice that the tests rejects regardless of the value of \\(\\mu_{1}\\), which is rather odd, since \\(\\mu_{1}\\) does not enter into the size of the test only the power. The probability of a type II error is \\[\n\\beta=P \\left[  \\text{accept }H_{0} \\mid H_{1}\\right]\n=P \\left[  \\overline{y}\\leq\\mu_{0}+\\frac{\\sigma}{\\sqrt{T}}z_{\\alpha\n} \\mid H_{1}\\right]  =\\int_{-\\infty}^{\\mu_{0}+\\frac{\\sigma}{\\sqrt{T}%\n}z_{\\alpha}}p\\left(  \\overline{y} \\mid \\mu_{1}\\right)  d\\overline{y}\\text{,}%\n\\] where \\(p\\left( \\overline{y} \\mid \\mu_{1}\\right) \\sim\\mathcal{N}\\left( \\mu _{1},\\sigma^{2}/T\\right)\\).\nThese tests can generate strikingly different conclusions. Consider a test of \\(H_{0}:\\mu=0\\) versus \\(H_{1}:\\mu=5\\), based on \\(T=100\\) observations drawn from \\(y_{t}\\sim\\mathcal{N}\\left( \\mu,10^{2}\\right)\\) with \\(\\overline{y}=2\\). For NP, since \\(\\sigma/\\sqrt{T}=1\\), \\(\\overline{y}\\) is two standard errors away from \\(0\\), thus \\(H_{0}\\) is rejected at the 5% level (the same conclusion holds for \\(p-\\)values). Since \\(p(\\overline {y}=2 \\mid H_{0})=0.054\\) and \\(p(\\overline{y}=2 \\mid H_{1})=0.0044\\), the Bayes factor is \\(\\mathcal{BF}_{0,1}=12.18\\) and \\(P \\left( H_{0} \\mid y\\right) =92.41\\%\\). Thus, the Bayesian is quite sure the null is true, while Neyman-Pearson reject the null.\nThe paradox can be seen in two different ways. First, although \\(\\overline{y}\\) is actually closer to \\(\\mu_{0}\\) than \\(\\mu_{1}\\), the NP test rejects \\(H_{0}\\). This is counterintuitive and makes little sense. The problem is one of calibration. The classical approach develops a test such that 5% of the time, a correct null would be rejected. The power of the test is easy to compute and implies that \\(\\beta=0.0012\\). Thus, this testing procedure will virtually never accept the null if the alternative is correct. For Bayesian procedure, assuming the prior odds is \\(1\\) and \\(L_{0}=L_{1}\\), then \\(\\alpha=\\beta=0.0062\\). Notice that the overall probability of making an error is 1.24% in the Bayesian procedure compared to 5.12% in the classical procedure. It should seem clear that the Bayesian approach is more reasonably, absent a specific motivation for inflating \\(\\alpha\\). Second, suppose the null and alternative were reversed, testing \\(H_{0}:\\mu=\\mu_{1}\\) versus \\(H_{1}:\\mu=\\mu_{0}\\) In the previous example, the Bayes approach gives the same answer, while NP once again rejects the null hypothesis! Again, this result is counterintuitive and nonsensical, but is common when arbitrarily fixing \\(\\alpha\\), which essentially hardwires the test to over-reject the null.\n\n\nExample 6.9 (Lindley’s paradox) Consider the case of testing whether or not a coin is fair, based on observed coin flips, \\[\nH_{0}:\\theta=\\frac{1}{2}\\text{ versus }H_{1}:\\theta\n\\neq\\frac{1}{2}\\text{,}%\n\\] based on \\(T\\) observations from \\(y_{t}\\sim Ber\\left( \\theta\\right)\\). As an example, Table 6.1 provides 4 datasets of differing lengths. Prior to considering the formal hypothesis tests, form your own opinion on the strength of evidence regarding the hypothesis in each data set. It is common for individuals, when confronted with this data to conclude that the fourth sample provides the strongest of evidence for the null and the first sample the weakest.\n\n\n\nTable 6.1: Lindley’s paradox\n\n\n\n\n\n\n#1\n#2\n#3\n#4\n\n\n\n\n# Flips\n50\n100\n400\n10,000\n\n\n# Heads\n32\n60\n220\n5098\n\n\nPercentage of heads\n64\n60\n55\n50.98\n\n\n\n\n\n\nFisher’s solution to the problem posits an unbiased estimator, the sample mean, and computes the \\(t-\\)statistic, which is calculated under \\(H_{0}\\): \\[\nt\\left(  y\\right)  =\\frac{\\overline{y}-E\\left[  \\overline{y} \\mid \\theta\n_{0}\\right]  }{se\\left(  \\overline{y}\\right)  }=\\sqrt{T}\\left(  2\\widehat\n{\\theta}-1\\right)  \\text{,}%\n\\] where \\(se\\left(\\overline{y}\\right)\\) is the standard error of \\(\\overline{y}\\). The Bayesian solution requires marginal likelihood under the null and alternative, which are \\[\np\\left(  y \\mid \\theta_{0}=1/2\\right)  =\\prod_{t=1}^{T}p\\left(  y_{t} \\mid \\theta\n_{0}\\right)  =\\left(  \\frac{1}{2}\\right)  ^{\\sum_{t=1}^{T}y_{t}}\\left( \\frac{1}{2}\\right)  ^{T-\\sum_{t=1}^{T}y_{t}}=\\left(  \\frac{1}{2}\\right)  ^{T},\n\\tag{6.1}\\] and, from Equation 6.1, \\(p\\left( y \\mid H_{1}\\right) =B\\left( a_{T},A_{T}\\right) /B\\left(a,A\\right)\\) assuming a beta prior distribution.\nTo compare the results, note first that in the datasets given above, \\(\\widehat{\\theta}\\) and \\(T\\) generate \\(t_{\\alpha}=1.96\\) in each case. Thus, for a significance level of \\(\\alpha=5\\%\\), the null is rejected for each sample size. Assuming a flat prior distribution, the Bayes factors are \\[\n\\mathcal{BF}_{0,1}=\\left\\{\n\\begin{array}\n[c]{l}%\n0.8178\\text{ for }N=50\\text{ }\\\\\n1.0952\\text{ for }N=100\\\\\n2.1673\\text{ for }N=400\\\\\n11.689\\text{ for }N=10000\n\\end{array}\n\\right.  ,\n\\] showing increasingly strong evidence in favor of \\(H_{0}\\). Assuming equal prior weight for the hypotheses, the posterior probabilities are 0.45, 0.523, 0.684, and 0.921, respectively. For the smallest samples, the Bayes factor implies roughly equal odds of the null and alternative. As the sample size increase, the weight of evidence favors the null, with a 92% probabability for \\(N=10K\\).\nNext, consider testing \\(H_{0}:\\theta_{0}=0\\) vs. \\(H_{1}:\\theta_{0}\\neq0,\\) based on \\(T\\) observations from \\(y_{t}\\sim \\mathcal{N}\\left( \\theta_{0},\\sigma^{2}\\right)\\), where \\(\\sigma^{2}\\) is known. This is the formal example used by Lindley to generate his paradox. Using \\(p-\\)values, the hypothesis is rejected if the \\(t-\\)statistic is greater than \\(t_{\\alpha}\\). To generate the paradox, consider datasets that are exactly \\(t_{\\alpha}\\) standard errors away from \\(\\overline{y}\\), that is, \\(\\overline {y}^{\\ast}=\\theta_{0}+\\sigma t_{\\alpha}/\\sqrt{n}\\), and a uniform prior over the interval \\(\\left( \\theta_{0}-I/2,\\theta_{0}+I/2\\right)\\). If \\(p_{0}\\) is the probability of the null, then, \\[\\begin{align*}\nP \\left(  \\theta=\\theta_{0} \\mid \\overline{y}^{\\ast}\\right)   &\n=\\frac{\\exp\\left(  -\\frac{1}{2}\\frac{T\\left(  \\overline{y}^{\\ast}-\\theta\n_{0}\\right)  ^{2}}{\\sigma^{2}}\\right)  p_{0}}{\\exp\\left(  -\\frac{1}{2}%\n\\frac{T\\left(  \\overline{y}^{\\ast}-\\theta_{0}\\right)  ^{2}}{\\sigma^{2}%\n}\\right)  p_{0}+\\left(  1-p_{0}\\right)  \\int_{\\theta_{0}-I/2}^{\\theta_{0}%\n+I/2}\\exp\\left(  -\\frac{1}{2}\\frac{T\\left(  \\overline{y}^{\\ast}-\\theta\\right)\n^{2}}{\\sigma^{2}}\\right)  I^{-1}d\\theta}\\\\\n&  =\\frac{\\exp\\left(  -\\frac{1}{2}t_{\\alpha}^{2}\\right)  p_{0}}{\\exp\\left( -\\frac{1}{2}t_{\\alpha}^{2}\\right)  p_{0}+\\frac{\\left(  1-p_{0}\\right)  }%\n{I}\\int_{\\theta_{0}-I/2}^{\\theta_{0}+I/2}\\exp\\left(  -\\frac{1}{2}\\left( \\frac{\\left(  \\overline{y}^{\\ast}-\\theta\\right)  }{\\sigma/\\sqrt{T}}\\right)\n\\right)  d\\theta}\\\\\n&  \\geq\\frac{\\exp\\left(  -\\frac{1}{2}t_{\\alpha}^{2}\\right)  p_{0}}{\\exp\\left( -\\frac{1}{2}t_{\\alpha}^{2}\\right)  p_{0}+\\frac{\\left(  1-p_{0}\\right)  }%\n{I}\\sqrt{2\\pi\\sigma^{2}/T}}\\rightarrow1\\text{ as }T\\rightarrow\\infty\\text{.}%\n\\end{align*}\\] In large samples, the posterior probability of the null approaches 1, whereas Fisher always reject the null. It is important to note that this holds for any \\(t_{\\alpha}\\), thus even if the test were performed at the 1% level or lower, the posterior probability would eventually reject the null.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#prior-sensitivity",
    "href": "06-hyp.html#prior-sensitivity",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.6 Prior Sensitivity",
    "text": "6.6 Prior Sensitivity\nOne potential criticism of the previous examples is the choice of the prior distribution. How do we know that, somehow, the prior is not biased against rejecting the null generating the paradoxes? Under this interpretation, the problem is not with the \\(p-\\)value but rather with the Bayesian procedure. One elegant way of dealing with the criticism is search over priors and prior parameters that minimize the probabability of the null hypothesis, thus biasing the Bayesian procedure against accepting the null hypothesis.\nTo see this, consider the case of testing \\(H_{0}:\\mu_{0}=0\\) vs. \\(H_{1}:\\mu_{0}\\neq0\\) with observations drawn from \\(y_{t} \\sim\\mathcal{N}\\left( \\theta_{0},\\sigma^{2}\\right)\\), with \\(\\sigma\\) known. With equal prior null and alternative probability, the probability of the null is \\(p\\left( H_{0} \\mid y\\right) =\\left( 1+\\left( \\mathcal{BF}_{0,1}\\right) ^{-1}\\right) ^{-1}\\). Under the null, \\[\np\\left(  y \\mid H_{0}\\right)  =\\left(  \\frac{1}{2\\pi\\sigma^{2}}\\right)\n^{\\frac{T}{2}}\\exp\\left(  -\\frac{1}{2}\\left(  \\frac{\\left(  \\overline\n{y}-\\theta_{0}\\right)  }{\\sigma/\\sqrt{T}}\\right)  ^{2}\\right)  \\text{.}%\n\\] The criticism applies to the priors under the alternative. To analyze the sensitivity, consider four classes of priors under the alternative: (a) the class of normal priors, \\(p\\left( \\theta \\mid H_{1}\\right) \\sim\\mathcal{N}\\left( a,A\\right)\\); (b) the class of all symmetric unimodal prior distributions; (c) the class of all symmetric prior distributions; and (d) the class of all proper prior distributions. These classes provide varying degrees of prior information, allowing a thorough examination of the strength of evidence.\nIn the first case, consider the standard conjugate prior distribution, \\(p\\left( \\mu \\mid H_{1}\\right) \\sim\\mathcal{N}\\left( \\mu_{0},A\\right)\\). Under the alternative, \\[\\begin{align*}\np\\left(  y \\mid H_{1}\\right)   &  =\\int p\\left(  y \\mid \\mu,H_{1}\\right)  p\\left(  \\mu \\mid H_{1}\\right)  d\\mu\\\\\n&  =\\int p\\left(  \\overline{y} \\mid \\mu,H_{1}\\right)  p\\left( \\mu \\mid H_{1}\\right)  d\\mu\\text{,}%\n\\end{align*}\\] using the fact that \\(\\overline{y}\\) is a sufficient statistic. Noting that \\(p\\left( \\overline{y} \\mid \\mu,H_{1}\\right) \\sim N\\left( \\mu ,\\sigma^{2}/T\\right)\\) and \\(p\\left( \\mu \\mid H_{1}\\right) \\sim N\\left( \\mu_{0},A\\right)\\), we can use the “substitute” instead of integrate trick to assert that \\[\n\\overline{y}=\\mu_{0}+\\sqrt{A}\\eta+\\sqrt{\\sigma^{2}/T}\\varepsilon\\text{,}%\n\\] where \\(\\eta\\) and \\(\\varepsilon\\) are standard normal. Then, \\(p\\left( \\overline{y} \\mid H_{1}\\right) \\sim\\mathcal{N}\\left( \\mu_{0},A+\\sigma^{2}/T\\right)\\). Thus, \\[\n\\mathcal{BF}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)  }{p\\left( y \\mid H_{1}\\right)  }=\\frac{p\\left(  \\overline{y} \\mid H_{0}\\right)\n}{p\\left(  \\overline{y} \\mid H_{1}\\right)  }=\\frac{\\left(  \\sigma^{2}/T\\right)\n^{-\\frac{1}{2}}}{\\left(  \\sigma^{2}/T+A\\right)  ^{-\\frac{1}{2}}}\\frac\n{\\exp\\left(  -\\frac{1}{2}t^{2}\\right)  }{\\exp\\left(  -\\frac{1}{2}\\frac\n{z^{2}\\sigma^{2}/T}{A+\\sigma^{2}/T}\\right)  }\\text{.} \\label{BF_normal}%\n\\] To operationalize the test, \\(A\\) must be selected. \\(A\\) is chosen to minimizing the posterior probabilities of the null, with \\(P_{norm}\\left( H_{0} \\mid y\\right)\\) being the resulting lower bound on the posterior probability of the null. For \\(z\\geq1\\), the lower bound on the posterior probability of the null is \\[\nP_{norm}\\left(  H_{0} \\mid y\\right)  =\\left[\n1+\\sqrt{e}\\exp\\left(  -.5t^{2}\\right)  \\right]  ^{-1},\n\\] which is derived in a reference cited in the notes. This choice provides a maximal bias of the Bayesian approach toward rejecting the null. It is important to note that this is not a reasonable prior, as it was intentionally constructed to bias the null toward rejection.\nFor the class of all proper prior distributions, it is also easy to derive the bound. From equation above, minimizing the posterior probability is equivalent to minimizing the Bayes factor, \\[\n\\mathcal{BF}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)  }{p\\left( y \\mid H_{1}\\right)  }\\text{.}%\n\\] Since \\[\np\\left(  y \\mid H_{1}\\right)  =\\int p\\left(  y \\mid \\theta,H_{1}\\right)  p\\left(  \\theta \\mid H_{1}\\right)  d\\theta\\leq p\\left( y \\mid \\widehat{\\theta}_{MLE},H_{1}\\right)  \\text{,}%\n\\] where \\(\\widehat{\\theta}_{MLE}=\\arg\\underset{\\theta\\neq0}{\\max}p\\left( y \\mid \\theta\\right)\\). The maximum likelihood estimator, maximizes the probability of the alternative, and provides a lower bound on the Bayes factor, \\[\n\\underline{\\mathcal{BF}}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)\n}{\\underset{\\theta\\neq0}{\\sup}p\\left(  y \\mid \\theta\\right)  }\\text{.}%\n\\] In this case, the bound is particularly easy to calculate and is given by \\[\nP_{all}\\left(  H_{0} \\mid y\\right)  =\\left( 1+\\exp\\left(  -\\frac{t^{2}}{2}\\right)  \\right)  ^{-1}\\text{.}%\n\\] A reference cited in the notes provides the bounds for the second and third cases, generating \\(P_{s,u}\\left( H_{0} \\mid y\\right)\\) and \\(P_{s}\\left( H_{0} \\mid y\\right)\\), respectively. All of the bounds only depend on the \\(t-\\)statistic and constants.\nTable 6.2 reports the \\(t-\\)statistics and associated \\(p-\\)values, with the remaining columns provide the posterior probability bounds. For the normal prior and choosing the prior parameter \\(A\\) to minimize the probability of the null, the posterior probability of the null is much larger than the \\(p-\\)value, in every case. For the standard case of a \\(t-\\)statistic of 1.96, \\(P\\left( H_{0} \\mid y\\right)\\) is more than six times greater than the \\(p-\\)value. For \\(t=2.576\\), \\(P\\left( H_{0} \\mid y\\right)\\) is almost 13 times greater than the \\(p-\\)value. These probabilities fall slightly for more general priors. For example, for the class of all priors, a t-statistic of 1.96/2.576 generates a lower bound for the posterior probability of 0.128/0.035\\(,\\) more than 2/3 times the \\(p-\\)value.\n\n\n\nTable 6.2: Comparison of strength of evidence against the point null hypothesis. The numbers are reproduced from Berger (1986).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(t\\)-stat\n\\(p\\)-value\n\\(P_{norm}\\left(H_{0} \\mid y\\right)\\)\n\\(P_{s,u}\\left( H_{0} \\mid y\\right)\\)\n\\(P_{s}\\left(H_{0} \\mid y\\right)\\)\n\\(P_{all}\\left(H_{0} \\mid y\\right)\\)\n\n\n\n\n1.645\n0.100\n0.412\n0.39\n0.34\n0.205\n\n\n1.960\n0.050\n0.321\n0.29\n0.227\n0.128\n\n\n2.576\n0.010\n0.133\n0.11\n0.068\n0.035\n\n\n3.291\n0.001\n0.0235\n0.018\n0.0088\n0.0044",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "07-sp.html",
    "href": "07-sp.html",
    "title": "7  Stochastic Processes",
    "section": "",
    "text": "7.1 Brownian Motion\nYet another fundamental concept that is useful for probabilistic reasoning is a stochastic process. An instance of a process is a function \\(X:~ \\Omega \\rightarrow S\\) from domain of index set \\(\\Omega\\) into another set of process values \\(S\\), called state-space. The state-space of a stochastic process is the set of all possible states that the process can be in. Each state in the state-space represents a possible outcome or condition of the system being modeled. The process then is the distribution over the space of functions from \\(\\Omega\\) to \\(S\\). The term process is used because the function \\(Y\\) is often thought of as a time-varying quantity, and the index set \\(\\Omega\\) is often interpreted as time. However, the index set can be any set, and the process can be a random function of any other variable. Both index set and state-space can be discrete or continuous. For example, discrete time index can represent days or rounds and continuous time index is a point on a time line. The state-space can be discrete (composed of distinct states, like the number of customers in a store) or continuous (such as the price of a stock). The state-space can be one-dimensional (only one aspect of the system is modeled) or multi-dimensional (multiple aspects are modeled simultaneously).\nA stochastic process is a family of random variables that describes the evolution through time the evolution of some (physical) process. We denote this by \\(X = \\{X(t),~t\\in T\\}\\), with \\(t\\) representing time and \\(X(t) = \\omega\\) is the state of the process at time \\(t\\).We will get a realization (a.k.a. sample path). In the case when time is discrete, the realization is a sequence of observed \\(X = \\Omega = \\{\\omega_1,\\omega_2,\\ldots\\}\\). Common discrete time process are markov chains. Brownian motion is a central process in continuous time and state with almost surely continuos but nowhere differentiable. Poisson process is commonly used to account for jumps in the process.\nHere are some widely used stochastic processes:\nIn contexts like agricultural field trials, the domain for analyzing yield is commonly referred to as the collection of plots. This term is broadly suitable for practical field purposes but is mathematically interpreted as the collection of planar Borel subsets across various growing seasons. In a basic clinical trial for a COVID-19 vaccine, like the AstraZeneca trial in 2021, the domain is typically referred to as the group of patients. This implies the inclusion of all eligible patients, regardless of whether they were actually recruited and observed in the trial. In research on speciation or sexual compatibility in fruit flies, the domain is defined as the set of male-female pairs, encompassing all potential pairs with the desired genetic traits. For a competition experiment, such as a chess or tennis tournament, the domain is described as the set of ordered pairs of participants, which includes all possible pairings, not just those who actually competed against each other at events like US Open in 2024.\nIn data analysis, both experimental and observational data can exhibit variability. This variability is often modeled using probability distributions. These distributions can either represent simple processes with independent elements (then we are back to i.i.d case) or more complex stochastic processes that display dependencies, whether they be serial, spatial, or of other types. Essentially, this modeling approach helps in understanding and predicting data behavior under various conditions. The early sections of Davison (2003) work offer an insightful primer on how to develop and apply these stochastic models across various fields. This introduction is particularly useful for grasping the fundamental concepts and practical applications of these models.\nBrownian Motion, named after botanist Robert Brown, is a fundamental concept in the theory of stochastic processes. It describes the random motion of particles suspended in a fluid (liquid or gas), as they are bombarded by the fast-moving molecules in the fluid.\nA one-dimensional Brownian Motion (also known as Wiener process) is a continuous time stochastic process \\(B(t)_{t\\ge 0}\\) with the following properties\nFormally brownian motion is a stochastic process \\(B(t)\\) is a family of real random variables indexed by the set of nonnegative real numbers \\(t\\).\nFigure 7.1 below shows tree sample paths of Brownian Motion.\n# Brownian Motion\nset.seed(92)\nt = seq(0, 1, 0.001)\nplot(t, cumsum(rnorm(1001, 0, sqrt(0.001))), type=\"l\", xlab=\"t\", ylab=\"B(t)\", lwd=2, ylim=c(-1.2, 2))\nlines(t, cumsum(rnorm(1001, 0, sqrt(0.001))), lwd=2, col=2)\nlines(t, cumsum(rnorm(1001, 0, sqrt(0.001))),lwd=2, col=3)\n\n\n\n\n\n\n\nFigure 7.1: Brownian Motion\nThus, for any times \\(0 \\leq t_1 &lt; t_2 &lt; \\ldots &lt; t_n\\), the random variables \\(B(t_2) - B(t_1)\\), \\(B(t_3) - B(t_2)\\), , \\(B(t_n) - B(t_{n-1})\\) are independent and the function \\(t \\mapsto B(t)\\) is continuous almost surely.\nSome properties of Brownian Motion are:\nHistorically, the most widely used models for stock market returns relied on the assumption that asset returns follow a normal or a lognormal distribution. The lognormal model for the asset returns was challenged after the October 1987 crash of the American stock market. On October 19 (black Monday) the Dow Jones index had fallen 508 points, or 23 percent. It was the worst single day in history for the US markets. The reason for the crash was rather simple, it was caused by the portfolio insurance product created by one of the financial firms. The idea of this insurance was to switch from equities to the US Treasury bills, as markets go down. Although the lognormal model does a good job at describing the historical data, the jump observed on that day had a probability close to zero, according to the lognormal model. The lognormal model underestimates the probability of a large change (thin tail). The widely used then Black-Sholes model for asset pricing was relying on the lognormal model, it was incapable of correctly pricing in the possibility of such a large drop.\nThe normal assumption of the asset returns was first proposed in 1900 in the PhD thesis of Louis Bachelier, who was a student of Henri Poincare. Bachelier was interested in developing statistical tools for pricing options (predicting asset returns) on the Paris stock exchange. Although Bachelier’s work laid the foundation for the modern theory of stochastic processes, he was never given credit by his contemporaries, including Einstein, L{'e}vi and Borel. In 1905 Einstein published a paper which used the same statistical model as Bachelier to describe the 1827 discovery by a botanist Robert Brown, who observed that pollen particles suspended in water followed irregular random trajectories. Thus, we call the stochastic process that describes these phenomena a Brownian motion. Einstein’s advisor at the University of Zurich was Hermann Minkowski who was a friend and a collaborator of Poincare. Thus, it is likely Einstein knew about the work of Bachelier, but he never mentioned it in his paper. This was not the first instance of when Einstein did not give proper credit. Poincare published a paper Poincaré (1898) on the relativity theory in 1898, seven years before Einstein. This paper was published in a philosophy journal and thus Poincare had avoided using any mathematical formulas except for the famous \\(E=mc^2\\). Poincare did discussed his results on the relativity theory with Minkowski. Minkowski asked Einstein to read Poincare’s work Arnol’d (2006). However, Einstein never referenced the work of Poincare until 1945. One of the reviewers for the 1905 paper on relativity by Einstein was Poincare and he wrote a very positive review mentioning it as a breakthrough. When Minkowski asked Poincare why he did not claim his priority on the theory, Poincare replied that our mission is to support young scientists. More about why credit is mistakenly given to Einstein for the relativity theory is discussed by Logunov Logunov (2004).\nEinstein was not the only one who ignored the work of Bachelier, Paul L{'e}vi did so as well. Paul L{'e}vi was considered a pioneer and authority on stochastic processes during Bachelier’s time, although Bruno de Finetti introduced a dual concept of infinite divisibility in 1929, before the works of L{'e}vi in early 1930s on this topic. L{'e}vi never mentioned the work of the obscure and little known mathematician Bachelier. The first to give credit to Bachelier was Kolmogorov in his 1931 paper Kolmogoroff (1931) (Russian translation A. N. Kolmogorov (1938) and English translation Shiryayev (1992)). Later Leonard Jimmie Savage translated Bachelier’s work to English and showed it to Paul Samuelson. Samuelson extended the work of Bachelier by considering the log-returns rather than absolute numbers, popularized the work of Bachelier among economists and the translation of Bachelier’s thesis was finally published in English in 1964 Cootner (1967). Many economists who extended the work of Bachelier won Nobel prizes, including Eugine Fama known for work on the efficient markets hypothesis, Paul Samuelson, and Myron Scholes for the Black-Sholes model, as well as Robert Merton.\nRobert Merton, who was a student of Samuelson, who proposed a major extension to the work of Bachelier, by introducing jumps to the model. The additive jump term addresses the issues, asymmetry, and heavy tails in the distribution. Merton’s Jump Stochastic volatility model has a discrete-time version for log-returns, \\(y_t\\), with jump times, \\(J_t\\), jump sizes, \\(Z_t\\), and spot stochastic volatility, \\(V_t\\), given by the dynamics \\[\\begin{align*}\n    y_{t} & \\equiv \\log \\left( S_{t}/S_{t-1}\\right) =\\mu + V_t \\varepsilon_{t}+J_{t}Z_{t} \\\\V_{t+1} & = \\alpha_v + \\beta_v V_t + \\sigma_v \\sqrt{V_t} \\varepsilon_{t}^v\n\\end{align*}\\] where \\(\\mathbb{P} \\left ( J_t =1 \\right ) = \\lambda\\), \\(S_t\\) denote a stock or asset price and log-returns \\(y^t = (y_1,\\ldots,y_t)\\) is the log-return. The errors \\((\\varepsilon_{t},\\varepsilon_{t}^v)\\) are possibly correlated bivariate normals. The investor must obtain optimal filters for \\((V_t,J_t,Z_t)\\), and learn the posterior densities of the parameters \\((\\mu, \\alpha_v, \\beta_v, \\sigma_v^2 , \\lambda )\\). These estimates will be conditional on the information available at each time.\nAlthough it was originally developed to model the financial markets by Louis Bachelier in 1900, the Brownian Motion has found applications in many other fields, biology (movement of biomolecules within cells), environmental science (diffusion processes, like the spread of pollutants in the air or water), and mathematics (stochastic calculus and differential equations).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#brownian-motion",
    "href": "07-sp.html#brownian-motion",
    "title": "7  Stochastic Processes",
    "section": "",
    "text": "\\(B(0) = 0\\) almost surely\n\\(B(t)\\) has stationary independent increments: \\(B(t) - B(s) \\sim N(0, t-s)\\) for \\(0 \\le s &lt; t\\)\n\\(B(t)\\) is continuous function of \\(t\\)\nFor each time \\(t &gt; 0\\), the random variable \\(B(t)\\) is normally distributed with mean 0 and variance \\(t\\), i.e., \\(B(t) \\sim N(0, t)\\).\n\n\n\n\n\n\n\nScale Invariance: If \\(B(t)\\) is a Brownian motion, then for any \\(a &gt; 0\\), the process \\(aB(t/a^2)\\) is also a Brownian motion.\nTime Inversion: If \\(B(t)\\) is a Brownian motion, then \\(tB(1/t)\\) is also a Brownian motion for \\(t &gt; 0\\).\nFractal Nature: Brownian motion paths are nowhere differentiable but continuous everywhere, reflecting a fractal-like nature.\n\n\n\n\n\n\n\nExample 7.1 (Brownian Motion for Sport Scores) A Model for Sports Scores\nIn order to define the implied volatility of a sports game we begin with a distributional model for the evolution of the outcome in a sports game which we develop from Stern (1994). The model specifies the distribution of the lead of team A over team B, \\(X(t)\\) for any \\(t\\) as a Brownian motion process. If \\(B(t)\\) denotes a standard Brownian motion with distributional property \\(B(t) \\sim N(0,t)\\) and we incorporate drift, \\(\\mu\\), and volatility, \\(\\sigma\\), terms, then the evolution of the outcome \\(X(t)\\) that is given by: \\[\nX(t)=\\mu t + \\sigma B(t) \\sim N( \\mu t , \\sigma^2 t).\n\\] This distribution of the game outcome is similar to the Black-Scholes model of the distribution of a stock price.\nThis specification results in several useful measures (or, this specification results in closed-form solutions for a number of measures of interest). The distribution of the final score follows a normal distribution, \\(X(1)\\sim N(\\mu, \\sigma^2)\\). We can calculate the probability of team A winning, denoted \\(p=\\mathbb{P}(X(1)&gt;0)\\), from the spread and probability distribution. Given the normality assumption, \\(X(1) \\sim N(\\mu, \\sigma^2)\\), we have \\[\np = \\mathbb{P}(X(1)&gt;0) = \\Phi \\left ( \\frac{\\mu}{\\sigma} \\right )\n\\] where \\(\\Phi\\) is the standard normal cdf. Table 7.1 uses \\(\\Phi\\) to convert team A’s advantage \\(\\mu\\) to a probability scale using the information ratio \\(\\mu/\\sigma\\).\n\n\n\nTable 7.1: Probability of Winning \\(p\\) versus the Sharpe Ratio \\(\\mu/\\sigma\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mu/\\sigma\\)\n0\n0.25\n0.5\n0.75\n1\n1.25\n1.5\n2\n\n\n\n\n\\(p=\\Phi(\\mu/\\sigma)\\)\n0.5\n0.60\n0.69\n0.77\n0.84\n0.89\n0.93\n0.977\n\n\n\n\n\n\nIf teams are evenly matched and \\(\\mu/\\sigma =0\\) then \\(p=0.5\\). Table 7.1 provides a list of probabilities as a function of \\(\\mu/\\sigma\\). For example, if the point spread \\(\\mu=-4\\) and volatility is \\(\\sigma=10.6\\), then the team has a \\(\\mu/\\sigma = -4/10.6 = - 0.38\\) volatility point disadvantage. The probability of winning is \\(\\Phi(-0.38) = 0.353 &lt; 0.5\\). A common scenario is that team A has an edge equal to half a volatility, so that \\(\\mu/\\sigma =0.5\\) and then \\(p= 0.69\\).\nOf particular interest here are conditional probability assessments made as the game progresses. For example, suppose that the current lead at time \\(t\\) is \\(l\\) points and so \\(X(t) = l\\). The model can then be used to update your assessment of the distribution of the final score with the conditional distribution \\((X(1) | X(t)=l )\\). To see this, we can re-write the distribution of \\(X(1)\\) given \\(X(t)\\) by noting that \\(X(1) = X(t)+ X(1) - X(t)\\). Using the formula above and substituting \\(t\\) for \\(1\\) where appropriate and noting that \\(X(t) = l\\) by assumption, this simplifies to \\[\nX(1)= l + \\mu(1- t) + \\sigma (B(1) - B(t)).\n\\] Here \\(B(1) - B(t)  \\stackrel{D}{=} B(1-t)\\) which is independent of \\(X(t)\\) with distribution \\(N(0,1-t)\\). The mean and variance of \\(X(1)|X(t)=l\\) decay to zero as \\(t \\rightarrow 1\\) and the outcome becomes certain at the realised value of \\(X(1)\\). We leave open the possibility of a tied game and overtime to determine the outcome.\nTo determine this conditional distribution, we note that there are \\(1-t\\) time units left together with a drift \\(\\mu\\) and as shown above in this case the uncertainty can be modeled as \\(\\sigma^2(1-t)\\). Therefore, we can write the distribution of the final outcome after \\(t\\) periods with a current lead of \\(l\\) for team A as the conditional distribution: \\[\n( X(1) | X(t)=l) =  (X(1)-X(t)) + l   \\sim N( l + \\mu(1 - t) , \\sigma^2 (1 - t) )\n\\] From the conditional distribution \\((X(1) | X(t)=l) \\sim N(l+\\mu(1-t), \\sigma^2 (1-t))\\), we can calculate the conditional probability of winning as the game evolves. The probability of team A winning at time \\(t\\) given a current lead of \\(l\\) point is: \\[\np_t = P ( X(1) &gt; 0 | X(t) = l) = \\Phi \\left ( \\frac{ l + \\mu ( 1 - t)  }{ \\sigma \\sqrt{ ( 1-t) } } \\right )\n\\]\n\n\n\n\n\n\nFigure 7.2: Score Evolution on a Discretized Grid\n\n\n\nFigure 7.2 A and B illustrate our methodology with an example. Suppose we are analyzing data for a Superbowl game between teams A and B with team A favored. Figure A presents the information available at the beginning of game from the perspective of the undergod team B. If the initial point spread–or the markets’ expectaion of the expected outcome–is \\(-4\\) and the volatility is \\(10.6\\)–assumed given for the moment (more on this below)–then the probability that the underdog team wins is \\(p = \\Phi ( \\mu /\\sigma ) = \\Phi ( - 4/ 10.6) = 35.3\\)%. This result relies on our assumption of a normal outcome distribution on the outcome as previously explained. Another way of saying this is \\(\\mathbb{P}(X(1)&gt;0)=0.353\\) for an outcome distribution \\(X(1) \\sim N(-4, 10.6^2)\\). Figure A illustrates this with the shaded red area under the curve.\nFigure 7.2 B illustrates the information and potential outcomes at half-time. Here we show the evolution of the actual score until half time as the solid black line. From half-time onwards we simulate a set of possible Monte Carlo paths to the end of the game.\nSpecifically, we discretise the model with time interval \\(\\Delta =1/200\\) and simulate possible outcomes given the score at half time. The volatility plays a key role in turning the point spread into a probability of winning as the greater the volatility of the distribution of the outcome, \\(X(1)\\), the greater the range of outcomes projected in the Monte Carlo simulation. Essentially the volatility provides a scale which calibrates the advantage implied by a given point spread.\nWe can use this relationship to determine how volatility decays over the course of the game. The conditional distribution of the outcome given the score at time \\(t\\), is \\((X(1)|X(t)=l)\\) with a variance of \\(\\sigma^2(1-t)\\) and volatility of \\(\\sigma \\sqrt{1-t}\\). The volatility is a decreasing function of \\(t\\), illustrating that the volatility dissipates over the course of a game. For example, if there is an initial volatility of \\(\\sigma = 10.6\\), then at half-time when \\(t=\\frac{1}{2}\\), the volatility is \\(10.6 / \\sqrt{2} = 7.5\\) volatility points left. Table 7.2, below, illustrates this relationship for additional points over the game.\n\n\n\nTable 7.2: Volatility Decay over Time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(t\\)\n0\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{3}{4}\\)\n1\n\n\n\n\n\\(\\sigma \\sqrt{1-t}\\)\n10.6\n9.18\n7.50\n5.3\n0\n\n\n\n\n\n\nTo provide insight into the final outcome given the current score, Table 7.1 and Table 7.2 can be combined to measure the current outcome, \\(l\\), in terms of standard deviations of the outcome.\nFor example, suppose that you have Team B, an underdog, so from their perspective \\(\\mu = -4\\) and at half-time team B has a lead of 15, \\(l= 15\\). Team B’s expected outcome as presented earlier is \\(l + \\mu (1-t)\\) or \\(15 - 4 \\times \\frac{1}{2} = 13\\). If initial volatility is \\(\\sigma = 10.60\\) then the remaining volatility at half-time is \\(10.6/\\sqrt{2} = 7.50\\) and team B’s expected outcome of \\(13\\) in terms of standard deviations is \\(13/7.5 = 1.73\\). Thus team B’s expected outcome is at the 99th percentile of the distribution, \\(\\Phi ( 1.73 ) = 0.96\\), implying a 96% chance of winning.\nImplied Volatility\nThe previous discussion assumed that the variance (or volatility) parameter \\(\\sigma\\) was a known constant. We return to this important quantity now. We are now in a position to define the implied volatility implicit in the two betting lines that are available. Given our model, we will use the money-line odds to provide a market assessment of the probability of winning, \\(p\\), and the point spread to assess the expected margin of victory, \\(\\mu\\). The money line odds are shown for each team A and B and provide information on the payoff from a bet on the team winning. This calculation will also typically require an adjustment for the bookmaker’s spread. With these we can infer the implied volatility, \\(\\sigma_{IV}\\), by solving \\[\n\\sigma_{IV}: \\; \\; \\;  \\; \\; p = \\Phi \\left ( \\frac{\\mu}{\\sigma_{IV}} \\right ) \\; \\; \\text{ which \\; gives} \\; \\;\n\\sigma_{IV} = \\frac{ \\mu }{ \\Phi^{-1} ( p ) } \\; .\n\\] Here \\(\\Phi^{-1}(p)\\) denotes the standard normal quantile function such that the area under the standard normal curve to the left of \\(\\Phi^{-1}(p)\\) is equal to \\(p\\). In our example we calculate this using the qnorm in R. Note that when \\(\\mu =0\\) and \\(p= \\frac{1}{2}\\) there’s no market information about the volatility as \\(\\mu / \\Phi^{-1} (p)\\) is undefined. This is the special case where the teams are seen as evenly matched- the expected outcome has a zero point spread and there is an equal probability that either team wins.\nTime Varying Implied Volatility\nUp to this point the volatility rate has been assumed constant through the course of the game, i.e., that the same value of \\(\\sigma\\) is relevant. The amount of volatility remaining in the game is not constant but the basic underlying parameters has been assumed constant. This need not be true and more importantly the betting markets may provide some information about the best estimate of the volatility parameter at a given point of time. This is important because time-varying volatility provides an interpretable quantity that can allow one to assess the value of a betting opportunity.\nWith the advent of on-line betting there is a virtually continuous traded contract available to assesses implied expectations of the probability of team A winning at any time \\(t\\). The additional information available from the continuous contract allows for further update of the implied conditional volatility. We assume that the online betting market gives us a current assessment of \\(p_t\\), that is the current probability that team A will win. We will then solve for \\(\\sigma^2\\) and in turn define resulting time-varying volatility, as \\(\\sigma_{IV,t}\\), using the resulting equation to solve for \\(\\sigma_{IV,t}\\) with \\[\np_t = \\Phi \\left ( \\frac{ l + \\mu(1-t)  }{\\sigma_{IV,t} \\sqrt{1-t}} \\right )\n\\; \\text{ which \\; gives} \\; \\;\n\\sigma_{IV,t} = \\frac{ l + \\mu ( 1-t ) }{ \\Phi^{-1} ( p_t )  \\sqrt{1-t}}\n\\] We will use our methodology to find evidence of time-varying volatility in the SuperBowl XLVII probabilities.\nSuper Bowl XLVII: Ravens vs San Francisco 49ers\nSuper Bowl XLVII was held at the Superdome in New Orleans on February 3, 2013 and featured the San Francisco 49ers against the Baltimore Ravens. Going into Super Bowl XLVII the San Francisco 49ers were favorites to win which was not surprising following their impressive season. It was a fairly bizarre Super Bowl with a \\(34\\) minute power outage affecting the game by ultimately an exciting finish with the Ravens causing an upset victory \\(34-31\\). We will build our model from the viewpoint of the Ravens. Hence \\(X(t)\\) will correspond to the Raven’s score minus the San Francisco 49ers. Table 7.3 provides the score at the end of each quarter.\n\n\n\nTable 7.3: SuperBowl XLVII by Quarter\n\n\n\n\n\n\\(t\\)\n0\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{3}{4}\\)\n1\n\n\n\n\nRavens\n0\n7\n21\n28\n34\n\n\n49ers\n0\n3\n6\n23\n31\n\n\n\\(X(t)\\)\n0\n4\n15\n5\n3\n\n\n\n\n\n\nTo determine the parameters of our model we first use the point spread which was set at the Ravens being a four point underdog, i.e. \\(\\mu=-4\\). This sets the mean of our outcome, \\(X(1)\\), as \\[\n\\mu = \\mathbb{E} \\left (X(1) \\right )=-4 .\n\\] In reality, it was an exciting game with the Ravens upsetting the 49ers by \\(34-31\\). Hence, the realised outcome is \\(X(1)= 34-31=3\\) with the point spread being beaten by \\(7\\) points or the equivalent of a touchdown.\n\n\n\n\n\n\nFigure 7.3: Superbowl XLVII: Ravens vs 49ers: TradeSports contracts traded and dynamic probability of the Ravens winning\n\n\n\nTo determine the markets’ assessment of the probability that the Ravens would win at the being of the game we use the money-line odds. These odds were quoted as San Francisco \\(-175\\) and Baltimore Ravens \\(+155\\). This implies that a bettor would have to place $175 to win $100 on the 49ers and a bet of $100 on the Ravens would lead to a win of $155. We can convert both of these money-lines to implied probabilities of the each team winning, by the equations \\[\np_{SF} = \\frac{175}{100+175} = 0.686 \\; \\; \\text{ and} \\; \\; p_{Ravens} = \\frac{100}{100+155} = 0.392\n\\] The probability sum to one plus the market overround: \\[\np_{SF} + p_{Ravens} = 0.686+0.392 = 1.078\n\\] namely a \\(7.8\\)% edge for the bookmakers. Put differently, if bettors place money proportionally across both teams then the bookies vig will be \\[\n\\text{Vig} = \\dfrac{0.078}{0.078+1} = 0.072\n\\] This means that the bookmakers is expected to make a profit of 7.2% of the total staked no matter what happens to the outocme of the game.\nTo account for this edge in our model, we use the mid-point of the spread to determine \\(p\\) implying that \\[\np = \\frac{1}{2} p_{Ravens} + \\frac{1}{2} (1 - p_{SF} ) = 0.353\n\\] From the Ravens perspective we have \\(p = \\mathbb{P}(X(1)&gt;0) =0.353\\).\nFigure 7.3 shows the evolution of the markets conditional probability of winning \\(p_t\\) for the Ravens. The data are from the online betting website TradeSports.com. Starting at \\(p=0.353\\) we see how dramatically the markets assessment of the Ravens winning can fluctuate. (NGP: This is really confusing. We need to say a bit more about what we did here. How do you get these probabilities. Key point is what are you assuming about sigma given the abaove discussion.) Given their commanding lead at half time, the probability has as high as \\(0.90\\). At the end of the four quarter when the 49ers nearly went into the lead with a touchdown, at one point the probability had dropped to \\(30\\)%.\nOur main question of interest is then: What implied volatility is consistent with market expectations?\nTo calculate the implied volatility of the Superbowl we substitute the pair \\((\\mu,p)\\) into our definition and solve for \\(\\sigma_{IV}\\). We obtain \\[\n\\sigma_{IV} = \\frac{\\mu}{\\Phi^{-1}(p)} = \\frac{-4}{-0.377}  = 10.60\n\\] where we have used \\(\\Phi^{-1} ( p) = qnorm(0.353) = -0.377\\). So on a volatility scale the \\(4\\) point advantage assessed for the 49ers is under a \\(\\frac{1}{2} \\sigma\\) favorite. From Table 2, this is consistent with a win probability of \\(p=\\Phi(\\frac{1}{2})=0.69\\). Another feature is that a \\(\\sigma=10.6\\) is historically low, as a typical volatility of an NFL game is \\(14\\) (see Stern, 1991). However, the more competitive the game one might expect a lower volatility. In reality, the outcome \\(X(1)=3\\) was withing one standard deviation of the model which had an expectation of \\(\\mu=-4\\) and volatility of \\(\\sigma= 10.6\\). Another question of interest is\nWhat’s the probability of the Ravens winning given their lead at half time?\n\nAt half time the Ravens where leading \\(21\\) to \\(6\\). This gives us \\(X(\\frac{1}{2})=21-6=15\\). From the online betting market we also have traded contracts on TradeSports.com that yield a current probability of \\(p_{\\frac{1}{2}} = 0.90\\). \nAn alternative view is to assume that the market assesses time varying volatility and the prices fully reflect the underlying probability. Here we ask the question\nWhat’s the implied volatility for the second half of the game?\nWe now have an implied volatility \\[\n\\sigma_{IV,t=\\frac{1}{2}} = \\frac{ l + \\mu ( 1-t ) }{ \\Phi^{-1} ( p_t )  \\sqrt{1-t}} = \\frac{15-2}{ \\Phi^{-1}(0.9) / \\sqrt{2} } = 14\n\\] where qnorm(0.9)=1.28. Notice that \\(14&gt; 10.6\\), our assessment of the implied volatility at the beginning of the game.\nWhat’s a valid betting strategy?\nAn alternative approach is to assume that the initial moneyline and point spread set the volatility and this stays constant throughout the game. This market is much larger than the online market and this is a reasonable assumption unless there has been material information as the game progresses such as a key injury.\nHence the market was expected a more typical volatility in the second half. If a bettor believed that there was no reason that \\(\\sigma\\) had changed from the initial \\(10.6\\) then their assessment of the Ravens win probability, under this models, would have been \\(\\Phi \\left ( 13/ (10.6/\\sqrt{2}) \\right ) = 0.96\\) and the \\(0.90\\) market rate would have been thought of as a betting opportunity.\nThe Kelly criterion (Kelly,1956) yields the betting rate \\[\n\\omega = p - \\frac{q}{0} = 0.96 - \\frac{0.1}{1/9} = 0.06\n\\] that is, \\(6\\)% of capital. A more realistic strategy is to use the fractional Kelly criterion whcih scales by a risk aversion parameter, \\(\\gamma\\). For example, in this case if \\(\\gamma =3\\), we would bet \\(0.06/3=0.01\\), or \\(2\\)% of our capital on this betting opportunity.\nFinally, odds changes can be dramatic at the end of the fourth quarter and this Super Bowl was no exception. With the score at \\(34-29\\), with \\(x\\) minutes to go, the 49ers were at \\(1\\)st and goal. A few minutes after this, the probability of the Ravens winning had dropped precipitously from over \\(90\\)% to \\(30\\)%, see Figure 7.3. On San Francisco’s final offensive play of the game, Kaepernick threw a pass on fourth down to Michael Crabtree, but Ravens cornerback Jimmy Smith appeared to hold the wide receiver during the incompletion, No call was given and the final result was a Ravens win.\n\n\nExample 7.2 (Yahoo Stock Price Simulation) Investing in volatile stocks can be very risky. The Internet stocks during the late 1990’s were notorious for their volatility. For example, the leading Internet stock Yahoo! started 1999 at $62,rose to $122, then fell back to $55 in August, only to end the year at $216. Even more remarkable is the fact that by January 2000, Yahoo! has risen more than 100-fold from its offering price of $1.32 on April 15, 1996. In comparison, theNasdaq 100, a benchmark market index, was up about 5-fold during the same period.\nStock prices fluctuate somewhat randomly. Maurice Kendall, in his seminal 1953 paper on the random walk nature of stock and commodity prices, observed that “The series looks like a wandering one, almost as if once a week the Demon of Chance drew a random number from a symmetrical population of fixed dispersion and added to it the current price to determine next week’s price (p. 87).” While a pure random walk model for Yahoo!’s stock price is in fact not reasonable since its price cannot fall below zero, an alternative model that appears to provide reasonable results assumes that the logarithms of price changes, or returns, follow a random walk. This alternative model is the basis for the results in this example.\nTo evaluate a stock investment, we take the initial price as \\(X_0\\) and then we need to determine what the stock price might be in year \\(T\\), namely \\(X_T\\). Our approach draws from the Black-Scholes Model for valuing stock options. Technically, the Black-Scholes Model assumes that \\(X_T\\) is determined by the solution to a stochastic differential equation. This leads to the Geometric Brownian Motion \\[\nX_T = X_0 \\exp\\left( (\\mu - 1/2\\sigma^2)T + \\sigma B_T  \\right),\n\\] where \\(B_T\\) is a standard Brownian motion, namely: \\(B_0 = 0\\), \\(B_t - B_s\\) is independent of \\(B_s\\) and its distribution only depends onb \\(t-s\\) and \\(B_t \\sim N(0,t)\\). Hence, \\(B_t = \\sqrt{t}Z\\), where \\(Z \\sim N(0,1)\\).\nThen, the expected value is \\[\\begin{align*}\nE(X_T) = &X_0 \\exp\\left( (\\mu - 1/2\\sigma^2)T \\right) E(\\exp(\\sigma B_T))\\\\\n& = X_0\\exp\\left( (\\mu - 1/2\\sigma^2)T \\right) E(\\exp(\\sigma \\sqrt{T}Z))\\\\\n& = X_0\\exp\\left( (\\mu - 1/2\\sigma^2)T \\right) E(\\exp(\\sigma \\sqrt{T}Z)) \\\\\n&= X_0\\exp\\left( (\\mu - 1/2\\sigma^2)T \\right) \\exp\\left( \\frac{1}{2}\\sigma^2T \\right) = X_0\\exp\\left( \\mu T \\right).\n\\end{align*}\\] The \\(E(\\exp(\\sigma \\sqrt{T}Z)) = \\exp\\left( 1/2\\sigma^2T \\right)\\) is due to the moment property of the log-normal distribution. We can interpret \\(\\mu\\) as the expected rate of return \\[\n\\hat \\mu = \\frac{1}{T}\\log\\left( \\frac{X_T}{X_0} \\right).\n\\] This provides a way to estimate the expected rate of return from the expected value of the stock price at time \\(T\\), by plugging in the observed values of \\(X_0\\) and \\(X_T\\).\nThe variance is \\[\\begin{align*}\n\\text{Var}(X_T) = &X_0^2 \\exp\\left( 2(\\mu - 1/2\\sigma^2)T \\right) \\text{Var}(\\exp(\\sigma B_T))\\\\\n& = X_0^2 \\exp\\left( 2(\\mu - 1/2\\sigma^2)T \\right) \\text{Var}(\\exp(\\sigma \\sqrt{T}Z))\\\\\n& = X_0^2 \\exp\\left( 2(\\mu - 1/2\\sigma^2)T \\right) \\exp\\left( \\sigma^2T \\right) - X_0^2\\exp\\left( 2(\\mu - 1/2\\sigma^2)T \\right)\\\\\n& = X_0^2\\exp\\left( 2\\mu T \\right)\\left( \\exp\\left( \\sigma^2T \\right) - 1 \\right).\n\\end{align*}\\]\nThe important consequence of the model for predicting future prices is that \\(\\log(X_T/X_0)\\) has a normal distribution with mean \\((\\mu-\\frac{1}{2} \\sigma^2)T\\) and variance \\(\\sigma^2 T\\) which is equivalent to saying that the ratio \\(X_T/X_0\\) has a log-normal distribution. It is interesting that although the Black-Scholes result is a standard tool for valuing options in finance the log-normal predictive distribution that follows from its assumptions is not commonly studied. In order to forecast \\(X_T\\) we need to estimate the unknowns \\(\\mu\\) and \\(\\sigma\\) (recall \\(X_0\\) is known). The unknown parameters \\(\\mu\\) and \\(\\sigma\\) can be interpreted as the instantaneous expected rate of return and the volatility, respectively. The mean parameter \\(\\mu\\) is known as the expected rate of return because the expected value of \\(X_T\\) is \\(X_0e^{\\mu T}\\). There are a number of ways of estimating the unknown parameters. One approach is to use an equilibrium model for returns, such as the Capital Asset Pricing Model or CAPM. We will discuss this model later. Another approach is to use historical data to estimate the parameters. For example, the expected rate of return can be estimated as the average historical return. The volatility can be estimated as the standard deviation of historical returns. The Black-Scholes model is a continuous time model, but in practice we use discrete time data. The Black-Scholes model can be adapted to discrete time by replacing the continuous time Brownian motion with a discrete time random walk.\n\n\n\n\n\nA. N. Kolmogorov. 1938. “On the Analytic Methods of Probability Theory.” Rossíiskaya Akademiya Nauk, no. 5: 5–41.\n\n\nArnol’d, Vladimir I. 2006. “Forgotten and Neglected Theories of Poincaré.” Russian Mathematical Surveys 61 (1): 1.\n\n\nCootner, Paul H. 1967. The Random Character of Stock Market Prices. MIT press.\n\n\nDavison, Anthony Christopher. 2003. Statistical Models. Vol. 11. Cambridge university press.\n\n\nKolmogoroff, Andrei. 1931. “Über Die Analytischen Methoden in Der Wahrscheinlichkeitsrechnung.” Mathematische Annalen 104 (1): 415–58.\n\n\nLogunov, A. A. 2004. “Henri Poincare and Relativity Theory.” https://arxiv.org/abs/physics/0408077.\n\n\nPoincaré, Henri. 1898. “La Mesure Du Temps.” Revue de métaphysique Et de Morale 6 (1): 1–13.\n\n\nShiryayev, A. N. 1992. “On Analytical Methods in Probability Theory.” In Selected Works of a. N. Kolmogorov: Volume II Probability Theory and Mathematical Statistics, edited by A. N. Shiryayev, 62–108. Dordrecht: Springer Netherlands.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "08-gp.html",
    "href": "08-gp.html",
    "title": "8  Gaussian Processes",
    "section": "",
    "text": "8.1 Making Predictions with Gaussian Processes\nA Gaussian Process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution. It’s a powerful tool for modeling and predicting in various fields, particularly useful for regression and classification tasks in machine learning. A finite collection of \\(n\\) points from Gaussian Process is completely specified by its \\(n\\)-dimensional mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\). The index of the GP is a real number \\(x\\) and values are also real numbers. The mean of the process (and a finite collection of points) is defined by function \\(m(x)\\) and covariance is defined by function \\(k(x, x')\\), where \\(x\\) and \\(x'\\) are points in the index space. The mean function defines the average value of the function at point \\(x\\), and the covariance function, also known as the kernel, defines the extent to which the values of the function at two points \\(x\\) and \\(x'\\) are correlated.\n\\(k(x, x')\\), where \\(x\\) and \\(x'\\) are points in the input space. The mean function defines the average value of the function at point \\(x\\), and the covariance function, also known as the kernel, defines the extent to which the values of the function at two points \\(x\\) and \\(x'\\) are correlated. In other words, the kernel function is a measure of similarity between two input points. The covariance between two points is higher if they are similar, and lower if they are dissimilar. Thus Gaussian Process is completely specified by its mean function and covariance function and an instance of a one-dimensional GP is a function \\(f(x): \\mathbb{R} \\rightarrow \\mathbb{R}\\), a typical notation is \\[\nf(x) \\sim \\mathcal{GP}(m(x), k(x, x')).\n\\] The mean function* \\(m(x) = \\mathbb{E}[f(x)]\\) is then represents the expected value of the function at point \\(x\\), and the covariance function: \\(k(x, x') = \\mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\\) describes the amount of dependence between the values of the function at two different points in the input space.\nTypically the mean function is less important than the covariance function. Most of the time data scientists will use a zero mean function, \\(m(x)=0\\), and focus on the covariance function. The kernel function is often chosen to be a function of the distance between the two points \\(|x-x'|\\) or \\(\\|x-x'\\|_2\\) in higher dimensions. The most commonly used kernel function is the squared exponential kernel, which is a function of the squared distance between the two points. The squared exponential kernel is given by: \\[\nk(x, x') = \\sigma^2 \\exp\\left(-\\frac{(x - x')^2}{2l^2}\\right)\n\\] where \\(\\sigma^2\\) is the variance parameter and \\(l\\) is the length scale parameter. The variance parameter controls the vertical variation of the function (amplitude), and the length scale parameter controls the horizontal variation (number of “bumps”). The length scale parameter determines how far apart two points must be to be considered dissimilar. The larger the length scale, the smoother the function. The length scale parameter is also called the bandwidth parameter. In this case the covariance decays exponentially with the distance between the points. Observe, that \\(k(x,x) = \\sigma^2\\) and \\(k(x,x') \\rightarrow 0\\) as \\(|x-x'| \\rightarrow \\infty\\).\nLet’s demonstrate GP using a simulated example. We start by generating a sequence 100 inputs (process indexes)\nand then define the mean function and the covariance function\nThe covariance function is a function of the distance between the two points and not the actual values of the points. The squared exponential kernel is infinitely differentiable, which means that the GP is a very smooth function. The squared exponential kernel is also called the radial basis function (RBF) kernel. The covariance matrix is then defined as\nand we can generate a sample from the GP using the mvrnorm function from the MASS package and plot a sample\nFigure 8.1 shows a collection of 100 points of function \\(f(x)\\) sampled from a Gaussian Process with zero mean and squared exponential kernel for the set of 100 indexes \\(x =(0,0.1,0.2,\\ldots,10)\\). By visually inspecting the finite realization in Figure 8.1 of the GP, we can see that the sampled function is smooth, with most of its values between -2 and 2. Notice, that each element of the covariance matrix is less than 1. Thus, by properties of the normal 95% of points of \\(Y\\) should be within 1.96 of the zero (the mean). We see a few bumps on the plot, because values of \\(Y\\) with indexes close to each other are highly correlated.\nLet’s generate a few more samples from the same GP and plot them together\nEach random finite collection is different than the next. They all have similar range, about the same number of bumps, and are smooth. That’s what it means to have function realizations under a GP prior: \\(Y = f(x) \\sim \\mathcal{GP}(0, k(x, x'))\\)\nIf we think that our observed data with input indexes \\(X = (x_1,\\ldots,x_n)\\) and outputs \\(Y = (y_1,\\ldots,y_n)\\) are a realization of a Gaussian Process, then we can use the GP to make predictions about the output values at new inputs \\(x_* \\in \\mathbb{R}^q\\). The joint distribution of the observed data \\(Y\\) and the new data \\(y_*\\) is given by \\[\n\\begin{bmatrix} Y \\\\ y_* \\end{bmatrix} \\sim \\mathcal{N} \\left ( \\begin{bmatrix} \\mu \\\\ \\mu_* \\end{bmatrix}, \\begin{bmatrix} K & K_* \\\\ K_*^T & K_{**} \\end{bmatrix} \\right )\n\\] where \\(K = k(X, X)\\in \\mathbb{R}^{n\\times n}\\), \\(K_* = k(X, x_*)\\in \\mathbb{R}^{n\\times q}\\), \\(K_{**} = k(x_*, x_*) \\in \\mathbb{R}^{q\\times q}\\), \\(\\mu = \\mathbb{E}[Y]\\), and \\(\\mu_* = \\mathbb{E}[y_*]\\). The conditional distribution of \\(y_*\\) given \\(y\\) is then given by \\[\ny_* \\mid Y \\sim \\mathcal{N}(\\mu_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}}).\n% y_* \\mid Y \\sim \\mathcal{N}(\\mu_* + K_* K^{-1} (y - \\mu), K_{**} - K_*^T K^{-1} K_*).\n\\] The mean of the conditional distribution is given by \\[\n\\mu_{\\mathrm{post}} = \\mu_* + K_*^TK^{-1} (Y - \\mu)\n\\tag{8.1}\\] and the covariance is given by \\[\n\\Sigma_{\\mathrm{post}} = K_{**} - K_*^T K^{-1} K_*.\n\\tag{8.2}\\]\nEquation 8.1 and Equation 8.2 are convenient properties of a multivariate normal distribution.\nNow, instead of using GP to fit a known function (\\(\\sin\\)), we will apply it to a real-world data set. We will use the motorcycle accident data set from the MASS package. The data set contains accelerator readings taken through time in a simulated experiment on the efficacy of crash helmets.\nIn summary, Gaussian Processes provide a robust and flexible framework for modeling and predicting in situations where uncertainty and correlation among data points play a critical role. Their versatility and powerful predictive capabilities make them a popular choice in various scientific and engineering disciplines. GPs are considered non-parametric, which means they can model functions of arbitrary complexity. Through the choice of the kernel function, GPs can model a wide range of correlations between the data points. The mean and covariance functions can incorporate prior knowledge about the behavior of the function being modeled. There are many areas of applications for GP. The two main applications are: (i) predictive modeling, (ii) optimization, (iii) uncertainty quantification. We will focus on the first two applications in the later sections. In predictive modeling we can use GPs to predict the value of a function at new points, taking into account the uncertainty of the prediction. GPs are particular useful in spatial data analysis, where the correlation between data points is often related to their physical distance. Thus, GPs are quite often used for environmental modeling to analyze temperature or pollution levels, over geographical areas.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gaussian Processes</span>"
    ]
  },
  {
    "objectID": "08-gp.html#making-predictions-with-gaussian-processes",
    "href": "08-gp.html#making-predictions-with-gaussian-processes",
    "title": "8  Gaussian Processes",
    "section": "",
    "text": "Example 8.1 (Gaussian Process for \\(\\sin\\) function) Let’s use the GP to make predictions about the output values at new inputs \\(x_*\\). We use \\(x\\) in the [0,\\(2\\pi\\)] range and \\(y\\) to be the \\(y = \\sin(x)\\). We start by simulating the “observed” \\(x\\)-\\(y\\) pairs.\n\nn = 8; eps=1e-6\nX = matrix(seq(0, 2*pi, length=n), ncol=1)\nY = sin(X)\nK = outer(X[,1],X[,1], sqexpcov) + diag(eps, n)\n\nThe additive term diag(eps, n) \\(=\\epsilon I\\) adds a diagonal matrix with the small \\(\\epsilon\\) on the diagonal. This term shifts the spectrum of the resulting covariance matrix \\(K\\) by \\(\\epsilon\\) to the right. This is done to add numerical stability in case one of the eigenvalues is close to zero. It simply gives us a guarantee that solving linear system (inverting) with matrix \\(K\\) will be a numerically stable operation. In machine learning they call this term the jitter. Now we implement a function that calculates the mean and covariance of the posterior distribution of \\(y_*\\) given \\(Y\\).\nNow we generate a new set of inputs \\(x_*\\) and calculate the covariance matrices \\(K_*\\) and \\(K_{**}\\).\n\nq = 100\nXX = matrix(seq(-0.5, 2*pi + 0.5, length=q), ncol=1)\nKX = outer(X[,1], XX[,1],sqexpcov)\nKXX = outer(XX[,1],XX[,1], sqexpcov) + diag(eps, q)\n\nNotice, we did not add \\(\\epsilon I\\) to \\(K_*\\) = KX matrix, but to add it to \\(K_{**}\\) = KXX to guarantee that the resulting posterior covariance matrix is non-singular (invert able). Now we can calculate the mean and covariance of the posterior distribution of \\(y_*\\) given \\(Y\\).\n\nSi = solve(K)\nmup = t(KX) %*% Si %*% Y # we assume mu is 0\nSigmap = KXX - t(KX) %*% Si %*% KX\n\nNow, we can generate a sample from the posterior distribution over \\(y_*\\), given \\(Y\\)\n\nYY = mvrnorm(100, mup, Sigmap)\n\nUsing our convenience function plot_gp we can plot the posterior distribution over \\(y_*\\), given \\(Y\\).\n\nplot_gp = function(mup, Sigmap, X, Y, XX, YY){\n  q1 = mup + qnorm(0.05, 0, sqrt(diag(Sigmap)))\n  q2 = mup + qnorm(0.95, 0, sqrt(diag(Sigmap)))\n  matplot(XX, t(YY), type=\"l\", col=\"gray\", lty=1, xlab=\"x\", ylab=\"y\")\n  points(X, Y, pch=20, cex=2)\n  lines(XX, sin(XX), col=\"blue\")\n  lines(XX, mup, lwd=2)\n  lines(XX, q1, lwd=2, lty=2, col=2)\n  lines(XX, q2, lwd=2, lty=2, col=2)\n}\n\n\nplot_gp(mup, Sigmap, X, Y, XX, YY)\n\n\n\n\n\n\n\nFigure 8.3: Posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\n\n\n\n\n\nExample 8.2 (Gaussian Process for Simulated Data using MLE) In previous example we assumed that the \\(x\\)-\\(y\\) relations are modeled by a GP with \\(\\sigma^2 = 1\\) and \\(2l^2 = 1\\). However, we can use the observed data to estimate those to parameters. In the context of GP models, they are call hyper-parameters. We will use Maximum Likelihood Estimation (MLE) procedure to estimate those hype-parameters. The likelihood of a data that follows multivariate normal distribution is given by \\[\np(Y \\mid X, \\sigma, l) = \\frac{1}{(2\\pi)^{n/2} |K|^{1/2}} \\exp \\left ( -\\frac{1}{2} Y^T K^{-1} Y \\right )\n\\] where \\(K = K(X,X)\\) is the covariance matrix. We assume mean is zero, to simplify the formulas. The log-likelihood is given by \\[\n\\log p(Y \\mid X, \\sigma, l) = -\\frac{1}{2} \\log |K| - \\frac{1}{2} Y^T K^{-1} Y - \\frac{n}{2} \\log 2\\pi.\n\\]\nLet’s implement a function that calculates the log-likelihood of the data given the hyper-parameters \\(\\sigma\\) and \\(l\\) and use optim function to find the maximum of the log-likelihood function.\n\nloglik = function(par, X, Y) {\n  sigma = par[1]\n  l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  return(-(-0.5 * log(det(K)) - 0.5 * t(Y) %*% Si %*% Y - (n/2)* log(2*pi)))\n}\npar = optim(c(1,1), loglik, X=X, Y=Y)$par\nprint(par)\n\n 1.5 2.4\n\n\nThe optim function returns the hyper-parameters that maximize the log-likelihood function. We can now use those hyper-parameters to make predictions about the output values at new inputs \\(x_*\\).\n\nl = par[2]; sigma = par[1]\npredplot = function(X, Y, XX, YY, l, sigma) {\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  KX = outer(X[,1], XX[,1],sqexpcov,l,sigma)\n  KXX = outer(XX[,1],XX[,1], sqexpcov,l,sigma) + diag(eps, q)\n  Si = solve(K)\n  mup = t(KX) %*% Si %*% Y # we assume mu is 0\n  Sigmap = KXX - t(KX) %*% Si %*% KX\n  YY = mvrnorm(100, mup, Sigmap)\n  plot_gp(mup, Sigmap, X, Y, XX, YY)\n}\npredplot(X, Y, XX, YY, l, sigma)\n\n\n\n\n\n\n\n\nWe can see that our uncertainty is much “tighter”, the posterior distribution is much narrower. This is because we used the observed data to estimate the hyper-parameters. We can also see that the posterior mean is closer to the true function \\(y = \\sin(x)\\). Although our initial guess of \\(\\sigma^2 = 1\\) and \\(2l^2 = 1\\) was not too far off, the model fits the data much better when we use the estimated hyper-parameters.\nThe function optim we used above uses a derivative-based optimization algorithm and when derivative is not provided by the user, it uses a numerical approximation. Although we can use numerical methods to calculate the derivative of the log-likelihood function, it is faster and more accurate to use analytical derivatives, when possible. In the case of the GP’s log-likelihood, the derivative can be analytically calculated. To do it, we need a couple of facts from matrix calculus. If elements of matrix \\(K\\) are functions of some parameter \\(\\theta\\), then \\[\n\\frac{\\partial  Y^T K^{-1} Y}{\\partial \\theta} =  Y^T \\frac{\\partial K^{-1}}{\\partial \\theta} Y.\n\\] The derivative of the inverse matrix \\[\n\\frac{\\partial K^{-1}}{\\partial \\theta} = -K^{-1} \\frac{\\partial K}{\\partial \\theta} K^{-1}.\n\\] and the log of the determinant of a matrix \\[\n\\frac{\\partial \\log |K|}{\\partial \\theta} = \\mathrm{tr} \\left ( K^{-1} \\frac{\\partial K}{\\partial \\theta} \\right ),\n\\] we can calculate the derivative of the log-likelihood function with respect to \\(\\theta\\) \\[\n\\frac{\\partial \\log p(Y \\mid X,\\theta)}{\\partial \\theta} = -\\frac{1}{2}\\frac{\\partial \\log |K|}{\\partial \\theta}  + \\frac{1}{2} Y^T \\frac{\\partial K^{-1}}{\\partial \\theta}  Y.\n\\] Putting it all together, we get \\[\n\\frac{\\partial \\log p(Y \\mid X,\\theta)}{\\partial \\theta} = -\\frac{1}{2} \\mathrm{tr} \\left ( K^{-1} \\frac{\\partial K}{\\partial \\theta} \\right ) + \\frac{1}{2} Y^T K^{-1} \\frac{\\partial K}{\\partial \\theta} K^{-1} Y.\n\\] In the case of squared exponential kernel, the elements of the covariance matrix \\(K\\) are given by \\[\nK_{ij} = k(x_i, x_j) = \\sigma^2 \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right ).\n\\] The derivative of the covariance matrix with respect to \\(\\sigma\\) is given by \\[\n\\frac{\\partial K_{ij}}{\\partial \\sigma} = 2\\sigma \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right );~\\frac{\\partial K}{\\partial \\sigma} = \\dfrac{2}{\\sigma}K.\n\\] The derivative of the covariance matrix with respect to \\(l\\) is given by \\[\n\\frac{\\partial K_{ij}}{\\partial l} = \\sigma^2 \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right ) \\frac{(x_i - x_j)^2}{l^3};~ \\frac{\\partial K}{\\partial l}  = \\frac{(x_i - x_j)^2}{l^3} K.\n\\] Now we can implement a function that calculates the derivative of the log-likelihood function with respect to \\(\\sigma\\) and \\(l\\).\n\n# Derivative of the log-likelihood function with respect to sigma\ndloglik_sigma = function(par, X, Y) {\n  sigma = par[1]; l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  dK = 2*K/sigma\n  tr = sum(diag(Si %*% dK))\n  return(-(-0.5 * tr + 0.5 * t(Y) %*% Si %*% dK %*% Si %*% Y))\n}\n# Derivative of the log-likelihood function with respect to l\ndloglik_l = function(par, X, Y) {\n  sigma = par[1]; l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov ,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  dK =   outer(X[,1],X[,1], function(x, x1) (x - x1)^2)/l^3 * K\n  tr = sum(diag(Si %*% dK))\n  return(-(-0.5 * tr + 0.5 * t(Y) %*% Si %*% dK %*% Si %*% Y))\n}\n# Gradient function that returns a vector of derivatives\ngnlg = function(par,X,Y) {\n  return(c(dloglik_sigma(par, X, Y), dloglik_l(par, X, Y)))\n}\n\nNow we can use the optim function to find the maximum of the log-likelihood function and provide the derivative function we just implemented.\n\npar1 = optim(c(1,1), fn=loglik, gr=gnlg ,X=X, Y=Y,method=\"BFGS\")$par\nl = par1[2]; sigma = par1[1]\nprint(par1)\n\n 1.5 2.4\n\n\nThe result is the same compared to when we called optim without the derivative function. Even execution time is the same for our small problem. However, at larger scale, the derivative-based optimization algorithm will be much faster.\nFurthermore, incited of coding our own derivative functions, we can use an existing package, such as the laGP package, developed by Bobby Gramacy to estimate the hyper-parameters. The laGP package uses the same optimization algorithm as we used above, but it also provides better selection of the covariance functions and implements approximate GP inference algorithms for large scale problems, when \\(n\\) becomes large and inversion of the covariance matrix \\(K\\) is prohibitively expensive.\n\nlibrary(laGP)\ngp = newGP(X, Y, 1, 0, dK = TRUE)\nres = mleGP(gp, tmax=20)\nl.laGP = sqrt(res$d/2)\nprint(l.laGP)\n\n 2.4\n\n\nIn the newGP function defines a Gaussian process with square exponential covariance function and assumes \\(\\sigma^2 = 1\\), then mleGP function uses optimization algorithm to maximize the log-likelihood and returns the estimated hyper-parameters d = \\(2l^2\\), we can see that the length scale is close to the one we estimated above. We will use the predplot convenience function to calculate the predictions and plot the data vs fit.\npredplot(X, Y, XX, YY, l, sigma)\npredplot(X, Y, XX, YY, l.laGP, 1)\n\n\n\n\n\n\n\n\n\n\n\n(a) MLE Fit\n\n\n\n\n\n\n\n\n\n\n\n(b) laGP Fit\n\n\n\n\n\n\n\nFigure 8.4: Posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\n\nWe can see that there is visually no difference between the two fits. Thus, it seem irrelevant weather we keep sigma fixed \\(\\sigma=1\\) or estimate it using MLE. However, is other applications when uncertainty is larger, the choice of \\(\\sigma\\) is important when we use GP for regression and classification tasks. Even for our example, if we ask our model to extrapolate\nXX1 = matrix(seq(-4*pi, 6*pi + 0.5, length=q), ncol=1)\npredplot(X, Y, XX1, YY, l, sigma)\npredplot(X, Y, XX1, YY, l.laGP, 1)\n\n\n\n\n\n\nMLE Fit\n\n\n\n\n\n\n\nlaGP Fit\n\n\n\n\n\n\nExtrapolation: Posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\n\nWe can see that outside of the range of the observed data, the model with \\(\\sigma=1\\) is more “confident” in its predictions.\n\n\n\nExample 8.3 (Gaussian Process for Motorcycle Accident Data) We first estimate the length scale parameter \\(l\\) using the laGP package.\n\nlibrary(MASS)\nX = mcycle$times\nY = mcycle$accel\ngp = newGP(matrix(X), Y, 2, 1e-6, dK = TRUE);\nmleGP(gp, tmax=10);\n\nNow we plot the data and the fit using the estimated length scale parameter \\(l\\).\n\nXX = matrix(seq(2.4, 55, length = 499), ncol=1)\np = predGP(gp, XX)\nN = 499\nq1 = qnorm(0.05, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nq2 = qnorm(0.95, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nq3 = qnorm(0.5, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nggplot() + geom_point(aes(x=X,y=Y)) + geom_line(aes(x=XX,y=q3)) + geom_ribbon(aes(x=XX,ymin=q1, ymax=q2), alpha=0.2)\n\n\n\n\nMotorcycle Accident Data. Black line is the mean of the posterior distribution over \\(y_*\\), given \\(Y\\). Blue lines are the 95% confidence interval.\n\n\n\n\nWe can see that our model is more confident for time values between 10 and 30. The confidence interval is wider for time values between 0 and 10 and between 30 and less confident at the end close to the 60 mark. For some reason the acceleration values were not measure evenly, if we look at the histogram of time values, we can see that there are more data points in the middle of the time range.\n\nhist(X)\n\n\n\n\nHistogram of time values\n\n\n\n\nThe \\(\\sqrt{n}\\) decay in variance of the posterior distribution is a property of the squared exponential kernel.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gaussian Processes</span>"
    ]
  },
  {
    "objectID": "09-rl.html",
    "href": "09-rl.html",
    "title": "9  Reinforcement Learning",
    "section": "",
    "text": "9.1 Multi-Armed Bandits\n“Information is the resolution of uncertainty. Clause Shannon, 1948”\nThus far we have learned about the concept of making decisions under uncertainty in Chapter 4 and Chapter 5. We also learned about two different way to collect the data, namely filed experiments and observational studies. In the former, we have control over the data data collection process prior the start of the experiment and can collect data that will help us make better decisions. In the latter, we have no control over the data generating process and we have to make due with the data we have.\nWhat if we have a choice of what data to collect while we are running the experiment? What if we can collect data that will help us make better decisions? This is the idea behind what is called a sequential design of experiment. In this setting the researcher can choose what data to collect next based on the data collected so far and there is a feedback loop between the data generating process and the decision making. This is illustrated in the diagram below\nThere are multiple applications where this general framework can be applied and multiple algorithms that implement this concept. In this section, we will consider the most widely used among them.\nThe first application of the reinforcement learnign (a.k.a. trial-and-error learning) was in the 1950s, when Clause Shennon tained a mechanical mouse Thesius to find its wat through a maze.\nWhen the number of options is large and the testing budget is limited, e.g. number of samples we can collect might not be enough to test multiple alternatives, we can use a different approach to A/B testing called multi-armed bandits. Although the math involved in analysis and design of the experiments is slightly more complected, the idea is simple. Instead of testing all alternatives at the same time, we test them sequentially. We start with a small number of alternatives and collect data. Based on the data we decide which alternative to test next. This way we can test more alternatives with the same number of samples. Thus, MABs allow us to explore and exploit simultaneously. Thus, MABs require that the outcome of each experiment is available immediately or with small enough delay to make a decision on the next experiment Scott (2015). On the other hand, MABs are more sample efficient and allow to find an optimal alternative faster. Thus, in the case when time is of essence and there as an opportunity cost associated with delaying the decision, MABs are a better choice.\nFormally, there are \\(K\\) alternatives (arms) and each arm \\(a\\) is associated with a reward distribution \\(v_a\\), the value of this arm. The goal is to find the arm with the highest expected reward and accumulate the highest total reward in doing so. The reward distribution is unknown and we can only observe the reward after we select an arm \\(a\\) but we assume that we know the distribution \\(f_a(y\\mid \\theta)\\), where \\(a\\) is the arm index, \\(y\\) is the reward, and \\(\\theta\\) is the set of unknown parameters to be learned. The value of arm \\(v_a(\\theta)\\) is known, given \\(\\theta\\). Here are a few examples:\nA variation of the second example would include controlling for background variables, meaning adding covariates that correspond to variables that are not under our control. For example, we might want to control for the time of the day or the user’s location. This would allow us to estimate the effect of the design variables text and color while controlling for the background variables. Another variation is to replace the binary outcome variable with a continuous variable, such as the time spent on the website or the amount of money spent on the website or count variable. In this case we simply simple linear regression or another appropriate generalized linear model. Although at any given time, we might have our best guess about the values of parameters \\(\\hat \\theta\\), acting in a greedy way and choosing the arm with the highest expected reward \\(\\hat a = \\arg\\max_a v_a(\\hat \\theta)\\), we might be missing out on a better alternative. The problem is that we are not sure about our estimates \\(\\hat \\theta\\) and we need to explore other alternatives. Thus, we need to balance exploration and exploitation. A widely used and oldest approach for managing the explore/exploit trade-off in a multi-armed bandit problem is Thompson sampling. The idea is to use Bayesian inference to estimate the posterior distribution of the parameters \\(\\theta\\) and then sample from this distribution to select the next arm to test. The algorithm is as follows.\nAs the algorithm progresses, the posterior distributions of the arms are updated based on observed data, and arms with higher estimated success probabilities are favored for selection. Over time, Thompson Sampling adapts its beliefs and preferences, leading to better exploration and exploitation of arms.\nIt’s important to note that the effectiveness of Thompson Sampling depends on the choice of prior distributions and the updating process. The algorithm’s Bayesian nature allows it to naturally incorporate uncertainty and make informed decisions in the presence of incomplete information. The initial priors \\((\\alpha_i, \\beta_i)\\) can be set based on any available information about the arms, or as uniform distributions if no prior knowledge exists. This is a basic implementation of Thompson sampling. Variations exist for handling continuous rewards, incorporating side information, and adapting to non-stationary environments.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#multi-armed-bandits",
    "href": "09-rl.html#multi-armed-bandits",
    "title": "9  Reinforcement Learning",
    "section": "",
    "text": "In online advertisements, we have \\(K\\) possible ads to be shown and probability of user clicking is given by vector \\(\\theta = (\\theta_1,\\ldots,\\theta_K)\\) of success probabilities for \\(K\\) independent binomial models, with \\(v_a(\\theta) = \\theta_a\\). The goal is to find the ad with the highest click-through rate.\nIn website design, we have two design variables, the color of a button (red or blue) and its pixel size (27 or 40), we introduce two dummy variables \\(x_c\\) for color, \\(x_s\\) for size and the probability of user clicking is given by \\[\n\\mathrm{logit} P(\\text{click} \\mid \\theta) = \\theta_0 + \\theta_x x_c + \\theta_s x_s + \\theta_{cs}x_cx_s,\n\\] with \\(v_a(\\theta) = P(\\text{click} \\mid \\theta)\\).\n\n\n\nInitial Beliefs. For each arm \\(k\\) (\\(k = 1,\\ldots,K\\)), define a prior belief about its reward distribution using a beta distribution with parameters \\(\\alpha_i\\) and \\(\\beta_i\\). These parameters represent the prior knowledge about the number of successes (\\(\\alpha_k\\)) and failures (\\(\\beta_k\\)) experienced with arm \\(k\\).\n\nSampling Parameters. For each arm \\(k\\), sample a reward \\[\n\\hat \\theta_k \\sim Beta(\\alpha_k, \\beta_k)\n\\]\nfrom the beta distribution. This simulates drawing a “potential” reward from the unknown true distribution of arm \\(i\\). A suboptimal greedy alternative is to select expected value of the parameter \\(\\hat \\theta_k = E(\\theta_k) =\\alpha_k/(\\alpha_k + \\beta_k)\\).\n\nChoosing the Best Arm. Select the arm \\(k\\) with the highest sampled reward \\(\\hat \\theta_k\\): \\[\na_t = \\arg\\max_i \\hat \\theta_k.\n\\]\nUpdating Beliefs. After observing the actual reward \\(R_t\\) for the chosen arm \\(a_t\\), update the parameters for that arm: \\[\n\\alpha_i = \\alpha_i + R_t, \\quad \\beta_i = \\beta_i + (1 - R_t)\n\\] This update incorporates the new information gained from the actual reward into the belief about arm \\(a_t\\)’s distribution.\nRepeat. Go back to step 2 and continue sampling, choosing, and updating until you reach a stopping point.\n\n\n\n\n9.1.1 When to End Experiments\nSteps 2 of the TS algorithm be replaced by calculating probability of an arm \\(a\\) being the best \\(w_{at}\\) and then choose the arm by sampling from the discrete distribution \\(w_{1t},\\ldots,w_{Kt}\\). The probability of an arm \\(a\\) being the best is given by \\[\nw_{at} = P(a \\text{ is optimal } \\mid y^t) = \\int P(a \\text{ is optimal } \\mid \\theta) P(\\theta \\mid y^t) d\\theta,\n\\] where \\(y^t = (y_1,\\ldots,y_t)\\) is the history of observations up to time \\(t\\). We can calculate the probabilities \\(w_{at}\\) using Monte Carlo. We can sample \\(\\theta^{(1)},\\ldots,\\theta^{(G)}\\) from the posterior distribution \\(p(\\theta \\mid y^t)\\) and calculate the probability as \\[\nw_{at}\\approx \\dfrac{1}{G}\\sum_{g=1}^G I(a = \\arg\\max_i v_i(\\theta^{(g)})),\n\\] where \\(I(\\cdot)\\) is the indicator function. This is simply the proportion of times the arm was the best in the \\(G\\) samples.\nAlthough, using a single draw from posterior \\(p(\\theta \\mid y^t)\\) (as in the original algorithm) is equivalent to sampling proportional to \\(w_{at}\\) , the explicitly calculated \\(w_{at}\\) yields a useful statistic that can be used to decide on when to end the experiment.\nWe will use regret statistic to decide when to stop. Regret is the difference in values between the truly optimal arm and the arm that is apparently optimal at time. Although we cannot know the regret, it is unobservable, we can compute samples from its posterior distribution. We simulate the posterior distribution of the regret by sampling \\(\\theta^{(1)},\\ldots,\\theta^{(G)}\\) from the posterior distribution \\(p(\\theta \\mid y^t)\\) and calculating the regret as \\[\nr^{(g)} =  (v_a^*(\\theta^{(g)}) - v_{a^*_t}(\\theta^{(g)})),\n\\] Here \\(a^*\\) is the arm deemed best across all Monte Carlo draws and the first term is the value of the best arm within draw \\(g\\). We choose \\(a^*_t\\) as \\[\na^*_t = \\arg\\max_a w_{at}.\n\\]\nOften, it is convenient to measure the regret on the percent scale and then we use \\[\nr^{(g)} \\leftarrow r^{(g)}/v_{a^*_t}(\\theta^{(g)})\n\\]\nLet’s demonstrate using a simulated data. The function below generates samples \\(\\theta^{(g)}\\)\n\nbandit = function(x,n,alpha = 1,beta = 1,ndraws = 5000) {\n  set.seed(17) # Kharlamov\n  K &lt;- length(x) # number of bandits\n  prob &lt;- matrix(nrow = ndraws,ncol = K)\n  no = n - x\n  for (a in 1:K) {# posterior draws for each arm\n    prob[, a] = rbeta(ndraws,x[a] + alpha,no[a] + beta)\n  }\n  prob\n}\n\nSay we have three arms with 20, 30, and 40 sessions that have generated 12, 20, and 30 conversions. We assume a uniform prior for each arm \\(\\theta_i \\sim Beta(1,1)\\) and generate 6 samples from the posterior of \\(\\theta \\mid y^t\\).\n\nx = c(12,20,30)\nn = c(20, 30,40)\nprob = bandit(x, n,ndraws=6)\n\n\n\n\n\n\\(\\theta_1\\)\n\\(\\theta_2\\)\n\\(\\theta_3\\)\n\n\n\n\n1\n0.60\n0.63\n0.58\n\n\n2\n0.62\n0.62\n0.74\n\n\n3\n0.69\n0.53\n0.67\n\n\n4\n0.49\n0.59\n0.73\n\n\n5\n0.61\n0.51\n0.69\n\n\n6\n0.47\n0.64\n0.69\n\n\n\n\n\nNow, we calculate the posterior probabilities \\(w_{at} = P(a \\text{ is optimal } \\mid y^t)\\) for each of the three arms\n\nwat = table(factor(max.col(prob),levels = 1:3))/6\n\n\n\n\n1\n2\n3\n\n\n\n\n0.17\n0.17\n0.67\n\n\n\n\n\nThus far, the third arm is the most likely to be optimal, with probability 67%. Now, we calculate the regret for each of the six draws from the posterior of \\(\\theta \\mid y^t\\).\n\nregret = (apply(prob,1,max) - prob[,3])/prob[,3]\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n0.09\n0\n0.03\n0\n0\n0\n\n\n\n\n\nWe compute value row by row by subtracting the largest element of that row from the element in column 3 (because arm 3 has the highest chance of being the optimal arm). All rows but 1 and 3 are zero. In the first row, the value is (0.63-0.58)/0.58 because column 2 is 0.05 larger than column 3. If we keep going down each row we get a distribution of values that we could plot in a histogram. Let’s generate one for a larger number of draws (10000).\n\nprob = bandit(x, n,ndraws=10000)\nregret = (apply(prob,1,max) - prob[,3])/prob[,3]\n\n\n\n\nThe histogram of the value remaining in an experiment (regret). The vertical line is the 95th percentile, or the potential value remaining.\n\n\n\nwat = table(factor(max.col(prob),levels = 1:3))/10000\n\n\n\n\n1\n2\n3\n\n\n\n\n0.08\n0.2\n0.72\n\n\n\nThe histogram of the value remaining in an experiment (regret). The vertical line is the 95th percentile, or the potential value remaining.\n\n\nArm 3 has a 72% probability of being the best arm, so the value of switching away from arm 3 is zero in 72% of the cases. The 95th percentile of the value distribution is the “potential value remaining” (CvR) in the experiment, which in this case works out to be about .16.\n\nquantile(regret,0.95)\n\n 95% \n0.17 \n\n\nYou interpret this number as “We’re still unsure about the CvR for arm 3, but whatever it is, one of the other arms might beat it by as much as 16%.”\nGoogle Analytics, for example, “ends the experiment when there’s at least a 95% probability that the value remaining in the experiment is less than 1% of the champion’s conversion rate. That’s a 1% improvement, not a one percentage point improvement. So if the best arm has a conversion rate of 4%, then we end the experiment if the value remaining in the experiment is less than .04 percentage points of CvR”.\n\n\n9.1.2 Contextual Bandits\nTraditional multi-armed bandit models, like the binomial model, assume independent observations with fixed reward probabilities. This works well when rewards are consistent across different groups and times. However, for situations with diverse user bases or fluctuating activity patterns, such as international audiences or browsing behavior, this assumption can be misleading.\nFor instance, companies with a global web presence may experience temporal effects as markets in Asia, Europe, and the Americas become active at different times of the day. Additionally, user behavior can change based on the day of the week, with people engaging in different browsing patterns and purchase behaviors. For example, individuals may research expensive purchases during work hours but make actual purchases on weekends.\nConsider an experiment with two arms, A and B. Arm A performs slightly better during the weekdays when users browse but don’t make purchases, while Arm B excels during the weekends when users are more likely to make purchases. If there is a substantial amount of traffic, the binomial model might prematurely conclude that Arm A is the superior option before any weekend traffic is observed. This risk exists regardless of whether the experiment is conducted as a bandit or a traditional experiment. However, bandit experiments are more susceptible to this issue because they typically run for shorter durations.\nTo mitigate the risk of being misled by distinct sub-populations, two methods can be employed. If the specific sub-populations are known in advance or if there is a proxy for them, such as geographically induced temporal patterns, the binomial model can be adapted to a logistic regression model. This modification allows for a more nuanced understanding of the impact of different factors on arm performance, helping to account for variations in sub-population behavior and temporal effects. \\[\n\\mathrm{logit} P(\\text{click}_a \\mid \\theta, x) = \\theta_{0a} + \\beta^Tx,\n\\] where \\(x\\) that describe the circumstances or context of the observation. The success probability for selecting arm \\(a\\) under the context \\(x\\) is represented as \\(P(\\text{click}_a \\mid \\theta, x)\\). Each arm \\(a\\) has its own specific coefficient denoted as \\(\\beta_{0a}\\) with one arm’s coefficient set to zero as a reference point. Additionally, there is another set of coefficients represented as \\(\\beta\\) that are associated with the contextual data and are learned as part of the model. The value function can then be \\[\nv_a(\\theta) = \\mathrm{logit}^{-1}(\\beta_{0a}).\n\\]\nIf we lack knowledge about the crucial contexts, one option is to make the assumption that contexts are generated randomly from a context distribution. This approach is often exemplified by the use of a hierarchical model like the beta-binomial model. \\[\\begin{align*}\n\\theta_{at} &\\sim Beta(\\alpha_a,\\beta_a)\\\\\n\\text{click}_a \\mid \\theta &\\sim Binomial(\\theta_{at}),\n\\end{align*}\\] where \\(\\theta = \\{\\alpha_a,\\beta_a ~:~ a = 1,\\ldots,K \\}\\), with value function \\(v_a(\\theta) = \\alpha_a/\\beta_a\\)\n\n\n9.1.3 Summary of MAB Experimentation\nDesign Phase: The design phase begins with defining your arms by identifying the different options you want to evaluate, such as different website layouts, pricing strategies, or marketing campaigns. Next, choose a bandit algorithm that balances exploration and exploitation in various ways. Popular choices include Epsilon-greedy, Thompson Sampling, and Upper Confidence Bound (UCB). Then set your parameters by configuring the algorithm parameters based on your priorities and expected uncertainty. For example, a higher exploration rate encourages trying new arms earlier. Finally, randomize allocation by assigning users to arms randomly, ensuring unbiased data collection.\nAnalysis Phase: During the analysis phase, track rewards by defining and measuring the reward metric for each arm, such as clicks, conversions, or profit. Monitor performance by regularly analyzing the cumulative reward and arm selection probabilities to see which arms are performing well and how the allocation strategy is adapting. Use statistical tools like confidence intervals or Bayesian methods to compare performance between arms and assess the significance of findings. Make adaptive adjustments by modifying the experiment based on ongoing analysis. You might adjust algorithm parameters, stop arms with demonstrably poor performance, or introduce new arms.\nPractical Considerations: Start with a small pool of arms to avoid information overload by testing a manageable number of options initially. Set a clear stopping criterion by deciding when to end the experiment based on a predetermined budget, time limit, or desired level of confidence in the results. Consider ethical considerations by ensuring user privacy and informed consent if the experiment involves personal data or user experience changes. Interpret results in context by remembering that MAB results are specific to the tested conditions and might not generalize perfectly to other contexts.\nBy following these steps and utilizing available resources, you can design and analyze effective MAB experiments to optimize your decision-making in various scenarios. Remember to adapt your approach based on your specific goals and context to maximize the benefits of this powerful technique.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#bellman-principle-of-optimality",
    "href": "09-rl.html#bellman-principle-of-optimality",
    "title": "9  Reinforcement Learning",
    "section": "9.2 Bellman Principle of Optimality",
    "text": "9.2 Bellman Principle of Optimality\n\n“An optimum policy has the propery that whatver the initial state and initial decision are, the remaining decision sequence must be optimum for the state resulting from the first decision.” – Richard Bellman\n\n\nExample 9.1 (Secretary Problem) The Secretary Problem, also known as the “marriage problem” or “sultan’s dowry problem,” is a classic problem in decision theory and probability theory. The scenario involves making a decision on selecting the best option from a sequence of candidates or options. The problem is often framed as hiring a secretary, but it can be applied to various situations such as choosing a house, a spouse, or any other scenario where you sequentially evaluate options and must make a decision.\nIn this problem, you receive \\(T\\) offers and must either accept (or reject) the offer “on the spot”. You cannot return to a previous offer once you have moved on to the next one. Offers are in random order and can be ranked against those previously seen. The aim is to maximize the probability of choosing the offer with the greatest rank. There is an optimal \\(r\\) (\\(1 \\le r &lt; T\\)) to be determined such that where we examine and reject the first \\(r\\) offers. Then of the remaining \\(T - r\\) offers we choose the first one that is best seen to date.\nA decision strategy involves setting a threshold such that the first candidate above this threshold is hired, and all candidates below the threshold are rejected. The optimal strategy, known as the “37% rule,” suggests that one should reject the first \\(r=T/e\\) candidates and then select the first candidate who is better than all those seen so far.\nThe reasoning behind the 37% rule is based on the idea of balancing exploration and exploitation. By rejecting the first \\(T/e\\) candidates, you gain a sense of the quality of the candidates but avoid committing too early. After that point, you select the first candidate who is better than the best among the initial \\(r\\) candidates.\nIt’s important to note that the 37% rule provides a probabilistic guarantee of selecting the best candidate with a probability close to 1/e (approximately 37%) as \\(T\\) becomes large.\nTo solve the secretary problem, we will use the principle of optimality due to Richard Bellman. The principle states that an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. In other words, the policy is optimal from the first decision onwards.\nThe solution to the secretary problem can be found via dynamic programming. Given an agent with utility function \\(u(x,d)\\), with current state \\(x\\), and decision \\(d\\). The law of motion of \\(x_t\\) is given by \\(x_{t+1} = p(x_t,d_t)\\). Bellman principle of optimality states that the optimal policy is given by the following recursion \\[\nV(x_t) = \\max_{d_t} \\left \\{ u(x_t,d_t) + \\gamma \\mathbb{E} \\left [ V(x_{t+1}) \\right ] \\right \\}\n\\] where \\(\\gamma\\) is the discount factor. The optimal policy is given by \\[\nd_t^* = \\arg \\max_{d_t} \\left \\{ u(x_t,d_t) + \\gamma \\mathbb{E} \\left [ V(x_{t+1}) \\right ] \\right \\}.\n\\]\nNow, back to the secretary problem. Let \\(y^t = (y_1,\\ldots,y_t)\\) denote the history of observations up to time \\(t\\). State \\(x_t=1\\) if the \\(t\\)th candidate is the best seen so far and \\(x_t=0\\) otherwise. The decision \\(d_t=1\\) if the \\(t\\)th candidate is hired and \\(d_t=0\\) otherwise. The utility function is given by \\(u(x_t,d_t) = x_t d_t\\).  The Bellman equation is given by \\[\nP(\\text{best of T}\\mid x_t=1) = \\dfrac{P(\\text{best of T})}{P(x_t=1)} = \\dfrac{1/T}{1/t} = \\dfrac{t}{T}.\n\\] The \\(t\\)th offer is the best seen so far places no restriction on the relative ranks of the first \\(t-1\\) offers. Therefore, \\[\np(x_t=1,y^{t-1}) = p(x_t=1)p(y^{t-1})\n\\] by the independence assumption. Hence, we have \\[\np(x_t=1 \\mid y^{t-1}) = p(x_t=1) = \\dfrac{1}{t}.\n\\]\nLet \\(p^*(x_{t-1}=0)\\) be the probability under the optimal strategy. Now we have to select the best candidate, given we have seen \\(t-1\\) offers so far and the last one was not the best or worse. The probability satisfies the Bellman equation \\[\np^*(x_{t-1}=0) = \\frac{t-1}{t} p^*(x_{t}=0) + \\frac{1}{t}\\max\\left(t/T, p^*(x_{t}=0)\\right).\n\\] This leads to \\[\np^*(x_{t-1}=0) = \\frac{t-1}{T} \\sum_{\\tau=t-1}^{T-1}\\dfrac{1}{\\tau}.\n\\]\nRemember, the strategy is to reject the first \\(r\\) candidates and then select the first. The probability of selecting the best candidate is given by \\[\nP(\\text{success}) = \\dfrac{1}{T}\\sum_{a=r+1}^T \\dfrac{r}{a} \\approx  \\dfrac{1}{T}\\int_{r}^{T}\\dfrac{r}{a} = \\dfrac{r}{T} \\log \\left ( \\dfrac{T}{r} \\right ).\n\\] We optimize over \\(r\\) by setting the derivative \\[\n\\frac{\\log \\left(\\frac{T}{r}\\right)}{T}-\\frac{1}{T}\n\\] to zero, to find the optimal \\(r=T/e\\).\nIf we plug in \\(r=T/e\\) back to the probability of success, we get \\[\nP(\\text{success}) \\approx \\dfrac{1}{e} \\log \\left ( e \\right ) = \\dfrac{1}{e}.\n\\]\nMonte Carlo Simulations\nSimulations are a powerful tool for making decisions when we deal with a complex system, which is difficult or impossible to analyze mathematically. They are used in many fields, including finance, economics, and engineering. They can also be used to test hypotheses about how a system works and to generate data for statistical analysis.\nWe start by showing how the secretary problem can be analyses using simulations rather than alanytical derivations provided above.\n\nset.seed(17) # Kharlamov\nnmc = 1000\nn = 1000\nsz = 300\nrules = round(n*seq(0.002,0.8,length.out = sz))\nrules = unique(rules[rules&gt;0])\nsz = length(rules)\ncnt = rep(0,sz)\nquality = rep(0,sz)\nfor (i in 1:sz)\n{\n  for (j in 1:nmc){\n    x = sample(1:n,n)\n    screen = x[1:(rules[i]-1)]\n    best_screen = max(screen)\n    xchoice = x[(rules[i]):n]\n    better_candidates = which(xchoice &gt; best_screen)\n    if (length(better_candidates)==0)\n      choice = x[n]\n    else\n      choice = xchoice[min(better_candidates)]\n    cnt[i] = cnt[i] + (choice == max(x))\n    quality[i] = quality[i] + choice\n  }\n}\nd = data.frame(cnt=cnt, quality=quality,nmc=nmc, rules=rules)\n\n\nplot(d$rules, d$cnt/d$nmc, type='l', col=3, lwd=3, xlab=\"Number of Candidates Screend\", \n     ylab=\"Probability of Picking the Best\")\n\n\n\n\n\n\n\nplot(d$rules, d$quality/1000, type='l', col=3, lwd=3, xlab=\"Number of Candidates Screend\", \n     ylab=\"Average Quality of Candidate\")",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#markov-decision-processes",
    "href": "09-rl.html#markov-decision-processes",
    "title": "9  Reinforcement Learning",
    "section": "9.3 Markov Decision Processes",
    "text": "9.3 Markov Decision Processes\nMarkov decision process (MDP) is a discrete time stochastic control process which provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. Almost all dynamic programming and reinforcement learning problems are formulated using the formalism of MDPs. MDPs are useful in various fields, including robotics, economics, and artificial intelligence. In fact, the multi-armed bandit problem considered before is a special case of MDP with one state. MDPs were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard’s 1960 book, Dynamic Programming and Markov Processes.\nA Markov Decision Process (MDP) is a mathematical framework used for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful in various fields, including robotics, economics, and artificial intelligence, particularly in reinforcement learning. An MDP is defined by:\n\nStates (\\(S\\)): A set of states representing different scenarios or configurations the system can be in. The key assumption here is the Markov property, which states that the future is independent of the past given the present. This means that the decision only depends on the current state, not on the sequence of events that preceded it.\nActions (\\(A\\)): A set of actions available in each state.\nTransition Probability (\\(P\\)): \\(P(s', r | s, a)\\) is the probability of transitioning to state \\(s'\\), receiving reward \\(r\\), given that action \\(a\\) is taken in state \\(s\\).\nReward (\\(R\\)): A reward function \\(R(s, a, s')\\) that gives the feedback signal immediately after transitioning from state \\(s\\) to state \\(s\\)’, due to action \\(a\\).\nDiscount Factor (\\(\\gamma\\)): A factor between 0 and 1, which reduces the value of future rewards.\n\n\n9.3.1 Mathematical Representation\nThe states \\(s_t\\) and rewards \\(R_t\\) in MDP are indexed by time \\(t\\). The state at time \\(t+1\\) is distributed according to the transition probability \\[\nP(s_{t+1}\\mid s_t,a_t).\n\\] The reward function is \\(R_s^a = E[R_{t+1} \\mid s, a]\\).\nThe Markov property of the state is that the transition probability depends only on the current state and action and not on the history of states and actions. \\[\nP(s_{t+1}\\mid s_t,a_t) = P(s_{t+1}\\mid s_t,a_t,s_{t-1},a_{t-1},\\ldots,s_0,a_0).\n\\] In other words, the future only depends on the present and not on the past history. The state is a sufficient statistic for the future.\nIn the case when the number of states if finite, we can represent the transition probability as a matrix \\(P_{ss'}^a = P(s_{t+1} = s' \\mid s_t = s, a_t = a)\\), where \\(s,s' \\in S\\) and \\(a \\in A\\). For a given action \\(a\\), the transition probability matrix \\(P^a\\) is a square matrix of size \\(|S| \\times |S|\\), where each row sums to 1 \\[\nP^a = \\begin{bmatrix}\nP_{11}^a & P_{12}^a & \\cdots & P_{1|S|}^a \\\\\nP_{21}^a & P_{22}^a & \\cdots & P_{2|S|}^a \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nP_{|S|1}^a & P_{|S|2}^a & \\cdots & P_{|S||S|}^a\n\\end{bmatrix}\n\\] The reward function is also a matrix \\(R_s^a = E[R_{t+1} \\mid s_t = s, a_t = a]\\).\nMarkov Reward Process\nLet’s consider a simpler example of Markov Process. This is a special case of MDP when there is no action and the transition probability is simply a matrix \\(P_{ss'} = P(s_{t+1} = s' \\mid s_t = s)\\), where \\(s,s' \\in S\\). For a given action \\(a\\), the transition probability matrix \\(P^a\\) is a square matrix of size \\(|S| \\times |S|\\), where each row sums to 1.\n\nExample 9.2 (Student Example) The graph below represents possible states (nodes) and transitions (links). Each node has reward assigned to it which corresponds to the reward function \\(R(s)\\). The transition probabilities are shown on the links. The graph is a Markov Chain, a special case of MDP with no actions.\n\n\nIf we are to pick an initial state and sample a trajectory (path on the graph above) by picking a random action at each state, we will get a random walk on the graph. The reward for each state is shown in the graph. The discounted value of the trajectory is then \\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1},\n\\] where \\(\\gamma\\) is the discount factor. The discount factor is a number between 0 and 1 that determines the present value of future rewards. A discount factor of 0 makes the agent “myopic” and only concerned about immediate rewards. A discount factor of 1 makes the agent strive for a long-term high reward. The discount factor is usually denoted by \\(\\gamma\\) and is a parameter of the MDP. The discount of \\(&lt;1\\) is used to avoid infinite returns in cyclic Markov chains and allows to discount less certain future rewards. The value of \\(\\gamma\\) is usually close to 1, e.g. 0.9 or 0.99. The value of \\(\\gamma\\) can be interpreted as the probability of the agent surviving from one time step to the next.\nLet’s calculate sample returns \\(G_t\\) for this Markov Chain. We first read in the reward matrix\n\n# Reward function\nR = read.csv(\"../data/student-reward.tab\", header = T, sep = \"\\t\", row.names=1)\nt(R) %&gt;% knitr::kable()\n\n\nRewards\n\n\n\nFacebook\nClass 1\nClass 2\nClass 3\nPub\nPass\nFail\nSleep\n\n\n\n\nReward\n-1\n-2\n-2\n-2\n3\n10\n-20\n0\n\n\n\n\ngetR = function(s) R[s,]\n\nand the transition probability matrix and the reward matrix\n\n# Read transition probability matrix\np = read.csv(\"../data/student-mdp.tab\", header = T, sep = \"\\t\", row.names=1)\nkbp = knitr::kable(p)\ngsub(0, ' ', kbp) # replace 0 with blank\n\n\n\n\n\nFacebook\nClass.1\nClass.2\nClass.3\nPub\nPass\nFail\nSleep\n\n\n\n\nFacebook\n.9\n.1\n.\n.\n.\n.\n.\n.\n\n\nClass 1\n.5\n.\n.5\n.\n.\n.\n.\n.\n\n\nClass 2\n.\n.\n.\n.8\n.\n.\n.\n.2\n\n\nClass 3\n.\n.\n.\n.\n.4\n.6\n.\n.\n\n\nPub\n.\n.2\n.3\n.3\n.\n.\n.2\n.\n\n\nPass\n.\n.\n.\n.\n.\n.\n.\n1.\n\n\nFail\n.\n.\n.\n.\n.\n.\n1.\n.\n\n\nSleep\n.\n.\n.\n.\n.\n.\n.\n1.\n\n\n\n\n\nNot let’s check that all of the rows sum to 1\n\np = as.matrix(p)\nrowSums(p) %&gt;% t() %&gt;% knitr::kable()\n\n\nTransition probability matrix row sums\n\n\nFacebook\nClass 1\nClass 2\nClass 3\nPub\nPass\nFail\nSleep\n\n\n\n\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\nGiven the transition probability matrix, we can sample possible trajectories. First, we define a tr(s,m) convenience function that generates a trajectory of length m starting from state s\n\nset.seed(17)\n# Sample column s' using probabilities in row s \njump = function(s) sample(rownames(p), 1, prob = p[s,])\n# Function to generate a trajectory\ntr = function(s,m) {\n  traj = c(s)\n  for (i in 1:m) {\n    traj = c(traj, jump(traj[i]))\n  }\n  return(traj)\n}\n\nNow, let’s generate 6 trajectories of length 5 starting from state “Pub”\n\ntraj = t(replicate(6,tr(\"Pub\",5)))\nknitr::kable(traj)\n\n\n\n\nPub\nClass 3\nPub\nClass 2\nClass 3\nPass\n\n\nPub\nClass 2\nClass 3\nPass\nSleep\nSleep\n\n\nPub\nClass 2\nClass 3\nPub\nFail\nFail\n\n\nPub\nFail\nFail\nFail\nFail\nFail\n\n\nPub\nFail\nFail\nFail\nFail\nFail\n\n\nPub\nClass 3\nPass\nSleep\nSleep\nSleep\n\n\n\n\n\nNow we can calculate the discounted value \\(G_t\\) of each of the trajectories\n\ntrajR = apply(traj,1:2, getR)\ndisc = 0.5^(0:5)\ntrajR %*% disc %&gt;% t() %&gt;% knitr::kable()\n\n\nDiscounted value of each trajectory\n\n\n2.7\n2.8\n0\n-16\n-16\n4.5\n\n\n\n\n\nLet’s calculate the discounted value for 1000 trajectories\n\n# Value function of a trajectory\nvalue = function(s,m, gamma=0.5) {\n  traj = tr(s,m)\n  disc = gamma^(0:m)\n  return(sum(sapply(traj,getR) * disc))\n}\nvpub = replicate(1000,value(\"Pub\",6))\nhist(vpub)\n\n\n\n\n\n\n\nmean(vpub)\n\n -1.2\n\n\nWe can see that the distribution of discounted rewards is by-model and depends on weather you get to “Fail” state or not.\n\nThe value of a state is the expected discounted reward starting from that state \\[\nV(s) = E[G_t \\mid s_t = s].\n\\] It evaluates the long-term value of state \\(s\\) (the goodness of a state). It can be drastically different from the reward value associated with the state. In our student example, the reward for the “Pub” state is 3, but the value is -1.2.\nThe value of a state can be calculated recursively using the Bellman equation \\[\\begin{align*}\nV(s) &= E[G_t \\mid s_t = s] \\\\\n&= E[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid s_t = s] \\\\\n&= E[R_{t+1} + \\gamma G_{t+1} \\mid s_t = s] \\\\\n&= E[R_{t+1} + \\gamma V(s_{t+1}) \\mid s_t = s] \\\\\n&= \\sum_{s'} P(s' \\mid s) \\left[R(s) + \\gamma V(s')\\right].\n\\end{align*}\\]\n\nExample 9.3 (Game of Chess as an MDP) Let’s consider a simple example of a game of chess.\nIn chess, a state \\(s\\) represents the configuration of the chessboard at any given time. This includes the positions of all the pieces (pawns, knights, bishops, rooks, queen, and king) for both players (white and black). Each possible arrangement of these pieces on the chessboard is a unique state. The game starts in a standard initial state (the standard chess setup) and progresses through a series of states as moves are made. If the game is played to completion, it ends in a terminal state (checkmate, stalemate, or draw). Also if we reach a state that has been seen before multiple times in a row, we can stop the game and declare a draw. Thus, we need to “remember” the states we have seen before and essentially expand the state space to include the number of times we have seen the state. In a timed game, the game can also end when a player runs out of time.\nActions \\(a\\) in chess are the legal moves that can be made by the player whose turn it is to move. This includes moving pieces according to their allowed movements, capturing opponent pieces, and special moves like castling or en passant. The set of actions available changes with each state, depending on the position of the pieces on the board.\nIn chess, the transition probability is deterministic for most part, meaning that the outcome of a specific action (move) is certain and leads to a predictable next state. For example, moving a knight from one position to another (assuming it’s a legal move) will always result in the same new configuration of the chessboard. However, in the context of playing against an opponent, there is uncertainty in predicting the opponent’s response, which can be seen as introducing a probabilistic element in the larger view of the game.\nDefining a reward function \\(R\\) in chess can be complex. In the simplest form, the reward could be associated with the game’s outcome: a win, loss, or draw. Wins could have positive rewards, losses negative, and draws could be neutral or have a small positive/negative value. Alternatively, more sophisticated reward functions can be designed to encourage certain strategies or positions, like controlling the center of the board, protecting the king, or capturing opponent pieces.\nChess is a game of perfect information, meaning all information about the game state is always available to both players. While the number of states in chess is finite, it is extremely large, making exhaustive state analysis (like traditional MDP methods) computationally impractical.\nIn practice, solving chess as an MDP, especially using traditional methods like value iteration or policy iteration, is not feasible due to the enormous state space. Modern approaches involve heuristic methods, machine learning, and deep learning techniques. For instance, advanced chess engines and AI systems like AlphaZero use deep neural networks and reinforcement learning to evaluate board positions and determine optimal moves, but they do not solve the MDP in the classical sense.\n\nThe goal in an MDP is to find a policy \\(a = \\pi(s)\\) (a function from states to actions) that maximizes the sum of discounted rewards: \\[\nV^\\pi(s) = E_{\\pi}[G_t \\mid S_t = s],\n\\] where \\[\nG_t = \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, \\pi(s_t), s_{t+1})\n\\]\nFunction \\(V^\\pi(s)\\) is the value of state s under policy \\(\\pi\\). Similarly we can define the action-value function \\(Q^\\pi(s,a)\\) as the value of taking action \\(a\\) in state \\(s\\) under policy \\(\\pi\\): \\[\nQ^\\pi(s,a) = E_{\\pi}[G_t \\mid S_t = s, A_t = a] = E_{\\pi}[G_t \\mid S_t = s, A_t = a].\n\\]\nBellman Equations for MDP simply states that the value of a state is the sum of the immediate reward and the discounted value of the next state \\[\nV^\\pi(s) = E_{\\pi}[R_{t+1} + \\gamma V^{\\pi}(S_{t+1})\\mid S_t = s] = \\sum_{a\\in A}\\pi(a\\mid s)\\left(R_s^a + \\gamma \\sum_{s'\\in S}P^a_{ss'}V^pi(s') \\right).\n\\] The action-value function satisfies the following Bellman equation \\[\nQ^\\pi(s,a) = E_{\\pi}[R_{t+1} + \\gamma Q^{\\pi}(S_{t+1}, A_{t+1})\\mid S_t = s, A_t = a].\n\\] The value function can be defined as an expectation over the action-value function \\[\nV^\\pi(s) = E_{\\pi}[Q^\\pi(s,a)\\mid S_t = s] = \\sum_{a\\in A}\\pi(a\\mid s)Q^\\pi(s,a).\n\\] In matrix form, we have \\[\nQ^\\pi(s,a) = R_s^a + \\gamma \\sum_{s'\\in S}P_{ss'}^a V^\\pi(s') = R_s^s + \\gamma \\sum_{s'\\in S}P_{ss'}^a\\sum_{a'\\in A}\\pi(a'\\mid s')Q^\\pi(s',a').\n\\] Now we can define the Bellman equation in the matrix form \\[\nV^\\pi = R^\\pi + \\gamma P^\\pi V^\\pi.\n\\] The direct solution is then \\[\nV^\\pi = (I - \\gamma P^\\pi)^{-1}R^\\pi.\n\\] The optimal value function \\(V^*(s)\\) is the value function for the optimal policy \\(\\pi^*(s)\\) \\[\nV^*(s) = \\max_\\pi V^\\pi(s).\n\\] The optimal action-value function \\(Q^*(s,a)\\) is the action-value function for the optimal policy \\(\\pi^*(s)\\) \\[\nQ^*(s,a) = \\max_\\pi Q^\\pi(s,a).\n\\] The optimal policy \\(\\pi^*(s)\\) is the policy that maximizes the value function \\[\n\\pi^*(s) = \\arg\\max_a Q^*(s,a).\n\\] The optimal value function satisfies the Bellman optimality equation \\[\nV^*(s) = \\max_a Q^*(s,a).\n\\] and vice-versa \\[\nQ^*(s,a) = R_s^a + \\gamma \\sum_{s'\\in S}P_{ss'}^a V^*(s').\n\\]\nThe Bellman optimality equation is non-linear and is typically solved iteratively using\n\nValue Iteration\nPolicy Iteration\nQ-learning: is an off-policy algorithm that learns the optimal policy by directly estimating the optimal action-value function \\(Q^*(s,a)\\). The algorithm iteratively updates the action-value function using the Bellman optimality backup. The off-policy means that the algorithm learns the optimal policy while following a different policy. The algorithm can learn the optimal policy while following a random policy, for example.\n\n\n\n9.3.2 MDP Solvers\nThe underlying approach behind all MDP solvers is to iteratively apply the Bellman equations until convergence. The main difference between the solvers is how they update the value function. All of them use dynamic programming approach to find optimal policy. A dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems. It is applicable to problems exhibiting the properties of overlapping subproblems and optimal substructure. If a problem can be solved by combining optimal solutions to non-overlapping subproblems, the strategy is called “divide and conquer” instead. This is why dynamic programming is applicable to solving MDPs.\nFirst, we consider how to find a the values of states under a given policy \\(\\pi\\). We can iteratively apply Bellman expectation backup. We update the values using the following update rule \\[\nV_{k+1}(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s, a)[R(s, a, s') + \\gamma V_k(s')].\n\\] We will introduce the short-cut natation \\[  \nP_{ss'}^a = P(s' \\mid s, a), \\quad R_s^a = \\sum_{s'} P(s' \\mid s, a)R(s, a, s').\n\\] Then in matrix form the update rule becomes \\[\nV_{k+1} = R^{\\pi} + \\gamma P^{\\pi} V_k.\n\\]\nThe policy iteration algorithm involves two main steps: policy evaluation and policy improvement, which are iteratively applied until convergence. We start with an arbitrary value function, often initialized to zero for all states. \\[\\begin{align*}\nV_0(s) &= 0 \\\\\nV_{k+1} = & R^{\\pi} + \\gamma P^{\\pi} V_k\\\\\n\\pi_{k+1} &= \\arg\\max_a R^a + \\gamma P^a V_{k+1} = \\arg\\max_a Q^\\pi(s,a)\n\\end{align*}\\] The last step is to simply choose the action that maximizes the expected return in each state. Although can be slow in practice, the convergence is guaranteed because the value function is a contraction mapping. We typically stop the iterations when the maximum change in the value function is below a threshold.\nIt can be used for calculating the optimal policy. The Bellman optimality equation is a fundamental part of finding the best policy in MDPs. \\[\nV^*(s) = \\max_a \\sum_{s', r} P(s', r | s, a)[r + \\gamma V^*(s')]\n\\] The optimal policy is then \\[\n\\pi^*(s) = \\arg\\max_a \\sum_{s', r} P(s', r | s, a)[r + \\gamma V^*(s')]\n\\] The optimal policy is the one that maximizes the value function. The optimal value function is the value function for the optimal policy. The optimal value function satisfies the Bellman optimality equation. The optimal policy can be found by maximizing the right hand side of the Bellman optimality equation.\nGiven an optimal policy, we can subdivie it into two parts: the optimal first action \\(A^*\\) and the optimal policy from the next state \\(S'\\). The optimal value \\(V^*\\) can be found using one-step lookahead \\[\nV^*(s) = \\max_a R_s^a +  \\gamma \\sum_{s'\\in S} P_{ss'}^a V^*(s')\n\\] This allows us to define another approach to solving MDPs, called value iteration. The value iteration algorithm starts with an arbitrary value function and iteratively applies the Bellman optimality backup. The algorithm updates the value function using the following update rule \\[\nV_{k+1}(s) = \\max_a R_s^a +  \\gamma \\sum_{s'\\in S} P_{ss'}^a V_k(s').\n\\] In matrix form, the update rule becomes \\[\nV_{k+1} = \\max_a R^a + \\gamma P^a V_k.\n\\] The algorithm stops when the maximum change in the value function is below a threshold. The optimal policy can be found by maximizing the right hand side of the Bellman optimality equation \\[\n\\pi^*(s) = \\arg\\max_a R_s^a +  \\gamma \\sum_{s'\\in S} P_{ss'}^a V^*(s').\n\\]\nIn practice, exactly solving the Bellman Expectation Equation in the policy evaluation step can be computationally expensive for large state spaces. Approximate methods may be used. Policy Iteration is particularly effective when the optimal policy needs to be very precise, as in high-stakes decision-making environments.\n\nExample 9.4 (MDP for a Maze) We use a mazemdp archive by Sally Gao, Duncan Rule, Yi Hao to demonstrate the value and policy iterations. Both are applied to the problem of finding an optimal policy for a maze. The maze is represented as a grid, with each cell either being a wall or empty. Agent (decision maker) does not know the maze structure and needs to find the optimal path from the start to the goal state. The agent starts in the bottom right corner and needs to reach the top left corner (marked as red). The agent can move up, down, left, or right, but not diagonally (actions). Moving into a wall keeps the agent in the same cell. Reaching the goal state gives a reward of +1, and all other transitions give a reward of 0. The goal is to find the optimal policy that maximizes the expected return (sum of discounted rewards) for the agent. In other words, the agent needs to find the shortest path to the exit.\nFigures below show the snapshot from policy (top row) and value (bottom row) iterations.\n\n\n\n\n\n\n\n\n\\(t=0\\)\n\n\n\n\n\n\n\n\\(t=12\\)\n\n\n\n\n\n\n\n\\(t=24\\)\n\n\n\n\n\n\nFigure 9.1: Policy Iteration Solver\n\n\n\n\n\n\n\n\n\n\n\n\\(t=0\\)\n\n\n\n\n\n\n\n\\(t=15\\)\n\n\n\n\n\n\n\n\\(t=30\\)\n\n\n\n\n\n\nFigure 9.2: Value Iteration Solver\n\n\n\nThe policy iterations converged after 24 iterations and it took\n\n\nExample 9.5 (MDP for a Forest Management) Let’s consider one of the classic example of a Markov Decision Process (MDP). Imagine you need to calculate an optimal policy to manage forest to prevent possible fires. The goal is to decide between two possible actions to either ‘Wait’ or ‘Cut’. They correspond to balancing between ecological preservation and economic gain, considering the random event of a fire. Let’s break down the elements of this model.\n\nStates: Represent the age of the forest. The states are denoted as \\(\\{0, 1,\\ldots, S-1\\}\\), where 0 is the youngest state (just after a fire or cutting) and \\(S-1\\) is the oldest state of the forest.\nActions: There are two actions available:\n\n‘Wait’ (Action 0): Do nothing and let the forest grow for another year.\n‘Cut’ (Action 1): Harvest the forest, which brings immediate economic benefit but resets its state to the youngest.\n\nProbabilities: There’s a probability ‘p’ each year that a fire occurs, regardless of the action taken. If a fire occurs, the forest returns to state 0.\nTransition Matrix (P): This matrix defines the probabilities of moving from one state to another, given a specific action.\n\nWe will use mdp_example_forest function from the MDPtoolbox package to generate the transition probability matrix and reward matrix for the Forest example.\n\nlibrary(MDPtoolbox)\n# Define the transition and reward matrices for the Forest example\nres = mdp_example_forest(S=4,r1=10,r2=1,p=0.01)\n\nThis function is generates a transition probability \\(P\\) of size \\((|A| \\times |S| \\times |S|\\), there are three states by default \\(S = \\{0,1,2\\}\\) and two actions.\n\nres$P\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,] 0.01 0.99 0.00 0.00\n[2,] 0.01 0.00 0.99 0.00\n[3,] 0.01 0.00 0.00 0.99\n[4,] 0.01 0.00 0.00 0.99\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    1    0    0    0\n[3,]    1    0    0    0\n[4,]    1    0    0    0\n\n\nAs well as the reward matrix \\(R\\) of size \\(|S| \\times |A|\\).\n\nres$R\n\n     R1 R2\n[1,]  0  0\n[2,]  0  1\n[3,]  0  1\n[4,] 10  1\n\n\n\nmdp_value_iteration(res$P,res$R, discount = 0.9, epsilon = 1e-6, max_iter = 10000)\n\n \"MDP Toolbox WARNING: max_iter is bounded by  5000\"\n \"MDP Toolbox: iterations stopped, epsilon-optimal policy found\"\n\n\n 13 21 30 40\n\n$policy\n[1] 1 1 1 1\n\n$iter\n[1] 5\n\n$time\nTime difference of 0.0018 secs\n\n$epsilon\n[1] 1e-06\n\n$discount\n[1] 0.9\n\nmdp_policy_iteration(res$P, res$R, discount=0.9)\n\n  4.7  5.2  5.2 92.1\n\n$policy\n[1] 1 2 2 1\n\n$iter\n[1] 1\n\n$time\nTime difference of 0.013 secs\n\n\n\nA more general form of a value function is the action-value function \\(Q^\\pi(s,a)\\), which represents the expected return when starting from state \\(s\\), taking action \\(a\\), and following policy \\(\\pi\\) thereafter. \\[\nQ^\\pi(s,a) = E_{\\pi}\\left[G_t \\mid s_t = s, a_t = a, a_t\\right].\n\\] We can derive both the value and optimal policy functions from the action-value function: \\[\nV^\\pi(s) = \\sum_{a \\in A} \\pi(a|s) Q^\\pi(s,a)\n\\] \\[\n\\pi^*(s) = \\arg\\max_a Q^*(s,a)\n\\]\n\n\n9.3.3 Model-Free Methods\nBoth policy and value iterations we’ve considered thus far assume that transition probabilities between states given actions are known. However, often it is not the case in many real-world problems. Model-free methods learn through trial and error, by interacting with the environment and observing the rewards. Typically, the interaction. The first method we consider is Monte Carlo methods. Monte Carlo methods for Markov Decision Processes (MDPs) are a class of algorithms used for finding optimal policies when the model of the environment (i.e., the transition probabilities and rewards) is unknown or too complex to model explicitly. These methods rely on learning from experience, specifically from complete episodes of interaction with the environment. Here’s a detailed look at how Monte Carlo methods work in the context of MDPs:\n\nGenerate Episodes: An episode is a sequence of states, actions, and rewards, from the start state to a terminal state. \\[\nS_0, A_0, R_1, S_1, A_1, R_2, \\ldots, S_{T-1}, A_{T-1}, R_T \\sim \\pi.\n\\] In Monte Carlo methods, these episodes are generated through actual or simulated interaction with the environment, based on a certain policy.\nEstimate Value Functions: Unlike dynamic programming methods, which update value estimates based on other estimated values, Monte Carlo methods update estimates based on actual returns received over complete episodes. This involves averaging the returns received after visits to each state. We use empirical mean to estimate the expected value.\nPolicy Improvement: After a sufficient number of episodes have been generated and value functions estimated, the policy is improved based on these value function estimates.\n\nMonte Carlo methods require complete episodes to update value estimates. This makes them particularly suitable for episodic tasks, where interactions naturally break down into separate episodes with clear starting and ending points. MC methods require sufficient exploration of the state space. This can be achieved through various strategies, like \\(\\epsilon\\)-greedy policies, where there’s a small chance of taking a random action instead of the current best-known action. In this case, the policy is given by \\[\n\\pi(a \\mid s) = \\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|A|} & \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\\n\\frac{\\epsilon}{|A|} & \\text{otherwise}\n\\end{cases}\n\\] where \\(\\epsilon\\) is the probability of taking a random action and \\(|A|\\) is the number of actions. The \\(\\epsilon\\)-greedy policy is an example of an exploration-exploitation strategy, where the agent explores the environment by taking random actions (exploration) while also exploiting the current knowledge of the environment by taking the best-known action (exploitation). The value of \\(\\epsilon\\) is typically decayed over time, so that the agent explores more in the beginning and exploits more later on.\nMonte Carlo methods are model-free, meaning they do not require a model of the environment (transition probabilities and rewards). They are also effective in dealing with high variance in returns, which can be an issue in some environments. However, they can be inefficient due to high variance and the need for many episodes to achieve accurate value estimates. They also require careful handling of the exploration-exploitation trade-off. The two main approaches for Monte Carlo methods are first-visit and every-visit methods.\n\nFirst-Visit MC: In this approach, the return for a state is averaged over all first visits to that state in each episode.\nEvery-Visit Monte Carlo: Here, the return is averaged over every visit to the state, not just the first visit in each episode.\n\nMonte Carlo Policy Iteration involves alternating between policy evaluation (estimating the value function of the current policy using Monte Carlo methods) and policy improvement (improving the policy based on the current value function estimate). This process is repeated until the policy converges to the optimal policy.\nTo find the optimal policy, a balance between exploration and exploitation must be maintained. This is achieved through strategies like \\(\\epsilon\\)-greedy exploration. In Monte Carlo Control, the policy is often improved in a greedy manner based on the current value function estimate.\nRecall, that an arithmetic average can be updated recursively \\[\n\\bar{x}_n = \\frac{1}{n}\\sum_{i=1}^n x_i = \\frac{1}{n}\\left(x_n + \\sum_{i=1}^{n-1} x_i\\right) = \\frac{1}{n}\\left(x_n + (n-1)\\bar{x}_{n-1}\\right) = \\bar{x}_{n-1} + \\frac{1}{n}(x_n - \\bar{x}_{n-1}).\n\\] This is called a running average. We can use this recursion to update the value function \\(V(s)\\) incrementally, each time we visit state \\(s\\) at time \\(t\\). \\[\nV(s_t) = V(s_t) + \\frac{1}{N(s_t)}(G_t - V(s_t)),\n\\] where \\(N(s_t)\\) is the number of times we visited state \\(s_t\\) before time \\(t\\) and \\(G_t\\) is the return at time \\(t\\). This is called first-visit Monte Carlo method. Alternatively, we can use every-visit Monte Carlo method, where we update the value function each time we visit state \\(s\\).\nAlternatively, we can use a learning rate \\(\\alpha\\) \\[\nV_{n+1} = V_n + \\alpha(G_n - V_n).\n\\] This is called constant step size update. The learning rate is a hyperparameter that needs to be tuned. The constant step size update is more convenient because it does not require to keep track of the number of visits to each state. The constant step size update is also more robust to non-stationary problems.\nTemporal Difference Learning (TD Learning) Similar to MC, TD methods learn directly from raw experience without a model of the environment. However, unlike MC methods, TD methods update value estimates based on other learned estimates, without waiting for the end of an episode. This is called bootstrapping. TD methods combine the sampling efficiency of Monte Carlo methods with the low variance of dynamic programming methods. They are also model-free and can learn directly from raw experience. However, they are more complex than MC methods and require careful tuning of the learning rate.\nA simple TD method is TD(0), which updates value estimates based on the current reward and the estimated value of the next state. The update rule is \\[\nV(S_t) = V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)),\n\\] where \\(\\alpha\\) is the learning rate. The TD(0) method is also called one-step TD because it only looks one step ahead. The \\(R_{t+1} + \\gamma V(S_{t+1})\\) term is called the TD target and is biased estimate of \\(V(S_t)\\). The difference \\(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\) is called the TD error. The TD target is an estimate of the return \\(G_t\\) and the TD error is the difference between the TD target and the current estimate \\(V(S_t)\\). Although, TD algorithms has lower variance than MC methods, it has higher bias. In practice TD methods are more efficient than MC methods.\n\n\n9.3.4 Q-Learning\nQ-learning is an off-policy algorithm that learns the optimal policy by directly estimating the optimal action-value function \\(Q^*(s,a)\\). The algorithm iteratively updates the action-value function using the Bellman optimality backup. The off-policy means that the algorithm learns the optimal policy while following a different policy. The algorithm can learn the optimal policy while following a random policy, for example. The algorithm can be summarized as follows: \\[\nQ(S_t,A_t) = Q(S_t,A_t) + \\alpha(R_{t+1} + \\gamma \\max_a Q(S_{t+1},a) - Q(S_t,A_t)),\n\\] where \\(\\alpha\\) is the learning rate. The algorithm can be summarized as follows:\n\nInitialize \\(Q(s,a)\\) arbitrarily\nRepeat for each episode:\n\nInitialize \\(S\\)\nRepeat for each step of the episode:\n\nChoose \\(A\\) from \\(S\\) using policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy)\nTake action \\(A\\), observe \\(R\\), \\(S'\\)\n\\(Q(S,A) = Q(S,A) + \\alpha(R + \\gamma \\max_a Q(S',a) - Q(S,A))\\)\n\\(S = S'\\)\n\nUntil \\(S\\) is terminal\n\n\nThen we can simplify the update rule to \\[\nQ(S_t,A_t) = (1-\\alpha)Q(S_t,A_t) + \\alpha(R_{t+1} + \\gamma \\max_a Q(S_{t+1},a)).\n\\]\n\nExample 9.6 (Q-Learning and Deal-No Deal) Deal-no-deal is a popular TV show where a contestant is presented with a number of sealed boxes, each containing a prize. The contestant selects a box and then proceeds to open the remaining boxes one by one. After a certain number of boxes have been opened, the “banker” makes an offer to buy the contestant’s box. The contestant can either accept the offer and sell the box or reject the offer and continue opening boxes. The game continues until the contestant either accepts an offer or opens all the boxes. The goal is to maximize the expected value of the prize in the contestant’s box. The Rule of Thumb is to continue as long as there are two large prizes left. Continuation value is large. For example, with three prizes and two large ones. Risk averse people will naively choose deal, when if they incorporated the continuation value they would choose no deal.\nLet \\(s\\) denote the current state of the system and \\(a\\) an action. The \\(Q\\)-value, \\(Q_t(s,a)\\), is the value of using action \\(a\\) today and then proceeding optimally in the future. We use \\(a=1\\) to mean no deal and \\(a=0\\) means deal. The Bellman equation for \\(Q\\)-values becomes \\[\nQ_{t} ( s , a) = u( s , a  ) + \\sum_{ s^\\star } P( s^\\star | s ,a ) \\max_{ a } Q_{t+1} ( s^\\star , a )\n\\] where \\(u(s,a)\\) is the immediate utility of taking action \\(a\\) in state \\(s\\). The value function and optimal action are given by \\[\nV(s) = \\max_a Q ( s , a ) \\; \\; \\text{and} \\; \\;  a^\\star = \\text{argmax}  Q ( s , a )\n\\]\nTransition Matrix: Consider the problem where you have three prizes left. Now \\(s\\) is the current state of three prizes. \\[\ns^\\star = \\{ \\text{all \\; sets \\; of \\; two \\; prizes} \\} \\; \\; \\text{and} \\; \\; P( s^\\star | s, a =1) = \\frac{1}{3}\n\\] where the transition matrix is uniform to the next state. There’s no continuation for \\(P( s^\\star | s, a =0)\\).\nUtility: The utility of the next state depends on the contestants value for money and the bidding function of the banker \\[\nu( B ( s^\\star ) ) = \\frac{ B ( s^\\star )^{1-\\gamma} -1 }{1 - \\gamma }\n\\] in power utility case.\nExpected value implies \\(B( s ) = \\bar{s}\\) where \\(s\\) are the remaining prizes.\nThe website uses the following criteria: with three prizes left: \\[\nB( s) = 0.305 \\times \\text{big} + 0.5 \\times \\text{small}\n\\] and with two prizes left \\[\nB( s) = 0.355 \\times \\text{big} + 0.5 \\times \\text{small}   \n\\]\nThree prizes left: $s = { 750 , 500 , 25 } $.\nAssume the contestant is risk averse with log-utility \\(U(x) = \\ln x\\). Banker offers the expected value we get \\[\nu( B( s = \\{ 750 , 500 , 25 \\}) ) = \\ln ( 1275/3  ) = 6.052\n\\] and so \\(Q_t ( s , a= 0 ) = 6.052\\).\nIn the continuation problem, \\(s^\\star = \\{ s_1^\\star , s_2^\\star , s_3^\\star  \\}\\) where \\(s_1^\\star = \\{750,500 \\}\\) and \\(s_2^\\star = \\{ 750,25 \\}\\) and \\(s_3^\\star = \\{ 500,25 \\}\\).\nWe’ll have offers \\(625 ,  387.5 , 137.5\\) under the expected value. As the banker offers expected value the optimal action at time \\(t+1\\) is to take the deal \\(a=0\\) with Q-values given by \\[\\begin{align*}\nQ_{t} ( s , a=1) & = \\sum_{ s^\\star } P( s^\\star | s ,a =1) \\max_{ a } Q_{t+1} ( s^\\star , a ) \\\\\n& = \\frac{1}{3} \\left (  \\ln (625) + \\ln (387.5) + \\ln (262.5) \\right ) = 5.989\n\\end{align*}\\] as immediate utility \\(u( s,a ) = 0\\). Hence as \\[\nQ_{t} ( s , a=1)=5.989 &lt;  6.052 = Q_{t} ( s , a=0)\n\\] the optimal action is \\(a^\\star = 0\\), deal. Continuation value is not large enough to overcome the generous (expected value) offered by the banker.\nSensitivity analysis: we perform it by assuming different Banker’s bidding function. If we use the function from the website (2 prizes): \\[\nB( s) = 0.355 \\times \\text{big} + 0.5 \\times \\text{small},\n\\] Hence \\[\\begin{align*}\nB( s_1^\\star = \\{750,500 \\}) &  = 516.25 \\\\\nB( s_2^\\star = \\{ 750,25 \\}) & =  278.75 \\\\\nB( s_3^\\star = \\{ 500,25 \\}) &  = 190  \n\\end{align*}\\]\nThe optimal action with two prizes left for the contestant is \\[\\begin{align*}\nQ_{t+1} ( s_1^\\star , a=1) & = \\frac{1}{2} \\left (  \\ln (750) + \\ln (500) \\right ) = 6.415 \\\\\n& &gt; 6.246 = Q_{t+1} ( s_1^\\star , a=0) = \\ln \\left ( 516.25 \\right ) \\\\\nQ_{t+1} ( s_1^\\star , a=1) & = \\frac{1}{2} \\left (  \\ln (750) + \\ln (25) \\right ) = 4.9194 \\\\\n& &lt; 5.63 = Q_{t+1} ( s_1^\\star , a=0)  = \\ln \\left ( 278.75 \\right ) \\\\\nQ_{t+1} ( s_1^\\star , a=1) & = \\frac{1}{2} \\left (  \\ln (500) + \\ln (25) \\right ) = 4.716 \\\\\n& &lt; 5.247 = Q_{t+1} ( s_1^\\star , a=0)  = \\left ( 516.25 \\right ) \\\\\n\\end{align*}\\] Hence future optimal policy with be no deal under \\(s_1^\\star\\), and deal under \\(s_2^\\star , s_3^\\star\\).\nTherefore solving for \\(Q\\)-values at the previous step gives \\[\\begin{align*}\nQ_{t} ( s , a=1) & = \\sum_{ s^\\star } P( s^\\star | s ,a =1) \\max_{ a } Q_{t+1} ( s^\\star , a ) \\\\\n& = \\frac{1}{3} \\left (  6.415+ 5.63 + 5.247 \\right ) = 5.764\n\\end{align*}\\] with a monetary equivalent as \\(\\exp(5.764  ) = 318.62\\).\nWith three prizes we have \\[\\begin{align*}\nQ_{t} ( s , a=0) & = u( B( s = \\{ 750 , 500 , 25 \\}) ) \\\\\n& = \\ln \\left ( 0.305 \\times 750 + 0.5 \\times 25 \\right ) \\\\\n& = \\ln ( 241.25 ) = 5.48.\n\\end{align*}\\] The contestant is offered $ 241.\nNow we have \\(Q_{t} ( s , a=1)= 5.7079  &gt; 5.48 = Q_{t} ( s , a=0)\\) and the optimal action is \\(a^\\star = 1\\), no deal. The continuation value is large. The premium is $ 241 compared to $319, a \\(33\\)% premium.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#bayesian-optimization",
    "href": "09-rl.html#bayesian-optimization",
    "title": "9  Reinforcement Learning",
    "section": "9.4 Bayesian Optimization",
    "text": "9.4 Bayesian Optimization\nBayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. It is particularly useful when the objective function is expensive to evaluate. Bayesian optimization uses a surrogate model to approximate the objective function and an acquisition function to decide where to sample next. The surrogate model is typically a Gaussian process (GP) model, which is a probabilistic model that defines a distribution over functions. The acquisition function is a heuristic that trades off exploration and exploitation to decide where to sample next. Bayesian optimization is a global optimization method, meaning it does not require derivatives and can find the global optimum of the objective function. It is also sample-efficient, meaning it can find the optimum with fewer samples than other methods. However, it can be slow in practice and is not suitable for high-dimensional problems.\nGiven a function \\(f(x)\\), that is not known analytically, it can represent, for example, output of a complex computer program. The goal is to optimize \\[\nx^* = \\arg\\min_x f(x).\n\\]\nThe Bayesian approach to this problem is the following:\n\nDefine a prior distribution over \\(f(x)\\)\nCalculate \\(f\\) at a few points \\(x_1, \\ldots, x_n\\)\nRepeat until convergence:\n\nUpdate the prior to get the posterior distribution over \\(f(x)\\)\nChoose the next point \\(x^+\\) to evaluate \\(f(x)\\)\nCalculate \\(f(x^+)\\)\n\nPick \\(x^*\\) that corresponds to the smallest value of \\(f(x)\\) among evaluated points\n\nThe prior distribution is typically a Gaussian process (GP) model, which is a probabilistic model that defines a distribution over functions. The GP model is defined by a mean function \\(m(x)\\) and a covariance function \\(k(x,x')\\). The mean function is typically set to zero. The covariance function is typically a squared exponential function \\[\nk(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2l^2}\\right),\n\\] where \\(\\sigma_f^2\\) is the signal variance and \\(l\\) is the length scale. The covariance function defines the similarity between two points \\(x\\) and \\(x'\\). The covariance function is also called a kernel function. The kernel function is a measure of similarity between inputs \\(x\\) and \\(x'\\).\nNow we need to decide where to sample next. We can use the acquisition function to decide where to sample next. The acquisition function is a heuristic that trades off exploration and exploitation to decide where to sample next. The expected improvement (EI) function is a popular acquisition function. Suppose \\[\nf^* = \\min y\n\\] is the maximum value of \\(f(x)\\) among evaluated points. At a given point \\(x\\) and function value \\(y = f(x)\\), the expected improvement function is defined as \\[\na(x) = \\mathbb{E}\\left[\\max(0, f^* - y)\\right],\n\\] The function that we calculate expectation of \\[\nu(x) = \\max(0, f^* - y)\n\\] is the utility function. Thus, the acquisition function is the expected value of the utility function.\nThe acquisition function is high when \\(y\\) is likely to be higher than \\(f^*\\), and low when \\(y\\) is likely to be lower than \\(f^*\\). Given the GP prior, we can calculate the acquisition function analytically. The posterior distribution of Normal \\(y \\sim N(\\mu,\\sigma^2)\\), then the acquisition function is \\[\\begin{align*}\na(x) &= \\mathbb{E}\\left[\\max(0, f^* - y)\\right] \\\\\n&= \\int_{-\\infty}^{\\infty} \\max(0, f^* - y) \\phi(y,\\mu,\\sigma^2) dy \\\\\n&= \\int_{-\\infty}^{f^*} (f^* - y) \\phi(y,\\mu,\\sigma^2) dy\n\\end{align*}\\] where \\(\\phi(y,\\mu,\\sigma^2)\\) is the probability density function of the normal distribution. A useful identity is \\[\n\\int y \\phi(y,\\mu,\\sigma^2) dy =\\frac{1}{2} \\mu  \\text{erf}\\left(\\frac{y-\\mu }{\\sqrt{2} \\sigma }\\right)-\\frac{\\sigma\n   e^{-\\frac{(y-\\mu )^2}{2 \\sigma ^2}}}{\\sqrt{2 \\pi }},\n\\] where \\(\\Phi(y,\\mu,\\sigma^2)\\) is the cumulative distribution function of the normal distribution. Thus, \\[\n\\int_{-\\infty}^{f^*} y \\phi(y,\\mu,\\sigma^2) dy = \\frac{1}{2} \\mu (1+\\text{erf}\\left(\\frac{f^*-\\mu }{\\sqrt{2} \\sigma\n   }\\right))-\\frac{\\sigma  e^{-\\frac{(f^*-\\mu )^2}{2 \\sigma ^2}}}{\\sqrt{2 \\pi}} = \\mu \\Phi(f^*,\\mu,\\sigma^2) + \\sigma^2 \\phi(f^*,\\mu,\\sigma^2).\n\\]\nwe can write the acquisition function as \\[\na(x) = \\dfrac{1}{2}\\left(\\sigma^2 \\phi(f^*,\\mu,\\sigma^2) + (f^*-\\mu)\\Phi(f^*,\\mu,\\sigma^2)\\right)\n\\]\nLet’s implement it\n\nacq &lt;- function(xx,p, fstar) {\n  x = matrix(xx, ncol=1)\n  d = fstar - p$mean\n  s = sqrt(diag(p$Sigma))\n  return(s*dnorm(d) + d*pnorm(d))\n}\n\n\nExample 9.7 (Taxi Fleet Optimisation) We will use the taxi fleet simulator from Emukit project. For a given demand (the frequency of trip requests) and the number of taxis in the fleet, it simulates the taxi fleet operations and calculates the profit. The simulator is a black-box function, meaning it does not have an analytical form and can only be evaluated at specific points. The goal is to find the optimal number of taxis in the fleet that maximizes the profit. We will use Bayesian optimization to solve this problem.\n\n\n\nTaxi Simulator Visualization\n\n\nWe start with initial set of three designs \\(x = (10,30,90)\\), where \\(x\\) is the number of the taxis in the fleet and observe the corresponding profits profit=(3.1,3.6,6.6). When \\(x=10\\), the demand for taxis exceed the supply passengers need to wait for their rides, leading to missed profit opportunities. At another extreme when we have 90 taxis, the profit is slightly better. However, there are many empty taxis, which is not profitable. The optimal number of taxis must be somewhere in the middle. Finally, we try 30 taxis and observe that the profit is higher than both of our previous attempts. However, should we increase or decrease the number of taxis from here? We can use Bayesian optimization to answer this question. First we define a convenience function to plot the GP emulator.\n\nplotgp = function(x,y,XX,p) {\n  q1 = qnorm(0.05, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  q2 = qnorm(0.95, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  q3 = qnorm(0.5, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  ggplot() + geom_point(aes(x=x,y=y)) + geom_line(aes(x=XX,y=q3)) + geom_ribbon(aes(x=XX,ymin=q1, ymax=q2), alpha=0.2)\n}\n\nNow, we fit the GP imulator using our initial set of observed taxi-profit pairs.\n\nlibrary(laGP)\nlibrary(mvtnorm)\nx = matrix(c(10,90,30), ncol=1)\nxx = seq(1,100, length=500)\nXX &lt;- matrix(xx, ncol = ncol(x))\nprofit = -c(3.1,3.6,6.6)\ngp &lt;- newGP(x, profit, 1000, 1e-6, dK = TRUE)\np &lt;- predGP(gp, XX)\nplotgp(x,profit,XX,p)\n\n\n\n\n\n\n\n\nInstead of maximizing the profit, we minimize the negative profit. We see that that there is potentially a better value at around 50 taxis. We can use the acquisition function to decide where to sample next. We define two functions: nextsample that uses the acquisition function to decide where to sample next and updgp that updates the GP emulator with the new sample. Then we call those two functions twice. First time, EI suggests 44 and second time it suggests 42. We update the GP emulator with the new samples and plot the updated emulator. We see that the GP emulator is updated to reflect the new samples.\nnextsample = function(){\n  ei = acq(xx,p,min(profit))\n  plot(xx,ei, type='l')\n  xnext = as.integer(xx[which.max(ei)])\n  return(xnext)\n}\nupdgp = function(xnext,f){\n  profit &lt;&lt;- c(profit, f)\n  x &lt;&lt;- c(x, xnext)\n  updateGP(gp, matrix(xnext,ncol=1), f)\n  p &lt;&lt;- predGP(gp, XX)\n  plotgp(x,profit,XX,p)\n}\nnextsample(); #44\nupdgp(44, -8.4);\nnextsample(); # 57\nupdgp(57, -7.1);\nnextsample(); # 45\nupdgp(45, -8.5);\nnextsample(); # 100\nupdgp(100, -3.3);\n\n\n\n\n\n\n 44\n\n\n\n\n\n\n\n\n\n\n 57\n\n\n\n\n\n\n\n\n\n\n 45\n\n\n\n\n\n\n\n\n\n\n 100\n\n\n\n\n\n\nIf we run nextsample one more time, we get 47, close to our current best of 45. Further, the model is confident at this location. It means that we can stop the algorithm and “declare a victory”\n\nnextsample()\n\n\n\n\n\n\n\n\n 47\n\n\n\n\n\n\n\nScott, Steven L. 2015. “Multi-Armed Bandit Experiments in the Online Service Economy.” Applied Stochastic Models in Business and Industry 31 (1): 37–45.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "10-data.html",
    "href": "10-data.html",
    "title": "10  Unreasonable Effectiveness of Data",
    "section": "",
    "text": "It is remarkable that a science which began with the consideration of games of chance should have become the most important object of human knowledge. Laplace, Pierre Simon, 1812\n\nData from telescopes have played a crucial role on the data analysis techniques developed in the 18th century. Internet and mobile devices play a similar role in the 21st century. Large amounts of astronomical data led scientists like Carl Friedrich Gauss, Pierre-Simon Laplace, and Sim{'e}on Denis Poisson to the development of data-driven methods, such as the method of least squares and the Poisson distribution. These methods not only revolutionized astronomy but also are used nowadays in various fields, such as physics, engineering, and economics. The development of these methods marked a significant shift in the scientific approach, enabling more rigorous analysis and interpretation of observational data. The integration of data and statistical methods laid the foundation for modern data science and statistics, demonstrating the power and versatility of data-driven approaches.\nBack in the 18th and 19th century data collection was often limited to manual measurements or observations, and the amount of available data was typically much smaller compared to the massive datasets encountered in modern data science. Scientists like Gauss and Poisson often conducted carefully designed experiments, collected their own data, and performed manual calculations without the aid of computers or advanced statistical software. The focus of their work was often on theoretical developments in mathematics, physics and astronomy, and the data was used to test and validate their theories. Let’s consider one of those studies from early 18th century.\n\nExample 10.1 (Boscovich and Shape of Earth) The 18th century witnessed heated debates surrounding the Earth’s precise shape. While the oblate spheroid model - flattened poles and bulging equator - held sway, inconsistencies in measurements across diverse regions fueled uncertainty about its exact dimensions. The French, based on extensive survey work by Cassini, maintained the prolate view while the English, based on gravitational theory of Newton (1687), maintained the oblate view.\nThe determination of the exact figure of the earth would require very accurate measurements of the length of a degree along a single meridian. The final answer to this debate was given by Roger Boscovich (1711-1787) who used geodetic surveying principles and in collaboration with English Jesuit Christopher Maire, in 1755, they embarked on a bold project: measuring a meridian arc spanning a degree of latitude between Rome and Rimini. He employed ingenious techniques to achieve remarkable accuracy for his era, minimizing errors and ensuring the reliability of his data. In 1755 they published “De litteraria expeditione per pontificiam ditionem” (On the Scientific Expedition through the Papal States) that contained results of their survey and its analysis. For more details about the work of Boscovich, see Altić (2013). Stigler (1981) gives an exhaustive introduction to the history of regression.\nThe data on meridian arcs used by Boscovich was crucial in determining the shape and size of the Earth. He combined data from five locations:\n\nd=read.csv(\"../data/boscovich.csv\")\nknitr::kable(d, digits = 8)\n\n\n\n\nLocation\nLatitude\nArcLength\nsin2Latitude\n\n\n\n\nQuito\n0\n56751\n0\n\n\nCape of Good Hope\n33\n57037\n2987\n\n\nRome\n43\n56979\n4648\n\n\nParis\n49\n57074\n5762\n\n\nLapland\n66\n57422\n8386\n\n\n\n\nplot(d$sin2Latitude,d$ArcLength, ylab=\"Arc Length\", xlab=expression(sin^2~(theta)), pch=16,ylim=c(56700,57450), xlim=c(-30,8590))\ntext(d$sin2Latitude,d$ArcLength-25, labels=d$Location)\n\n\n\n\n\n\n\n\nThe arc length is measured in toises. A toise is a pre-metric unit of length approximately equal to 6.39 feet. It is clear from the table and from the table and from the plot that the the arc length goes up as the latitude increases and the relationship between the arc length and the sine squared of the latitude is approximately linear and the relationship is \\[\n\\text{Arc Length} = \\beta_0 +  \\beta_1 \\sin^2 \\theta\n\\] where \\(\\theta\\) is the latitude. Here \\(\\beta_0\\) is the length of a degree of arc at the equator, and \\(\\beta_1\\) is how much longer a degree of arc is at the pole. The question that Boscovich asked is how can we combine those five data points to estimate the parameters \\(\\beta_0\\) and \\(\\beta_1\\)? His first attempt to answer this question involved calculating ten slopes for each pair of points and then averaging them. The table below shows the ten slopes.\n\nd = read.csv(\"../data/boscovich.csv\")\nsl = matrix(NA,5,5)\nfor (i in 1:5) {\n    for(j in 1:(i-1)) {\n        dx = d$sin2Latitude[i] - d$sin2Latitude[j]\n        dy = d$ArcLength[i] - d$ArcLength[j]\n        sl[i,j]=dy/dx\n    }\n}\nrownames(sl) = d$Location\ncolnames(sl) = d$Location\noptions(knitr.kable.NA = '')\nknitr::kable(sl, digits = 4)\n\n\nTen slopes for each pair of the five cities from the Boscovich data\n\n\n\nQuito\nCape of Good Hope\nRome\nParis\nLapland\n\n\n\n\nQuito\n\n\n\n\n\n\n\nCape of Good Hope\n0.096\n\n\n\n\n\n\nRome\n0.049\n-0.035\n\n\n\n\n\nParis\n0.056\n0.013\n0.085\n\n\n\n\nLapland\n0.080\n0.071\n0.118\n0.13\n\n\n\n\nTen slopes for each pair of the five cities from the Boscovich data\n\nplot(d$sin2Latitude,d$ArcLength, ylab=\"Arc Length\", xlab=expression(sin^2~(theta)), pch=16)\ntext(d$sin2Latitude,d$ArcLength-25, labels=d$Location)\nfor (i in 1:4){\n  for (j in (i+1):5){\n    slope = (d$ArcLength[i] - d$ArcLength[j])/(d$sin2Latitude[i] - d$sin2Latitude[j])\n    intercept = d$ArcLength[i] - slope*d$sin2Latitude[i]\n    abline(a=intercept, b=slope)\n  }\n}\n\n\n\n\nTen slopes for each pair of the five cities from the Boscovich data\n\n\n\n\nThe average of the ten slopes is 0.0667. Notice the slope between Cape of Good Hope and Rome is negative. This is due to the measurement error. Boscovich then calculated an average after removing this outlier. The average of the remaining nine slopes is 0.078. In both cases he used length of the arc at Quito as estimate of the intercept \\(\\beta_0\\). Figure 10.1 (a) shows the line that corresponds to the parameter estimates obtained by Boscovich. Figure 10.1 (b) is the same plot but with the modern least squares line.\nd=read.csv(\"../data/boscovich.csv\")\nplot(d$sin2Latitude,d$ArcLength, ylab=\"Arc Length\", xlab=expression(sin^2~(theta)), pch=16,ylim=c(56700,57450), xlim=c(-30,8590))\nabline(56751,0.06670097, lwd=3, col=\"red\")\nplot(d$sin2Latitude,d$ArcLength, ylab=\"Arc Length\", xlab=expression(sin^2~(theta)), pch=16,ylim=c(56700,57450), xlim=c(-30,8590))\nabline(lm(ArcLength~sin2Latitude, data=d), lwd=3, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Boscovich’s first attempt to estimate the parameters\n\n\n\n\n\n\n\n\n\n\n\n(b) Modern least squares approach\n\n\n\n\n\n\n\nFigure 10.1: Comparison of Boscovich’s first attempt to estimate the parameters and the modern least squares approach\n\n\n\nThis is a very reasonable approach! However, Boscovich was not satisfied with this approach and he wanted to find a better way to combine the data. He was looking for a method that would minimize the sum of the absolute deviations between the data points and the fitted curve. Two years later he developed a pioneering technique called “least absolute deviations,” which revolutionized data analysis. This method, distinct from the prevalent “least squares” approach, minimized the sum of absolute deviations between data points and the fitted curve, proving particularly effective in handling measurement errors and inconsistencies.\nArmed with his meticulous measurements and innovative statistical analysis, Boscovich not only confirmed the oblate spheroid shape of the Earth but also refined its dimensions. His calculations yielded a more accurate value for the Earth’s equatorial radius and the flattening at the poles, providing crucial support for Newton’s theory of gravitation, which predicted this very shape.\n\nMotivated by the analysis of planetary orbits and determining the shape of the Earth, later in 1805, Adrien-Marie Legendre (1752 - 1833) published the first clear and concise explanation of the least squares method in his book “Nouvelles m{'e}thodes pour la d{'e}termination des orbites des cometes”. The method of least squares is a powerful statistical technique used today fit a mathematical model to a set of data points. Its goal is to find the best-fitting curve that minimizes the sum of the squared distances (a.k.a residuals) between the curve and the actual data points. Compared to the approach proposed by Boscovich, the least squares method is less robust to measurement errors and inconsistencies. However, from computational point of view, it is more efficient and there are various algorithms exist for efficient calculation of curve parameters. This computational efficiency is crucial for modern data analysis, where datasets can be massive and complex, making least squares a fundamental tool in statistics and data analysis, offering a powerful and widely applicable approach to data fitting and model building.\nLegendre provided a rigorous mathematical foundation for the least squares method, demonstrating its theoretical underpinnings and proving its optimality under certain conditions. This mathematical basis helped establish the credibility and legitimacy of the method, paving the way for its wider acceptance and application. Legendre actively communicated his ideas and collaborated with other mathematicians, such as Carl Friedrich Gauss (1777-1855), who also contributed significantly to the development of the least squares method. While evidence suggests Gauss used the least squares method as early as 1795, his formal publication came later than Legendre’s in 1809. Despite the delay in publication, Gauss independently discovered the method and applied it to various problems, including celestial mechanics and geodesy. He developed efficient computational methods for implementing the least squares method, making it accessible for practical use by scientists and engineers. While Legendre’s clear exposition and early publication brought the least squares method to the forefront, Gauss’s independent discovery, theoretical development, practical applications, and contributions to computational methods were equally crucial in establishing the method’s significance and impact. Both mathematicians played vital roles in shaping the least squares method into the powerful statistical tool it is today.\nAnother French polymath Pierre-Simon Laplace (1749 - 1827) extended the methods of Boscovich and showed that the curve fitting problem could be solved by ordering the candidate slopes and finding the weighted median. Besides that Laplace made fundamental contributions to probability theory, developing the Bayesian approach to inference. Most of the work of Laplace was in the field of celestial mechanics, where he used data from astronomical observations to develop mathematical models and equations describing the gravitational interactions between celestial bodies. His analytical methods and use of observational data were pioneering in the field of celestial mechanics. Furthermore, developed methods for estimating population parameters from samples, such as the mean and variance and pioneered the use of random sampling techniques, which are essential for ensuring the validity and generalizability of statistical inferences. These contributions helped lay the foundation for modern sampling theory and survey design, which are crucial for conducting reliable and representative studies. Overall, Laplace’s contributions to data analysis were profound and enduring. His work in probability theory, error analysis, sampling methods, and applications significantly advanced the field and laid the groundwork for modern statistical techniques. He also played a crucial role in promoting statistical education and communication, ensuring that these valuable tools were accessible and utilized across various disciplines.\nBoscovich used what we call today a linear regression analysis. This type of analysis relies on the assumption that the relationship between the independent (arc length) and dependent (sinus squared of the latitude) variables is linear. Francis Galton was the person who coined the term “regression” in the context of statistics. One of the phenomena he studied was the relationship between the heights of parents and their children. He found that the heights of children tended to “regress” towards the average height of the population, which led him to use the term “regression” to describe this phenomenon. Galton promoted a data-driven approach to research that continues to shape statistical practice today. Furthermore, he introduced the concept of quantiles, which divide a population into equal-sized subpopulations based on a specific variable. This allowed for a more nuanced analysis of data compared to simply considering the mean and median. He also popularized the use of percentiles, which are specific quantiles used to express the proportion of a population below a certain value.\nGalton used regression analysis to show that the the offspring of exceptionally large or small seed size of sweet peas from tended to be closer to the average size. He also used it for family studies and investigated the inheritance of traits such as intelligence and talent. He used regression analysis to assess the degree to which these traits are passed down from parents to offspring.\nGalton’s overall approach to statistics was highly influential. He emphasized the importance of quantitative analysis, data-driven decision-making, and empirical research, which paved the way for modern statistical methods and helped to establish statistics as a recognized scientific discipline.\n\n\n\n\nAltić, Mirela Slukan. 2013. “Exploring Along the Rome Meridian: Roger Boscovich and the First Modern Map of the Papal States.” In History of Cartography: International Symposium of the ICA, 2012, 71–89. Springer.\n\n\nStigler, Stephen M. 1981. “Gauss and the Invention of Least Squares.” The Annals of Statistics, 465–74.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unreasonable Effectiveness of Data</span>"
    ]
  },
  {
    "objectID": "11-pattern.html",
    "href": "11-pattern.html",
    "title": "11  Pattern Matching",
    "section": "",
    "text": "11.1 Why Pattern Matching?\n“Prediction is very difficult, especially about the future. Niels Bohr, Danish physicist and Nobel laureate”\nThe history of data analysis is closely intertwined with the development of pattern matching techniques. The ability to identify and understand patterns in data has been crucial for scientific discoveries, technological advancements, and decision-making. From the early days of astronomy to modern machine learning, pattern matching has played a pivotal role in advancing our understanding of the world around us. This chapter explores the key concepts of pattern matching, its historical development, and its impact on data analysis.\nData science involves two major steps, collection and cleaning of data and building a model or applying an algorithm. In this chapter we present the process of building predictive models. To illustrate the process think of your data as being generated by a black box on which a set of input variables \\(x\\) go through the box and generate an output variable \\(y\\).\nFor Gauss, Laplace and many other scientist, the main problem was the problem of estimating parameters, while the relationship between the variables was known and was usually linear, like in the shape of the earth example of multiplicative, e.g. Newton’s second law \\(F = ma\\). However, in many cases, the relationship between the variables is unknown and cannot be described by a simple mathematical model. Halevy, Norvig, and Pereira (2009) discuss the problem of human behavior and natural languages. Neither can be described by a simple mathematical model.\nThis is case, the pattern matching approach is a way to use data to find those relations. In data analysis, pattern matching is the process of identifying recurring sequences, relationships, or structures within a dataset. It’s like looking for a specific puzzle piece within a larger picture. By recognizing these patterns, analysts can gain valuable insights into the data, uncover trends, make predictions, and ultimately improve decision-making. Sometimes initial pattern matching analysis leads to a scientific discovery. Consider a case of mammography and early pattern matching.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#why-pattern-matching",
    "href": "11-pattern.html#why-pattern-matching",
    "title": "11  Pattern Matching",
    "section": "",
    "text": "Example 11.1 (Mammography and Early Pattern Matching) The use of mammograms for breast cancer detection relied on simple pattern matching in its initial stages. Radiologists visually examined the X-ray images for specific patterns indicative of cancer, such as: dense areas of tissue appearing different from surrounding breast tissue (a.k.a masses) and small white spots of calcium deposits called microcalcifications. These patterns were associated with early-stage cancer and could be easily missed by visual inspection alone.\nRadiologists relied on their expertise and experience to identify these patterns and distinguish them from normal breast tissue variations. This process was subjective and prone to errors, particularly with subtle abnormalities or in dense breasts. Subtle abnormalities, especially in dense breasts, could be easily missed using visual assessment alone. Despite these limitations, pattern matching played a crucial role in the early detection of breast cancer, saving countless lives. It served as the foundation for mammography as a screening tool.\nAlbert Solomon, a German surgeon, played a pivotal role in the early development of mammography (Nicosia et al. (2023)). His most significant contribution was his 1913 monograph, “Beitr{\"a}ge zur Pathologie und Klinik der Mammakarzinome” (Contributions to the Pathology and Clinic of Breast Cancers). In this work, he demonstrated the potential of X-ray imaging for studying breast disease. He pioneered the use of X-rays, he compared surgically removed breast tissue images with the actual tissue and was able to identify characteristic features of cancerous tumors, such as their size, shape, and borders. He was one of the first to recognize the association between small calcifications appearing on X-rays and breast cancer.\nPresence of calcium deposits is correlated with brest cancer and is still prevailing imaging biomarkers for its detection. Although discovery of the deposit-cancer asosciation induced scientific discoveries, the molecular mechanisms that leads to the formation of these calcium deposits, as well as the significance of their presence in human tissues, have not been completely understood (Bonfiglio et al. (2021)).\n\n\n11.1.1 Richard Feynman on Pattern Matching and Chess\nRichard Feynman, the renowned physicist, was a strong advocate for the importance of pattern matching and its role in learning and problem-solving. He argued that in many scientific discoveries, start with pattern matching. He emphasized that experts in any field, whether it’s chess, science, or art, develop a strong ability to identify and understand relevant patterns in their respective domains.\nHe often used the example of chess to illustrate this concept (Feynman (n.d.)). Feynman argued that a skilled chess player doesn’t consciously calculate every possible move. Instead, they recognize patterns on the board and understand the potential consequences of their actions. For example, a chess player might recognize that having a knight in a certain position is advantageous and will lead to a favorable outcome. This ability to identify and understand patterns allows them to make quick and accurate decisions during the game. Through playing and analyzing chess games, players develop mental models that represent their understanding of the game’s rules, strategies, and potential patterns. These mental models allow them to anticipate their opponent’s moves and formulate effective responses.\nHe emphasized that this skill could be transferred to other domains, such as scientific research, engineering, and even everyday problem-solving.\nHere is a quote from his interview\n\n\n\n\n\n\nRichard Feynman\n\n\n\nLet’s say a chess game. And you don’t know the rules of the game, but you’re allowed to look at the board from time to time, in a little corner, perhaps. And from these observations, you try to figure out what the rules are of the game, what [are] the rules of the pieces moving.\nYou might discover after a bit, for example, that when there’s only one bishop around on the board, that the bishop maintains its color. Later on you might discover the law for the bishop is that it moves on a diagonal, which would explain the law that you understood before, that it maintains its color. And that would be analogous we discover one law and later find a deeper understanding of it.\nAh, then things can happen–everything’s going good, you’ve got all the laws, it looks very good–and then all of a sudden some strange phenomenon occurs in some corner, so you begin to investigate that, to look for it. It’s castling–something you didn’t expect …..\nAfter you’ve noticed that the bishops maintain their color and that they go along on the diagonals and so on, for such a long time, and everybody knows that that’s true; then you suddenly discover one day in some chess game that the bishop doesn’t maintain its color, it changes its color. Only later do you discover the new possibility that the bishop is captured and that a pawn went all the way down to the queen’s end to produce a new bishop. That could happen, but you didn’t know it.\n\n\nIn an interview on Artificial General Intelligence (AGI), he compares human and machine intelligence\n\n\n\n\n\n\nRichard Feynman\n\n\n\nFirst of all, do they think like human beings? I would say no and I’ll explain in a minute why I say no. Second, for “whether they be more intelligent than human beings: to be a question, intelligence must first be defined. If you were to ask me are they better chess players than any human being? Possibly can be , yes ,”I’ll get you some day”. They’re better chess players than most human beings right now!\n\n\nBy 1996, computers had become stronger than GMs. With the advent of deep neural networks in 2002, Stockfish15 is way stronger. A turning point on our understanding of AI algorithms was AlphaZero and Chess\nAlphaGo coupled with deep neural networks and Monte Carlo simulation provided a gold standard for chess. AlphaZero showed that neural networks can self-learn by competing against itself. Neural networks are used to pattern match and interpolate both the policy and value function. This implicitly performs “feature selection”. Whilst humans have heuristics for features in chess, such as control center, king safety and piece development, AlphaZero “learns” from experience. With a goal of maximizing the probability of winning, neural networks have a preference for initiative, speed and momentum and space over minor material such as pawns. Thus reviving the old school romantic chess play.\nFeynman discusses how machines show intelligence:\n\n\n\n\n\n\nRichard Feynman\n\n\n\nWith regard to the question of whether we can make it to think like [human beings], my opinion is based on the following idea: That we try to make these things work as efficiently as we can with the materials that we have. Materials are different than nerves, and so on. If we would like to make something that runs rapidly over the ground, then we could watch a cheetah running, and we could try to make a machine that runs like a cheetah. But, it’s easier to make a machine with wheels. With fast wheels or something that flies just above the ground in the air. When we make a bird, the airplanes don’t fly like a bird, they fly but they don’t fly like a bird, okay? They don’t flap their wings exactly, they have in front, another gadget that goes around, or the more modern airplane has a tube that you heat the air and squirt it out the back, a jet propulsion, a jet engine, has internal rotating fans and so on, and uses gasoline. It’s different, right?\nSo, there’s no question that the later machines are not going to think like people think, in that sense. With regard to intelligence, I think it’s exactly the same way, for example they’re not going to do arithmetic the same way as we do arithmetic, but they’ll do it better.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#correlations",
    "href": "11-pattern.html#correlations",
    "title": "11  Pattern Matching",
    "section": "11.2 Correlations",
    "text": "11.2 Correlations\nArguable, the simplest form of pattern matching is correlation. Correlation is a statistical measure that quantifies the strength of the relationship between two variables. It is a measure of how closely two variables move in relation to each other. Correlation is often used to identify patterns in data and determine the strength of the relationship between two variables. It is a fundamental statistical concept that is widely used in various fields, including science, engineering, finance, and business.\nLet’s consider the correlation between returns on Google stock and S&P 500 stock index. The correlation coefficient is a measure of the strength and direction of the linear relationship between two variables. It is a number between -1 and 1.\n\nExample 11.2 (Google Stock Returns) Figure 11.1 shows the scattershot of Google and S&P 500 daily returns\n\ngoog = read.csv(\"../data/GOOG2019.csv\") \nrgoog = goog$Adj.Close[2:251]/goog$Adj.Close[1:250] - 1 \nsp = read.csv(\"../data/SP2019.csv\");   rsp = sp$Adj.Close[2:251]/sp$Adj.Close[1:250] - 1 \nplot(rgoog, rsp, col=\"lightblue\", pch=21, bg=\"grey\", xlab=\"GOOG return\", ylab=\"SP500 return\") \n\n\n\n\n\n\n\nFigure 11.1: Scattershot of Google and S&P 500 daily returns\n\n\n\n\n\nLet’s calculate the covariance and correlation between the daily returns of the Google stock and S&P 500.\n\nvar_goog = mean((rgoog - mean(rgoog))^2) \nvar_sp = mean((rsp - mean(rsp))^2) \ncov = mean((rgoog - mean(rgoog))*(rsp - mean(rsp))); print(cov) \n\n 8e-05\n\ncor = cov/(sqrt(var_goog)*sqrt(var_sp)); print(cor)\n\n 0.67",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#pattern-matching-to-data",
    "href": "11-pattern.html#pattern-matching-to-data",
    "title": "11  Pattern Matching",
    "section": "11.3 Pattern Matching to Data",
    "text": "11.3 Pattern Matching to Data\nThere are plenty of engineering applications. Given the availability of sensors, predictive maintenance became a widespread approach. By predicting when equipment is likely to fail, engineers can take proactive steps to prevent failures and reduce downtime. Another example is traffic forecasting, which is used by drivers and engineers to plan future road and bridge construction projects. Prediction and forecasting are also used in weather forecasting, resource management, and many other applications.\nMany problems in machine learning involve an output \\(y_i\\) with \\(x_i\\) a high-dimensional input variable. The objective function takes the form, with \\(\\theta=(\\theta_1, \\ldots,  \\theta_p)\\), \\[\nl(\\theta) =\\sum_{i=1}^n l(y_i, f_{\\theta} (x_i)) +\\lambda \\sum_{j=1}^p \\phi(\\theta_j),\n\\] Deep learning constructs as a composition (rather than the traditional additive) of semi-affine functions. As such the optimization problem of training a deep learner involves a highly nonlinear objective function. Stochastic gradient descent (SGD) is a popular tool based on back-propagation (a.k.a. the chain rule). Our goal is to show how data augmentation techniques can be seamlessly applied in this context too and provide efficiency gains. The goal then is to find a maximum a posteriori (MAP) mode from a probabilistic model defined by the conditional distributions \\[\\begin{align*}\n&p(y | \\theta)\\propto \\exp\\{- l (y, f_{\\theta}(x))\\},\\; \\;  p(\\theta)\\propto \\exp\\{-\\lambda\\phi(\\theta)\\},\\\\\n&p(\\theta | y)=\\frac{p(y | \\theta)p(\\theta)}{p(y)} \\propto  \\exp\\{- l(y, f_{\\theta}(x))-\\lambda \\phi(\\theta)\\}.\n\\end{align*}\\] Here \\(p(\\theta)\\) can be interpreted as a prior probability distribution and the log-prior as the regularization penalty.\nTraditional modeling culture employs statistical models characterized by single-layer transformations, where the relationship between input variables and output is modeled through direct, interpretable mathematical formulations. These approaches typically involve linear combinations, additive structures, or simple nonlinear transformations that maintain analytical tractability and statistical interpretability. The list of widely used models includes:\n\nGeneralized Linear Models (GLM) \\(y = f^{-1}(\\beta^T x)\\)\nGeneralized Additive Models (GAM) \\(y = \\beta_0 + f_1(x_1) + \\dots + f_k(x_k)\\)\nPrincipal Component Regression (PCR) \\(y = \\beta^T (W x),\\quad W \\in \\mathbb{R}^{k \\times p},\\quad k &lt; p\\)\nSliced Inverse Regression (SIR) \\(y = f(\\beta_1^T x,\\, \\beta_2^T x,\\, \\ldots,\\, \\beta_k^T x,\\, \\epsilon)\\)\n\nIn contrast to these traditional single-layer approaches, Deep Learning employs sophisticated high-dimensional multi-layer neural network architectures that can capture complex, non-linear relationships in data through hierarchical feature learning. Each layer transforms the input data through by applying an affine transformation and a non-linear activation function. The depth and complexity of these architectures allow deep learning models to automatically discover intricate patterns and representations from raw input data, making them particularly effective for tasks involving high-dimensional inputs such as image recognition, natural language processing, and complex time series analysis. Unlike traditional statistical models that rely on hand-crafted features, deep learning models learn hierarchical representations directly from the data, with early layers capturing simple features (like edges in images) and deeper layers combining these into more complex, abstract representations.\nWe wish to find map \\(f\\) such that \\[\\begin{align*}\ny &= f ( x ) \\\\\ny &=  f ( x_1 , \\ldots , x _p )\n\\end{align*}\\]\nEssentiall, the goal is to perform the pattern matching, also known as nonparametric regression. It involves finding complex relationships in data without assuming a specific functional form. In deep learning, we use composite functions rather than additive functions. We write the superposition of univariate functions as \\[\nf = f_1 \\circ \\ldots \\circ f_L   \\; \\; \\text{versus}  \\; \\; f_1 +  \\ldots + f_L\n\\] where the composition of functions creates a hierarchical structure. The optimization process that finds the parametrs of those functions relies on Stochastic Gradient Descent (SGD), which iteratively updates parameters to minimize the loss function. Back-propagation, which is essentially the chain rule from calculus, is used to efficiently compute gradients through the network layers.\nA typical prediction problem involves building a rule that maps observed inputs \\(x\\) into the output \\(y\\). The inputs \\(x\\) are often called predictors, features, or independent variables, while the output \\(y\\) is often called the response or dependent variable. The goal is to find a predictive rule \\[\ny = f(x).\n\\]\nThe map \\(f\\) can be viewed as a black box which describes how to find the output \\(y\\) from the input \\(x\\). One of the key requirement of \\(f\\) is that we should be able to efficiently find this function using an algorithm. In the simple case \\(y\\) and \\(x\\) are both univariate (scalars) and we can view the map as\n\n\nThe goal of machine learning is to reconstruct this this map from observed data. In a multivariate setting \\(x = (x_1,\\ldots,x_p)\\) is a list of \\(p\\) variables. This leads to a model of the form \\(y = f(x_1,\\ldots,x_p)\\). There are a number of possible goals of analysis, such as estimation, inference or prediction. The main one being prediction.\nThe prediction task is to calculate a response that corresponds to a new feature input variable. Example of af an inference is the task of establishing a causation, with the goal of extracting information about the nature of the black box association of the response variable to the input variables.\nIn either case, the goal is to use data to find a pattern that we can exploit. The pattern will be ``statistical” in its nature. To uncover the pattern we use a training dataset, denoted by \\[\nD = (y_i,x_i)_{i=1}^n\n\\]\nwhere \\(x_i\\) is a set of \\(p\\) predictors ans \\(y_i\\) is response variable. Prediction problem is to use a training dataset \\(D\\) to design a rule that can be used for predicting output values \\(y\\) for new observations \\(x\\).\nLet \\(f(x)\\) be predictor of \\(y\\), we will use notation \\[\n\\hat{y} = f(x).\n\\]\nTo summarize, we will use the following notation.\n\n\n\n\\(y\\)\noutput variable (response/outcome)\n\n\n\\(x\\)\ninput variable (predictor/covariate/feature)\n\n\n\\(f(x)\\)\npredictive rule\n\n\n\\(\\hat y\\)\npredicted output value\n\n\n\nWe distinguish several types of input or output variables. First, binary variables that can only have two possible values, e.g. yes/no, left/right, 0/1, up/down, etc. A generalization of binary variable is a categorical variable that can take a fixed number of possible values, for example, marriage status. Additionally, some of the categorical variable can have a natural order to them, for example education level or salary range. Those variables are called ordinal. Lastly, the most common type of a variable is quantitative which is described by a real number.\nDepending on the type of the output variable, there are three types of prediction problems. When \\(y\\) is binary \\(y\\in \\{0,1\\}\\) or categorical, \\(y\\in \\{0,\\ldots,K\\}\\), for \\(K\\) possible categories, the prediction problem is called classification. When \\(y\\) is any number \\(y \\in R\\), this is known as a regression. Finally, when \\(y\\) is ordinal, this problem known as ranking.\nThere are several simple predictive rules we can use to predict the output variable \\(y\\). For example, in the case of regression problem, the simplest rule is to predict the average value of the output variable. This rule is called the mean rule and is defined as \\[\nf(x) = \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i.\n\\]\nNotice, this model does not depend on the input variable \\(x\\) and will predict the same value for all observations. This rule is simple and easy to implement, but it is not very accurate. A more sophisticated rule is the nearest neighbor rule. This rule predicts the output value \\(y\\) for a new observation \\(x\\) by finding the closest observation in the training dataset and using its output value. The nearest neighbor rule is defined as \\[\nf(x) = y_{i^*},\n\\]\nwhere \\(i^* = \\arg\\min_{i=1,\\ldots,n} \\|x_i - x\\|\\) is the index of the closest observation in the training dataset. These two models represent two extreme cases of predictive rules: the mean rule is “stubborn” (it always predicts the same value) and the nearest neighbor rule is “flexible” (can be very sensitive to small changes in the inputs). Using the language of statistics the mean rule is of high bias and low variance, while the nearest neighbor rule is of low bias and high variance. Although those two rules are simple, they sometimes lead to useful models that can be used in practice. Further, those two models represent a trade-off between accuracy and complexity (a.k.a bias-variance trade-off). We will discuss this trade-off in more detail in the later section.\nThe mean model and nearest neighbor model belong to a class of so-called non-parametric models. The non-parametric models do not make explicit assumption about the form of the function \\(f(x)\\). In contrast, parametric models assume that the predictive rule \\(f(x)\\) belongs to a specific family of functions and each function in this family is defined by setting values of the parameters.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#prediction-accuracy",
    "href": "11-pattern.html#prediction-accuracy",
    "title": "11  Pattern Matching",
    "section": "11.4 Prediction Accuracy",
    "text": "11.4 Prediction Accuracy\nAfter we fit our model and find the optimal value of the parameter \\(\\theta\\), denoted by \\(\\hat \\theta\\), we need to evaluating the accuracy of a predictive model. It involves comparing the model’s predictions to actual outcomes. We can simply use the value of the loss function from the training step to evaluate model’s predictive power. However, this only tells us how well the model fits the training data. It doesn’t tell us how well the model will perform on unseen data. To evaluate the model’s performance on unseen data, we need to use a different approach.\nThe most common approach is to split the data into training and test sets. The training set is used to train the model, while the test set is used to evaluate its performance. This approach is known as the train-test split. It is a simple and effective way to evaluate how well model predicts unseen for unseen inputs.\nAnother approach is to use cross-validation. It involves splitting the data into smaller subsets and using them to train and test the model multiple times. When our sample size is small, this allows for a more robust estimate of the model’s performance than simply splitting the data into a single training and test set. For small data sets, simple train-test split approach will be sensitive to choice of test samples, thus the estimated predicted performance will be unstable (high variance). Cross-validation helps to reduce this variance by averaging the performance across multiple folds. This makes the performance estimate more robust and less sensitive to the choice of test samples.\nCross-validation involves several steps:\n\nSplit the data: The data is randomly divided into \\(k\\) equal-sized chunks (folds).\nTrain and test the model: For each fold, the model is trained on \\(k-1\\) folds and tested on the remaining fold. This process is repeated \\(k\\) times, ensuring each fold is used for testing once.\nEvaluate the model: The performance of the model is evaluated on each fold using a chosen metric, such as accuracy, precision, recall, or F1 score.\nReport the average performance: The average of the performance metrics across all k folds is reported as the final estimate of the model’s performance.\n\nA common choice for \\(k\\) is 5 or 10. When \\(K=n\\), this is known as leave-one-out cross-validation. This method can be computationally expensive but is less likely to overfit the data. Stratified cross-validation ensures that each fold contains approximately the same proportion of each class as in the entire dataset. This is important for imbalanced datasets where one class is significantly larger than the others.\nNotice, that cross-validation requires re-training the model multiple times, which can be computationally expensive. Thus, for large datasets, we typically prefer simple train-test split. However, for small datasets, cross-validation can provide a more robust estimate of the model’s performance.\nEither method is limited to evaluating the model’s performance on data that is available to the modeler. What if we start using our model on data that is different from the training and test sets? Unlike in physics, when a model represents a law that is universal, in data science, we are dealing with data that is generated by a process that is not necessarily universal. For example, if we are building a model to predict the price of a house, we can train and test the model on data from a specific city. However, if we start using the model to predict the price of a house in a different city, the model might not perform as well. This is because the data from the new city might be different from the data used to train and test the model. This is known as the problem of generalization. It refers to the ability of a model to perform well on data that is different from the training and test sets.\n\n11.4.1 Evaluation Metrics for Regression\nThere are several metrics that can be used to evaluate the performance of regression models. We can simply use the same function as we use for fitting the model, e.g. least squares \\[\n\\text{MSE} = \\dfrac{1}{m}\\sum_{i=1}^n (y_i -\\hat y_i)^2,\n\\] here \\(\\hat y_i\\) is the predicted value of the i-th data point by the model \\(\\hat y_i = f(x_i,\\hat\\theta)\\) and \\(m\\) is the total number of data points used for the evaluation. This metric is called the Mean Squared Error (MSE). It is the average squared difference between the actual and predicted values. Lower MSE indicates better model performance, as it means the model’s predictions are closer to the actual values.\nA slight variation of this metric is Root Mean Squared Error (RMSE). This is the square root of MSE and is also commonly used due to its units being the same as the target variable. \\[\n\\text{RMSE} = \\sqrt{\\text{MSE}}.\n\\] However, MSE is sensitive to outliers, as it squares the errors, giving more weight to large errors. This can lead to misleading results when the data contains outliers.\nMedian Absolute Error (MAE) solves the sensetivity to the outliers problem. It is the median of the absolute errors, providing a more robust measure than MAE for skewed error distributions \\[\n\\text{MAE} = \\dfrac{1}{m}\\sum_{i=1}^n |y_i -\\hat y_i|.\n\\] A variation of it is the Mean Absolute Percentage Error (MAPE), which is the mean of the absolute percentage errors \\[\n\\text{MAPE} = \\dfrac{1}{m}\\sum_{i=1}^n \\left | \\dfrac{y_i -\\hat y_i}{y_i} \\right |.\n\\]\nAlternative way to measure the predictive quility is to use the coefficient of determination, also known as the R-squared value, which measures the proportion of variance in the target variable that is explained by the model. Higher R-squared indicates better fit. However, R-squared can be misleading when comparing models with different numbers of features. R-squared is defined as follows \\[\nR^2 = 1 - \\dfrac{\\sum_{i=1}^n (y_i -\\hat y_i)^2}{\\sum_{i=1}^n (y_i -\\bar y_i)^2},\n\\] where \\(\\bar y_i\\) is the mean of the target variable. R-squared is a relative measure of fit, so it can be used to compare different models. However, it is not an absolute measure of fit, so it cannot be used to determine whether a model is good or bad. It is also sensitive to the number of features in the model, so it cannot be used to compare models with different numbers of features.\nFinally, we can use graphics to evaluate the model’s performance. For example, we can scatterplot the actual and predicted values of the target variable to visually compare them. We can also plot the histogram of a boxplot of the residuals (errors) to see if they are normally distributed.\n\n\n11.4.2 Evaluation Metrics for Classification\nAccuracy is the most fundamental metric used to evaluate models. It is defined as the ratio of the number of correct predictions to the total number of predictions. The formula is given by \\[\\text{Accuracy} = \\frac{\\text{TP+TN}}{\\text{TP+TN+FP+FN}},\\] where TP, TN, FP, and FN are the numbers of true positives, true negatives, false positives, and false negatives, respectively. However, it can be misleading for imbalanced datasets where one class is significantly larger than others. For example, if 95% of the data belongs to one class, a model that always predicts this class will be 95% accurate, even though it’s not very useful.\nA more comprehensive understanding of model performance can be achieved by calculaitng the sensitivity (a.k.a precision) and specificity (a.k.a. recall) as well as confusion matrix discussed in Section 2.4. The confusion matrix is\n\n\n\nActual/Predicted\nPositive\nNegative\n\n\n\n\nPositive\nTP\nFN\n\n\nNegative\nFP\nTN\n\n\n\nPrecision measures the proportion of positive predictions that are actually positive. It is useful for evaluating how good the model is at identifying true positives. Recall measures the proportion of actual positives that are correctly identified by the model. It is useful for evaluating how good the model is at not missing true positives.\nThen we can use those to calculate F1 Score which is is a harmonic mean of precision and recall, providing a balanced view of both metrics. Higher F1 score indicates better overall performance. If misclassifying certain instances is more costly than others, weighted metrics account for these different costs. For imbalanced datasets, metrics like F1 score or balanced accuracy are important to avoid misleading interpretations.\nSometimes, we use multiple metrics to get a comprehensive assessment of the model’s performance. Additionally, consider comparing the model’s performance to a baseline model or other existing models for the same task. Sometimes, it is hard to beat a “coin flip” classification model, when the model predicts the class randomly with equal probability. In regression, a simple baseline model is \\(f(x_i) = \\bar y\\), which is the mean of the target variable.\n\n\n11.4.3 Some Examples of Prediction Problems\nPrediction and forecasting is most frequent problem in data analysis and the most common approach to solve the prediction problem is via the pattern matching. Prediction and forecasting are two closely related concepts that are often used interchangeably. In business and engineering the main motivation for prediction and forecasting is to make better decisions. In science, the main motivation is to test and validate theories. Prediction and forecasting help to identify trends and patterns in historical data that would otherwise remain hidden. This allows analysts to make informed decisions about the future based on what they know about the past. By using prediction models, analysts can identify potential risks and opportunities that may lie ahead. This information can then be used to develop proactive strategies to mitigate risks and capitalize on opportunities. In many business applications the concern is improving efficiency of a system. For example to improve logistic chains and to optimally allocate resources, we need to forecast demand and supply and to predict the future prices of the resources. By predicting future sales, businesses can better plan their marketing and sales efforts. This can lead to increased sales and profitability. Prediction and forecasting can be used to identify and mitigate potential risks, such as financial losses, supply chain disruptions, and operational failures.\n\nExample 11.3 (Obama Elections) Elections 2012: Bayes and Nate Silver\n\nlibrary(plyr)\n# Source: \"http://www.electoral-vote.com/evp2012/Pres/pres_polls.csv\"\nelection.2012 = read.csv(\"../data/pres_polls.csv\")\n# Remove a pollster: elect2012 &lt;- election.2012[!grepl('Rasmussen', election.2012$Pollster),]\nelect2012 &lt;- election.2012\n# Aggregrate the data\nelect2012 &lt;- ddply(elect2012, .(state), subset, Day == max(Day))\nelect2012 &lt;- ddply(elect2012, .(state), summarise, R.pct = mean(GOP), O.pct = mean(Dem), EV = mean(EV))\n\nknitr::kable(elect2012[1:25,], caption = \"Election 2012 Data\",longtable=TRUE)\nknitr::kable(elect2012[26:51,], caption = \"Election 2012 Data\",longtable=TRUE)\n\n\n\n\nElection 2012 Data\n\n\nstate\nR.pct\nO.pct\nEV\n\n\n\n\nAlabama\n61\n38\n9\n\n\nAlaska\n55\n42\n3\n\n\nArizona\n54\n44\n11\n\n\nArkansas\n61\n37\n6\n\n\nCalifornia\n38\n59\n55\n\n\nColorado\n47\n51\n9\n\n\nConnecticut\n40\n58\n7\n\n\nD.C.\n7\n91\n3\n\n\nDelaware\n40\n59\n3\n\n\nFlorida\n49\n50\n29\n\n\nGeorgia\n53\n45\n16\n\n\nHawaii\n28\n71\n4\n\n\nIdaho\n65\n33\n4\n\n\nIllinois\n41\n57\n20\n\n\nIndiana\n54\n44\n11\n\n\nIowa\n47\n52\n6\n\n\nKansas\n60\n38\n6\n\n\nKentucky\n61\n38\n8\n\n\nLouisiana\n58\n41\n8\n\n\nMaine\n41\n56\n4\n\n\nMaryland\n37\n62\n10\n\n\nMassachusetts\n38\n61\n11\n\n\nMichigan\n45\n54\n16\n\n\nMinnesota\n45\n53\n10\n\n\nMississippi\n56\n44\n6\n\n\n\n\n\n\nElection 2012 Data\n\n\n\nstate\nR.pct\nO.pct\nEV\n\n\n\n\n26\nMissouri\n54\n44\n10\n\n\n27\nMontana\n55\n41\n3\n\n\n28\nNebraska\n61\n38\n5\n\n\n29\nNevada\n46\n52\n6\n\n\n30\nNew Hampshire\n46\n52\n4\n\n\n31\nNew Jersey\n41\n58\n14\n\n\n32\nNew Mexico\n43\n53\n5\n\n\n33\nNew York\n36\n63\n29\n\n\n34\nNorth Carolina\n51\n48\n15\n\n\n35\nNorth Dakota\n59\n39\n3\n\n\n36\nOhio\n48\n50\n18\n\n\n37\nOklahoma\n67\n33\n7\n\n\n38\nOregon\n43\n54\n7\n\n\n39\nPennsylvania\n47\n52\n20\n\n\n40\nRhode Island\n36\n63\n4\n\n\n41\nSouth Carolina\n55\n44\n9\n\n\n42\nSouth Dakota\n58\n40\n3\n\n\n43\nTennessee\n60\n39\n11\n\n\n44\nTexas\n57\n41\n38\n\n\n45\nUtah\n73\n25\n6\n\n\n46\nVermont\n31\n67\n3\n\n\n47\nVirginia\n48\n51\n13\n\n\n48\nWashington\n42\n56\n12\n\n\n49\nWest Virginia\n62\n36\n5\n\n\n50\nWisconsin\n46\n53\n10\n\n\n51\nWyoming\n69\n28\n3\n\n\n\n\n\n\nRun the Simulation and plot probabilities by state\n\nlibrary(MCMCpack)\nprob.Obama &lt;- function(mydata) {\n    p &lt;- rdirichlet(1000, 500 * c(mydata$R.pct, mydata$O.pct, 100 - mydata$R.pct - \n        mydata$O.pct)/100 + 1)\n    mean(p[, 2] &gt; p[, 1])\n}\nwin.probs &lt;- ddply(elect2012, .(state), prob.Obama)\nwin.probs$Romney &lt;- 1 - win.probs$V1\nnames(win.probs)[2] &lt;- \"Obama\"\nwin.probs$EV &lt;- elect2012$EV\nwin.probs &lt;- win.probs[order(win.probs$EV), ]\nrownames(win.probs) &lt;- win.probs$state\n\n\nlibrary(usmap)\nplot_usmap(data = win.probs, values = \"Obama\") + \n  scale_fill_continuous(low = \"red\", high = \"blue\", name = \"Obama Win Probability\", label = scales::comma) + theme(legend.position = \"right\")\n\n\n\n\nProbabilities of Obama winning by state\n\n\n\n\nWe use those probabilities to simulate the probability of Obama winning the election. First, we calculate the probability of Obama having &gt;270 EV or more\n\nsim.election &lt;- function(win.probs) {\n    winner &lt;- rbinom(51, 1, win.probs$Obama)\n    sum(win.probs$EV * winner)\n}\n\nsim.EV &lt;- replicate(10000, sim.election(win.probs))\noprob &lt;- sum(sim.EV &gt;= 270)/length(sim.EV)\noprob\n\n 0.97\n\n\n\nlibrary(lattice)\n# Lattice Graph\ndensityplot(sim.EV, plot.points = \"rug\", xlab = \"Electoral Votes for Obama\", \n    panel = function(x, ...) {\n        panel.densityplot(x, ...)\n        panel.abline(v = 270)\n        panel.text(x = 285, y = 0.01, \"270 EV to Win\")\n        panel.abline(v = 332)\n        panel.text(x = 347, y = 0.01, \"Actual Obama\")\n}, main = \"Electoral College Results Probability\")\n\n\n\n\n\n\n\n\nResults of recent state polls in the 2008 United States Presidential Election between Barack Obama and John McCain.\n\n# Source: LearnBayes library\n#| fig-height: 6\nelection.2008 = read.csv(\"../data/election2008.csv\")\ndata(election.2008)\nattach(election.2008)\n\n##  Dirichlet simulation\n\n\nprob.Obama = function(j)\n {\n p=rdirichlet(5000,500*c(M.pct[j],O.pct[j],100-M.pct[j]-O.pct[j])/100+1)\n mean(p[,2]&gt;p[,1])\n }\n\n## sapply function to compute Obama win prob for all states\n\nObama.win.probs=sapply(1:51,prob.Obama)\n\n##  sim.EV function\n\nsim.election = function()\n {\n winner = rbinom(51,1,Obama.win.probs)\n sum(EV*winner)\n }\n\nsim.EV = replicate(1000,sim.election())\n\n## histogram of simulated election\nhist(sim.EV,min(sim.EV):max(sim.EV),col=\"blue\",prob=T)\nabline(v=365,lwd=3)   # Obama received 365 votes\ntext(375,30,\"Actual \\n Obama \\n total\")\n\n\n\n\n\n\n\n\n\nHere are a few important considerations when building predictive models:\n1. Model Selection: Choosing the right model for the relationship between \\(x\\) and \\(y\\) is crucial and involves navigating a fundamental trade-off between model complexity and generalization ability. If the chosen model is too simple (e.g., linear regression when the true relationship is polynomial), it might underfit the data and fail to capture important relationships, leading to high bias and poor performance on both training and test data. Conversely, a model that is too complex (e.g., high-degree polynomials or deep neural networks with insufficient data) might overfit the data by memorizing training examples rather than learning the underlying pattern, resulting in excellent training performance but poor generalization to unseen examples. This problem becomes even more complex when dealing with non-linear relationships, high-dimensional data, or noisy data, where the optimal model complexity is not immediately obvious and may require systematic experimentation with different model architectures, regularization techniques, and hyperparameter tuning to find the right balance between capturing the true signal while avoiding noise.\n2. Overfitting and Underfitting: Overfitting occurs when the model fits the training data too closely, capturing not only the true underlying relationship but also random noise and idiosyncrasies specific to the training dataset. This phenomenon typically manifests when a model has too many parameters relative to the amount of training data available, allowing it to essentially “memorize” the training examples rather than learning the generalizable patterns. The model may achieve excellent performance metrics on the training data (low training error) but will perform poorly on new, unseen data (high generalization error). This is because the model has learned to recognize specific noise patterns in the training data that don’t exist in the broader population. Common signs of overfitting include a large gap between training and validation/test performance, or performance that improves on training data while degrading on validation data during training iterations.\nUnderfitting occurs when the model is too simple and fails to capture the true relationship between x and y, often due to insufficient model complexity or inadequate training. This can happen when using a model that is inherently too simple for the problem at hand (e.g., linear regression for a highly non-linear relationship), when the model hasn’t been trained for enough iterations, or when regularization is applied too aggressively. Underfitting results in poor performance on both training and test data, as the model lacks the capacity to learn the underlying patterns in the data. The model essentially misses important features or relationships that are necessary for accurate predictions. Unlike overfitting, underfitting typically shows similar poor performance across training, validation, and test sets, indicating that the model is not capturing the signal in the data regardless of the dataset.\n3. Data Quality and Quantity: The accuracy of predictions heavily relies on the quality and quantity of the available data. If the data is noisy, inaccurate, or incomplete, it can lead to misleading predictions. A sufficient amount of data is also crucial to ensure the model can learn the underlying relationship effectively. Insufficient data can result in underfitting and poor generalization.\nData quality issues can manifest in various forms, including missing values, inconsistent formatting, labeling errors, and biased sampling. These problems are particularly acute in machine learning applications where large volumes of labeled data are required for training. To address these challenges, companies have emerged that specialize in data quality improvement and annotation services.\nCompanies like Scale AI and Toloka provide platforms that help organizations improve data quality through human-in-the-loop annotation and validation processes. These platforms employ large networks of human annotators who can perform tasks such as image labeling, text classification, data validation, and quality control. Scale AI, for example, offers services for creating high-quality training datasets through human annotation, with built-in quality control mechanisms that include multiple annotators per task and consensus-based validation. Their platform can handle various data types including images, text, and video, making it suitable for computer vision, natural language processing, and other AI applications.\nToloka, similarly, provides a crowdsourcing platform that connects businesses with a global network of contributors who can perform data labeling, content moderation, and quality assessment tasks. Their platform includes quality control features such as skill-based routing, where tasks are assigned to annotators based on their demonstrated expertise, and dynamic overlap, where multiple workers verify the same data to ensure accuracy.\nThese platforms help address several key data quality challenges: they can identify and correct labeling errors through consensus mechanisms, handle missing data through targeted collection efforts, and ensure consistency in data formatting and annotation standards. By leveraging human expertise at scale, these services enable organizations to create more reliable training datasets, which in turn leads to better-performing machine learning models and more accurate predictions.\n4. Model Explainability: In many applications, it is crucial to understand how the model arrives at its predictions. This is particularly important in areas like healthcare or finance, where transparency and interpretability are essential. Some models, particularly complex ones like deep neural networks, can be difficult to interpret, making it challenging to understand the rationale behind their predictions. However, modern machine learning has developed several techniques to address this challenge and make complex models more interpretable.\nThe importance of explainability extends beyond mere curiosity about model behavior. In healthcare applications, doctors need to understand why a model recommended a particular diagnosis or treatment plan to make informed decisions and maintain trust in the system. A model that predicts a patient has a 90% chance of having cancer but cannot explain which symptoms or test results led to this conclusion would be of limited clinical value. Similarly, in financial services, regulators require explanations for credit decisions to ensure compliance with fair lending laws and to prevent discriminatory practices. When a loan application is denied, both the applicant and regulatory bodies need to understand the specific factors that influenced this decision.\nIn legal and compliance contexts, explainability becomes a legal requirement. The European Union’s General Data Protection Regulation (GDPR) includes a “right to explanation” that allows individuals to request information about automated decisions that affect them. This has created a legal imperative for organizations to develop explainable AI systems. In criminal justice applications, where AI systems might be used for risk assessment or sentencing recommendations, the stakes are particularly high. Judges, lawyers, and defendants all need to understand how these systems arrive at their conclusions to ensure fair and just outcomes.\nOne prominent approach is the use of interpretable surrogate models, such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations). These methods work by approximating the complex model’s behavior in the vicinity of a specific prediction using simpler, more interpretable models like linear regression or decision trees. LIME, for instance, creates local explanations by sampling points around the prediction of interest and fitting a linear model to explain the model’s behavior in that neighborhood. This allows us to understand which features contributed most to a particular prediction, even for complex models like deep neural networks.\nAnother powerful technique is attention mechanisms, which have become increasingly popular in natural language processing and computer vision. Attention mechanisms allow models to “focus” on specific parts of the input when making predictions, providing a form of built-in interpretability. For example, in image classification tasks, attention maps can highlight which regions of an image the model is focusing on when making its prediction, making it easier to understand the model’s decision-making process.\nGradient-based methods offer another approach to model interpretability. Techniques like Grad-CAM (Gradient-weighted Class Activation Mapping) use gradients to identify which parts of the input are most important for the model’s prediction. By computing the gradient of the model’s output with respect to the input features, these methods can create heatmaps that show which features or regions contributed most to the final prediction.\nFor tree-based models like random forests and gradient boosting machines, built-in feature importance measures provide natural interpretability. These methods can rank features based on their contribution to the model’s predictive performance, offering insights into which variables are most important for making predictions.\nModel distillation techniques represent another approach, where a complex model (the teacher) is used to train a simpler, more interpretable model (the student) that mimics the teacher’s behavior. The student model, being simpler, is easier to interpret while maintaining much of the teacher’s predictive performance.\nFinally, counterfactual explanations provide a different type of interpretability by showing what changes to the input would be needed to change the model’s prediction. For example, if a loan application is rejected, a counterfactual explanation might show that the application would have been approved if the applicant’s income were $10,000 higher or if their credit score were 50 points better.\nThese modern interpretability techniques have made it possible to understand and explain the behavior of even the most complex models, addressing the “black box” problem that has historically limited the adoption of advanced machine learning methods in critical applications where transparency is essential.\n5. Computational Cost: Training and using prediction models can be computationally expensive, especially for complex models with large datasets. This can limit their applicability in resource-constrained environments. Finding a balance between model complexity, accuracy, and computational cost is critical for practical applications.\nThe computational demands of machine learning models have been significantly addressed through the development of specialized hardware, particularly Graphics Processing Units (GPUs). Originally designed for rendering graphics in video games, GPUs have become essential for deep learning due to their parallel processing architecture. Unlike traditional Central Processing Units (CPUs) that process tasks sequentially, GPUs can perform thousands of mathematical operations simultaneously, making them ideal for the matrix multiplications and tensor operations that are fundamental to neural network training. This parallel processing capability has reduced training times from weeks to hours or even minutes for many deep learning models, democratizing access to advanced machine learning techniques.\nHowever, the computational cost challenge extends beyond just training to the deployment phase, where models need to run efficiently in production environments. This has led to the emergence of edge computing as a crucial solution. Edge computing involves processing data and running models closer to where the data is generated, rather than sending everything to centralized cloud servers. This approach offers several advantages for machine learning applications: reduced latency for real-time predictions, lower bandwidth costs by processing data locally, and improved privacy by keeping sensitive data on local devices.\nEdge computing is particularly important for applications requiring real-time decision making, such as autonomous vehicles, industrial IoT systems, and mobile applications. For example, a self-driving car cannot afford the latency of sending sensor data to a cloud server and waiting for predictions to return; it needs to process information and make decisions locally within milliseconds. Similarly, smart manufacturing systems use edge computing to monitor equipment and predict maintenance needs in real-time without the delays associated with cloud processing.\nQuantization and lower precision calculations have emerged as powerful techniques for reducing computational costs while maintaining model performance. Traditional neural networks use 32-bit floating-point numbers (FP32) for all calculations, which provides high precision but requires significant computational resources and memory. Quantization reduces the precision of these numbers, typically to 16-bit (FP16), 8-bit integers (INT8), or even 4-bit integers (INT4), dramatically reducing both memory usage and computational requirements. For example, converting from FP32 to INT8 can reduce memory usage by 75% and computational cost by 2-4x, while often maintaining acceptable accuracy levels. This is particularly valuable for deployment on edge devices with limited resources, such as smartphones, IoT devices, and embedded systems. Modern hardware, including specialized AI accelerators like Google’s Tensor Processing Units (TPUs) and NVIDIA’s Tensor Cores, are specifically designed to handle these lower precision calculations efficiently, further reducing the computational cost barrier.\nThe trade-offs between computational cost and model performance are becoming increasingly sophisticated. Techniques like model pruning, which removes unnecessary connections from neural networks, can create smaller, faster models. Knowledge distillation allows large, complex models to transfer their knowledge to smaller, more efficient models that can run on resource-constrained devices.\nThese developments have created a spectrum of deployment options, from powerful cloud-based systems that can run the most complex models to lightweight edge devices that can perform basic predictions locally. The choice depends on the specific requirements of the application, including latency requirements, accuracy needs, privacy concerns, and cost constraints. As hardware continues to improve and optimization techniques become more sophisticated, the computational cost barrier to deploying machine learning models continues to decrease, opening up new possibilities for AI applications in previously inaccessible domains.\n6. Ethical Considerations: Predictions can have significant real-world consequences, raising ethical concerns about bias, fairness, and potential misuse. It is crucial to consider the potential harms and unintended consequences of predictions and implement safeguards to mitigate them.\nThe ethical implications of predictive models have become increasingly prominent as these systems are deployed in critical domains such as healthcare, criminal justice, employment, and financial services. One of the most significant concerns is algorithmic bias, which can perpetuate or amplify existing societal inequalities. For example, facial recognition systems have been shown to have higher error rates for people of color, potentially leading to wrongful arrests or surveillance. Similarly, hiring algorithms trained on historical data may perpetuate gender or racial biases present in past hiring decisions, creating a feedback loop that reinforces discrimination.\nFairness in machine learning has emerged as a critical research area, with multiple definitions and approaches to ensure equitable treatment across different demographic groups. Statistical parity, equalized odds, and individual fairness are among the various fairness metrics that can be applied depending on the specific context and requirements of the application. However, achieving fairness often involves trade-offs with model accuracy, and different fairness definitions may conflict with each other, requiring careful consideration of which definition is most appropriate for a given use case.\nThe potential for misuse of predictive models is another significant concern. Models designed for legitimate purposes can be repurposed for harmful applications, such as using facial recognition for mass surveillance or employing predictive policing algorithms that disproportionately target certain communities. Additionally, the increasing sophistication of deepfake technology, which uses predictive models to generate realistic but fake images, videos, or audio, raises concerns about misinformation and manipulation.\nPrivacy concerns arise when predictive models require access to sensitive personal data. The collection, storage, and processing of personal information for training and deploying these models can violate individual privacy rights and create risks of data breaches. Differential privacy techniques, which add carefully calibrated noise to data or model outputs, have emerged as a promising approach to protect individual privacy while maintaining model utility.\nTransparency and accountability are essential for addressing ethical concerns. Organizations deploying predictive models must be able to explain their decisions and be held accountable for any harms that result. This includes maintaining audit trails, implementing human oversight mechanisms, and establishing clear procedures for addressing complaints or errors. The concept of “algorithmic impact assessments” has been proposed as a framework for evaluating the potential social impacts of automated decision-making systems before deployment.\nRegulatory frameworks are evolving to address these ethical challenges. The European Union’s General Data Protection Regulation (GDPR) includes provisions for automated decision-making and profiling, while various jurisdictions are developing specific regulations for AI systems. These regulations often require transparency, human oversight, and the ability to contest automated decisions.\nTechnical approaches to addressing ethical concerns include adversarial training to reduce bias, interpretability techniques to increase transparency, and robust testing procedures to identify potential harms before deployment. Regular monitoring and updating of deployed models is also crucial, as societal norms and legal requirements evolve over time.\nAddressing these challenges requires careful consideration of the specific problem, selection of appropriate techniques, and continuous evaluation and improvement of the prediction model. It also requires collaboration between technical experts, domain specialists, ethicists, and stakeholders to ensure that predictive models serve the public good while minimizing potential harms.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#prediction-vs-interpretation",
    "href": "11-pattern.html#prediction-vs-interpretation",
    "title": "11  Pattern Matching",
    "section": "11.5 Prediction vs Interpretation",
    "text": "11.5 Prediction vs Interpretation\nAs we have discussed at the beginning of this chapter the predictive rule can be used for two purposes: prediction and interpretation. The goal of interpretation is to understand the relationship between the input and output variables. The two goals are not mutually exclusive, but they are often in conflict. For example, a model that is good at predicting the target variable might not be good at interpreting the relationship between the input and output variables. A nice feature of a linear model is that it can be used for both purposes, unlike more complex predictive rules with many parameters that can be difficult to interpret.\nTypically the problem of interpretation requires a simpler model. We prioritize models that are easy to interpret and explain, even if they have slightly lower predictive accuracy. Also, evaluation metrics are different, we typically use coefficient of determination (R-squared) or p-values, which provide insights into the model’s fit and the significance of the estimated relationships.\nThe choice between using a model for prediction or interpretation depends on the specific task and desired outcome. If the primary goal is accurate predictions, a complex model with high predictive accuracy might be preferred, even if it is less interpretable. However, if understanding the underlying relationships and causal mechanisms is crucial, a simpler and more interpretable model might be chosen, even if it has slightly lower predictive accuracy. Typically interpretive models are used in scientific research, social sciences, and other fields where understanding the underlying causes and relationships is crucial.\nIn practice, it’s often beneficial to consider both prediction and interpretation when building and evaluating models. However, it is not unusual to build two different models, one for prediction and one for interpretation. This allows for a more nuanced analysis of the data and can lead to better insights than using a single model for both purposes.\nBreiman’s Two Cultures\nStatistical prediction problems are of great practical and theoretical interest. The deep learning predictor has a number of advantages over traditional predictors, including that\n\ninput data can include all data of possible relevance to the prediction problem at hand\nnonlinearities and complex interactions among input data are accounted for seamlessly\noverfitting is more easily avoided than traditional high dimensional procedures\nthere exists fast, scale computational frameworks (TensorFlow)\n\nLet \\(x\\) be a high dimensional input containing a large set of potentially relevant data. Let \\(y\\) represent an output (or response) to a task which we aim to solve based on the information in \\(x\\). Brieman [2000] summaries the difference between statistical and machine learning philosophy as follows.\n\n“There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.”\n\n\n“The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.”\n\n\n“Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.”\n\n\n\n\n\nBonfiglio, Rita, Annarita Granaglia, Raffaella Giocondo, Manuel Scimeca, and Elena Bonanno. 2021. “Molecular Aspects and Prognostic Significance of Microcalcifications in Human Pathology: A Narrative Review.” International Journal of Molecular Sciences 22 (120).\n\n\nFeynman, Richard. n.d. “Feynman :: Rules of Chess.”\n\n\nHalevy, Alon, Peter Norvig, and Fernando Pereira. 2009. “The Unreasonable Effectiveness of Data.” IEEE Intelligent Systems 24 (2): 8–12.\n\n\nNicosia, Luca, Giulia Gnocchi, Ilaria Gorini, Massimo Venturini, Federico Fontana, Filippo Pesapane, Ida Abiuso, et al. 2023. “History of Mammography: Analysis of Breast Imaging Diagnostic Achievements over the Last Century.” Healthcare 11 (1596).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html",
    "href": "12-theoryai.html",
    "title": "12  Theory of AI",
    "section": "",
    "text": "12.1 Penalty and Regularisation\nWe use observed input-output pairs \\((x_i,y_i)\\) to learn a function \\(f\\) that maps \\(x_i\\) to \\(y_i\\). The goal is to learn a function \\(f\\) that generalizes well to unseen data. We can measure the quality of a function \\(f\\) by its empirical risk, which is the expected loss of \\(f\\) on a new input-output pair \\((x,y)\\):\n\\[\nR(f) = \\sum_{i=1}^N  l(y_i,f(x_i)) + \\lambda \\phi(f)\n\\] where \\(l\\) is a loss function, \\(\\phi\\) is a regularization function, and \\(\\lambda\\) is a regularization parameter. The loss function \\(l\\) measures the difference between the output of the function \\(f\\) and the true output \\(y\\). The regularization function \\(\\phi\\) measures the complexity of the function \\(f\\). The regularization parameter \\(\\lambda\\) controls the tradeoff between the loss and the complexity.\nThe loss is simply a negative log-likelihood from a probabilistic model specified for the data generating process. For example, when \\(y\\) is numeric and \\(y_i \\mid x_i \\sim N(f(x_i),\\sigma^2)\\), we get the squared loss \\(l(y,f(x)) = (y-f(x))^2\\). When \\(y_i\\in \\{0,1\\}\\) is binary, we use the logistic loss \\(l(y,f(x)) = \\log(1+\\exp(-yf(x)))\\).\nThe Bayesian version of the risk is the Bayes risk, which is the expected loss of \\(f\\) on a new input-output pair \\((x,y)\\) under the posterior distribution of \\(f\\): \\[\nR(f) = \\int\\int\\int l(y,f(x)) p(y|x,f) p(f)dydfdx\n\\] where \\(p(y|x,f)\\) is the predictive posterior distribution of \\(y\\) given \\(x\\) and \\(f\\), and \\(p(f)\\) is the prior distribution of \\(f\\). When \\(f\\) is a parametric model parametrized by \\(\\theta\\), \\(p(f)\\) becomes the prior distribution of the parameters of the model \\(p(\\theta)\\).\nTo find the optimal predictor that minimizes this risk (the Bayes predictor), we focus on minimizing the expected loss for a specific input \\(x\\).\n\\[\n\\arg\\min_f \\int l(y,f(x)) p(f\\mid x,y) df\n\\]\nThere is a duality between using regularization term in optimisation problem and assuming a prior distribution over the parameters of the model \\(f\\). The regularization parameter \\(\\lambda\\) is related to the variance of the prior distribution. When \\(\\lambda=0\\), the function \\(f\\) is the maximum likelihood estimate of the parameters. When \\(\\lambda\\) is large, the function \\(f\\) is the prior mean of the parameters. When \\(\\lambda\\) is infinite, the function \\(f\\) is the prior mode of the parameters. When \\(\\lambda\\) is negative, the function \\(f\\) is the posterior mean of the parameters. When \\(\\lambda\\) is very negative, the function \\(f\\) is the posterior mode of the parameters.\nThe goal is to find a function \\(f\\) that minimizes the risk \\(R(f)\\). This is called the empirical risk minimization problem. Finding minimum is a difficult problem when the risk function \\(R(f)\\) is non-convex. In practice, we often use gradient descent to find a local minimum of the risk function \\(R(f)\\).\nThere are multiple ways to choose the penalty term \\(\\phi(f)\\). Sections below describe the most popular approaches.\nThe problem of finding a good model boils down to finding \\(\\phi\\) that minimize some form of Bayes risk for the problem at hand.\nThere are a number of commonly used penalty functions (a.k.a. log prior density). For example, the $ l^2$-norm corresponds to s normal prior. The resulting Bayes rule will take the form of a shrinkage estimator, a weighted combination between data and prior beliefs about the parameter. An $ l^1 $-norm will induce a sparse solution in the estimator and can be used an a variable selection operator. The $ l_0 $-norm directly induces a subset selection procedure.\nThe amount of regularisation $ $ gauges the trade-off between the compromise between the observed data and the initial prior beliefs.\nThere are two main approaches to finding a good model:\nThe posterior is given by Bayes rule \\[\np( \\theta | y ) = \\frac{ f( y | \\theta ) p( \\theta ) }{ m(y) } \\; \\; {\\rm where} \\; \\; m(y) = \\int f( y| \\theta ) p( \\theta ) d \\theta\n\\] Here $ m(y) $ is the marginal beliefs about the data. This can also be used to choose the amount of regularisation via the type II maximum likelihood estimator (MMLE) defined by \\[\n\\hat{\\tau} = \\arg \\max \\log m( y | \\tau )\n\\] where again $ m( y | ) = f( y | ) p( | ) $.\nFor example, in the normal-normal model, with $ $, we can integrate out the high dimensional $ $ and find $ m( y | ) $ in closed form as $ y_i N ( 0 , ^2 + ^2 ) $ \\[\nm( y | \\tau ) = ( 2 \\pi)^{-n/2} ( \\sigma^2 + \\tau^2 )^{- n/2}  \\exp \\left ( - \\frac{ \\sum y_i^2 }{ 2 ( \\sigma^2 + \\tau^2) }\n\\] The original JS estimator shrinks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want o shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS is \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), which serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate proper from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\nRather than having to perform high dimensional integration with the likes of MCMC etc, a common approach is to use a maximum a posteriori (MAP) estimator defined by \\[\n\\hat{\\theta} = \\arg \\max \\log p ( \\theta | y )\n\\] This can directly lead to sparsity as in the case of $ _1 $-norm optimisation.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#penalty-and-regularisation",
    "href": "12-theoryai.html#penalty-and-regularisation",
    "title": "12  Theory of AI",
    "section": "",
    "text": "Full Bayes: This approach places a prior distribution on the parameters and computes the full posterior distribution.\n\n\n\n\nRegularization Methods: These approaches add penalty terms to the objective function to control model complexity. Common examples include ridge regression (L2 penalty), lasso regression (L1 penalty), and elastic net (combination of L1 and L2 penalties).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#normal-means-problem",
    "href": "12-theoryai.html#normal-means-problem",
    "title": "12  Theory of AI",
    "section": "12.2 Normal Means Problem",
    "text": "12.2 Normal Means Problem\nThe canonical problem is estimation in the normal means problem. First, consider the univariate case where the signal, \\(y\\), has mean, $ $, and Gaussian error, $ \\(. The model is\\)$ y = + ; ; {} ; ; N ( 0 , ^2 ) \\[\nFor the prior distribution  we assume a conjugate $ \\theta \\sim N ( \\mu , \\sigma^2 ) $. The optimal Bayes rule under squared error loss\nis given by the posterior mean, $ E( \\theta | y ) $, where\n\\] ( y ) = is only a global shrinkage rule–in the sense that there are no local parameters to learn about sparsity. A simple sparsity example shows the issue with \\(L^2\\)-regularisation. Consider the sparse \\(r\\)-spike shows the problem with focusing solely on rules with the same shrinkage weight (albeit benefiting from pooling of information).\nLet the true parameter value be \\(\\theta_p = \\left ( \\sqrt{d/p} , \\ldots , \\sqrt{d/p} , 0 , \\ldots , 0 \\right )\\). James-Stein is equivalent to the model \\[\ny_i = \\theta_i + \\epsilon_i \\; \\mathrm{ and} \\; \\theta_i \\sim \\mathcal{N} \\left ( 0 , \\tau^2 \\right )\n\\] This dominates the plain MLE but loses admissibility! This is due to the fact that a “plug-in” estimate of global shrinkage \\(\\hat{\\tau}\\) is used.\nFrom a risk perspective \\(E \\Vert \\hat{\\theta}^{JS} - \\theta \\Vert \\leq p , \\forall \\theta\\) showing the inadmissibility of the MLE. At origin the risk is \\(2\\).\n\nExample 12.1 (Stein’s Paradox) Stein’s paradox, as explained Efron and Morris (1977), is a phenomenon in statistics that challenges our intuitive understanding of estimation. The paradox arises when trying to estimate the mean of a multivariate normal distribution. Traditionally, the best guess about the future is usually obtained by computing the average of past events. However, Charles Stein showed that there are circumstances where there are estimators better than the arithmetic average. This is what’s known as Stein’s paradox.\nIn 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution that has uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein’s rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson’s chi-square test with results from a computer simulation.\nIn each of these examples, the mean square error of these rules is less than half that of the sample mean. This result is paradoxical because it contradicts the elementary law of statistical theory. The philosophical implications of Stein’s paradox are also significant. It has influenced the development of shrinkage estimators and has connections to Bayesianism and model selection criteria.\nSuppose that we have \\(n\\) independent observations \\(y_{1},\\ldots,y_{n}\\) from a \\(N\\left(  \\theta,\\sigma^{2}\\right)\\) distribution. The maximum likelihood estimator is \\(\\widehat{\\theta}=\\bar{y}\\), the sample mean. The Bayes estimator is the posterior mean, \\[\n\\widehat{\\theta}=\\mathbb{E}\\left[  \\theta\\mid y\\right]  =\\frac{\\sigma^{2}}{\\sigma^{2}+n}% \\bar{y}.\n\\] The Bayes estimator is a shrinkage estimator, it shrinks the MLE towards the prior mean. The amount of shrinkage is determined by the ratio of the variance of the prior and the variance of the likelihood. The Bayes estimator is also a function of the MLE \\[\n\\widehat{\\theta}=\\frac{\\sigma^{2}}{\\sigma^{2}+n}\\bar{y}+\\frac{n}{\\sigma^{2}+n}\\widehat{\\theta}.\n\\] This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior. The Bayes estimator is a function of the MLE \\[\n\\widehat{\\theta}=\\frac{\\sigma^{2}}{\\sigma^{2}+n}\\bar{y}+\\frac{n}{\\sigma^{2}+n}\\widehat{\\theta}.\n\\] This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior.\n\n\nExample 12.2 (Example: James-Stein for Baseball Batting Averages) We reproduce the baseball batting average example from Efron and Morris (1977). Data below has the number of hits for 18 baseball player after 45 at-beat in 1970 season\n\n# Data source: https://www1.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt\nbaseball = read.csv(\"../data/EfronMorrisBB.txt\", sep = \"\\t\", stringsAsFactors = FALSE) %&gt;% select(LastName,AtBats,BattingAverage,SeasonAverage)\n\nNow, we can eatimate overall mean and variance\n\nmu_hat &lt;- mean(baseball$BattingAverage)\nsigma2_hat &lt;- var(baseball$BattingAverage)\n\nAs well as the posterior mean for each player (James-Stein estimator)\n\nbaseball &lt;- baseball %&gt;%\n  mutate(\n    JS = (sigma2_hat / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * mu_hat +\n      ((BattingAverage * (1 - BattingAverage) / AtBats) / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * BattingAverage\n  )\nkable(baseball)\n\n\n\n\nLastName\nAtBats\nBattingAverage\nSeasonAverage\nJS\n\n\n\n\nClemente\n45\n0.400\n0.352\n0.3358644\n\n\nRobinson\n45\n0.378\n0.306\n0.3237686\n\n\nHoward\n45\n0.356\n0.283\n0.3117929\n\n\nJohnstone\n45\n0.333\n0.238\n0.2994785\n\n\nBerry\n45\n0.311\n0.276\n0.2879767\n\n\nSpencer\n45\n0.311\n0.274\n0.2879767\n\n\nKessinger\n45\n0.289\n0.266\n0.2768343\n\n\nAlvarado\n45\n0.267\n0.224\n0.2661503\n\n\nSanto\n45\n0.244\n0.267\n0.2555955\n\n\nSwaboda\n45\n0.244\n0.233\n0.2555955\n\n\nPetrocelli\n45\n0.222\n0.261\n0.2462289\n\n\nRodriguez\n45\n0.222\n0.225\n0.2462289\n\n\nScott\n45\n0.222\n0.296\n0.2462289\n\n\nUnser\n45\n0.222\n0.258\n0.2462289\n\n\nWilliams\n45\n0.222\n0.251\n0.2462289\n\n\nCampaneris\n45\n0.200\n0.279\n0.2377410\n\n\nMunson\n45\n0.178\n0.302\n0.2303313\n\n\nAlvis\n45\n0.156\n0.183\n0.2242473\n\n\n\n\n\nPlot below shows the observed averages vs. James-Stein estimate\n\nggplot(baseball, aes(x = BattingAverage, y = JS)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"Observed Batting Average\",\n    y = \"James-Stein Estimate\",\n    title = \"Empirical Bayes Shrinkage of Batting Averages (2016)\"\n  )\n\n\n\n\n\n\n\n\nCalculate mean squared error (MSE) for observed and James-Stein estimates\n\nmse_observed &lt;- mean((baseball$BattingAverage - mu_hat)^2)\nmse_js &lt;- mean((baseball$JS - mu_hat)^2)\n\ncat(sprintf(\"MSE (Observed): %.6f\\n\", mse_observed))\n\n## MSE (Observed): 0.004584\n\ncat(sprintf(\"MSE (James-Stein): %.6f\\n\", mse_js))\n\n## MSE (James-Stein): 0.001031\n\n\nWe can see that the James-Stein estimator has a lower MSE than the observed batting averages. This is a demonstration of Stein’s paradox, where the James-Stein estimator, which shrinks the estimates towards the overall mean, performs better than the naive sample mean estimator.\n\na = matrix(rep(1:3, nrow(baseball)), 3, nrow(baseball))\nb = matrix(c(baseball$BattingAverage, baseball$SeasonAverage, baseball$JS),    3, nrow(baseball), byrow=TRUE)\n\nmatplot(a, b, pch=\" \", ylab=\"predicted average\", xaxt=\"n\", xlim=c(0.5, 3.1), ylim=c(0.13, 0.42))\nmatlines(a, b)\ntext(rep(0.7, nrow(baseball)), baseball$BattingAverage, baseball$LastName, cex=0.6)\ntext(1, 0.14, \"First 45\\nat bats\", cex=0.5)\ntext(2, 0.14, \"Average\\nof remainder\", cex=0.5)\ntext(3, 0.14, \"J-S\\nestimator\", cex=0.5)\n\n\n\n\n\n\n\n\nNow if we look at the season dynamics for Clemente\n\n# Data source: https://www.baseball-almanac.com/players/hittinglogs.php?p=clemero01&y=1970\ncl = read.csv(\"../data/clemente.csv\")\nx = cumsum(cl$AB)\ny = cumsum(cl$H)/cumsum(cl$AB)\n# Plot x,y startind from index 2\nind = c(1,2)\nplot(x[-ind],y[-ind], type='o', ylab=\"Betting Average\", xlab=\"Number at Bats\")\n# Add horizontal line for season average 145/412 and add text above line `Seaosn Average`\ntext(200, 145/412 + 0.005, \"Season Average\", col = \"red\")\nabline(h = 145/412, col = \"red\", lty = 2)\n# Ted williams record is .406 in in 1941, so you know the first data points are noise\ntext(200, baseball$JS[1] + 0.005, \"JS\", col = \"red\")\nabline(h = baseball$JS[1], col = \"red\", lty = 2)\ntext(200, baseball$BattingAverage[1] + 0.005, \"After 45 Bets\", col = \"red\")\nabline(h = baseball$BattingAverage[1], col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nThe motivation for the shrinkage prior rather than a flat uniform prior are the following probabilistic arguments.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#ell_2-shrinkage",
    "href": "12-theoryai.html#ell_2-shrinkage",
    "title": "12  Theory of AI",
    "section": "12.3 \\(\\ell_2\\) Shrinkage",
    "text": "12.3 \\(\\ell_2\\) Shrinkage\nThe original JS estimator shranks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want o shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS is \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), which serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate propr from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\n\n12.3.1 Sparse \\(r\\)-spike problem\nFor the sparse \\(r\\)-spike problem we require a different rule. For a sparse signal, however, \\(\\hat \\theta_{JS}\\) performs poorly when the true parameter is an \\(r\\)-spike where \\(\\theta_r\\) has \\(r\\) coordinates at \\(\\sqrt{p/r}\\) and the rest set at zero with norm \\({\\Vert \\theta_r \\Vert}^2 =p\\).\nThe classical risk satisfies \\(R \\left ( \\hat \\theta_{JS} , \\theta_r \\right ) \\geq p/2\\) where the simple thresholding rule \\(\\sqrt{2 \\ln p}\\) performs with risk \\(\\sqrt{\\ln p}\\) in the \\(r\\)-spike sparse case even though it is inadmissible in MSE for a non-sparse signal. Then is due to the fact that for $ _p $ we have \\[\n\\frac{p \\Vert \\theta \\Vert^2}{p + \\Vert \\theta \\Vert^2} \\leq R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\leq\n2 + \\frac{p \\Vert \\theta \\Vert^2}{ d + \\Vert \\theta \\Vert^2}.\n\\] This implies that \\(R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\geq (p/2)\\). Hence, simple thresholding rule beats James-Stein this with a risk given by \\(\\sqrt{\\log p }\\). This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.\nA Bayes rule that inherits good MSE properties but also simultaneously provides asymptotic minimax estimation risk for sparse signals. HS estimator uniformly dominates the traditional sample mean estimator in MSE and has good posterior concentration properties for nearly black objects. Specifically, the horseshoe estimator attains asymptotically minimax risk rate \\[\n\\sup_{ \\theta \\in l_0[p_n] } \\;\n\\mathbb{E}_{ y | \\theta } \\|\\hat y_{hs} - \\theta \\|^2 \\asymp\np_n \\log \\left ( n / p_n \\right ).\n\\] The “worst’’ \\(\\theta\\) is obtained at the maximum difference between \\(\\left| \\hat \\theta_{HS} - y \\right|\\) where \\(\\hat \\theta_{HS} = \\mathbb{E}(\\theta|y)\\) can be interpreted as a Bayes posterior mean (optimal under Bayes MSE).\nOne such estimator that achieves the optimal minimax rate is the horseshoe estimator proposed by Carvalho, Polson, and Scott (2010) t\n\n\n12.3.2 Efron Example\nEfron provide an example which shows the importance of specifying priors in high dimensions. The key idea behind James-Stein shrinkage is that one when one can “borrow strength” across components. In this sense the multivariate parameter estimation problem is easier than the univariate one.\nStein’s phenomenon where \\(y_i | \\theta_i \\sim N(\\theta_i, 1)\\) and \\(\\theta_i \\sim N(0, \\tau^2)\\) where $ $ illustrates this point well. This leads to the improper “non-informative” uniform prior. The corresponding generalized Bayes rule is the vector of means—which we know is inadmissible. so no regularisation leads to an estimator with poor risk property.\nLet $ || y || = _{i=1}^p y_i^2 \\(. Then, we can make the following probabilistic statements from the model,\\)$ P( | y | &gt; | | ) &gt; \\[\nNow for the posterior, this inequallty is reversed under a flat Lebesgue measure,\n\\] P( | | &gt; | y | ; | ; y ) &gt; $$ which is in conflict with the classical statement. This is a property of the prior which leads to a poor rule (the overall average) and risk.\nThe shrinkage rule (a.k.a. normal prior) where $ ^2 $ is “estimated” from the data avoids this conflict. More precisely, we have \\[\n\\hat{\\theta}(y) = \\left( 1 - \\frac{k-2}{\\|y\\|^2} \\right) y \\quad \\text{and} \\quad E\\left( \\| \\hat{\\theta} - \\theta \\| \\right) &lt; k, \\; \\forall \\theta.\n\\] Hence, when \\(\\|y\\|^2\\) is small the shrinkage factor is more extreme. For example, if \\(k=10\\), \\(\\|y\\|^2=12\\), then \\(\\hat{\\theta} = (1/3) y\\). Now we have the more intuitive result that $ P( | | &gt; | y | ; | ; y ) &lt; $.\nThis shows that careful specification of default priors matter in high dimensions is necessary.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#ell_1-sparsity",
    "href": "12-theoryai.html#ell_1-sparsity",
    "title": "12  Theory of AI",
    "section": "12.4 \\(\\ell_1\\) Sparsity",
    "text": "12.4 \\(\\ell_1\\) Sparsity",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#ell_0-subset-selection",
    "href": "12-theoryai.html#ell_0-subset-selection",
    "title": "12  Theory of AI",
    "section": "12.5 \\(\\ell_0\\) Subset Selection",
    "text": "12.5 \\(\\ell_0\\) Subset Selection\nThe canonical problem is estimaiton of the normal means problem. Here we have \\(y_i = \\theta_i + e_i,~i=1,\\ldots,p\\) and \\(e_i \\sim N(0, \\sigma^2)\\). The goal is to estimate the vector of means \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\). This is also a proxy for non-parametric regression, where \\(\\theta_i = f(x_i)\\). Aslo typically \\(y_i\\) is a mean of \\(n\\) observations, i.e. \\(y_i = \\frac{1}{n} \\sum_{j=1}^n x_{ij}\\). Much has been written on the properties of the Bayes risk as a function of \\(n\\) and \\(p\\). Much work has also been done on the asymptotic properties of the Bayes risk as \\(n\\) and \\(p\\) grow to infinity. We now sumamrise some of the standard risk results.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#james-stein-estimator",
    "href": "12-theoryai.html#james-stein-estimator",
    "title": "12  Theory of AI",
    "section": "12.6 James-Stein Estimator",
    "text": "12.6 James-Stein Estimator\nThe classic James-Stein shrinkage rule, \\(\\hat y_{js}\\), uniformly dominates the traditional sample mean estimator, \\(\\hat{\\theta}\\), for all values of the true parameter \\(\\theta\\). In classical MSE risk terms: \\[\nR(\\hat y_{js}, \\theta) \\defeq E_{y|\\theta} {\\Vert \\hat y_{js} - \\theta \\Vert}^2 &lt; p\n    = E_{y|\\theta} {\\Vert y - \\theta \\Vert}^2, \\;\\;\\; \\forall \\theta\n\\] For a sparse signal, however, \\(\\hat y_{js}\\) performs poorly when the true parameter is an \\(r\\)-spike where \\(\\theta_r\\) has \\(r\\) coordinates at \\(\\sqrt{p/r}\\) and the rest set at zero with norm \\({\\Vert \\theta_r \\Vert}^2 =p\\).\nThe classical risk satisfies \\(R \\left ( \\hat y_{js} , \\theta_r \\right ) \\geq p/2\\) where the simple thresholding rule \\(\\sqrt{2 \\ln p}\\) performs with risk \\(\\sqrt{\\ln p}\\) in the \\(r\\)-spike sparse case even though it is inadmissible in MSE for a non-sparse signal.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#r-spike-problem",
    "href": "12-theoryai.html#r-spike-problem",
    "title": "12  Theory of AI",
    "section": "12.7 R-spike Problem",
    "text": "12.7 R-spike Problem\nFrom a historical perspective, James-Stein (a.k.a \\(L^2\\)-regularisation)(Stein 1964) is only a global shrinkage rule–in the sense that there are no local parameters to learn about sparsity. A simple sparsity example shows the issue with \\(L^2\\)-regularisation. Consider the sparse \\(r\\)-spike shows the problem with focusing solely on rules with the same shrinkage weight (albeit benefiting from pooling of information).\nLet the true parameter value be \\(\\theta_p = \\left ( \\sqrt{d/p} , \\ldots , \\sqrt{d/p} , 0 , \\ldots , 0 \\right )\\). James-Stein is equivalent to the model \\[\ny_i = \\theta_i + \\epsilon_i \\; \\mathrm{ and} \\; \\theta_i \\sim \\mathcal{N} \\left ( 0 , \\tau^2 \\right )\n\\] This dominates the plain MLE but loses admissibility! This is due to the fact that a “plug-in” estimate of global shrinkage \\(\\hat{\\tau}\\) is used. Tiao and Tan’s original “closed-form” analysis is particularly relevant here. They point out that the mode of \\(p(\\tau^2|y)\\) is zero exactly when the shrinkage weight turns negative (their condition 6.6). From a risk perspective \\(E \\Vert \\hat{\\theta}^{JS} - \\theta \\Vert \\leq p , \\forall \\theta\\) showing the inadmissibility of the MLE. At origin the risk is \\(2\\), but! \\[\n\\frac{p \\Vert \\theta \\Vert^2}{p + \\Vert \\theta \\Vert^2} \\leq R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\leq\n2 + \\frac{p \\Vert \\theta \\Vert^2}{ d + \\Vert \\theta \\Vert^2}.\n\\] This implies that \\(R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\geq (p/2)\\). Hence, simple thresholding rule beats James-Stein this with a risk given by \\(\\sqrt{\\log p }\\). This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.\nThe horseshoe estimator, which we will discuss in more detail later, \\(\\hat y_{hs}\\), was proposed by Carvalho, Polson, and Scott (2010) to provide a Bayes rule that inherits good MSE properties but also simultaneously provides asymptotic minimax estimation risk for sparse signals. HS estimator uniformly dominates the traditional sample mean estimator in MSE and has good posterior concentration properties for nearly black objects. Specifically, the horseshoe estimator attains asymptotically minimax risk rate \\[\n\\sup_{ \\theta \\in l_0[p_n] } \\;\n\\mathbb{E}_{ y | \\theta } \\|\\hat y_{hs} - \\theta \\|^2 \\asymp\np_n \\log \\left ( n / p_n \\right ).\n\\] The “worst’’ \\(\\theta\\) is obtained at the maximum difference between \\(\\left| \\hat y_{hs} - y \\right|\\) where \\(\\hat y_{hs} = \\mathbb{E}(\\theta|y)\\) can be interpreted as a Bayes posterior mean (optimal under Bayes MSE).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#ell_2-shrinkage-1",
    "href": "12-theoryai.html#ell_2-shrinkage-1",
    "title": "12  Theory of AI",
    "section": "12.8 \\(\\ell_2\\) Shrinkage",
    "text": "12.8 \\(\\ell_2\\) Shrinkage\n\nExample 12.3 (Stein’s Paradox) Stein’s paradox, as explained Efron and Morris (1977), is a phenomenon in statistics that challenges our intuitive understanding of estimation. The paradox arises when trying to estimate the mean of a multivariate normal distribution. Traditionally, the best guess about the future is usually obtained by computing the average of past events. However, Charles Stein showed that there are circumstances where there are estimators better than the arithmetic average. This is what’s known as Stein’s paradox.\nIn 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution that has uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein’s rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson’s chi-square test with results from a computer simulation.\nIn each of these examples, the mean square error of these rules is less than half that of the sample mean. This result is paradoxical because it contradicts the elementary law of statistical theory. The philosophical implications of Stein’s paradox are also significant. It has influenced the development of shrinkage estimators and has connections to Bayesianism and model selection criteria.\nSuppose that we have \\(n\\) independent observations \\(y_{1},\\ldots,y_{n}\\) from a \\(N\\left(  \\theta,\\sigma^{2}\\right)\\) distribution. The maximum likelihood estimator is \\(\\widehat{\\theta}=\\bar{y}\\), the sample mean. The Bayes estimator is the posterior mean, \\[\n\\widehat{\\theta}=\\mathbb{E}\\left[  \\theta\\mid y\\right]  =\\frac{\\sigma^{2}}{\\sigma^{2}+n}% \\bar{y}.\n\\] The Bayes estimator is a shrinkage estimator, it shrinks the MLE towards the prior mean. The amount of shrinkage is determined by the ratio of the variance of the prior and the variance of the likelihood. The Bayes estimator is also a function of the MLE \\[\n\\widehat{\\theta}=\\frac{\\sigma^{2}}{\\sigma^{2}+n}\\bar{y}+\\frac{n}{\\sigma^{2}+n}\\widehat{\\theta}.\n\\] This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior. The Bayes estimator is a function of the MLE \\[\n\\widehat{\\theta}=\\frac{\\sigma^{2}}{\\sigma^{2}+n}\\bar{y}+\\frac{n}{\\sigma^{2}+n}\\widehat{\\theta}.\n\\] This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior.\nThe original JS estimator shranks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and MOrris and Lindley showed that you want o shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS is \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), whcih serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate propr from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\nWe reproduce the baseball bartting average example from Efron and Morris (1977). Data below has the number of hits for 18 baseball player after 45 at-beat in 1970 season\n\n# Data source: https://www1.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt\nbaseball = read.csv(\"../data/EfronMorrisBB.txt\", sep = \"\\t\", stringsAsFactors = FALSE) %&gt;% select(LastName,AtBats,BattingAverage,SeasonAverage)\n\nNow, we can eatimate overall mean and variance\n\nmu_hat &lt;- mean(baseball$BattingAverage)\nsigma2_hat &lt;- var(baseball$BattingAverage)\n\nAs well as the osterior mean for each player (James-Stein estimator)\n\nbaseball &lt;- baseball %&gt;%\n  mutate(\n    JS = (sigma2_hat / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * mu_hat +\n      ((BattingAverage * (1 - BattingAverage) / AtBats) / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * BattingAverage\n  )\nkable(baseball)\n\n\n\n\nLastName\nAtBats\nBattingAverage\nSeasonAverage\nJS\n\n\n\n\nClemente\n45\n0.400\n0.352\n0.3358644\n\n\nRobinson\n45\n0.378\n0.306\n0.3237686\n\n\nHoward\n45\n0.356\n0.283\n0.3117929\n\n\nJohnstone\n45\n0.333\n0.238\n0.2994785\n\n\nBerry\n45\n0.311\n0.276\n0.2879767\n\n\nSpencer\n45\n0.311\n0.274\n0.2879767\n\n\nKessinger\n45\n0.289\n0.266\n0.2768343\n\n\nAlvarado\n45\n0.267\n0.224\n0.2661503\n\n\nSanto\n45\n0.244\n0.267\n0.2555955\n\n\nSwaboda\n45\n0.244\n0.233\n0.2555955\n\n\nPetrocelli\n45\n0.222\n0.261\n0.2462289\n\n\nRodriguez\n45\n0.222\n0.225\n0.2462289\n\n\nScott\n45\n0.222\n0.296\n0.2462289\n\n\nUnser\n45\n0.222\n0.258\n0.2462289\n\n\nWilliams\n45\n0.222\n0.251\n0.2462289\n\n\nCampaneris\n45\n0.200\n0.279\n0.2377410\n\n\nMunson\n45\n0.178\n0.302\n0.2303313\n\n\nAlvis\n45\n0.156\n0.183\n0.2242473\n\n\n\n\n\nPlot below shows the observed averages vs. James-Stein estimate\n\nggplot(baseball, aes(x = BattingAverage, y = JS)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"Observed Batting Average\",\n    y = \"James-Stein Estimate\",\n    title = \"Empirical Bayes Shrinkage of Batting Averages (2016)\"\n  )\n\n\n\n\n\n\n\n\nCalculate mean squared error (MSE) for observed and James-Stein estimates\n\nmse_observed &lt;- mean((baseball$BattingAverage - mu_hat)^2)\nmse_js &lt;- mean((baseball$JS - mu_hat)^2)\n\ncat(sprintf(\"MSE (Observed): %.6f\\n\", mse_observed))\n\n## MSE (Observed): 0.004584\n\ncat(sprintf(\"MSE (James-Stein): %.6f\\n\", mse_js))\n\n## MSE (James-Stein): 0.001031\n\n\nWe can see that the James-Stein estimator has a lower MSE than the observed batting averages. This is a demonstration of Stein’s paradox, where the James-Stein estimator, which shrinks the estimates towards the overall mean, performs better than the naive sample mean estimator.\n\na = matrix(rep(1:3, nrow(baseball)), 3, nrow(baseball))\nb = matrix(c(baseball$BattingAverage, baseball$SeasonAverage, baseball$JS),    3, nrow(baseball), byrow=TRUE)\n\nmatplot(a, b, pch=\" \", ylab=\"predicted average\", xaxt=\"n\", xlim=c(0.5, 3.1), ylim=c(0.13, 0.42))\nmatlines(a, b)\ntext(rep(0.7, nrow(baseball)), baseball$BattingAverage, baseball$LastName, cex=0.6)\ntext(1, 0.14, \"First 45\\nat bats\", cex=0.5)\ntext(2, 0.14, \"Average\\nof remainder\", cex=0.5)\ntext(3, 0.14, \"J-S\\nestimator\", cex=0.5)\n\n\n\n\n\n\n\n\nNow if we look at the season dynamics for Clemente\n\n# Data source: https://www.baseball-almanac.com/players/hittinglogs.php?p=clemero01&y=1970\ncl = read.csv(\"../data/clemente.csv\")\nx = cumsum(cl$AB)\ny = cumsum(cl$H)/cumsum(cl$AB)\n# Plot x,y startind from index 2\nind = c(1,2)\nplot(x[-ind],y[-ind], type='o', ylab=\"Betting Average\", xlab=\"Number at Bats\")\n# Add horizontal line for season average 145/412 and add text above line `Seaosn Average`\ntext(200, 145/412 + 0.005, \"Season Average\", col = \"red\")\nabline(h = 145/412, col = \"red\", lty = 2)\n# Ted williams record is .406 in in 1941, so you know the first data points are noise\ntext(200, baseball$JS[1] + 0.005, \"JS\", col = \"red\")\nabline(h = baseball$JS[1], col = \"red\", lty = 2)\ntext(200, baseball$BattingAverage[1] + 0.005, \"After 45 Bets\", col = \"red\")\nabline(h = baseball$BattingAverage[1], col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nThe motivation for the shrinkage prior rather than a flat uniform prior are the following probabilistic arguments.\n\nExample 12.4 (Efron Example) Efron shows the importance of priors in high dimensions when one can “borrow strength” (a.k.a. regularisation) across components.\nStein’s phenomenon where \\(y_i | \\theta_i \\sim N(\\theta_i, 1)\\) and \\(\\theta_i \\sim N(0, \\tau^2)\\) illustrates this point well. From the model,\n\\[\nP\\left( \\| y \\| &gt; \\| \\theta \\| \\right) &gt; \\frac{1}{2}\n\\]\nUnder a flat Lebesgue measure, this inequality is reversed in the posterior, namely\n\\[\nP\\left( \\| \\theta \\| &gt; \\| y \\| \\; | \\; y \\right) &gt; \\frac{1}{2}\n\\]\nIn conflict with the classical statement. However, if we use Stein’s rule (posterior where \\(\\tau^2\\) is estimated via empirical Bayes) we have\n\\[\n\\hat{\\theta}(y) = \\left( 1 - \\frac{k-2}{\\|y\\|^2} \\right) y \\quad \\text{and} \\quad E\\left( \\| \\hat{\\theta} - \\theta \\| \\right) &lt; k, \\; \\forall \\theta.\n\\]\nHence, when \\(\\|y\\|^2\\) is small the shrinkage factor is more extreme.\nFor example, if \\(k=10\\), \\(\\|y\\|^2=12\\), then \\(\\hat{\\theta} = (1/3) y\\). Now we have the more intuitive result:\n\\[\nP\\left( \\| \\theta \\| &gt; \\| y \\| \\; | \\; y \\right) &lt; \\frac{1}{2}\n\\]\nShowing that default priors matter in high dimensions.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#ell_1-sparsity-1",
    "href": "12-theoryai.html#ell_1-sparsity-1",
    "title": "12  Theory of AI",
    "section": "12.9 \\(\\ell_1\\) Sparsity",
    "text": "12.9 \\(\\ell_1\\) Sparsity",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#ell_0-subset-selection-1",
    "href": "12-theoryai.html#ell_0-subset-selection-1",
    "title": "12  Theory of AI",
    "section": "12.10 \\(\\ell_0\\) Subset Selection",
    "text": "12.10 \\(\\ell_0\\) Subset Selection",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#bayesain-model-selection-via-regularisation",
    "href": "12-theoryai.html#bayesain-model-selection-via-regularisation",
    "title": "12  Theory of AI",
    "section": "12.11 Bayesain Model Selection via Regularisation",
    "text": "12.11 Bayesain Model Selection via Regularisation\nFrom Bayesian perspective regularization is nothing but incorporation of prior information into the model. Remember, that a Bayesian model is specified by likelihood and prior distributions. Bayesian regularization methods include the Bayesian bridge, horseshoe regularization, Bayesian lasso, Bayesian elastic net, spike-and-slab lasso, and global-local shrinkage priors. Bayesian \\(\\ell_0\\) regularization is an attractive solution for high dimensional variable selection as it directly penalizes the number of predictors. The caveat is the need to search over all possible model combinations, as a full solution requires enumeration over all possible models which is NP-hard.\nIn Bayesian approach, regularization requires the specification of a loss, denoted by \\(l\\left(\\beta\\right)\\) and a penalty function, denoted by \\(\\phi_{\\lambda}(\\beta)\\), where \\(\\lambda\\) is a global regularization parameter. From a Bayesian perspective, \\(l\\left(\\beta\\right)\\) and \\(\\phi_{\\lambda}(\\beta)\\) correspond to the negative logarithms of the likelihood and prior distribution, respectively. Regularization leads to an maximum a posteriori (MAP) optimization problem of the form \\[\n\\underset{\\beta \\in R^p}{\\mathrm{minimize}\\quad}\nl\\left(\\beta\\right) + \\phi_{\\lambda}(\\beta) \\; .\n\\] Taking a probabilistic approach leads to a Bayesian hierarchical model \\[\np(y \\mid \\beta) \\propto \\exp\\{-l(\\beta)\\} \\; , \\quad p(\\beta) \\propto \\exp\\{ -\\phi_{\\lambda}(\\beta) \\} \\ .\n\\] The solution to the minimization problem estimated by regularization corresponds to the posterior mode, \\(\\hat{\\beta} = \\mathrm{ arg \\; max}_\\beta \\; p( \\beta|y)\\), where \\(p(\\beta|y)\\) denotes the posterior distribution. Consider a normal mean problem with \\[\n\\label{eqn:linreg}\ny = \\theta+ e \\ , \\ \\  \\text{where } e \\sim N(0, \\sigma^2),~-\\infty \\le \\theta \\le \\infty \\ .\n\\] What prior \\(p(\\theta)\\) should we place on \\(\\theta\\) to be able to separate the “signal” \\(\\theta\\) from “noise” \\(e\\), when we know that there is a good chance that \\(\\theta\\) is sparse (i.e. equal to zero). In the multivariate case we have \\(y_i = \\theta_i + e_i\\) and sparseness is measured by the number of zeros in \\(\\theta = (\\theta_1\\ldots,\\theta_p)\\). The Bayesan Lasso assumes double exponential (a.k.a Laplace) prior distribution where \\[\np(\\theta_i \\mid b) = 0.5b\\exp(-|\\theta|/b).\n\\] We use Bayes rule to calculate the posterior as a product of Normal likelihood and Laplace prior \\[\n\\log p(\\theta \\mid y, b) \\propto ||y-\\theta||_2^2 + \\dfrac{2\\sigma^2}{b}||\\theta||_1.\n\\] For fixed \\(\\sigma^2\\) and \\(b&gt;0\\) the posterior mode is equivalent to the Lasso estimate with \\(\\lambda = 2\\sigma^2/b\\). Large variance \\(b\\) of the prior is equivalent to the small penalty weight \\(\\lambda\\) in the Lasso objective function.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#shrinkage-ell_2-norm",
    "href": "12-theoryai.html#shrinkage-ell_2-norm",
    "title": "12  Theory of AI",
    "section": "12.12 Shrinkage (\\(\\ell_2\\) Norm)",
    "text": "12.12 Shrinkage (\\(\\ell_2\\) Norm)\nWe can estimate the risk bounds of \\(\\ell_2\\) Norm regularisation. The classic bias-variance tradeoff is given by the MSE risk bound. \\[\nR(\\theta,\\hat \\theta) = E_{y|\\theta} \\left [ \\Vert \\hat \\theta - \\theta \\Vert^2 \\right ] = \\Vert \\hat \\theta - \\theta \\Vert^2 + E_{y|\\theta} \\left [ \\Vert \\hat \\theta - \\mathbb{E}(\\hat \\theta) \\Vert^2 \\right ]\n\\]\nIn a case of multiple parameters, the Stein bound is \\[\nR(\\theta,\\hat \\theta_{JS}) &lt; R(\\theta,\\hat \\theta_{MLE}) \\;\\;\\; \\forall \\theta \\in \\mathbb{R}^p, \\;\\;\\; p \\geq 3.\n\\] In an applied problem the gap in risk between MLE and JS estimators can be large. For example, in the normal means problem with \\(p=100\\) and \\(n=100\\), the risk of the MLE is \\(R(\\theta,\\hat \\theta_{MLE}) = 100\\) while the risk of the JS estimator is \\(R(\\theta,\\hat \\theta_{JS}) = 1.5\\). The JS estimator is 67 times more efficient than the MLE. The JS estimator is also minimax optimal in the sense that it attains the minimax risk bound for the normal means problem. The minimax risk bound is the smallest risk that can be attained by any estimator.\nJS is a specific estimate and it motivates the ridge estimator. The ridge estimator is a shrinkage estimator with the penalty function being the squared norm of the parameter vector. The ridge estimator is \\[\n\\hat \\theta_{ridge} = \\left (  X^T X + \\lambda I \\right )^{-1} X^T y\n\\] where \\(\\lambda\\) is the regularization parameter.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#sparsity-ell_1-norm",
    "href": "12-theoryai.html#sparsity-ell_1-norm",
    "title": "12  Theory of AI",
    "section": "12.13 Sparsity (\\(\\ell_1\\) Norm)",
    "text": "12.13 Sparsity (\\(\\ell_1\\) Norm)\nHigh-dimensional predictor selection and sparse signal recovery are routine statistical and machine learning tasks. There is a vast and growing literator with lasso (leat absolute selection) being the gold standard. Horseshoe priors are the Bayesian counterpart. Sparsity relies on the property of a few large signals among many (nearly zero) noisy observations. The goal is to find a niddle in the haystack. Suppose that we observe data from Normal means model \\[\ny_i \\mid \\theta_i \\sim N(\\theta_i,1),~ 1\\le i\\le p, ~ \\theta = (\\theta_1,\\ldots,\\theta_p),\n\\] where parameter \\(\\theta\\) lies in the ball \\[\n||\\theta||_{\\ell_0} = \\{\\theta : \\text{number of  }\\theta_i \\ne 0 \\le p_n\\}.\n\\]\nEven threshholding can beat MLE, when the signal is sparse. The thresholding estimator is \\[\n\\hat \\theta_{thr} = \\left \\{ \\begin{array}{ll} \\hat \\theta_i & \\mbox{if} \\; \\hat \\theta_i &gt; \\sqrt{2 \\ln p} \\\\ 0 & \\mbox{otherwise} \\end{array} \\right .\n\\]\nSparse signal detection provides a challenge to statistical methodology; consider the classical normal means inference problem. Suppose that we observe data from the probability model \\(( y_i | \\theta_i ) \\sim N( \\theta_i,1)\\). We wish to provide an estimator \\(\\hat y_{hs}\\) for the vector of normal means \\(\\theta = ( \\theta_1, \\ldots , \\theta_p )\\). Sparsity occurs when a large portion of the parameter vector contains zeroes. The ’’ultra-sparse`` or “nearly black” vector case occurs when \\(p_n\\), denoting the number of non-zero parameter values, and for \\(\\theta \\in l_0 [ p_n]\\), which denotes the set \\(\\# ( \\theta_i \\neq 0 ) \\leq p_n\\) where \\(p_n = o(n)\\) where \\(p_n \\rightarrow \\infty\\) as \\(n \\rightarrow \\infty\\).\nThe predictive rule is often represented by a mathematical model, such as a linear regression model or a neural network. The model is trained using historical data, which consists of observed inputs and outputs \\((x_1,y_1),\\ldots, (x_n,y_n)\\).\nThe model is then used to predict the output \\(y\\) for new inputs \\(x\\). The accuracy of the model is evaluated using a performance metric, such as the mean squared error or the mean absolute error. The model is then updated and retrained using new data to improve its accuracy. This process is repeated until the model achieves the desired level of accuracy.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#lasso",
    "href": "12-theoryai.html#lasso",
    "title": "12  Theory of AI",
    "section": "12.14 LASSO",
    "text": "12.14 LASSO\nThe Laplace distribution can be represented as scale mixture of Normal distribution(Andrews and Mallows 1974) \\[\n\\begin{aligned}\n\\theta_i \\mid \\sigma^2,\\tau \\sim &N(0,\\tau^2\\sigma^2)\\\\\n\\tau^2  \\mid \\alpha \\sim &\\exp (\\alpha^2/2)\\\\\n\\sigma^2 \\sim & \\pi(\\sigma^2).\\end{aligned}\n\\] We can show equivalence by integrating out \\(\\tau\\) \\[\np(\\theta_i\\mid \\sigma^2,\\alpha) =  \\int_{0}^{\\infty} \\dfrac{1}{\\sqrt{2\\pi \\tau}}\\exp\\left(-\\dfrac{\\theta_i^2}{2\\sigma^2\\tau}\\right)\\dfrac{\\alpha^2}{2}\\exp\\left(-\\dfrac{\\alpha^2\\tau}{2}\\right)d\\tau = \\dfrac{\\alpha}{2\\sigma}\\exp(-\\alpha/\\sigma|\\theta_i|).\n\\] Thus it is a Laplace distribution with location 0 and scale \\(\\alpha/\\sigma\\). Representation of Laplace prior is a scale Normal mixture allows us to apply an efficient numerical algorithm for computing samples from the posterior distribution. This algorithms is called a Gibbs sample and it iteratively samples from \\(\\theta \\mid a,y\\) and \\(b\\mid \\theta,y\\) to estimate joint distribution over \\((\\hat \\theta, \\hat b)\\). Thus, we so not need to apply cross-validation to find optimal value of \\(b\\), the Bayesian algorithm does it “automatically”. We will discuss Gibbs algorithm later in the book.\nWhen prior is Normal \\(\\theta_i \\sim N(0,\\sigma_{\\theta}^2)\\), the posterior mode is equivalent to the ridge estimate. The relation between variance of the prior and the penalty weight in ridge regression is inverse proportional \\(\\lambda\\propto 1/\\sigma_{\\theta}^2\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#subset-selection-ell_0-norm",
    "href": "12-theoryai.html#subset-selection-ell_0-norm",
    "title": "12  Theory of AI",
    "section": "12.15 Subset Selection (\\(\\ell_0\\) Norm)",
    "text": "12.15 Subset Selection (\\(\\ell_0\\) Norm)\nSkike-and-slab (George and McCulloh) or Bernoulli-Gaussian (Polson Sun)",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#bridge-ell_alpha",
    "href": "12-theoryai.html#bridge-ell_alpha",
    "title": "12  Theory of AI",
    "section": "12.16 Bridge (\\(\\ell_{\\alpha}\\))",
    "text": "12.16 Bridge (\\(\\ell_{\\alpha}\\))\nThis is a non-convex penalty when \\(0&lt;\\alpha&lt;1\\). It is an NP-hard problem. When \\(\\alpha=1\\) or \\(\\alpha=2\\) we have optimisation problems that are “solvable” for large scale cases. However, when \\(0\\le \\alpha&lt;1\\) the current optimisation algorithms won’t work.\nThe real killer is that you can use data to estimate \\(\\alpha\\) and \\(\\lambda\\) (let the data speak for itself) Box and Tiao (1992).\nBayesian analogue of the bridge estimator in regression is \\[\ny = X\\beta + \\epsilon\n\\]\nfor some unknown vector \\(\\beta = (\\beta_1, \\ldots, \\beta_p)'\\). Given choices of \\(\\alpha \\in (0,1]\\) and \\(\\nu \\in \\mathbb{R}^+\\), the bridge estimator \\(\\hat{\\beta}\\) is the minimizer of\n\\[\nQ_y(\\beta) = \\frac{1}{2} \\|y - X\\beta\\|^2 + \\nu \\sum_{j=1}^p |\\beta_j|^\\alpha.\n\\tag{12.1}\\]\nThis bridges a class of shrinkage and selection operators, with the best-subset-selection penalty at one end, and the \\(\\ell_1\\) (or lasso) penalty at the other. An early reference to this class of models can be found in Frank and Friedman (1993), with recent papers focusing on model-selection asymptotics, along with strategies for actually computing the estimator (Huang, Horowitz, and Ma (2008), Mazumder, and and Hastie (2011)).\nBridge approach differs from this line of work in adopting a Bayesian perspective on bridge estimation. Specifically, we treat\n\\[\np(\\beta \\mid y) \\propto \\exp\\{-Q_y(\\beta)\\}\n\\]\nas a posterior distribution having the minimizer of Equation 12.1 as its global mode. This posterior arises in assuming a Gaussian likelihood for \\(y\\), along with a prior for \\(\\beta\\) that decomposes as a product of independent exponential-power priors (Box and Tiao (1992)):\n\\[\np(\\beta \\mid \\alpha, \\nu) \\propto \\prod_{j=1}^p \\exp\\left(-\\left|\\frac{\\beta_j}{\\tau}\\right|^\\alpha\\right), \\quad \\tau = \\nu^{-1/\\alpha}. \\tag{2}\n\\]\nRather than minimizing (1), we proceed by constructing a Markov chain having the joint posterior for \\(\\beta\\) as its stationary distribution.\n\n12.16.1 Spike-and-Slab Prior\nOur Bayesian formulation of allows to specify a wide range of range of regularized formulations for a regression problem. In this section we consider a Bayesian model for variable selection. Consider a linear regression problem \\[\ny = \\beta_1x_1+\\ldots+\\beta_px_p + e \\ , \\ \\  \\text{where } e \\sim N(0, \\sigma^2),~-\\infty \\le \\beta_i \\le \\infty \\ .\n\\] We would like to solve the problem of variable selections, i.e. identify which input variables \\(x_i\\) to be used in our model. The gold standard for Bayesian variable selection are spike-and-slab priors, or Bernoulli-Gaussian mixtures. Whilst spike-and-slab priors provide full model uncertainty quantification, they can be hard to scale to very high dimensional problems and can have poor sparsity properties. On the other hand, techniques like proximal algorithms can solve non-convex optimization problems which are fast and scalable, although they generally don’t provide a full assessment of model uncertainty.\nTo perform a model selection, we would like to specify a prior distribution \\(p\\left(\\beta\\right)\\), which imposes a sparsity assumption on \\(\\beta\\), where only a small portion of all \\(\\beta_i\\)’s are non-zero. In other words, \\(\\|\\beta\\|_0 = k \\ll p\\), where \\(\\|\\beta\\|_0 \\defeq \\#\\{i : \\beta_i\\neq0\\}\\), the cardinality of the support of \\(\\beta\\), also known as the \\(\\ell_0\\) (pseudo)norm of \\(\\beta\\). A multivariate Gaussian prior (\\(l_2\\) norm) leads to poor sparsity properties in this situation. Sparsity-inducing prior distributions for \\(\\beta\\) can be constructed to impose sparsity include the double exponential (lasso).\nUnder spike-and-slab, each \\(\\beta_i\\) exchangeably follows a mixture prior consisting of \\(\\delta_0\\), a point mass at \\(0\\), and a Gaussian distribution centered at zero. Hence we write,\n\\[\n\\label{eqn:ss}\n\\beta_i | \\theta, \\sigma_\\beta^2 \\sim (1-\\theta)\\delta_0 + \\theta N\\left(0, \\sigma_\\beta^2\\right) \\ .\n\\] Here \\(\\theta\\in \\left(0, 1\\right)\\) controls the overall sparsity in \\(\\beta\\) and \\(\\sigma_\\beta^2\\) accommodates non-zero signals. This family is termed as the Bernoulli-Gaussian mixture model in the signal processing community.\nA useful re-parameterization, the parameters \\(\\beta\\) is given by two independent random variable vectors \\(\\gamma = \\left(\\gamma_1, \\ldots, \\gamma_p\\right)'\\) and \\(\\alpha = \\left(\\alpha_1, \\ldots, \\alpha_p\\right)'\\) such that \\(\\beta_i  =  \\gamma_i\\alpha_i\\), with probabilistic structure \\[\n\\label{eq:bg}\n\\begin{array}{rcl}\n\\gamma_i\\mid\\theta & \\sim & \\text{Bernoulli}(\\theta) \\ ;\n\\\\\n\\alpha_i \\mid \\sigma_\\beta^2 &\\sim & N\\left(0, \\sigma_\\beta^2\\right) \\ .\n\\\\\n\\end{array}\n\\] Since \\(\\gamma_i\\) and \\(\\alpha_i\\) are independent, the joint prior density becomes \\[\np\\left(\\gamma_i, \\alpha_i \\mid \\theta, \\sigma_\\beta^2\\right) =\n\\theta^{\\gamma_i}\\left(1-\\theta\\right)^{1-\\gamma_i}\\frac{1}{\\sqrt{2\\pi}\\sigma_\\beta}\\exp\\left\\{-\\frac{\\alpha_i^2}{2\\sigma_\\beta^2}\\right\\}\n\\ , \\ \\ \\ \\text{for } 1\\leq i\\leq p \\ .\n\\] The indicator \\(\\gamma_i\\in \\{0, 1\\}\\) can be viewed as a dummy variable to indicate whether \\(\\beta_i\\) is included in the model.\nLet \\(S = \\{i: \\gamma_i = 1\\} \\subseteq \\{1, \\ldots, p\\}\\) be the “active set\" of \\(\\gamma\\), and \\(\\|\\gamma\\|_0 = \\sum\\limits_{i = 1}^p\\gamma_i\\) be its cardinality. The joint prior on the vector \\(\\{\\gamma, \\alpha\\}\\) then factorizes as \\[\n\\begin{array}{rcl}\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2\\right) & = & \\prod\\limits_{i = 1}^p p\\left(\\alpha_i, \\gamma_i \\mid \\theta, \\sigma_\\beta^2\\right) \\\\\n& = &\n\\theta^{\\|\\gamma\\|_0}\n\\left(1-\\theta\\right)^{p - \\|\\gamma\\|_0}\n\\left(2\\pi\\sigma_\\beta^2\\right)^{-\\frac p2}\\exp\\left\\{-\\frac1{2\\sigma_\\beta^2}\\sum\\limits_{i = 1}^p\\alpha_i^2\\right\\} \\ .\n\\end{array}\n\\]\nLet \\(X_\\gamma \\defeq \\left[X_i\\right]_{i \\in S}\\) be the set of “active explanatory variables\" and \\(\\alpha_\\gamma \\defeq \\left(\\alpha_i\\right)'_{i \\in S}\\) be their corresponding coefficients. We can write \\(X\\beta = X_\\gamma \\alpha_\\gamma\\). The likelihood can be expressed in terms of \\(\\gamma\\), \\(\\alpha\\) as \\[\np\\left(y \\mid \\gamma, \\alpha, \\theta, \\sigma_e^2\\right)\n=\n\\left(2\\pi\\sigma_e^2\\right)^{-\\frac n2}\n\\exp\\left\\{\n-\\frac1{2\\sigma_e^2}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n\\right\\} \\ .\n\\]\nUnder this re-parameterization by \\(\\left\\{\\gamma, \\alpha\\right\\}\\), the posterior is given by\n\\[\n\\begin{array}{rcl}\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2, \\sigma_e^2, y\\right) & \\propto &\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2\\right)\np\\left(y \\mid \\gamma, \\alpha, \\theta, \\sigma_e^2\\right)\\\\\n& \\propto &\n\\exp\\left\\{-\\frac1{2\\sigma_e^2}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n-\\frac1{2\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2\n-\\log\\left(\\frac{1-\\theta}{\\theta}\\right)\n\\left\\|\\gamma\\right\\|_0\n\\right\\} \\ .\n\\end{array}\n\\] Our goal then is to find the regularized maximum a posterior (MAP) estimator \\[\n\\arg\\max\\limits_{\\gamma, \\alpha}p\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2, \\sigma_e^2, y \\right) \\ .\n\\] By construction, the \\(\\gamma\\) \\(\\in\\left\\{0, 1\\right\\}^p\\) will directly perform variable selection. Spike-and-slab priors, on the other hand, will sample the full posterior and calculate the posterior probability of variable inclusion. Finding the MAP estimator is equivalent to minimizing over \\(\\left\\{\\gamma, \\alpha\\right\\}\\) the regularized least squares objective function\n\\[\n\\min\\limits_{\\gamma, \\alpha}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n+ \\frac{\\sigma_e^2}{\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2\n+ 2\\sigma_e^2\\log\\left(\\frac{1-\\theta}{\\theta}\\right)\n\\left\\|\\gamma\\right\\|_0 \\ .\n\\tag{12.2}\\] This objective possesses several interesting properties:\n\nThe first term is essentially the least squares loss function.\nThe second term looks like a ridge regression penalty and has connection with the signal-to-noise ratio (SNR) \\(\\sigma_\\beta^2/\\sigma_e^2\\). Smaller SNR will be more likely to shrink the estimates towards \\(0\\). If \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the prior uncertainty on the size of non-zero coefficients is much larger than the noise level, that is, the SNR is sufficiently large, this term can be ignored. This is a common assumption in spike-and-slab framework in that people usually want \\(\\sigma_\\beta \\to \\infty\\) or to be “sufficiently large\" in order to avoid imposing harsh shrinkage to non-zero signals.\nIf we further assume that \\(\\theta &lt; \\frac12\\), meaning that the coefficients are known to be sparse a priori, then \\(\\log\\left(\\left(1-\\theta\\right) / \\theta\\right) &gt; 0\\), and the third term can be seen as an \\(\\ell_0\\) regularization.\n\nTherefore, our Bayesian objective inference is connected to \\(\\ell_0\\)-regularized least squares, which we summarize in the following proposition.\n(Spike-and-slab MAP & \\(\\ell_0\\) regularization)\nFor some \\(\\lambda &gt; 0\\), assuming \\(\\theta &lt; \\frac12\\), \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the Bayesian MAP estimate defined by Equation 12.2 is equivalent to the \\(\\ell_0\\) regularized least squares objective, for some \\(\\lambda &gt; 0\\), \\[\n\\min\\limits_{\\beta}\n\\frac12\\left\\|y - X\\beta\\right\\|_2^2\n+ \\lambda\n\\left\\|\\beta\\right\\|_0 \\ .\n\\tag{12.3}\\]\nFirst, assuming that \\[\n\\theta &lt; \\frac12, \\ \\ \\  \\sigma_\\beta^2 \\gg \\sigma_e^2, \\ \\ \\  \\frac{\\sigma_e^2}{\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2 \\to 0 \\ ,\n\\] gives us an objective function of the form \\[\n\\min\\limits_{\\gamma, \\alpha}\n\\frac12 \\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n+ \\lambda\n\\left\\|\\gamma\\right\\|_0,  \\ \\ \\ \\  \\text{where } \\lambda \\defeq \\sigma_e^2\\log\\left(\\left(1-\\theta\\right) / \\theta\\right) &gt; 0 \\ .\n\\tag{12.4}\\]\nEquation Equation 12.4 can be seen as a variable selection version of equation Equation 12.3. The interesting fact is that Equation 12.3 and Equation 12.4 are equivalent. To show this, we need only to check that the optimal solution to Equation 12.3 corresponds to a feasible solution to Equation 12.4 and vice versa. This is explained as follows.\nOn the one hand, assuming \\(\\hat\\beta\\) is an optimal solution to Equation 12.3, then we can correspondingly define \\(\\hat\\gamma_i \\defeq I\\left\\{\\hat\\beta_i \\neq 0\\right\\}\\), \\(\\hat\\alpha_i \\defeq \\hat\\beta_i\\), such that \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) is feasible to Equation 12.4 and gives the same objective value as \\(\\hat\\beta\\) gives Equation 12.3.\nOn the other hand, assuming \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) is optimal to Equation 12.4, implies that we must have all of the elements in \\(\\hat\\alpha_\\gamma\\) should be non-zero, otherwise a new \\(\\tilde\\gamma_i \\defeq I\\left\\{\\hat\\alpha_i \\neq 0\\right\\}\\) will give a lower objective value of Equation 12.4. As a result, if we define \\(\\hat\\beta_i \\defeq \\hat\\gamma_i\\hat\\alpha_i\\), \\(\\hat\\beta\\) will be feasible to Equation 12.3 and gives the same objective value as \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) gives Equation 12.4.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#horseshoe-prior",
    "href": "12-theoryai.html#horseshoe-prior",
    "title": "12  Theory of AI",
    "section": "12.17 Horseshoe Prior",
    "text": "12.17 Horseshoe Prior\n\n\n\n\n\n\n\n\n\nThe sparse normal means problem is concerned with inference for the parameter vector \\(\\theta = ( \\theta_1 , \\ldots , \\theta_p )\\) where we observe data \\(y_i = \\theta_i + \\epsilon_i\\) where the level of sparsity might be unknown. From both a theoretical and empirical viewpoint, regularized estimators have won the day. This still leaves open the question of how does specify a penalty, denoted by \\(\\pi_{HS}\\), (a.k.a. log-prior, \\(- \\log p_{HS}\\))? Lasso simply uses an \\(L^1\\)-norm, \\(\\sum_{i=1}^K | \\theta_i |\\), as opposed to the horseshoe prior which (essentially) uses the penalty \\[\n\\pi_{HS} ( \\theta_i | \\tau ) = - \\log p_{HS} ( \\theta_i | \\tau ) = - \\log \\log \\left ( 1 + \\frac{2 \\tau^2}{\\theta_i^2} \\right ) .\n\\] The motivation for the horseshoe penalty arises from the analysis of the prior mass and influence on the posterior in both the tail and behaviour at the origin. The latter is the key determinate of the sparsity properties of the estimator.\nThe horseshoe Carvalho, Polson, and Scott (2010) is a Bayesian method for ‘needle-in-a-haystack’ type problems where there is some sparsity, meaning that there are some signals amid mostly noise.\nWe introduce the horseshoe in the context of the normal means model, which is given by \\[Y_i = \\beta_i + \\varepsilon_i, \\quad i = 1, \\ldots, n,\\] with \\(\\varepsilon_i\\) i.i.d. \\(\\mathcal{N}(0, \\sigma^2)\\). The horseshoe prior is given by \\[\\begin{align*}\n\\beta_i &\\sim \\mathcal{N}(0, \\sigma^2 \\tau^2 \\lambda_i^2)\\\\\n\\lambda_i &\\sim C^+(0, 1),\n\\end{align*}\\] where \\(C^+\\) denotes the half-Cauchy distribution. Optionally, hyperpriors on \\(\\tau\\) and \\(\\sigma\\) may be specified, as is described further in the next two sections.\nTo illustrate the shrinkage behaviour of the horseshoe, let’s plot the posterior mean for \\(\\beta_i\\) as a function of \\(y_i\\) for three different values of \\(\\tau\\).\n\nlibrary(horseshoe)\nlibrary(ggplot2)\ntau.values &lt;- c(0.005, 0.05, 0.5)\ny.values &lt;- seq(-5, 5, length = 100)\ndf &lt;- data.frame(tau = rep(tau.values, each = length(y.values)),\n                 y = rep(y.values, 3),\n                 post.mean = c(HS.post.mean(y.values, tau = tau.values[1], Sigma2=1), \n                               HS.post.mean(y.values, tau = tau.values[2], Sigma2=1), \n                               HS.post.mean(y.values, tau = tau.values[3], Sigma2=1)) )\n\nggplot(data = df, aes(x = y, y = post.mean, group = tau, color = factor(tau))) + \n  geom_line(size = 1.5) + \n  scale_color_brewer(palette=\"Dark2\") + \n  geom_abline(lty = 2) + geom_hline(yintercept = 0, colour = \"grey\") + \n  theme_classic() + ylab(\"\") + labs(color = \"Tau\") +\n  ggtitle(\"Horseshoe posterior mean for three values of tau\") \n\n\n\n\n\n\n\n\nSmaller values of \\(\\tau\\) lead to stronger shrinkage behaviour of the horseshoe. Observations that are in absolute value at most equal to \\(\\sqrt{2\\sigma^2\\log(1/\\tau)}\\) are shrunk to values close to zero (Van der Pas et al (2014)). For larger observed values, the horseshoe posterior mean will tend to the identity (that is, barely any shrinkage, the estimate will be very close to the observed value). The optimal value of \\(\\tau\\) is the proportion of true signals. This value is typically not known in practice but can be estimated, as described further in the next sections.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "12-theoryai.html#the-normal-means-problem",
    "href": "12-theoryai.html#the-normal-means-problem",
    "title": "12  Theory of AI",
    "section": "12.18 The normal means problem",
    "text": "12.18 The normal means problem\nThe normal means model is: \\[Y_i = \\beta_i + \\varepsilon_i, \\quad i = 1, \\ldots, n,\\] with \\(\\varepsilon_i\\) i.i.d. \\(\\mathcal{N}(0, \\sigma^2)\\).\nFirst, we will be computing the posterior mean only, with known variance \\(\\sigma^2\\) The function HS.post.mean computes the posterior mean of \\((\\beta_1, \\ldots, \\beta_n)\\). It does not require MCMC and is suitable when only an estimate of the vector \\((\\beta_1, \\ldots, \\beta_n)\\) is desired. In case uncertainty quantification or variable selection is also of interest, or no good value for \\(\\sigma^2\\) is available, please see below for the function HS.normal.means.\nThe function HS.post.mean requires the observed outcomes, a value for \\(\\tau\\) and a value for \\(\\sigma\\). Ideally, \\(\\tau\\) should be equal to the proportion of nonzero \\(\\beta_i\\)’s. Typically, this proportion is unknown, in which case it is recommended to use the function HS.MMLE to find the marginal maximum likelihood estimator for \\(\\tau\\).\nAs an example, we generate 50 data points, the first 10 of which are coming from true signals. The first 10 \\(\\beta_i\\)’s are equal to five and the remaining \\(\\beta_i\\)’s are equal to zero. Let’s first plot the true parameters (black) and observations (blue).\n\ndf &lt;- data.frame(index = 1:50,\n                 truth &lt;- c(rep(5, 10), rep(0, 40)),\n                 y &lt;- truth + rnorm(50) #observations\n                 )\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  ggtitle(\"Black = truth, Blue = observations\")\n\n\n\n\n\n\n\n\nWe estimate \\(\\tau\\) using the MMLE, using the known variance.\n\n(tau.est &lt;- HS.MMLE(df$y, Sigma2 = 1))\n\n## [1] 0.7505667\n\n\nWe then use this estimate of \\(\\tau\\) to find the posterior mean, and add it to the plot in red.\n\npost.mean &lt;- HS.post.mean(df$y, tau.est, 1)\ndf$post.mean &lt;- post.mean\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean), size = 2, col = \"red\") +\n  ggtitle(\"Black = truth, Blue = observations, Red = estimates\")\n\n\n\n\n\n\n\n\nIf the posterior variance is of interest, the function HS.post.var can be used. It takes the same arguments as HS.post.mean.\n\n12.18.1 Posterior mean, credible intervals and variable selection, possibly unknown \\(\\sigma^2\\)\nThe function HS.normal.means is the main function to use for the normal means problem. It uses MCMC and results in an object that contains all MCMC samples as well as the posterior mean for all parameters (\\(\\beta_i\\)’s, \\(\\tau\\), \\(\\sigma\\)), the posterior median for the \\(\\beta_i\\)’s, and credible intervals for the \\(\\beta_i\\)’s.\nThe key choices to make are:\n\nHow to handle \\(\\tau\\). The recommended option is “truncatedCauchy” (a half-Cauchy prior truncated to \\([1/n, 1]\\)). See the manual for other options.\nHow to handle \\(\\sigma\\). The recommended option is “Jeffreys” (Jeffrey’s prior). See the manual for other options.\n\nOther options that can be set by the user are the level of the credible intervals (default is 95%), and the number of MCMC samples (default is 1000 burn-in samples and then 5000 more).\nLet’s continue the example from the previous section. We first create a ‘horseshoe object’.\n\nhs.object &lt;- HS.normal.means(df$y, method.tau = \"truncatedCauchy\", method.sigma = \"Jeffreys\")\n\nWe extract the posterior mean of the \\(\\beta_i\\)’s and plot them in red.\n\ndf$post.mean.full &lt;- hs.object$BetaHat\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full), size = 2, col = \"red\") +\n  ggtitle(\"Black = truth, Blue = observations, Red = estimates\")\n\n\n\n\n\n\n\n\nWe plot the marginal credible intervals (and remove the observations from the plot for clarity).\n\ndf$lower.CI &lt;- hs.object$LeftCI\ndf$upper.CI &lt;- hs.object$RightCI\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full), size = 2, col = \"red\") +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI), width = .1, col = \"red\") +\n  ggtitle(\"Black = truth, Red = estimates with 95% credible intervals\")\n\n\n\n\n\n\n\n\nFinally, we perform variable selection using HS.var.select. In the normal means problem, we can use two decision rules. We will illustrate them both. The first method checks whether zero is contained in the credible interval, as studied by Van der Pas et al (2017).\n\ndf$selected.CI &lt;- HS.var.select(hs.object, df$y, method = \"intervals\")\n\nThe result is a vector of zeroes and ones, with the ones indicating that the observations is suspected to correspond to an actual signal. We now plot the results, coloring the estimates/intervals blue if a signal is detected and red otherwise.\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full, col = factor(selected.CI)), \n             size = 2) +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI, col = factor(selected.CI)),\n                width = .1) +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Black = truth, Blue = selected as signal, Red = selected as noise\")\n\n\n\n\n\n\n\n\nThe other variable selection method is the thresholding method of Carvalho et al (2010). The posterior mean can be written as \\(c_iy_i\\) where \\(y_i\\) is the observation and \\(c_i\\) some number between 0 and 1. A variable is selected if \\(c_i \\geq c\\) for some user-selected threshold \\(c\\) (default is \\(c = 0.5\\)). In the example:\n\ndf$selected.thres &lt;- HS.var.select(hs.object, df$y, method = \"threshold\")\n\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full, col = factor(selected.thres)), \n             size = 2) +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI, col = factor(selected.thres)),\n                width = .1) +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Black = truth, Blue = selected as signal, Red = selected as noise\")\n\n\n\n\n\n\n\n\n\n\n\n\nAndrews, D. F., and C. L. Mallows. 1974. “Scale Mixtures of Normal Distributions.” Journal of the Royal Statistical Society. Series B (Methodological) 36 (1): 99–102. https://www.jstor.org/stable/2984774.\n\n\nBox, George E. P., and George C. Tiao. 1992. Bayesian Inference in Statistical Analysis. New York: Wiley-Interscience.\n\n\nCarvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010. “The Horseshoe Estimator for Sparse Signals.” Biometrika, asq017.\n\n\nDiaconis, P., and D. Ylvisaker. 1983. “Quantifying Prior Opinion.”\n\n\nEfron, Bradley, and Carl Morris. 1977. “Stein’s Paradox in Statistics.” Scientific American 236 (5): 119–27.\n\n\nFrank, Ildiko E., and Jerome H. Friedman. 1993. “A Statistical View of Some Chemometrics Regression Tools.” Technometrics 35 (2): 109–35. https://www.jstor.org/stable/1269656.\n\n\nHuang, Jian, Joel L. Horowitz, and Shuangge Ma. 2008. “Asymptotic Properties of Bridge Estimators in Sparse High-Dimensional Regression Models.” The Annals of Statistics 36 (2): 587–613.\n\n\nMazumder, Rahul, Friedman, and Trevor and Hastie. 2011. “SparseNet: Coordinate Descent With Nonconvex Penalties.” Journal of the American Statistical Association 106 (495): 1125–38.\n\n\nStein, Charles. 1964. “Inadmissibility of the Usual Estimator for the Variance of a Normal Distribution with Unknown Mean.” Annals of the Institute of Statistical Mathematics 16 (1): 155–60.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-regression.html",
    "href": "13-regression.html",
    "title": "13  Linear and Multiple Regression",
    "section": "",
    "text": "13.1 Linear Regression\nThe simplest form of a parametric model is a linear model that assumes a linear relationship between the input variables and the output variable. There are a number of possibilities to specify such a family of functions, for example as a linear combinations of inputs \\[\nf(x)=\\beta_0+\\beta_1x_1 + \\ldots + \\beta_p x_p = \\beta^Tx,\n\\] we assume that vector \\(x\\) has \\(p+1\\) components \\(x = (1,x_1,\\ldots,x_p)\\) and \\(\\beta = (\\beta_0,\\beta_1,\\ldots,\\beta_p)\\) is a vector of parameters.\nA more general form of a linear model is a linear combination of basis functions \\[\nf(x)= \\beta_0 + \\beta_1 \\psi_1(x) + \\ldots + \\beta_M \\psi_M(x) = \\beta^T \\psi(x),\n\\] where \\(\\psi_1,\\ldots,  \\psi_M\\) are the basis functions and \\(\\psi(x) = (1, \\psi_1(x),\\ldots,\\psi_M(x))\\).\nNotice in the later case, the function \\(f\\) is linear in the parameters \\(\\beta = (\\beta_0,\\beta_1,\\ldots, \\beta_p)\\) but not in the input variables \\(x\\). The goal of the modeler to choose an appropriate set of predictors and basis functions that will lead to a good reconstruction of the input-output relations. After we’ve specified what the function \\(f\\) is, we need to find the best possible values of parameters \\(\\beta\\).\nFinding a predictive rule \\(f(x)\\) starts by defining the criterion of what is a good prediction. We assume that the function \\(f(x)\\) is parameterized by a vector of parameters \\(\\beta\\) and we want to find the best value of \\(\\beta\\) that will give us the best prediction. Thus, we will use notation \\(f(x,\\beta)\\).\nWe will us a loss function that quantifies the difference between the predicted and actual values of the output variable. It is closely related to the loss function used in decision theory (thus the name). In decision theory, a loss function is a mathematical representation of the “cost” associated with making a particular decision in a given state of the world. It quantifies the negative consequences of choosing a specific action and helps guide decision-makers towards optimal choices. You can think of the loss function in predictive problems as a “cost” associated with making an inaccurate prediction given the values of the input variables \\(x\\).\nThe least squares loss function is the sum of squared differences between the predicted and actual values. Given observed data set \\(\\{(x_1,y_1),\\ldots,(x_n,y_n)\\}\\), the least squares estimator is the value of \\(\\beta\\) that minimizes the sum of squared errors \\[\n\\mini_{\\beta}\\sum_{i=1}^n (y_i - f(x_i,\\beta))^2\n\\] It is easy to show that in the case of normal distribution, the least squares estimator is the maximum likelihood estimator.\nIn the unconditional case, when we do not observe any inputs \\(x\\), the least squares estimator is the sample mean. We can solve his minimization problem by taking derivative of the loss function and setting it to zero \\[\n\\frac{d}{d\\beta}\\sum_{i=1}^n (y_i - \\beta)^2 = -2\\sum_{i=1}^n (y_i - \\beta) = 0\n\\] which gives us the solution \\[\n\\hat{\\beta} = \\frac{1}{n}\\sum_{i=1}^n y_i\n\\] which is the sample average.\nThe least absolute deviations (Quantile) loss function is the sum of absolute differences between the predicted and actual values. It is used for regression problems with continuous variables. The goal is to minimize the sum of absolute errors (SAE) to improve the predictive performance of the model. Given observed data set the least absolute deviations estimator is the value of \\(\\beta\\) that minimizes the sum of absolute errors \\[\n\\mini_{\\beta}\\sum_{i=1}^n |y_i - f(x_i,\\beta)|\n\\] The least absolute deviations estimator is also known as the quantile estimator, where the quantile is set to 0.5 (the median). This is because the least absolute deviations estimator is equivalent to the median of the data (the 0.5 quantile).\nAgain, in the unconditional case, when we do not observe any inputs \\(x\\), the least absolute deviations estimator is the sample median. We can solve his minimization problem by taking derivative of the loss function and setting it to zero \\[\n\\frac{\\mathrm{d} \\left | x \\right | }{\\mathrm{d} x} = \\operatorname{sign} \\left( x \\right)\n\\] where \\(\\operatorname{sign} \\left( x \\right)\\) is the sign function. Hence, deriving the sum above yields \\[\n\\sum_{i=1}^n \\operatorname{sign}(y_i - \\beta).\n\\] This equals to zero only when the number of positive items equals the number of negative which happens when \\(\\beta\\) is the median.\nA more rigorous and non-calculus proof is due to Schwertman, Gilks, and Cameron (1990). Let \\(y_1,\\ldots,y_n\\) be the observed data and \\(\\hat{\\beta}\\) be the least absolute deviations estimator. Then we have \\[\n\\sum_{i=1}^n |y_i - \\hat{\\beta}| \\leq \\sum_{i=1}^n |y_i - \\beta|\n\\] for any \\(\\beta\\). Let \\(y_{(1)},\\ldots,y_{(n)}\\) be the ordered data. Then we have \\[\n\\sum_{i=1}^n |y_i - \\hat{\\beta}| \\leq \\sum_{i=1}^n |y_i - y_{(i)}|\n\\] Let \\(y_{(n/2)}\\) be the median of the data. Then we have \\[\n\\sum_{i=1}^n |y_i - \\hat{\\beta}| \\leq \\sum_{i=1}^n |y_i - y_{(n/2)}|\n\\] which implies that \\(\\hat{\\beta}\\) is the median of the data.\nThe generalization of the median estimator to the case of estimating value of quantile \\(\\tau\\) is as follows \\[\n\\mini_{\\beta}\\sum_{i=1}^n \\rho_{\\tau}(y_i - \\beta)\n\\] where \\(\\rho_{\\tau}(x) = x(\\tau - \\mathbb{I}(x &lt; 0))\\) is the quantile loss function. If we set \\(\\tau = 0.5\\), the loss function becomes the absolute value function and we get the median estimator. The expected loss is \\[\nE \\rho_{\\tau}(y - \\beta) = (\\tau-1)\\int_{-\\infty}^{\\beta} (y-\\beta)dF(y) + \\tau\\int_{\\beta}^{\\infty} (y-\\beta)dF(y)\n\\] Differentiating the expected loss function with respect to \\(\\beta\\) and setting it to zero gives the quantile estimator \\[\n\\hat{\\beta}_{\\tau} = F^{-1}(\\tau)\n\\] where \\(F^{-1}\\) is the quantile function of the distribution of \\(y\\). Thus, the problem of finding a quantile is solved via optimisation.\nA key difference between the least squares and least absolute deviations estimators is their sensitivity to outliers. The least squares estimator is sensitive to outliers because it squares the errors, giving more weight to large errors. In contrast, the least absolute deviations estimator is less sensitive to outliers because it takes the absolute value of the errors, giving equal weight to all errors. This makes the least absolute deviations estimator more robust to outliers than the least squares estimator.\nAnother difference is the computational complexity. Least squares estimator can be found by solving a linear system of equations. There are fast and efficient algorithms for it, making the least squares estimator computationally efficient. In contrast, the least absolute deviations estimator cannot requires more computationally expensive numerical optimization algorithms.\nThere is also a hybrid loss function, called Huber loss , which combines the advantages of squared errors and absolute deviations. It uses SE for small errors and AE for large errors, making it less sensitive to outliers.\nWe used lm function to fit the linear model for the housing data. This function uses least squares loss function to estimate the parameters of the line. One of the nice properties of the least squares estimator is that it has a closed-form solution. This means that we can find the values of the parameters that minimize the loss function by solving a system of linear equations rather than using an optimisation algorithm. The linear system is obtaind by taking the gradient (multivariate derivative) of the loss function with respect to the parameters and setting it to zero. The loss function \\[\nL(\\beta; ~D) = \\sum_{i=1}^n (y_i - f(x_i,\\beta))^2\n\\] is a quadratic function of the parameters, so the solution is unique and can be found analytically. The gradient of the loss function with respect to the parameters is \\[\n\\Delta L(\\beta; ~D) = -2\\sum_{i=1}^n (y_i - f(x_i,\\beta))\\Delta f(x_i,\\beta).\n\\] Given that \\(f(x_i,\\beta) = \\beta^T \\psi(x_i)\\), the gradient of the loss function with respect to the parameters is \\[\n\\Delta L_{\\beta}  = -2\\sum_{i=1}^n (y_i - \\beta^T \\psi(x_i))\\psi(x_i)^T.\n\\] Setting the gradient to zero gives us the normal equations \\[\n\\sum_{i=1}^n y_i \\psi(x_i) = \\sum_{i=1}^n \\beta^T \\psi(x_i) \\psi(x_i)^T.\n\\] In matrix form, the normal equations are \\[\n\\Psi^Ty = \\Psi^T\\Psi\\beta\n\\] where \\(\\Psi\\) is the design matrix with rows \\(\\psi(x_i)^T\\) and \\(y\\) is the vector of output values. The solution to the normal equations is \\[\n\\hat{\\beta} = (\\Psi^T\\Psi)^{-1}\\Psi^Ty.\n\\]\nThe function solve indeed finds the same values as the lm function. Essentially this is what the lm function does under the hood. The solve function uses the elimination method to solve the system of linear equations. The method we all learned when we were introduced to linear equations. The technique is known in linear algebra as LU decomposition.\nIn out housing example we used a linear model to predict the price of a house based on its square footage. The model is simple and easy to interpret, making it suitable for both prediction and interpretation. The model provides insights into the relationship between house size and price, allowing us to understand how changes in house size affect the price. The model can also be used to make accurate predictions of house prices based on square footage. This demonstrates the versatility of linear models for both prediction and interpretation tasks.\nLet’s consider another example of using a linear model that allows us to understand the relations between the performance of stock portfolio managed by John Maynard Keynes and overall market performance.\nRegression analysis is the most widely used statistical tool for understanding relationships among variables. It provides a conceptually simple method for investigating functional relationships between one or more factors and an outcome of interest. This relationship is expressed in the form of an equation, which we call the model, connecting the response or dependent variable and one or more explanatory or predictor variable.\nIt is convenient to introduce a regression model using the language of probability and uncertainty. A regression model assumes that mean of output variable \\(y\\) depends linearly on predictors \\(x_1,\\ldots,x_p\\) \\[\ny = \\beta_0 +  \\beta_1 x + \\ldots + \\beta_p x_p + \\epsilon,~ \\text{where}~\\epsilon \\sim N(0, \\sigma^2).\n\\] Often, we use simpler dot-product notation \\[\ny = \\beta^Tx + \\epsilon,\n\\] where \\(\\beta = (\\beta_0,\\beta_1,\\ldots,\\beta_p)\\) is the vector regression coefficients and \\(x = (1,x_1,\\ldots,x_p)\\) is the vector of predictors, with 1 appended to the beginning.\nLine coefficients \\(\\beta_i\\)s have the same interpretation as in the deterministic approach. However, the additional term \\(\\epsilon\\) is a random variable that captures the uncertainty in the relationship between \\(y\\) and \\(x\\), it is called the error term or the residual. The error term is assumed to be normally distributed with mean zero and variance \\(\\sigma^2\\). Thus, the linear regression model has a new parameter \\(\\sigma^2\\) that models dispersion of \\(y_i\\) around the mean \\(\\beta^Tx\\), let’s see an example.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "13-regression.html#estimates-and-intervals",
    "href": "13-regression.html#estimates-and-intervals",
    "title": "13  Linear and Multiple Regression",
    "section": "13.2 Estimates and Intervals",
    "text": "13.2 Estimates and Intervals\nIn our housing example, we estimated the parameter \\(\\beta_1\\) to be equal to 113.12 and made a conclusion that the the price of the house goes up by that amount when the lining area goes up by one unit. However, the estimated value is based on a sample. The sample is a result of well-designed data collection procedure and is representative of the population, and we should expect the estimated value to be close to the true value. However, the estimated value is not the true value of the parameter, but an estimate of it. The true value of the parameter is unknown and is the estimated value is subject to sampling error.\nThis means that the estimated value of the parameter is not the true value of the parameter, but an estimate of it. The true value of the parameter is unknown and can only be estimated from the sample data. The estimated value of the parameter is subject to sampling error, which is modeled by the normal distribution. The standard error of the estimate is a measure of the uncertainty in the estimated value of the parameter. The standard error is calculated from the sample data and is used to calculate confidence intervals and p-values for the estimated parameter.\nused the lm function to estimate the parameters of the linear model. The estimated values of the parameters are given in the Estimate column of the output. The estimated value of the intercept is \\(\\hat \\beta_0 = 13.44\\) and the estimated value of the slope is \\(\\hat \\beta_1 = 113.12\\). These values are calculated using the least squares loss function, which minimizes the sum of squared differences between the predicted and actual values of the output variable. The estimated values of the parameters are subject to sampling error, which is modeled by the normal distribution. The standard error of the estimates is given in the Std. Error column of the output. The standard error is a measure of the uncertainty in the estimated values of the parameters. The t-statistic is the ratio of the estimated coefficient to its standard error. The p-value is the probability of observing a value at least as extreme as the one observed, assuming the null hypothesis is true. In this case, the p-value for the livingArea coefficient is less than 0.05, so we conclude that the coefficient is statistically significant. This means that the size of the house is a statistically significant predictor of the price. The Residual standard error is the standard deviation of the residuals \\(\\hat y_i - y_i,~i=1,\\ldots,n\\).\n\nExample 13.3 (House Prices) Let’s go back to the Saratoga Houses dataset\n\nd = read.csv(\"../data/SaratogaHouses.csv\")\nd$price = d$price/1000; d$livingArea = d$livingArea/1000\nl = lm(price ~ livingArea, data=d)\nsummary(l)\n\n\nCall:\nlm(formula = price ~ livingArea, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-277.0  -39.4   -7.7   28.4  553.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    13.44       4.99    2.69   0.0072 ** \nlivingArea    113.12       2.68   42.17   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 69 on 1726 degrees of freedom\nMultiple R-squared:  0.507, Adjusted R-squared:  0.507 \nF-statistic: 1.78e+03 on 1 and 1726 DF,  p-value: &lt;2e-16\n\n\nThe output of the lm function has several components. Besides calculating the estimated values of the coefficients, given in the Estimate column, the method also calculates standard error (Std. Error) and t-statistic (t value) for each coefficient. The t-statistic is the ratio of the estimated coefficient to its standard error. The p-value (Pr(&gt;|t|)) is the probability of observing a value at least as extreme as the one observed, assuming the null hypothesis is true. The null hypothesis is that the coefficient is equal to zero. If the p-value is less than 0.05, we typically reject the null hypothesis and conclude that the coefficient is statistically significant. In this case, the p-value for the livingArea coefficient is less than 0.05, so we conclude that the coefficient is statistically significant. This means that the size of the house is a statistically significant predictor of the price. The Residual standard error the standard deviation of the residuals \\(\\hat y_i - y_i,~i=1,\\ldots,n\\).\n\nThe estimated values of the parameters were calculated using least squares loss function discussed above. The residual standard error is also relatively easy to calculate from the model residuals \\[\ns_e = \\sqrt{ \\frac{1}{n-2} \\sum_{i=1}^n ( \\hat y_i - y_i )^2 }.\n\\] Now the question is, how the p-value for the estimates was calculated? And why did we assume that \\(\\epsilon\\) is normally distributed in the first place? The normality of \\(\\epsilon\\) and as a consequence, the conditional normality of \\(y \\mid x \\overset{iid}{\\sim}  N(\\beta^Tx, \\sigma^2)\\) is easy to explain, it is simply due to mathematical convenience. Plus, this assumption happen to describe the reality well in a wide range of applications. One of those conveniences, is our ability to estimate to calculate mean and variance of the distribution of \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\).\nTo understand how to calculate the p-values, we first notice that there is uncertainty about the values of the parameters \\(\\beta_i\\)s. To get a feeling for the amount of variability in our experimentation. Imagine that we have two sample data sets. For example, we have housing data from two different realtor firms. Do you think the estimated value of price per square foot will be the same for both of those? The answer is no. Let’s demonstrate with an example, we simulate 20 data sets from the same distribution and estimate 20 different linear models.\n\nset.seed(92) #Kuzy\nd = read.csv(\"../data/SaratogaHouses.csv\")\nd$price = d$price/1000; d$livingArea = d$livingArea/1000\nplot(d$livingArea, d$price, pch=20, col=\"blue\", xlab=\"Living Area\", ylab=\"Price\")\nfor (i in 1:10) {\n  # Sample with replacement\n  dnew = d[sample(1:nrow(d),replace=T),]\n  # Fit a linear model\n  l = lm(price ~ livingArea, data=dnew, subset = sample(1:nrow(d),100))\n  abline(l, col=\"green\", lwd=3)\n}\n\n\n\n\n\n\n\nFigure 13.1: Twenty different linear models estimated using randomly selected subsample of the data.\n\n\n\n\n\nFigure 13.1 shows the results of this simulation. We can see that the estimated coefficients \\(\\hat \\beta_i\\) are different for each of the 20 samples. This is due to the sampling error. The sampling error is the difference between the estimated value of a parameter and its true value. The value of \\(\\beta_1\\) will differ from sample to sample. In other words, it will be a random variable. The sampling distribution of \\(\\beta_1\\) describes how it varies over different samples with the \\(x\\) values fixed. Statistical view of linear regression allows us to calculate confidence and prediction intervals for estimated parameters. It turns out that when least squares principle is used, the estimated \\(\\hat\\beta_1\\) is normally distributed: \\(\\hat\\beta_1 \\sim N( \\beta_1 , s_1^2 )\\). Let’s see how we can derive this result.\nThe extension of the central limit theorem, sometimes called the Lindeberg CLT, states that a linear combination of independent random variables that satisfy some mild condition are approximately normally distributed. We can show that estimates of \\((\\beta_0\\ldots,\\beta_p)\\) are linear combinations of the observed values of \\(y\\) and are therefore normally distributed. Indeed, if we write the linear regression model in matrix form \\[\nY = X \\beta + \\epsilon,\n\\] where \\(Y\\) is the vector of observed values of the dependent variable, \\(X\\) is the matrix of observed values of the independent variables, \\(\\beta\\) is the vector of unknown parameters, and \\(\\epsilon\\) is the vector of errors. Then, if we take the derivative of the loss function for linear regression and set it to zero, we get the following expression for the estimated parameters \\[\n\\hat \\beta =  AY,\n\\] where \\(A = (X^TX)^{-1}X^T\\). Due to Lindeberg central limit theorem, \\(\\hat \\beta\\) is normally distributed. This is a useful property that allows us to calculate confidence intervals and p-values for the estimated parameters.\nNow, we need to compute the mean and variance of \\(\\hat \\beta\\). The mean is easy to compute, since the expectation of the sum is the sum of expectations, we have \\[\n\\hat{\\beta} = A(X\\beta + \\epsilon)\n\\] \\[\n\\hat{\\beta} = \\beta + A\\epsilon\n\\]\nThe expectation of \\(\\hat{\\beta}\\) is: \\[\nE[\\hat{\\beta}] = E[\\beta + A\\epsilon] = E[\\beta] + E[A\\epsilon] = \\beta\n\\] Since \\(\\beta\\) is constant, and \\(E[A\\epsilon] = 0\\) (\\(E[\\epsilon] = 0\\)).\nThe variance is given by \\[\nVar(\\hat{\\beta}) = Var(A\\epsilon)\n\\] If we assume that \\(\\epsilon\\) is independent of \\(X\\), we can write and elements of \\(\\epsilon\\) are uncorrelated, we can write: \\[\nVar(\\hat{\\beta}) = AVar(\\epsilon)A^T\n\\] Given \\(Var(\\epsilon) = \\sigma^2 I\\), we have: \\[\nVar(\\hat{\\beta}) = \\sigma^2 (X^TX)^{-1}\n\\]\nPutting together the expectation and variance, and the fact that we get the following distribution for \\(\\hat{\\beta}\\): \\[\n\\hat{\\beta} \\sim N(\\beta, \\sigma^2 (X^TX)^{-1}).\n\\]\nStatistical approach to linear regression is useful. We can think of the estimated coefficients \\(\\hat \\beta_i\\) as an average amount of change in \\(y\\), when \\(x_i\\) goes up by one unit. Since this average was calculated using a sample data, it is subject to sampling error and the sampling error is modeled by the normal distribution. Assuming that residuals \\(\\epsilon\\) are independently normally distributed with a variance that does not depend on \\(x\\) (homoscedasticity), we can calculate the mean and variance of the distribution of \\(\\hat \\beta_i\\). This is a useful property that allows us to calculate confidence intervals and p-values for the estimated parameters.\nIn summary, the statistical view of the linear regression model is useful for understanding the uncertainty associated with the estimated parameters. It also allows us to calculate confidence intervals and prediction intervals for the output variable.\n\nAverage value of output \\(y\\) is a linear function of input \\(x\\) and lie on the straight line of regression \\(\\hat y_i = \\beta^Tx_i\\).\nThe values of \\(y\\) are statistically independent.\nThe true value of \\(y = \\hat y_i + \\epsilon_i\\) is a random variable, and it is normally distributed around the mean with variance \\(\\sigma^2\\). This variance is the same for all values of \\(y\\).\nThe estimated values of the parameters \\(\\hat \\beta_i\\) are calculated from observed data and are subject to the sampling error and we are not certain about them. This uncertainty is modeled by the normally distributed around the true values \\(\\beta\\). Given that errors \\(\\epsilon_i\\) are homoscedastic and independent, we have \\(Var(\\hat{\\beta}) = \\sigma^2 (X^TX)^{-1}\\).\n\nAgain, consider a house example. Say in our data we have 10 houses with the same squire footage, say 2000. Now the third point states, that the prices of those houses should follow a normal distribution and if we are to compare prices of 2000 sqft houses and 2500 sqft houses, they will have the same standard deviation. The second point means that price of one house does not depend on the price of another house.\nAll of the assumptions in the regression model can be written using probabilist notations The looks like: \\[\ny \\mid x \\overset{iid}{\\sim}  N(\\beta^Tx, \\sigma^2).\n\\]\nIn the case when we have only one predictor the variance of the estimated slope \\(\\hat \\beta_1\\) is given by \\[\nVar(\\hat \\beta_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n ( x_i - \\bar{x} )^2 } = \\frac{ \\sigma^2 }{ (n-1) s_x^2 },\n\\] where \\(s_x^2\\) is the sample variance of \\(x\\). Thus, there are three factors that impact the size of standard error for \\(\\beta_1\\): sample size (\\(n\\)), error variance (\\(s^2\\)), and \\(x\\)-spread, \\(s_x\\).\nWe can empirically demonstrate the sampling error by simulating several samples from the same distribution and estimating several linear models. We can see that the estimated coefficients \\(\\hat \\beta_i\\) are normally distributed around the true values \\(\\beta_i\\). If we plot coefficients for 1000 different models, we can see that the empirical distribution resembles a normal distribution.\n\nset.seed(92) #Kuzy\n# Read housing data\nd = read.csv(\"../data/SaratogaHouses.csv\")\nd$price = d$price/1000; d$livingArea = d$livingArea/1000\n# Simulate 1000 samples\nn = 1000\n# Create a matrix to store the results\nbeta = matrix(0, nrow=n, ncol=2)\n# Simulate 1000 samples\nfor (i in 1:n) {\n  # Sample with replacement\n  dnew = d[sample(1:nrow(d),replace=T),]\n  # Fit a linear model\n  l = lm(price ~ livingArea, data=dnew)\n  # Store the coefficients\n  beta[i,] = coef(l)\n}\nind = 2\n# Plot the results\nplot(beta[,1], beta[,2], pch=20, col=\"blue\", xlab=\"Intercept\", ylab=\"Slope\")\nabline(h=coef(l)[2], lwd=3, col=\"red\")\nabline(v=coef(l)[1], lwd=3, col=\"red\")\n\n\n\n\n\n\n\nX = cbind(1, d$livingArea)\n# Calculate the variance of the coefficients\nvar = sigma(l)^2 * solve(t(X) %*% X)\nvarb = var[ind,ind]/0.63\nhist(beta[,ind], col=\"blue\", xlab=\"Intercept\", main=\"\",freq=F)\n\n\n\n\n\n\n\nbt = seq(80,140,0.1)\n\nAccounting for uncertainty in \\(\\hat \\beta\\)s we can calculate confidence intervals for the predicted average \\(\\hat y\\). When we additionally account for the uncertainty in the predicted value \\(\\hat y\\), we can calculate prediction intervals.\nAnother advantage of adopting a statistical view of the linear regression model is ability to quantify information about potential outliers. Outliers are points that are extreme relative to our model predictions. Recall, that residual is \\(e_i  = y_i- \\hat y_i\\). Since our predicted value \\(\\hat y_i\\) follows a normal distribution, the residual also follows a normal distribution, since it is a difference of normal random variable \\(\\hat y_i\\) and a constant \\(y_i\\). It easy to see that \\[e_i \\sim N(0, s_e^2),\\] where \\(s_e^2\\) is an empirical estimate of the error’s variance.\nConsider the relation between the fitted values \\(\\hat y_i\\) and residuals \\(e_i\\). Our predictions are given by the line. The residual \\(e_i\\) and predicted value \\(\\hat y_i\\) for the \\(i\\)th observation are related via \\[\ny_i = \\hat{y}_i + ( y_i - \\hat{y}_i ) = \\hat{y}_i + e_i.\n\\]\nResiduals allow us to define outliers. They simply have large residuals. We re-scale the residuals by their standard errors. This lets us define \\[r_i = \\frac{ e_i }{ s_{ e} } =\\frac{y_i - \\hat{y}_i   }{ s_{ e } }\\] Since residuals follow normal distribution \\(e \\sim N(0,\\sigma^2)\\), in 95% of the time we expect the standardized residuals to satisfy \\(- 2 &lt; r_i &lt; 2\\). Any observation with is an extreme outlier, it is three sigmas away from the mean.\nAnother types of observations we are interested are the influential points. Those are are observations that affect the magnitude of our estimates \\(\\hat{\\beta}\\)’s. They are important to find as they typically have economic consequences. We will use to assess the significance of an influential point. Cook’s distance associated with sample \\(i\\) measure the change in estimated model parameters \\(\\hat \\beta\\) when sample \\(i\\) is removed from the training data set.\nIntuitively, we model regression-back-to-the-mean effect. This is one of the most interesting statistical effects you’ll see in daily life. In statistics, regression does not mean “going backwards”, but rather the tendency for a variable that is extremely high or low to move closer to the average upon subsequent measurement. For example, Francis Galton, who was a cousin of Charles Darwin, in his study on regression to the mean height showed that if your parents are taller than the average, you’ll regress back to the average. While people might expect the children of tall parents to be even taller and the children of short parents to be even shorter, Galton found that this wasn’t the case. Instead, he observed that the heights of the children tended to be closer to the average height for the population. Galton termed this phenomenon “regression towards mediocrity” (now more commonly known as “regression to the mean”). It meant that extreme characteristics (in this case, height) in parents were likely to be less extreme (closer to the average) in their children. It is a classic example that helped introduce and explain this statistical concept. Galton’s finding was one of the first insights into what is now a well-known statistical phenomenon. It doesn’t imply that all individual cases will follow this pattern; rather, it’s a trend observed across a population. It’s important to understand that regression to the mean doesn’t suggest that extreme traits diminish over generations but rather that an extreme measurement is partly due to random variation and is likely to be less extreme upon subsequent measurement.\nAnother example was documented by Daniel Kahneman and Amos Tversky in their book Thinking, Fast and Slow. They found that when a person performs a task, their performance is partly due to skill and partly due to luck. They observed that when a person performs a task and achieves an extreme result, their subsequent performance is likely to be less extreme. Particularly they studied effect of criticism and praise used by Israeli Air Force fighter pilots trainers. After criticism, the low-scoring pilots were retested. Often, their scores improve. At first glance, this seems like a clear effect of feedback from the trainer. However, some of this improvement is likely a statistical artifact and demonstrates the regression to the mean effect.\nWhy? Those pilots who initially scored poorly were, statistically speaking, somewhat unlucky. Their low scores may have been due to a bad day, stress, or other factors. When retested, their scores are likely to be closer to their true skill level, which is closer to the average. This natural movement towards the average can give the illusion that the intervention (praise or criticism) was more effective than it actually was. Conversely, if the top performers were praised and retested, we might find their scores decrease slightly, not necessarily due to the inefficacy of the praise but due to their initial high scores being partly due to good luck or an exceptionally good day. In conclusion, in pilot training and other fields, it’s important to consider regression to the mean when evaluating the effectiveness of interventions. Without this consideration, one might draw incorrect conclusions about the impact of training or other changes.\n\nExample 13.4 (Google vs S&P 500) We will demonstrate how we can use statistical properties of a linear regression model to understand the relationship between returns of a google stock and the S&P 500 index. We will use Capital Asset Pricing Model (CAPM) regression model to estimate the expected return of an investment into Google stock and to price the risk. The CAPM model is\n\\[\n\\mathrm{GOOG} = \\alpha + \\beta \\mathrm{SP500} + \\epsilon\n\\] On the left hand side, we have the return that investors expect to earn from investing into Google stock. In the CAPM model, this return is typically modeled as a dependent variable.\nThe input variable SP500 represents the average return of the entire US market. Beta measures the volatility or systematic risk of a security or a portfolio in comparison to the market as a whole. A beta greater than 1 indicates that the security is more volatile than the market, while a beta less than 1 indicates it is less volatile. Alpha is the intercept of the regression line, it measures the excess return of the security over the market. The error term \\(\\epsilon\\) captures the uncertainty in the relationship between the returns of Google stock and the market.\nIn a CAPM regression analysis, the goal is to find out how well the model explains the returns of a security based on its beta. This involves regressing the security’s excess returns (returns over the risk-free rate) against the excess returns of the market. The slope of the regression line represents the beta, and the intercept should ideally be close to the risk-free rate, although in practice it often deviates. This model helps in understanding the relationship between the expected return and the systematic risk of an investment.\nBased on the uncertainty associated with the estimates for alpha and beta, we can formulate some of hypothesis tests, for example\n\n\\(H_0 :\\) is Google related to the market?\n\\(H_0 :\\) does Google out-perform the market in a consistent fashion?\n\n\ngetSymbols(Symbols = c(\"GOOG\",\"SPY\"),from='2017-01-03',to='2023-12-29')\n\n \"GOOG\" \"SPY\" \n\ngret = as.numeric(dailyReturn(GOOG))\nspyret = as.numeric(dailyReturn(SPY))\nl = lm(gret ~ spyret)\ntidy(l) %&gt;% knitr::kable(digits=4)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0003\n0.0003\n1.1\n0.27\n\n\nspyret\n1.1706\n0.0240\n48.8\n0.00\n\n\n\nGoogle vs S&P 500 returns between 2017-2023\n\n\n\nplot(gret, spyret, pch=20, col=\"blue\", xlab=\"Google Return\", ylab=\"SPY Return\")\nabline(l, lwd=3, col=\"red\")\n\n\n\n\n\n\n\n\nHere’s what we get after we fit the model using function\nOur best estimates are \\[\n\\hat \\alpha = 0.0004 \\; , \\; \\hat{\\beta} = 1.01\n\\]\nNow we can provide the results for hypothesis we set at he beginning. Given that the p-value for \\(H_0: \\beta = 0\\) is &lt;2e-16 we can reject the null hypothesis and conclude that Google is related to the market. The p-value for \\(H_0: \\alpha = 0\\) is 0.06, which is grater than 0.05, so we cannot reject the null hypothesis and conclude that Google does not out-performs the market in a consistent fashion in the 2017-2023 period.\nFurther, we can answer some of the other important questions, such as how much will Google move if the market goes up \\(10\\)%?\n\nalpha = coef(l)[1]\nbeta = coef(l)[2]\n# Calculate the expected return\nalpha + beta*0.1\n\n(Intercept) \n       0.12 \n\n\nHowever, if we look at the earlier period between 2005-2016 (the earlier days of Google) the results will be different.\n\ngetSymbols(Symbols = c(\"GOOG\",\"SPY\"),from='2005-01-03',to='2016-12-29');\n\n \"GOOG\" \"SPY\" \n\ngret = as.numeric(dailyReturn(GOOG))\nspyret = as.numeric(dailyReturn(SPY))\nl = lm(gret ~ spyret)\ntidy(l) %&gt;% knitr::kable(digits=4)\n\n\n\nTable 13.1: Google vs S&P 500 returns between 2005-2016\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0006\n0.0003\n2.2\n0.03\n\n\nspyret\n0.9231\n0.0230\n40.1\n0.00\n\n\n\n\n\n\n\n\n\nplot(gret, spyret, pch=20, col=\"blue\", xlab=\"Google Return\", ylab=\"SPY Return\")\nabline(l, lwd=3, col=\"red\")\n\n\n\n\n\n\n\nFigure 13.2: Google vs S&P 500 returns between 2005-2016\n\n\n\n\n\nIn this period Google did consistently outperform the market. The p-value for \\(H_0: \\alpha = 0\\) is 0.03.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "13-regression.html#capm-model-for-yahoo-stock",
    "href": "13-regression.html#capm-model-for-yahoo-stock",
    "title": "13  Linear and Multiple Regression",
    "section": "13.3 CAPM Model for Yahoo! Stock",
    "text": "13.3 CAPM Model for Yahoo! Stock\nRather than estimate \\(\\mu\\) directly, the CAPM estimates the difference between \\(\\mu\\) and the risk-free rate \\(r_f\\). This quantity \\(\\mu-r_f\\) is known as the expected excess return (excess relative to a risk-free investment). The CAPM relates the expected excess return of a stock to that of an underlying benchmark, typically a broad-based market index. Let \\(\\mu_M\\) and \\(\\sigma_M\\) denote the return and volatility on the market index. The implication of CAPM is that there is a linear relationship between the expected excess return of a stock, \\(\\mu-r_f\\), and the excess return of the market, \\(\\mu_M-r_f\\).\n\\[\n\\text{Excess \\; Return}_{\\text{Stock}} =  \\beta \\;  \\text{Excess \\;\nReturn}_{\\text{Market}}\n\\] \\[\n\\mu-r_f = \\beta(\\mu_M - r_f )\n\\] Put simply, the expected excess return of a stock is \\(\\beta\\) times the excess expected return of the market. Beta (\\(\\beta\\)) is a measure of a stock’s risk in relation to the market. A beta of 1.3 implies that the excess return on the stock is expected to move up or down 30% more than the market. A beta bigger than one implies the stock is riskier than the market and goes up (and down) faster than the market goes up (and down). A beta less than one implies the stock is less risky than the market.\nUsing the CAPM, the expected return of the stock can now be defined as the risk free interest rate plus beta times the expected excess return of the market, \\[\n\\mu = \\text{Expected \\; Return}_{\\text{Stock}} = r_f+\\beta (\\mu_M-r_f)\n\\] Beta is typically estimated from a regression of the individual stock’s returns on those of the market. The other parameters are typically measured as the historical average return on the market \\(\\mu_M\\) and the yield on Treasury Bills \\(r_f\\). Together these form an estimate of \\(\\mu\\). The volatility parameter \\(\\sigma\\) is estimated by the standard deviation of historical returns.\nOur qualitative discussion implicitly took the year as the unit of time. For our example, we make one minor change and consider daily returns so that \\(\\mu\\) and \\(\\sigma\\) are interpreted as a daily rate of return and daily volatility (or standard deviation). We use an annual risk-free rate of 5%; this makes a daily risk-free rate of .019%, \\(r_f = 0.00019\\), assuming there are 252 trading days in a year. A simple historical average is used to estimate the market return (\\(\\mu_M\\)) for the Nasdaq 100. The average annual return is about 23%, with corresponding daily mean \\(\\mu_M = 0.00083\\). A regression using daily returns from 1996-2000 leads to an estimate of \\(\\beta = 1.38\\). Combining these (pieces) leads to an estimated expected return of Yahoo!, \\(\\mu_{Yahoo} = 0.00019+1.38(0.00083-0.00019) = 0.00107\\) on a daily basis. Note that the CAPM model estimates a future return that is much lower than the observed rate over the last three-plus years of .42% per day or 289% per year.\nTo measure the riskiness of Yahoo! notice that the daily historical volatility is 5%, i.e. \\(\\sigma = 0.05\\). On an annual basis this implies a volatility of \\(\\sigma \\sqrt{T} = 0.05 \\sqrt{252} = 0.79\\), that is 79%. For comparison, the benchmark Nasdaq 100 has historical daily volatility 1.9% and an annual historical volatility of 30%. The estimates of all the parameters are recorded in Table 13.2.\n\n\n\nTable 13.2: Key Parameter Estimates Based on Daily Returns 1996–2000\n\n\n\n\n\n\n\n\n\n\n\nAsset\nExpected return\nVolatility\nRegression coefft (s.e.)\n\n\n\n\nYahoo!\n\\(\\mu = 0.00107\\)\n\\(\\sigma = 0.050\\)\n\\(\\beta = 1.38 (.07)\\)\n\n\nNasdaq 100\n\\(\\mu_M = 0.00083\\)\n\\(\\sigma_M = 0.019\\)\n1\n\n\nTreasury Bills\n\\(r_f = 0.00019\\)\n–\n–",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "13-regression.html#factor-regression-and-feature-engineering",
    "href": "13-regression.html#factor-regression-and-feature-engineering",
    "title": "13  Linear and Multiple Regression",
    "section": "13.4 Factor Regression and Feature Engineering",
    "text": "13.4 Factor Regression and Feature Engineering\nA linear model assumes that output variable is proportional to the input variable plus an offset. However, this is not always the case. Often, we need to transform input variables by combining multiple inputs into a single predictor, for example by taking a ratio or putting inputs on a different scale, e.g. log-scale. In machine learning, this process is called feature engineering.\nOne of the classic examples of feature engineering is Fama-French three-factor model which is used in asset pricing and portfolio management. The model states that asset returns depend on (1) market risk, (2) the outperforming of small versus big companies, and (3) the outperformance of high book/market versus small book/market companies, mathematically \\[r = R_f + \\beta(R_m - R_f) + b_s\\cdot SMB + b_v\\cdot HML + \\alpha\\] Here \\(R_f\\) is risk-free return, \\(R_m\\) is the return of market, \\(SMB\\) stands for \"Small market capitalization Minus Big\" and \\(HML\\) for \"High book-to-market ratio Minus Low\"; they measure the historic excess returns of small caps over big caps and of value stocks over growth stocks. These factors are calculated with combinations of portfolios composed by ranked stocks (BtM ranking, Cap ranking) and available historical market data.\n\n13.4.1 Logarithmic and Power Transformations\nConsider, the growth of the Apple stock between 2000 and 2024. With the exception of the 2008 financial crisis period and 2020 COVID 19 related declines, the stock price has been growing exponentially. Figure 13.3 shows the price of the Apple stock between 2000 and 2024. The price is closely related to the company’s growth.\n\ngetSymbols(Symbols = \"AAPL\",from='2000-01-01',to='2023-12-31');\n\n \"AAPL\"\n\nplot(AAPL$AAPL.Adjusted, type='l', col=\"blue\", xlab=\"Date\", ylab=\"Price\")\n\n\n\n\n\n\n\nFigure 13.3: Apple stock price growth in the 2000-2024 period\n\n\n\n\n\nThe 2008 and 2020 declines are more related to extraneous factors, rather than the growth of the company. Thus, we can conclude that the overall growth of the company is exponential. Indeed, if we try to fit a linear model to the time-price data, we will see that the model does not fit the data well\n\ntime = index(AAPL) - start(AAPL)\nplot(time,AAPL$AAPL.Adjusted, type='l', col=\"blue\", xlab=\"Date\", ylab=\"Price\")\nabline(lm(AAPL$AAPL.Adjusted ~ time), col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\nJust by visual inspection we can see that a straight line will not be a good fit for this data. The growth of a successful company typically follows the rule of compounding. Compounding is a fundamental principle that describes how some quantity grows over time when this quantity increases by a fixed percentage periodically. This is a very common phenomenon in nature and business. For example, if two parents have 2.2 children on average, then the population increases by 10% every generation. Another example is growth of investment in a savings account.\nA more intuitive example is probably an investment in a savings account. If you invest \\(1000\\) in a savings account with \\(10\\%\\) annual interest rate and you get payed once a year, then your account value will be \\(1100\\) by the end of the year. However, if you get get payed \\(n\\) times a year, and initially invest \\(y_0\\) final value \\(y\\) of the account after \\(t\\) years will be \\[\ny = y_0 \\times (1 + r/n)^{nt}\n\\] where \\(r\\) is the annual interest rate. When you get payed every month (\\(n=12\\)), a traditional payout schedule used by banks, then \\[\ny = 1000 \\times (1 + 0.1/12)^{12} = 1105.\n\\] A value slightly higher than the annual payout of 1100.\nThe effect of compounding is minimal in the short term. However, the effect of compounding is more pronounced when the growth rate is higher and time periods are longer. For example at \\(r=2\\), \\(n=365\\) and 4-year period \\(t=4\\), you get \\[\ny = 1000 \\times (1 + 2/365)^{3\\times 365} = 2,916,565.\n\\] Your account is close to 3 million dollars! Compared to \\(n=1\\) scenario \\[\ny = 1000 \\times (1 + 2)^{4} = 81,000,\n\\] when you will end up with mealy 81 thousand. This is why compounding is often referred to as the “eighth wonder of the world” in investing contexts, emphasizing its power in growing wealth over time.\nIn general, as \\(n\\) goes up, the growth rate of the quantity approaches the constant\n\nn = 1:300\nr = 1\nplot(n, (1+r/n)^n, type='l', col=\"blue\", xlab=\"n\", ylab=\"Future Value\")\nabline(h=exp(r), col=\"red\", lwd=3)\n\n\n\n\n\n\n\nFigure 13.4: Growth of an investment in a savings account when n increases and return rate is 100% per year\n\n\n\n\n\nFigure 13.4 shows the growth of an investment in a savings account when \\(n\\) increases and return rate is \\(100\\%\\) per year. We can see that the growth rate approaches the constant \\(e \\approx 2.72\\) as \\(n\\) increases. \\[\n(1+r/n)^n \\rightarrow e^r,~\\text{as}~n \\rightarrow \\infty.\n\\] This limit was first delivered by Leonhard Euler and the number \\(e\\) is known as Euler’s number.\nComing back to the growth of the Apple company, we can think of it growing at small constant rate every day. The relation between the time and size of Apple is multiplicative. Meaning when when time increases by one day, the size of the company increases by a small constant percentage. This is a multiplicative relation. In contrast, linear relation is additive, meaning that when time increases by one day, the size of the company increases by a constant amount. The exponential growth model is given by the formula \\[\ny = y_0 \\times e^{\\beta^Tx}.\n\\] There are many business and natural science examples where multiplicative relation holds. IF we apply the \\(\\log\\) function to both sides of the equation, we get \\[\n\\log y = \\log y_0 + \\beta^Tx.\n\\] This is a linear relation between \\(\\log y\\) and \\(x\\). Thus, we can use linear regression to estimate the parameters of the exponential growth model by putting the output variable \\(y\\) on the log-scale.\nAnother example of nonlinear relation that can be analyzed using linear regression is when variables are related via a power law. This concept helps us modeling proportional relationships or ratios. In a multiplicative relationship, when one variable changes on a percent scale, the other changes in a directly proportional manner, as long as the multiplying factor remains constant. For example, the relation between the size of a city and the number of cars registered in the city is given by a power law. When the size of the city doubles, the number of cars registered in the city is also expected to double. The power law model is given by the formula \\[\ny = \\beta_0 x^{\\beta_1}.\n\\] If we apply the \\(\\log\\) function to both sides of the equation, we get \\[\n\\log y = \\log \\beta_0 + \\beta_1 \\log x.\n\\] This is a linear relation between \\(\\log y\\) and \\(\\log x\\). Thus, we can use linear regression to estimate the parameters of the power law model by putting the output variable \\(y\\) and input variable \\(x\\) on the log-scale.\nHowever, there are several caveats when putting variables on the log-scale. We need to make sure that the variable is positive. It means that we cannot apply log transformations to dummy or count variables.\n\nExample 13.5 (World’s Smartest Mammal) We will demonstrate the power relation using the data on the brain (measured in grams) and body (measured in kilograms) weights for 62 mammal species. The data was collected by Harry J. Jerison in 1973. The data set contains the following variables:\n\nmammals = read.csv(\"../data/mammals.csv\")\nknitr::kable(head(mammals))\n\n\n\n\nMammal\nBrain\nBody\n\n\n\n\nAfrican_elephant\n5712.0\n6654.00\n\n\nAfrican_giant_pouched_rat\n6.6\n1.00\n\n\nArctic_Fox\n44.5\n3.38\n\n\nArctic_ground_squirrel\n5.7\n0.92\n\n\nAsian_elephant\n4603.0\n2547.00\n\n\nBaboon\n179.5\n10.55\n\n\n\n\n\nLet’s build a linear model.\n\nattach(mammals)\nmodel = lm(Brain~Body)\nplot(Body,Brain, pch=21, bg=\"lightblue\")\nabline(model, col=\"red\", lwd=3)\n\n\n\n\nBrain vs Body weight for 62 mammal species\n\n\n\n\nWe see a few outliers with suggests that normality assumption is violated. We can check the residuals by plotting residuals against fitted values and plotting fitted vs true values.\nplot(model$fitted.values, model$residuals, pch=21, bg=\"lightblue\", xlab=\"Fitted y\", ylab=\"Residuals\")\nabline(h=0, col=\"red\", lwd=3)\nplot(Brain, model$fitted.values, pch=21, bg=\"lightblue\", xlab=\"True y\", ylab=\"Fitted y\")\nabline(a=0,b=1, col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\n\nFigure 13.5: Fitted y vs Residuals\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.6: Fitted y vs True y\n\n\n\n\n\n\nRemember, that residuals should roughly follow a normal distribution with mean zero and constant variance. We can see that the residuals are not normally distributed and the variance increases with the fitted values. This is a clear indication that we need to transform the data. We can try a log-log transformation.\n\nmodel = lm(log(Brain)~log(Body))\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.18\n0.11\n20\n0\n\n\nlog(Body)\n0.74\n0.03\n23\n0\n\n\n\n\n\nplot(model$fitted.values, model$residuals, pch=21, bg=\"lightblue\")\nabline(h=0, col=\"red\", lwd=3)\nplot(log(Brain), model$fitted.values, pch=21, bg=\"lightblue\")\nabline(a=0,b=1, col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\n\nFigure 13.7: Fitted y vs Residuals\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.8: Fitted y vs True y\n\n\n\n\n\n\nThat is much better! The residuals variance is constant and the plot of fitted vs true values shows a linear relationship. The log-log model is given by the formula \\[\n\\log \\mathrm{Brain} = 2.18 + 0.74 \\log \\mathrm{Body}.\n\\] seem to achieve two important goals, namely linearity and constant variance. The coefficients are highly significant.\nAlthough the log-log model fits the data rather well, there are a couple of outliers there. Let us print the observations with the largest residuals.\n\n# res = rstudent(model)\nres = model$residuals/sd(model$residuals)\noutliers = order(res,decreasing = T)[1:10]\ncbind(mammals[outliers,],\n      Std.Res = res[outliers], Residual=model$residuals[outliers],\n      Fit = exp(model$fitted.values[outliers])) %&gt;% knitr::kable(digits=2)\n\n\n\n\n\nMammal\nBrain\nBody\nStd.Res\nResidual\nFit\n\n\n\n\n11\nChinchilla\n64\n0.42\n3.41\n2.61\n4.7\n\n\n34\nMan\n1320\n62.00\n2.53\n1.93\n190.7\n\n\n50\nRhesus_monkey\n179\n6.80\n2.06\n1.58\n36.9\n\n\n6\nBaboon\n180\n10.55\n1.64\n1.26\n51.1\n\n\n42\nOwl_monkey\n16\n0.48\n1.44\n1.10\n5.1\n\n\n10\nChimpanzee\n440\n52.16\n1.26\n0.96\n167.7\n\n\n27\nGround_squirrel\n4\n0.10\n1.18\n0.91\n1.6\n\n\n43\nPatas_monkey\n115\n10.00\n1.11\n0.85\n49.1\n\n\n60\nVervet\n58\n4.19\n1.06\n0.81\n25.7\n\n\n3\nArctic_Fox\n44\n3.38\n0.92\n0.71\n22.0\n\n\n\n\n\nThere are two outliers, the Chinchilla and the Human, both have disproportionately large brains!\nIn fact, the Chinchilla has the largest standartised residual of 3.41. Meaning that predicted value of 4.7 g is 3.41 standard diviaitons away from the recorded value of 64 g. This suggests that the Chinchilla is a master race of supreme intelligence! However, afer checking more carefully we realized that there was a recording error and the acual weight of an average Chinchilla’s brain is 6.4. We mistyped the decimal separator! Thus the actual residual is 0.4.\n\nabs(model$fitted.values[11] - log(6.4))/sd(model$residuals)\n\n 11 \n0.4 \n\n\nIn reality Chinchilla’s brain is not far from an average mamal of this size!\n\n\nExample 13.6 (Newfood) A six month market test has been performed on the Newfood product, which is a breakfast cereal. The goal is to build a multiple regression model that provides accurate sales forecasts. This dataset represents the outcome of a controlled experiment in which the values of the independent variables that affect sales were carefully chosen by the analyst.\nThe analysis aims to identify the factors that contribute to sales of a new breakfast cereal and to quantify the effects of business decisions such as the choice of advertising level, location in store, and pricing strategies.\n\n\n\nvariable\ndescription\n\n\n\n\nsales\nnew cereal sales\n\n\nprice\nprice\n\n\nadv\nlow or high advertising (\\(0\\) or \\(1\\))\n\n\nlocat\nbread or breakfast section (\\(0\\) or \\(1\\))\n\n\ninc\nneighborhood income\n\n\nsvol\nsize of store\n\n\n\nFirst, we need to understand which variables need to be transformed. We start by running the “kitchen-sink” regression with all variables. Then we perform diagnostic checks to assess model assumptions and identify potential issues. Based on these diagnostics, we decide which variables should be transformed. After running the new model with transformations, we perform additional diagnostics and variable selection to refine the model. Using the final model after transformations and eliminating variables, we examine what the largest Cook’s distance is to identify influential observations. Finally, we provide a summary of coefficients and their statistical significance.\nFirst, let’s examine the correlation matrix to understand the relationships between all variables in the dataset. This will help us identify potential multicollinearity issues and understand the strength and direction of associations between variables before building our regression model.\n\nnewfood = read.csv(\"../data/newfood.csv\")\nattach(newfood)\nnames(newfood)\n\n \"sales\"  \"price\"  \"adv\"    \"locat\"  \"income\" \"svol\"   \"city\"   \"indx\"  \n\n# knitr::kable()\nhead(newfood)\n\n\n\n\n\nsales\nprice\nadv\nlocat\nincome\nsvol\ncity\nindx\n\n\n\n\n225\n24\n0\n0\n7.3\n34\n3\n1\n\n\n190\n24\n0\n0\n7.3\n34\n3\n2\n\n\n205\n24\n0\n0\n7.3\n34\n3\n3\n\n\n323\n24\n0\n0\n8.3\n41\n4\n1\n\n\n210\n24\n0\n0\n8.3\n41\n4\n2\n\n\n241\n24\n0\n0\n8.3\n41\n4\n3\n\n\n\n\n\n# correlation matrix\ncm = cor(cbind(sales,price,adv,locat,income,svol))\ncm[upper.tri(cm, diag = TRUE)] = NA\n# knitr::kable(as.table(round(cm, 3)))\nas.table(round(cm, 3)) %&gt;% knitr::kable()\n\n\n\n\n\nsales\nprice\nadv\nlocat\nincome\nsvol\n\n\n\n\nsales\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nprice\n-0.66\nNA\nNA\nNA\nNA\nNA\n\n\nadv\n0.00\n0.00\nNA\nNA\nNA\nNA\n\n\nlocat\n0.00\n0.00\n0.00\nNA\nNA\nNA\n\n\nincome\n0.16\n-0.13\n-0.75\n0.00\nNA\nNA\n\n\nsvol\n0.38\n-0.18\n-0.74\n-0.04\n0.81\nNA\n\n\n\n\n\nRemember, correlations between variables are not the same as regression coefficients (\\(\\beta\\)’s)! Looking at the correlation matrix, we can see that total sales volume (svol) is negatively correlated with advertising (adv), and income (income) is also negatively correlated with advertising (adv). The wuesiton is how might these negative correlations impact our ability to estimate the true advertising effects in our regression model?\n\nas.table(round(cm[2:4, 1:3], 3)) %&gt;% knitr::kable()\n\n\n\n\n\nsales\nprice\nadv\n\n\n\n\nprice\n-0.66\nNA\nNA\n\n\nadv\n0.00\n0\nNA\n\n\nlocat\n0.00\n0\n0\n\n\n\n\n\nThere’s no correlation in the \\(X\\)’s by design! Let’s start by only including price, adv, locat\n\nmodel = lm(sales~price+adv+locat)\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n562.31\n53.1\n10.58\n0.00\n\n\nprice\n-12.81\n1.8\n-7.20\n0.00\n\n\nadv\n0.22\n14.5\n0.02\n0.99\n\n\nlocat\n-0.22\n14.5\n-0.02\n0.99\n\n\n\n\n\nWhy is the marketer likely to be upset by this regression?! Why is the economist happy? Let’s add income and svol to the regression and use log-log model.\n\nmodel = lm(log(sales)~log(price)+adv+locat+log(income)+log(svol))\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.41\n1.39\n6.06\n0.00\n\n\nlog(price)\n-1.74\n0.22\n-7.90\n0.00\n\n\nadv\n0.15\n0.10\n1.49\n0.14\n\n\nlocat\n0.00\n0.06\n0.02\n0.99\n\n\nlog(income)\n-0.52\n0.50\n-1.06\n0.29\n\n\nlog(svol)\n1.03\n0.26\n4.04\n0.00\n\n\n\n\n\nWhy no logs for adv and locat variables? The log(svol) coefficient is close to one!\nThe reason we don’t apply logarithms to adv and locat variables is because they are binary categorical variables (taking values 0 or 1). Taking the logarithm of 0 is undefined, and taking the logarithm of 1 equals 0, which would not provide any meaningful transformation. For binary variables, the exponential transformation in the final model interpretation directly gives us the multiplicative effect on sales when the variable changes from 0 to 1.\nRegarding the log(svol) coefficient being close to one (1.03), this suggests that sales scale approximately proportionally with store volume. A coefficient of 1.0 would indicate perfect proportional scaling, meaning a 1% increase in store volume would lead to a 1% increase in sales. Our coefficient of 1.03 indicates slightly more than proportional scaling - a 1% increase in store volume leads to a 1.03% increase in sales, suggesting some economies of scale or network effects in larger stores.\nOn the transformed scale (log-log model), \\[\n\\log sales=8.41 - 1.74 \\log price + 0.150 {\\text{adv}} + 0.001 {\\text{locat}} - 0.524 \\log inc  + 1.03 \\log svol\n\\] On the un-transformed scale, \\[\n\\text{sales} = e^{8.41} ( \\text{price} )^{-1.74} e^{ 0. 15 \\text{adv} } e^{ 0.001 \\text{locat}} ( \\text{inc} )^{-0.524}  ( \\text{svol} )^{1.03}\n\\] In the log-log regression model, the relationship between sales and the continuous variables (price, income, and store volume) follows a power function relationship. This means that a 1% change in these variables leads to a proportional change in sales according to their respective coefficients. Specifically, a 1% increase in price leads to a 1.74% decrease in sales, a 1% increase in income leads to a 0.524% decrease in sales, and a 1% increase in store volume leads to a 1.03% increase in sales.\nIn contrast, the binary variables (advertising and location) follow an exponential relationship with sales. When advertising is present (adv=1), sales increase by a factor of e^0.15 = 1.16, representing a 16% improvement. Similarly, when a store is in a good location (locat=1), sales increase by a factor of e^0.001 = 1.001, representing a 0.1% improvement. This exponential relationship arises because these variables are binary (0 or 1) and cannot be log-transformed, so their effects are multiplicative on the original sales scale.\nThe log-log regression model reveals several important relationships between the independent variables and sales performance.\n\nPrice elasticity is \\(\\hat{\\beta}_{\\text{price}} = - 1.74\\). A \\(1\\)% increase in price will drop sales \\(1.74\\)%\n\\(\\mathrm{adv}=1\\) increases sales by a factor of \\(e^{0.15} = 1.16\\). That’s a \\(16\\)% improvement\n\nWe should delete the locat variable from our regression model because it is statistically insignificant. The coefficient for locat has a very small magnitude (0.001) and a high p-value, indicating that there is insufficient evidence to reject the null hypothesis that this variable has no effect on sales. Including statistically insignificant variables in a model can lead to overfitting and reduce the model’s predictive accuracy on new data. By removing locat, we create a more parsimonious model that focuses only on the variables that have meaningful relationships with the outcome variable.\nNow, we are ready to use our model for prediction. predict.lm provides a \\(\\hat{Y}\\)-prediction given a new \\(X_f\\)\n\nmodelnew = lm(log(sales)~log(price)+adv+log(income)+log(svol))\nmodelnew  %&gt;% tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.41\n1.37\n6.1\n0.00\n\n\nlog(price)\n-1.74\n0.22\n-8.0\n0.00\n\n\nadv\n0.15\n0.10\n1.5\n0.14\n\n\nlog(income)\n-0.52\n0.49\n-1.1\n0.29\n\n\nlog(svol)\n1.03\n0.25\n4.1\n0.00\n\n\n\n\nnewdata=data.frame(price=30,adv=1,income=8,svol=34)\npredict.lm(modelnew,newdata,se.fit=T,interval=\"confidence\",level=0.99,) %&gt;% knitr::kable(digits=2)\n\n\n\n\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n5.2\n4.9\n5.5\n\n\n\n\n\n\n\n\nx\n\n\n\n\n0.1\n\n\n\n\n\n\n\n\nx\n\n\n\n\n67\n\n\n\n\n\n\n\n\nx\n\n\n\n\n0.25\n\n\n\n\n\n\n\n\n\nExponentiate-back to find \\(\\text{sales} = e^{5.1739} = 176.60\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "13-regression.html#interactions",
    "href": "13-regression.html#interactions",
    "title": "13  Linear and Multiple Regression",
    "section": "13.5 Interactions",
    "text": "13.5 Interactions\nIn many situations, \\(X_1\\) and \\(X_2\\) interact when predicting \\(Y\\). An interaction occurs when the effect of one independent variable on the dependent variable changes at different levels of another independent variable. For example, consider a study analyzing the effect of study hours \\(X_1\\) and a tutoring program \\(X_2\\), a binary variable where 0 = no tutoring, 1 = tutoring) on test scores \\(Y\\). Without an interaction term, we assume the effect of study hours on test scores is the same regardless of tutoring. With an interaction term, we can explore whether the effect of study hours on test scores is different for those who receive tutoring compared to those who do not. Here are a few more examples when there is potential interraction.\n\nDoes gender change the effect of education on wages?\nDo patients recover faster when taking drug A?\nHow does advertisement affect price sensitivity?\nInteractions are useful. Particularly with dummy variables.\nWe build a kitchen-sink model with all possible dummies (day of the week, gender,...)\n\nIf we think that the effect of \\(X_1\\) on \\(Y\\) depends on the value of \\(X_2\\), we model it using a liner relation \\[\n\\beta_1 = \\beta_{10} + \\beta_{11} X_2\n\\] and the model without interaction \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\) becomes \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\epsilon.\n\\] The interaction term captures the effect of \\(X_1\\) on \\(Y\\) when \\(X_2=1\\). The coefficient \\(\\beta_3\\) is the difference in the effect of \\(X_1\\) on \\(Y\\) when \\(X_2=1\\) and \\(X_2=0\\). If \\(\\beta_3\\) is significant, then there is an interaction effect. If \\(\\beta_3\\) is not significant, then there is no interaction effect.\nIn R:\n\nmodel = lm(y = x1 * x2)\n\ngives \\(X_1+X_2+X_1X_2\\), and\n\nmodel = lm(y = x1:x2)\n\ngives only \\(X_1 X_2\\)\nThe coefficients \\(\\beta_1\\) and \\(\\beta_2\\) are marginal effects.\nIf \\(\\beta_3\\) is significant there’s an interaction effect and ee leave \\(\\beta_1\\) and \\(\\beta_2\\) in the model whether they are significant or not.\n\\(X_1\\) and \\(D\\) dummy\n\n\\(X_2 = D\\) is a dummy variable with values of zero or one.\nModel: typically we run a regression of the form \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1 \\star D + \\epsilon\\]\nThe coefficient \\(\\beta_1 + \\beta_2\\) is the effect of \\(X_1\\) when \\(D=1\\). The coefficient \\(\\beta_1\\) is the effect when \\(D=0\\).\n\n\nExample 13.7 (Orange Juice)  \n\noj = read.csv(\"./../data/oj.csv\")\nknitr::kable(oj[1:5,1:10], digits=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstore\nbrand\nweek\nlogmove\nfeat\nprice\nAGE60\nEDUC\nETHNIC\nINCOME\n\n\n\n\n2\ntropicana\n40\n9.0\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n2\ntropicana\n46\n8.7\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n2\ntropicana\n47\n8.2\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n2\ntropicana\n48\n9.0\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n2\ntropicana\n50\n9.1\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n\n\nmodel = lm(logmove ~ log(price)*feat, data=oj)\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.66\n0.02\n588\n0\n\n\nlog(price)\n-0.96\n0.02\n-51\n0\n\n\nfeat\n1.71\n0.03\n56\n0\n\n\nlog(price):feat\n-0.98\n0.04\n-23\n0\n\n\n\n\nmodel = lm(log(price)~ brand-1, data = oj)\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nbranddominicks\n0.53\n0\n254\n0\n\n\nbrandminute.maid\n0.79\n0\n382\n0\n\n\nbrandtropicana\n1.03\n0\n500\n0\n\n\n\n\n\nbrandcol &lt;- c(\"green\",\"red\",\"gold\")\noj$brand = factor(oj$brand)\nboxplot(log(price) ~ brand, data=oj, col=brandcol)\nplot(logmove ~ log(price), data=oj, col=brandcol[oj$brand], pch=20)\n\n\n\n\n\n\n\n\n\n\n\n83 Chicagoland Stores (Demographic info for each)\nPrice, sales (log units moved), and whether advertised (feat)\n\nOrange Juice: Price vs Sales\n\n\n\n\n\n\n\n\n\nOrange Juice: Price vs log(Sales)\n\nplot(logmove ~ price, data=oj, col=brandcol[oj$brand], pch=16, cex=0.5, ylab=\"log(move)\")\n\n\n\n\n\n\n\n\nOrange Juice: Price vs log(Sales)\n\nl1 &lt;- loess(logmove ~ price, data=oj, span=2)\nsmoothed1 &lt;- predict(l1) \nind = order(oj$price)\nplot(logmove ~ price, data=oj, col=brandcol[oj$brand], pch=16, cex=0.5, ylab=\"log(move)\")\nlines(smoothed1[ind], x=oj$price[ind], col=\"blue\", lwd=2)\n\n\n\n\n\n\n\n\nOrange Juice: log(Price) vs log(Sales)\n\nplot(logmove ~ log(price), data=oj, col=brandcol[oj$brand], pch=16, cex=0.5, ylab=\"log(move)\")\nl2 &lt;- lm(logmove ~ log(price), data=oj)\nsmoothed2 &lt;- predict(l2) \nind = order(oj$price)\nlines(smoothed2[ind], x=log(oj$price[ind]), col=\"blue\", lwd=2)\n\n\n\n\n\n\n\n\nWhy? Multiplicative (rather than additive) change.\nNow we are interested in how does advertisement affect price sensitivity?\nOriginal model \\[\n\\log(\\mathrm{sales}) = \\beta_0 + \\beta_1\\log(\\mathrm{price}) + \\beta_2 \\mathrm{feat}.\n\\] If we feature the brand (in-store display promo or flyer ad), does it affect price sensitivity \\(\\beta_1\\)? If we assume it does \\[\n\\beta_1 = \\beta_3 +  \\beta_4\\mathrm{feat}.\n\\] The new model is \\[\n\\log(\\mathrm{sales}) = \\beta_0 + (\\beta_3 +  \\beta_4\\mathrm{feat})\\log(\\mathrm{price}) + \\beta_2 \\mathrm{feat}.\n\\] After expanding \\[\n\\log(\\mathrm{sales}) = \\beta_0 + \\beta_3\\log(\\mathrm{price}) +  \\beta_4\\mathrm{feat}*\\log(\\mathrm{price}) + \\beta_2 \\mathrm{feat}.\n\\]\n\n## and finally, consider 3-way interactions\nojreg &lt;- lm(logmove ~ log(price)*feat, data=oj)\nojreg %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.66\n0.02\n588\n0\n\n\nlog(price)\n-0.96\n0.02\n-51\n0\n\n\nfeat\n1.71\n0.03\n56\n0\n\n\nlog(price):feat\n-0.98\n0.04\n-23\n0\n\n\n\n\nlm(logmove ~ log(price), data=oj) %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.4\n0.02\n679\n0\n\n\nlog(price)\n-1.6\n0.02\n-87\n0\n\n\n\n\n\n\nlm(logmove ~ log(price)+feat + brand, data=oj) %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.28\n0.01\n708\n0\n\n\nlog(price)\n-2.53\n0.02\n-116\n0\n\n\nfeat\n0.89\n0.01\n85\n0\n\n\nbrandminute.maid\n0.68\n0.01\n58\n0\n\n\nbrandtropicana\n1.30\n0.01\n88\n0\n\n\n\n\n\n\nlm(logmove ~ log(price)*feat, data=oj) %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.66\n0.02\n588\n0\n\n\nlog(price)\n-0.96\n0.02\n-51\n0\n\n\nfeat\n1.71\n0.03\n56\n0\n\n\nlog(price):feat\n-0.98\n0.04\n-23\n0\n\n\n\n\n\n\nlm(logmove ~ brand-1, data=oj) %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nbranddominicks\n9.2\n0.01\n885\n0\n\n\nbrandminute.maid\n9.2\n0.01\n889\n0\n\n\nbrandtropicana\n9.1\n0.01\n879\n0\n\n\n\n\n\nAdvertisement increases price sensitivity from -0.96 to -0.958 - 0.98 = -1.94!\nWhy?\nOne of the reasons is that the price was lowered during the Ad campaign!\n\ndoj = oj %&gt;% filter(brand==\"dominicks\")\npar(mfrow=c(1,3), mar=c(4.2,4.6,2,1))\nboxplot(price ~  feat, data = oj[oj$brand==\"dominicks\",], col=c(2,3), main=\"dominicks\", ylab=\"Price ($)\")\nboxplot(price ~  feat, data = oj[oj$brand==\"minute.maid\",], col=c(2,3), main=\"minute.maid\")\nboxplot(price ~  feat, data = oj[oj$brand==\"tropicana\",], col=c(2,3), main=\"tropicana\")\n\n\n\n\n\n\n\n\n0 = not featured, 1 = featured",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "13-regression.html#dummies",
    "href": "13-regression.html#dummies",
    "title": "13  Linear and Multiple Regression",
    "section": "13.6 Dummies",
    "text": "13.6 Dummies\nWe want to understand effect of the brand on the sales \\[\\log(\\mathrm{sales}) = \\beta_0 + \\beta_1\\log(\\mathrm{price}) + \\xcancel{\\beta_2\\mathrm{brand}}\\]\nBut brand is not a number!\nHow can you use it in your regression equation?\nWe introduce dummy variables\n\n\n\nBrand\nIntercept\nbrandminute.maid\nbrandtropicana\n\n\n\n\nminute.maid\n1\n1\n0\n\n\ntropicana\n1\n0\n1\n\n\ndominicks\n1\n0\n0\n\n\n\n\\[\\log(\\mathrm{sales}) = \\beta_0 + \\beta_1\\log(\\mathrm{price}) + \\beta_{21}\\mathrm{brandminute.maid} + \\beta_{22}\\mathrm{brandtropicana}\\]\nR will automatically do it it for you\n\nprint(lm(logmove ~ log(price)+brand, data=oj))\n\n\nCall:\nlm(formula = logmove ~ log(price) + brand, data = oj)\n\nCoefficients:\n     (Intercept)        log(price)  brandminute.maid    brandtropicana  \n           10.83             -3.14              0.87              1.53  \n\n\n\\[\\log(\\mathrm{sales}) = \\beta_0 + \\beta_1\\log(\\mathrm{price}) + \\beta_3\\mathrm{brandminute.maid} + \\beta_4\\mathrm{brandtropicana}\\]\n\\(\\beta_3\\) and \\(\\beta_4\\) are “change relative to reference\" (dominicks here).\nHow does brand affect price sensitivity?\nInteractions: logmove ~ log(price) * brand\nNo Interactions: logmove ~ log(price) + brand\n\n\n\nParameter\nInteractions\nNo Interactions\n\n\n\n\n(Intercept)\n10.95\n10.8288\n\n\nlog(price)\n-3.37\n-3.1387\n\n\nbrandminute.maid\n0.89\n0.8702\n\n\nbrandtropicana\n0.96239\n1.5299\n\n\nlog(price):brandminute.maid\n0.057\n\n\n\nlog(price):brandtropicana\n0.67\n\n\n\n\n\nExample 13.8 (Golf Performance Data) Dave Pelz has written two best-selling books for golfers, Dave Pelz’s Short Game Bible, and Dave Pelz’s Putting Bible. These books have become essential reading for serious golfers looking to improve their performance through data-driven analysis and scientific methodology.\nDave Pelz was formerly a “rocket scientist” (literally) at NASA, where he worked on the Apollo space program. His background in physics and engineering provided him with the analytical skills to revolutionize golf instruction through data analytics. His systematic approach to analyzing golf performance helped him refine his teaching methods and develop evidence-based strategies for improving players’ games. Through his research, Pelz discovered that it’s the short-game that matters most for overall scoring performance.\nOne of Pelz’s most famous findings concerns the optimal speed for a putt. Through extensive data collection and analysis, he determined that the best chance to make a putt is one that will leave the ball \\(17\\) inches past the hole, if it misses. This counterintuitive result challenges the common belief that golfers should aim to leave putts just short of the hole. Pelz’s research showed that putts hit with this specific speed have the highest probability of going in, as they account for the natural variations in green speed, slope, and other factors that affect putt trajectory.\nNow, we demonstrate how to use data to improve your golf game. We analyze the dataset that contains comprehensive year-end performance statistics for 195 professional golfers from the 2000 PGA Tour season. This rich dataset captures technicical abilities of the players as well as financial success (measured by the amount of prize money they made). Each observation represents season’s averages of the players’ performance and total prize money. List below shows the variables in the dataset.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nnevents\nThe number of official PGA events included in the statistics\n\n\nmoney\nThe official dollar winnings of the player\n\n\ndrivedist\nThe average number of yards driven on par 4 and par 5 holes\n\n\ngir\nGreens in regulation, measured as the percentage of time that the first (tee) shot on a par 3 hole ends up on the green, or the second shot on a par 4 hole ends up on the green, or the third shot on a par 5 hole ends up on the green\n\n\navgputts\nThe average number of putts per round\n\n\n\nWe will analyze these data to determine which of the variables nevents, drivedist, gir, and avgputts is most important for winning money on the PGA Tour. We begin by performing a regression of Money on all explanatory variables:\n\nd00 = read_csv(\"../data/pga-2000.csv\")\nd18 = read_csv(\"../data/pga-2018.csv\")\n\n\nmodel18 = lm(money ~ nevents + drivedist + gir + avgputts, data=d18)\nmodel00 = lm(money ~ nevents + drivedist + gir + avgputts, data=d00)\nmodel00 %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.5e+07\n4206466\n3.5\n0.00\n\n\nnevents\n-3.0e+04\n11183\n-2.7\n0.01\n\n\ndrivedist\n2.1e+04\n6913\n3.1\n0.00\n\n\ngir\n1.2e+05\n17429\n6.9\n0.00\n\n\navgputts\n-1.5e+07\n2000905\n-7.6\n0.00\n\n\n\n\n\nLet’s look at the residuals:\n\narrows(x0 = 7.5,y0 = 20,x1 = 8.5,y1 = 2,length = 0.1)\ntext(x = 7,y = 22,labels = \"Tiger Woods\", cex=1.5)\n\n\n\n\n\n\n\n\nIt seems like we need to measure money on a log scale. Let’s transform with log(Money) as it has much better residual diagnostic plots.\n\nm = lm(formula = log(money) ~ nevents + drivedist + gir + avgputts, data = d00)\nm %&gt;% tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n36.15\n3.58\n10.10\n0.00\n\n\nnevents\n-0.01\n0.01\n-0.94\n0.35\n\n\ndrivedist\n0.01\n0.01\n2.40\n0.02\n\n\ngir\n0.17\n0.01\n11.18\n0.00\n\n\navgputts\n-21.13\n1.70\n-12.42\n0.00\n\n\n\n\n\n\npar(mar = c(4,4.5,0,0),mfrow=c(1,1))\nmodel00log %&gt;% tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n36.15\n3.58\n10.10\n0.00\n\n\nnevents\n-0.01\n0.01\n-0.94\n0.35\n\n\ndrivedist\n0.01\n0.01\n2.40\n0.02\n\n\ngir\n0.17\n0.01\n11.18\n0.00\n\n\navgputts\n-21.13\n1.70\n-12.42\n0.00\n\n\n\n\nhist(rstandard(model00log), breaks=20, col=\"lightblue\", xlab = \"Standartized Residual\", main=\"\")\narrows(x0 = 3,y0 = 20,x1 = 3.2,y1 = 2,length = 0.1)\ntext(x = 3,y = 22,labels = \"Tiger Woods\", cex=1.5)\n\n\n\n\n\n\n\n\nUsing log scale for money gives us a better model. We will keep it for now. How about selectng variables. Notice, that \\(t\\)-stats for nevents is \\(&lt;1.5\\). Thus, we can remove it.\n\nm1 = lm(formula = log(money) ~ drivedist + gir + avgputts, data = d00)\nm1 %&gt;% tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n36.17\n3.58\n10.1\n0.00\n\n\ndrivedist\n0.01\n0.01\n2.5\n0.01\n\n\ngir\n0.17\n0.01\n11.2\n0.00\n\n\navgputts\n-21.37\n1.68\n-12.7\n0.00\n\n\n\n\n\nIt is obvious that fewer putts indicate a better golfer. However, decreasing the average number of putts per round by one is extremely difficult to achieve.\nEvaluating the Coefficients\n\nGreens in Regulation (GIR) has a \\(\\hat{\\beta} = 0.17\\). If I can increase my GIR by one, I’ll earn \\(e^{0.17} = 1.18\\)% An extra \\(18\\)%\nDriveDis has a \\(\\hat{\\beta} = 0.014\\). A \\(10\\) yard improvement, I’ll earn \\(e^{0.014 \\times 10} =e^{0.14}  = 1.15\\)% An extra \\(15\\)%\n\nCaveat: Everyone has gotten better since 2000!\nTiger Woods was nine standard deviations better than what the model predicted, while taking the natural logarithm of money earnings significantly improves the residual diagnostics and an exponential model appears to fit the data well as evidenced by the good residual diagnostic plots; furthermore, the t-ratios for the number of events variable are consistently under 1.5, indicating it may not be a significant predictor.\nThe outliers represent the biggest over and under-performers in terms of money winnings when compared with their performance statistics, and Tiger Woods, Phil Mickelson, and Ernie Els won major championships by performing exceptionally well during tournaments with substantial prize money available.\nWe can see the over-performers and under-performers in the data.\n\n\n\nOver-Performers\n\n\n\nname\nmoney\nPredicted\nError\n\n\n\n\n1\nTiger Woods\n9188321\n3584241\n5604080\n\n\n2\nPhil Mickelson\n4746457\n2302171\n2444286\n\n\n3\nErnie Els\n3469405\n1633468\n1835937\n\n\n4\nHal Sutton\n3061444\n1445904\n1615540\n\n\n20\nNotah Begay III\n1819323\n426061\n1393262\n\n\n182\nSteve Hart\n107949\n-1186685\n1294634\n\n\n\n\n\nNow, let’s extract the list of underperformers, which are given by large negative residuals. According to our model, Glasson and Stankowski should win more money based on their performance statistics, but they are not achieving the expected earnings. This could be due to several factors: they might be performing well in practice rounds but struggling under tournament pressure, they could be playing in fewer high-payout events, or their performance metrics might not capture other important aspects of tournament success like clutch putting or mental toughness during critical moments.\n\n\n\nUnder-Performers\n\n\n\nname\nmoney\nPredicted\nError\n\n\n\n\n47\nFred Couples\n990215\n1978477\n-988262\n\n\n52\nKenny Perry\n889381\n1965740\n-1076359\n\n\n70\nPaul Stankowski\n669709\n1808690\n-1138981\n\n\n85\nBill Glasson\n552795\n1711530\n-1158735\n\n\n142\nJim McGovern\n266647\n1397818\n-1131171\n\n\n\n\n\nLets look at 2018 data, the highest earners are\n\n\n\nHighest earners 2018\n\n\nname\nnevents\nmoney\ndrivedist\ngir\navgputts\n\n\n\n\nJustin Thomas\n23\n8694821\n312\n69\n1.7\n\n\nDustin Johnson\n20\n8457352\n314\n71\n1.7\n\n\nJustin Rose\n18\n8130678\n304\n70\n1.7\n\n\nBryson DeChambeau\n26\n8094489\n306\n70\n1.8\n\n\nBrooks Koepka\n17\n7094047\n313\n68\n1.8\n\n\nBubba Watson\n24\n5793748\n313\n68\n1.8\n\n\n\n\n\nOverperformers\n\n\n\nOverperformers 2018\n\n\n\nname\nmoney\nPredicted\nError\n\n\n\n\n1\nJustin Thomas\n8694821\n5026220\n3668601\n\n\n2\nDustin Johnson\n8457352\n6126775\n2330577\n\n\n3\nJustin Rose\n8130678\n4392812\n3737866\n\n\n4\nBryson DeChambeau\n8094489\n3250898\n4843591\n\n\n5\nBrooks Koepka\n7094047\n4219781\n2874266\n\n\n6\nBubba Watson\n5793748\n3018004\n2775744\n\n\n9\nWebb Simpson\n5376417\n2766988\n2609429\n\n\n11\nFrancesco Molinari\n5065842\n2634466\n2431376\n\n\n12\nPatrick Reed\n5006267\n2038455\n2967812\n\n\n84\nSatoshi Kodaira\n1471462\n-1141085\n2612547\n\n\n\n\n\nUnderperformers\n\n\n\nUnderperformers 2018\n\n\n\nname\nmoney\nPredicted\nError\n\n\n\n\n102\nTrey Mullinax\n1184245\n3250089\n-2065844\n\n\n120\nJ.T. Poston\n940661\n3241369\n-2300708\n\n\n135\nTom Lovelady\n700783\n2755854\n-2055071\n\n\n148\nMichael Thompson\n563972\n2512330\n-1948358\n\n\n150\nMatt Jones\n538681\n2487139\n-1948458\n\n\n158\nHunter Mahan\n457337\n2855898\n-2398561\n\n\n168\nCameron Percy\n387612\n3021278\n-2633666\n\n\n173\nRicky Barnes\n340591\n3053262\n-2712671\n\n\n176\nBrett Stegmaier\n305607\n2432494\n-2126887\n\n\n\n\n\nOur analysis reveals three particularly interesting effects from the golf performance data, with Tiger Woods demonstrating exceptional performance as an outlier that is eight standard deviations above the model’s predictions, indicating his extraordinary success relative to his statistical metrics, while the model shows that increasing driving distance by ten yards corresponds to a fifteen percent increase in earnings, suggesting that power off the tee provides a significant competitive advantage in professional golf, and additionally, improving greens in regulation (GIR) by one percentage point leads to an eighteen percent increase in earnings, highlighting the importance of approach shot accuracy in determining financial success on the PGA Tour, with the model also successfully identifying both under-performers and over-performers, players whose actual earnings significantly differ from what their statistical performance would predict, providing valuable insights into which players may be exceeding or falling short of expectations based on their measurable skills, demonstrating the practical applications of statistical modeling in sports analytics and performance evaluation.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "13-regression.html#bayesian-regression",
    "href": "13-regression.html#bayesian-regression",
    "title": "13  Linear and Multiple Regression",
    "section": "13.7 Bayesian Regression",
    "text": "13.7 Bayesian Regression\nConsider a linear regression \\[\nf(x) = x^T\\beta + \\epsilon,~~~\\epsilon \\sim N(0,\\sigma_e).\n\\] We put a zero mean Gaussian prior on the model parameters \\[\n\\beta \\sim N(0,\\Sigma).\n\\] Bayesian inference is to calculate posterior given the data \\[\np(\\beta\\mid y,X) = \\dfrac{p(y\\mid X,\\beta)p(\\beta)}{p(y\\mid X)}.\n\\] Product of two Gaussian density functions lead to another Gaussian \\[\n\\begin{aligned}\np(\\beta\\mid y,X)  & \\propto \\exp\\left(-\\dfrac{1}{2\\sigma_e^2}(y-\\beta^TX)^T(y-\\beta^TX)\\right)\\exp\\left(-\\dfrac{1}{2}\\beta^T\\Sigma^{-1}\\beta\\right)\\\\\n& \\propto \\exp\\left(-\\dfrac{1}{2}(\\beta - \\bar\\beta)^T\\left(\\dfrac{1}{\\sigma_e^2XX^T + \\Sigma^{-1}}\\right)(\\beta-\\bar\\beta)\\right)\n\\end{aligned}\n\\]\nThus, the posterior is \\[\n\\beta\\mid X,y \\sim N(\\bar\\beta,A^{-1}),\n\\] where \\(A = \\left(\\sigma_e^{-2}XX^T + \\Sigma\\right)\\), and \\(\\bar\\beta = \\sigma_e^{-2}A^{-1}Xy\\).\n\nExample 13.9 (Posterior) Consider a model with \\(p = 1\\) \\[\ny = \\beta_0 + \\beta_1 x + \\epsilon, ~~ \\beta_i \\sim N(0,1),~~~\\sigma_e = 1\n\\] Let’s plot a sample from the prior set of functions\n\n\n\nSample from prior distribution over possible linear models\n\n\nNow, say we observed two points \\((1,1)\\) and \\((2,2)\\), we can calculate the posterior \\(\\beta \\mid X,y \\sim N(0.833, 0.166)\\)\n\n\n\nSample from posterior distribution over possible linear models\n\n\nWhy our posterior mean is not 1?\n\n\n13.7.1 Horseshoe for Linear regression\nThe linear regression model is given by \\[Y = X\\beta + \\varepsilon,\\]\nwhere \\(Y\\) and \\(\\varepsilon\\) are vectors of length \\(n\\), \\(\\beta\\) is a vector of length \\(p\\) and \\(X\\) is an \\(n \\times p\\)-matrix. We assume \\(\\varepsilon \\sim \\mathcal{N}(0, I_n)\\). The main function for the horseshoe for the linear regression model is horseshoe and it implements the algorithm of Bhattacharya et al (2016).\nThe options of horseshoe are the same as for HS.normal.means (discussed above, although in case of linear regression it is less clear which prior to use for \\(\\tau\\)). We illustrate the use of horseshoe via an example.\nWe create a 50 by 100 design matrix \\(X\\) filled with realizations of independent normal random variables. The first 10 entries of the vector \\(\\beta\\) are set equal to six (the signals) and the remaining 90 entries are set equal to zero (the noise).\n\nX &lt;- matrix(rnorm(50*100), 50)\nbeta &lt;- c(rep(6, 10), rep(0, 90))\ny &lt;- X %*% beta + rnorm(50)\n\nWe use the horseshoe and plot the posterior mean and marginal 95% credible interval per parameter in red. The true parameter values are shown in black.\n\nlibrary(horseshoe)\nhs.object &lt;- horseshoe(y, X, method.tau = \"truncatedCauchy\", method.sigma =\"Jeffreys\")\n\n 1000\n 2000\n 3000\n 4000\n 5000\n 6000\n\ndf &lt;- data.frame(index = 1:100,\n                 truth = beta,\n                 post.mean = hs.object$BetaHat,\n                 lower.CI &lt;- hs.object$LeftCI,\n                 upper.CI &lt;- hs.object$RightCI\n                 )\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean), size = 2, col = \"red\") +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI), width = .1, col = \"red\") +\n  ggtitle(\"Black = truth, Red = estimates with 95% credible intervals\")\n\n\n\n\n\n\n\n\nWe again perform variable selection. The function, HS.var.select, is the same as described above for the normal means problem. Here we show how it works when variables are selected by checking whether 0 is in the credible interval. For the thresholding procedure, please refer to the normal means example above.\nWe perform variable selection:\n\ndf$selected.CI &lt;- HS.var.select(hs.object, df$y, method = \"intervals\")\n\nThe result is a vector of zeroes and ones, with the ones indicating that the observations is suspected to correspond to an actual signal. We now plot the results, coloring the estimates/intervals blue if a signal is detected and red otherwise.\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean, col = factor(selected.CI)), \n             size = 2) +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI, col = factor(selected.CI)),\n                width = .1) +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Black = truth, Blue = selected as signal, Red = selected as noise\")\n\n\n\n\n\n\n\n\n\n\n\n\nSchwertman, Neil C, AJ Gilks, and J Cameron. 1990. “A Simple Noncalculus Proof That the Median Minimizes the Sum of the Absolute Deviations.” The American Statistician 44 (1): 38–39.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "14-logistic.html",
    "href": "14-logistic.html",
    "title": "14  Classification: Logistic Regression",
    "section": "",
    "text": "14.1 Logistic Regression\nClassification is a type of predictive modeling where the goal is to predict a categorical variable based on a set of input variables. The categorical variable is often called the response variable and the input variables are called the predictors. The response variable is typically binary, meaning it can take on only two values, such as 0 or 1, or it can be a multi-class variable, meaning it can take on more than two values.\nWe start by assuming a binomial likelihood function for the response variable. The binomial likelihood function is a function of the probability of the response variable taking on a value of 1, given the input variables. The binomial likelihood function is defined as follows \\[\nP(y_i = 1\\mid p_i) = p_i^{y_i} (1-p_i)^{1-y_i},\n\\] where \\(p_i\\) is the funciton of the inputs \\(x_i\\) and coefficients \\(\\beta\\) that gives us the probability of the response variable taking on a value of 1, given the input variables. A typical approach to calculate \\(p_i\\) is to use logistic function \\[\np_i  = f(x_i,\\beta)= \\frac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}},\n\\] where \\(\\beta\\) is a vector of parameters. The logistic function is a sigmoid function that maps any real number to a number between zero and one.\nGiven the observed data set \\(\\{(x_i,y_i)\\}_{i=1}^n\\), where each \\(y_i\\) is either 0 or 1, the goal is to predict a probability that the next observation will be 1. Binomial log-likelihood minimisation leads to the maximum likelihood estimator for parameters \\(\\beta\\) (a.k.a cross-entropy estimator). The maximum likelihood estimator is defined as follows \\[\n\\mini_{\\beta} -\\sum_{i=1}^n y_i \\log \\left ( f(x_i,\\beta) \\right ) + (1-y_i) \\log \\left ( 1-f(x_i,\\beta) \\right ).\n\\]\nIn the unconditional case, when we do not observe any inputs \\(x\\), the cross-entropy estimator is again, the sample mean. If we take the derivative of the above expression with respect to \\(\\beta\\) and set it to zero, we get \\[\n\\frac{d}{d\\beta} -\\sum_{i=1}^n y_i \\log \\left ( \\beta \\right ) + (1-y_i) \\log \\left ( 1-\\beta \\right ) = -\\sum_{i=1}^n \\frac{y_i}{\\beta} - \\frac{1-y_i}{1-\\beta} = 0\n\\] which gives us the solution \\[\n\\hat{\\beta} = \\frac{1}{n}\\sum_{i=1}^n y_i.\n\\] which is the sample mean.\nUnlike the least squares estimator, there is no analytical solution to the problem of minimizing cross-entropy. However, there are efficient numerical optimization algorithms that can be used to find the optimal solution. As we will show later, the cross-entropy estimator is equivalent to the maximum likelihood estimator, assuming that \\(y\\) is the Bernoulli random variable.\nIn the case when we have more than two classes \\(y \\in \\{1,\\ldots,K\\}\\), we simply build \\(K\\) models \\(f_1(x_i,\\beta),\\ldots, f(x_K,\\beta)\\), one for each class and then use the softmax function to convert the output of each model into a number between zero and one. The softmax function is defined as follows \\[\n\\mathrm{softmax}\\left(f_j(x,\\beta)\\right) = \\frac{\\exp(f_j(x,\\beta))}{\\sum_{i=1}^K \\exp(f_i(x,\\beta))}.\n\\] The softmax function is a generalization of the sigmoid function to the case of more than two classes. It is often used as the activation function in the output layer of neural networks for multi-class classification problems. It converts the output of each model into a probability distribution over the classes, making it suitable for multi-class classification with probabilistic outputs.\nIn summary, choosing the right loss function for your predictive rule depends on several factors, including type of prediction task: regression vs classification and importance of sensitivity to outliers.\nFor that, we need to specify the loss function that will measure the mismatch between an observed and a predicted values. The loss function is a measure of how well the model fits the data and is used to estimate the parameters of the model. The goal is to find the values of the parameters that minimize the loss function. A most popular choice for the loss function is the squared error loss function. \\[\nL(\\beta; ~D) = \\sum_{i=1}^n (y_i - f(x_i))^2 \\rightarrow \\mathrm{minimize}_{\\beta}\n\\]\nWhen the value \\(y\\) we are trying to predict is categorical (or qualitative) we have a classification problem. For a binary output we predict the probability its going to happen \\[\np ( Y=1 | X = x ),\n\\] where \\(X = (x_1,\\ldots,x_p)\\) is our usual list of predictors.\nSuppose that we have a binary response, \\(y\\) taking the value \\(0\\) or \\(1\\)\nThe goal is to predict the probability that \\(y\\) equals \\(1\\). You can then do and categorize a new data point. Assessing credit risk and default data is a typical problem. - \\(y\\): whether or not a customer defaults on their credit card (No or Yes). - \\(x\\): The average balance that customer has remaining on their credit card after making their monthly payment, plus as many other features you think might predict \\(Y\\).\nA linear model is a powerful tool to find relations among different variables \\[\ny = \\beta^Tx + \\epsilon.\n\\] It works assuming that \\(y\\) variable is contentious and ranges in \\((-\\infty,+\\infty)\\). Another assumption is that conditional distribution of \\(y\\) is normal \\(p(y\\mid \\beta^Tx) \\sim N(\\beta^Tx, \\sigma^2)\\)\nWhat do we do when assumptions about conditional normal distributions do not hold. For example \\(y\\) can be a binary variable with values 0 and 1. For example \\(y\\) is\nWe model response \\(\\{0,1\\}\\), using a continuous variable \\(y \\in [0,1]\\) which is interpreted as the probability that response equals to 1. \\[\np( y= 1 | x_1, \\ldots , x_p  ) = F \\left ( \\beta_1 x_1 + \\ldots + b_p x_p   \\right )\n\\] where \\(f\\) is increasing and \\(0&lt; f(x)&lt;1\\).\nIt seems logical to find a transformation \\(F\\) so that \\(F(\\beta^Tx + \\epsilon) \\in [0,1]\\). Then we can predict using \\(F(\\beta^Tx)\\) and intercepting interpret the result as a probability, i.e if \\(F(\\beta^Tx) = z\\) then we interpret it as \\(p(y=1) = z\\). Such function \\(F\\) is called a link function.\nDo we know a function that maps any real number to a number in \\([0,1]\\) interval? What about commutative distribution function \\(F(x) = p(Z \\le x)\\)? If we choose CDF \\(\\Phi(x)\\) for \\(N(0,1)\\) then we have \\[\\begin{align*}\n\\hat y = p(y=1) &= \\Phi(\\beta^Tx) \\\\\n\\Phi^{-1}(\\hat y) = & \\beta^Tx + \\epsilon\n\\end{align*}\\] This is a linear model for \\(\\Phi^{-1}(\\hat y)\\), but not for \\(y\\)! You can thing of this as a change of units for variable \\(y\\). In this specific case, when we use normal CDF, the resulting model is called probit, it stands for probability unit. The resulting link function is \\(\\Phi^{-1}\\) and now \\(\\Phi^{-1}(Y)\\) follows a normal distribution! This term was coined in the 1930’s by biologists studying the dosage-cure rate link. We can fit a probit model using glm function in R.\nset.seed(92) # Kuzy\nx = seq(-3,3,length.out = 100)\ny = pnorm(x+rnorm(100))&gt;0.5\nprobitModel = glm(y~x, family=binomial(link=\"probit\"))\nmc = as.double(coef(probitModel))\n# we want to predict outcome for x = -1\nxnew = -1\n(yt = mc[1] + mc[2]*xnew)\n\n -0.86\n\n(pnorm(yt))\n\n 0.19\n\n(pred = predict(probitModel, list(x = c(xnew)), type=\"response\"))\n\n   1 \n0.19\nnd = dnorm(mc[1] + mc[2]*x)\nplot(x,nd, type='l', col=\"red\", xlab=\"x\", ylab = \"P(y=1)\")\npolygon(c(-3,x[x&lt; -1],-1),c(0,nd[x&lt; -1],0), col=\"blue\")\nOur prediction is the blue area which is equal to 0.195.\nplot(x,y, type='p', col=\"red\", xlab=\"x\", ylab = \"P(y=1)\")\npred_probit = predict(probitModel, list(x=x), type=\"response\")\nlines(x,pred_probit, type='l')\nA couple of observations: (i) this fits the data much better than the linear estimation, and (i) it always lies between 0 and 1. Instead of thinking of \\(y\\) as a probability and transforming right hand side of the linear model we can think of transforming \\(y\\) so that transformed variable lies in \\((-\\infty,+\\infty)\\). We can use odds ratio, that we talked about before \\[\n\\dfrac{y}{1-y}\n\\]\nOdds ration lies in the interval \\((0,+\\infty)\\). Almost what we need, but not exactly. Can we do another transform that maps \\((0,+\\infty)\\) to \\((-\\infty,+\\infty)\\)? \\[\n\\log\\left(\\dfrac{y}{1-y}\\right)\n\\] will do the trick! This function is called a logit function and it is This function is called a logit function and it is the inverse of the sigmoidal \"logistic\" function or logistic transform. The is linear in \\[\n\\log \\left ( \\frac{ p \\left ( y=1|x \\right ) }{ 1 -  p \\left ( Y=1|x \\right ) } \\right ) = \\beta_0 + \\beta_1 x_1 + \\ldots + x_p.\n\\] These model are easy to fit in R:\nglm( y ~ x1 + x2,  family=\"binomial\")\nOutside of specific field, i.e. behavioral economics, the logistic function is much more popular of a choice compared to probit model. Besides that fact that is more intuitive to work with logit transform, it also has several nice properties when we deal with multiple classes (more then 2). Also, it is computationally easier then working with normal distributions. The density function of the logit is very similar to the probit one.\nlogitModel  = glm(y~x, family=binomial(link=\"logit\"))\npred_logit = predict(logitModel, list(x = x), type=\"response\")\nplot(x,pred_probit, pch=20, col=\"red\", cex=0.9, ylab=\"y\")\nlines(x,pred_logit, type='p', pch=20, cex=0.5, col=\"blue\")\nlines(x,y, type='p', pch=21, cex=0.5, bg=\"lightblue\")\nlegend(\"bottomright\",pch=20, legend=c(\"Logit\", \"Probit\"), col=c(\"blue\",\"red\"),y.intersp = 2)\nWe can easily derive an inverse of the logit function to get back the original \\(y\\) \\[\n\\log\\left(\\dfrac{y}{1-y}\\right) = \\beta^Tx;~~ y=\\dfrac{e^{\\beta^Tx}}{1+e^{\\beta^Tx}}\n\\]",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Classification: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "14-logistic.html#logistic-regression",
    "href": "14-logistic.html#logistic-regression",
    "title": "14  Classification: Logistic Regression",
    "section": "",
    "text": "Win or lose\nSick or healthy\nBuy or not buy\nPay or default\n\n\n\n\n\nOutcome of an election\nResult of spam filter\nDecision variable about loan approval\n\n\n\n\n\n\n\n\n\n\n\n\nis for indicates \\(y=0\\) or \\(1\\)\nhas a bunch of other options.\n\n\n\n\n\nExample 14.1 (Example: NBA point spread)  \nNBA = read.csv(\"../data/NBAspread.csv\")\nattach(NBA)\nn = nrow(NBA)\nhist(NBA$spread[favwin==1], col=5, main=\"\", xlab=\"spread\")\nhist(NBA$spread[favwin==0], add=TRUE, col=6)\nlegend(\"topright\", legend=c(\"favwin=1\", \"favwin=0\"), fill=c(5,6), bty=\"n\")\nboxplot(NBA$spread ~ NBA$favwin, col=c(6,5), horizontal=TRUE, ylab=\"favwin\", xlab=\"spread\")\n\n\n\n\n\n\n\n\n\n\nDoes the Vegas point spread predict whether the favorite wins or not? Turquoise = Favorites does win, Purple = Favorite does not win. In R: the output gives us\n\nnbareg = glm(favwin~spread-1, family=binomial)\nsummary(nbareg)\n\n\nCall:\nglm(formula = favwin ~ spread - 1, family = binomial)\n\nCoefficients:\n       Estimate Std. Error z value Pr(&gt;|z|)    \nspread   0.1560     0.0138    11.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 766.62  on 553  degrees of freedom\nResidual deviance: 527.97  on 552  degrees of freedom\nAIC: 530\n\nNumber of Fisher Scoring iterations: 5\n\ns = seq(0,30,length=100)\nfit = exp(s*nbareg$coef[1])/(1+exp(s*nbareg$coef[1]))\nplot(s, fit, typ=\"l\", col=4, lwd=2, ylim=c(0.5,1), xlab=\"spread\", ylab=\"P(favwin)\")\n\n\n\n\n\n\n\n\nThe \\(\\beta\\) measures how our log-odds change! \\(\\beta = 0.156\\)\nLet’s do the NBA Point Spread Prediction. “Plug-in” the values for the new game into our logistic regression \\[\n{ P \\left ( \\mathrm{ favwin}  \\mid  \\mathrm{ spread} \\right ) = \\frac{ e^{ \\beta x } }{ 1 + e^{\\beta x} } }\n\\] Check that when \\(\\beta =0\\) we have \\(p= \\frac{1}{2}\\).\nGiven our new values spread\\(=8\\) or spread\\(=4\\), the win probabilities are \\(77\\)% and \\(65\\)%, respectively. Clearly, the bigger spread means a higher chance of winning.\n\n\nExample 14.2 (Logistic Regression for Tennis Classification) Data science plays a major role in tennis, you can learn about recent AI tools developed by IBM from this This Yahoo Article.\nWe will analyze the Tennis Major Tournament Match Statistics Data Set from the UCI ML repository. The data set has one per each game from four major Tennis tournaments in 2013 (Australia Open, French Open, US Open, and Wimbledon).\nLet’s load the data and familiarize ourselves with it\n\nd = read.csv(\"./../data/tennis.csv\")\ndim(d)\n\n 943  44\n\nstr(d[,1:5])\n\n'data.frame':   943 obs. of  5 variables:\n $ Player1: chr  \"Lukas Lacko\" \"Leonardo Mayer\" \"Marcos Baghdatis\" \"Dmitry Tu\"..\n $ Player2: chr  \"Novak Djokovic\" \"Albert Montanes\" \"Denis Istomin\" \"Michael \"..\n $ Round  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Result : int  0 1 0 1 0 0 0 1 0 1 ...\n $ FNL1   : int  0 3 0 3 1 1 2 2 0 3 ...\n\n\nLet’s look at the few coluns of the randomly selected five rows of the data\n\nd[sample(1:943,size = 5),c(\"Player1\",\"Player2\",\"Round\",\"Result\",\"gender\",\"surf\")]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer1\nPlayer2\nRound\nResult\ngender\nsurf\n\n\n\n\n532\nFlorian Mayer\nJuan Monaco\n1\n1\nM\nHard\n\n\n816\nL.Kubot\nJ.Janowicz\n5\n0\nM\nGrass\n\n\n431\nSvetlana Kuznetsova\nEkaterina Makarova\n1\n1\nW\nClay\n\n\n568\nMarcos Baghdatis\nGo Soeda\n1\n1\nM\nHard\n\n\n216\nMandy Minella\nAnastasia Pavlyuchenkova\n2\n0\nW\nHard\n\n\n\n\n\n\nWe have data for 943 matches and for each match we have 44 columns, including names of the players, their gender, surface type and match statistics. Let’s look at the number of break points won by each player. We will plot BPW (break points won) by each player on the scatter plot and will colorize each dot according to the outcome\n\nn = dim(d)[1]\nplot(d$BPW.1+rnorm(n),d$BPW.2+rnorm(n), pch=21, col=d$Result+2, cex=0.6, bg=\"yellow\", lwd=0.8,\n     xlab=\"BPW by Player 1\", ylab=\"BPW by Player 2\")\nlegend(\"bottomright\", c(\"P1 won\", \"P2 won\"), col=c(3,2), pch=21, bg=\"yellow\", bty='n')\n\n\n\n\n\n\n\n\nWe can clearly see that number of the break points won is a clear predictor of the match outcome. Which is obvious and follows from the rules, to win a match, a player must win break points. Now, we want to understand the impact of a winning a break point on the overall match outcome. We do it by building a logistic regression model\n\nwhich(is.na(d$BPW.1)) # there is one row with NA value for the BPW.1 value and we remove it\n\n 171\n\nd = d[-171,]; n = dim(d)[1]\nm = glm(Result ~ BPW.1 + BPW.2-1, data=d, family = \"binomial\" )\nsummary(m)\n\n\nCall:\nglm(formula = Result ~ BPW.1 + BPW.2 - 1, family = \"binomial\", \n    data = d)\n\nCoefficients:\n      Estimate Std. Error z value Pr(&gt;|z|)    \nBPW.1   0.4019     0.0264    15.2   &lt;2e-16 ***\nBPW.2  -0.4183     0.0277   -15.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1305.89  on 942  degrees of freedom\nResidual deviance:  768.49  on 940  degrees of freedom\nAIC: 772.5\n\nNumber of Fisher Scoring iterations: 5\n\n\nR output does not tell us how accurate our model is but we can quickly check it by using the table function. We will use \\(0.5\\) as a threshold for our classification.\n\ntable(d$Result, as.integer(m$fitted.values&gt;0.5))\n\n   \n      0   1\n  0 416  61\n  1  65 400\n\n\nThus, our model got (416+416)/942 = 0.88% of the predictions correctly!\nEssentially, the logistic regression is trying to draw a line that separates the red observations from the green one. In out case, we have two predictors \\(x_1\\) = BPW.1 and \\(x_2\\) = BPW.2 and our model is \\[\n\\log\\left(\\dfrac{p}{1-p}\\right) = \\beta_1x_1 + \\beta_2 x_2,\n\\] where \\(p\\) is the probability of player 1 winning the match. We want to find the line along which the probability is 1/2, meaning that \\(p/(1-p) = 1\\) and log-odds \\(\\log(p/(1-p)) = 0\\), thus the equation for the line is \\(\\beta_1x_1 + \\beta_2 x_2 = 0\\) or \\[\nx_2 = \\dfrac{-\\beta_1}{\\beta_2}x_1\n\\]\nLet’s see the line found by the glm function\n\nlegend(\"bottomright\", c(\"P1 won\", \"P2 won\"), col=c(3,2), pch=21, bg=\"yellow\", bty='n')\n\nx = seq(0,30,length.out = 200)\ny  =  -m$coefficients[1]*x/m$coefficients[2]\nlines(x,y, lwd=2, col=\"red\") \n\n\n\n\n\n\n\n\nThere are a couple of observations. First, effect of a break point on the game outcome is significant and symmetric, effect of loosing break point is the same as the effect of winning one. We also can interpret the effect of winning a break point in the following way. We will keep BPW.2 = 0 and will calculate what happens to the probability of winning when BPW.1 changes from 0 to 1. The odds ration for player 1 winning when BPW.1 = 0 is exp(0) which is 1, meaning that the probability that P1 wins is 1/2. Now when BPW.1 = 1, the odds ratio is 1.5\n\nexp(0.4019)\n\n 1.5\n\n\nWe can calculate probability of winning from the regression equation \\[\n\\dfrac{p}{1-p} = 1.5,~~~p = 1.5(1-p),~~~2.5p = 1.5,~~~p = 0.6\n\\] Thus probability of winning goes from 50% to 60%, we can use predict function to get this result\n\npredict.glm(m,newdata = data.frame(BPW.1 = c(0), BPW.2 = c(0)), type=\"response\")\n\n  1 \n0.5 \n\npredict.glm(m,newdata = data.frame(BPW.1 = c(1), BPW.2 = c(0)), type=\"response\")\n\n  1 \n0.6 \n\n\nWhat happens to the chances of winning when P1 wins three more break points compared to the opponent\n\npredict.glm(m,newdata = data.frame(BPW.1 = c(0), BPW.2 = c(0)), type=\"response\")\n\n  1 \n0.5 \n\npredict.glm(m,newdata = data.frame(BPW.1 = c(3), BPW.2 = c(0)), type=\"response\")\n\n   1 \n0.77 \n\n\nChances go up by 27%.\nTennis is arguably the sport in which mean and women are treated equally. Both man and women matches are shown during the prime-time on TV, they both have the same prize money. However, one of the comments you hear often is that Women’s matches are “less predictable”, meaning that an upset (when the favorite looses) is more likely to happen in a women’s match compared to man Matches. We can test thus statement by looking at the residuals. The large the residual the less accurate our prediction was.\n\noutlind = which(d$res&lt;2)\nboxplot(d$res[outlind] ~ d$gender[outlind], col=c(2,3), xlab=\"Gender\",ylab=\"Residual\")\n\n\n\n\n\n\n\n\nLet’s do a formal T-test on the residuals foe men’s and women’s matches\n\nmen = d %&gt;% filter(res&lt;2, gender==\"M\") %&gt;% pull(res)\nwomen = d %&gt;% filter(res&lt;2, gender==\"W\") %&gt;% pull(res)\nt.test(men, women, alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  men and women\nt = -5, df = 811, p-value = 3e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.105 -0.043\nsample estimates:\nmean of x mean of y \n      1.2       1.3 \n\n\nLooks like the crowd wisdom that Women’s matches are less predictable is correct.\n\n\nExample 14.3 (Credit Card Default) We have 10,000 observations\n\nDefault = read.csv(\"../data/CreditISLR.csv\", stringsAsFactors = T)\nhead(Default)\n\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n730\n44362\n\n\nNo\nYes\n817\n12106\n\n\nNo\nNo\n1074\n31767\n\n\nNo\nNo\n529\n35704\n\n\nNo\nNo\n786\n38464\n\n\nNo\nYes\n920\n7492\n\n\n\n\n\n\nLet’s build a logistic regression model\n\nglm.fit=glm(default~balance,data=Default,family=binomial)\nsummary(glm.fit)\n\n\nCall:\nglm(formula = default ~ balance, family = binomial, data = Default)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -10.65133    0.36116   -29.5   &lt;2e-16 ***\nbalance       0.00550    0.00022    24.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600\n\nNumber of Fisher Scoring iterations: 8\n\n\nWe use it now to predict default\n\npredict.glm(glm.fit,newdata = list(balance=1000))\n\n   1 \n-5.2 \n\n-1.065e+01 + 5.499e-03*1000\n\n -5.2\n\npredict.glm(glm.fit,newdata = list(balance=1000), type=\"response\")\n\n     1 \n0.0058 \n\nexp(-1.065e+01 + 5.499e-03*1000)/(1+exp(-1.065e+01 + 5.499e-03*1000))\n\n 0.0058\n\n\nPredicting default\n\ny = predict.glm(glm.fit,newdata = x, type=\"response\")\nplot(x$balance,y, pch=20, col=\"red\", xlab = \"balance\", ylab=\"Default\")\nlines(Default$balance, as.integer(Default$default)-1, type='p',pch=20)",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Classification: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "14-logistic.html#confusion-matrix",
    "href": "14-logistic.html#confusion-matrix",
    "title": "14  Classification: Logistic Regression",
    "section": "14.2 Confusion Matrix",
    "text": "14.2 Confusion Matrix\nWe can use accuracy rate: \\[\n\\text{accuracy} = \\dfrac{\\text{\\# of Correct answers}}{n}\n\\] or its dual, error rate \\[\n\\text{error rate} = 1 - \\text{accuracy}.\n\\] You remember, we haw two types of errors. We can use confusion matrix to quantify those\n\n\n\n\nPredicted: YES\nPredicted: NO\n\n\n\n\nActual: YES\nTPR\nFNR\n\n\nActual: NO\nFPR\nTNR\n\n\n\nTrue positive rate (TPR) is the sensitivity and false positive rate (FPR) is the specificity of our predictive model\nExample: Evolute the previous model Accuracy = 0.96\n\n\n\n\nPredicted: YES\nPredicted: NO\n\n\n\n\nActual: YES\nTPR=0.6\nFNR=0.4\n\n\nActual: NO\nFPR=0.03\nTNR=0.97\n\n\n\nI used \\(p=0.2\\) as a cut-off. What if I use smaller or larger \\(p\\), e.g. \\(p=0\\)?\nROC Curve Shows what happens for different cut-off values\nFirst, we define a function, that calculates the ROC\n\nroc &lt;- function(p,y, ...){\n  y &lt;- factor(y)\n  n &lt;- length(p)\n  p &lt;- as.vector(p)\n  Q &lt;- p &gt; matrix(rep(seq(0,1,length=100),n),ncol=100,byrow=TRUE)\n  specificity &lt;- colMeans(!Q[y==levels(y)[1],])\n  sensitivity &lt;- colMeans(Q[y==levels(y)[2],])\n  plot(1-specificity, sensitivity, type=\"l\", ...)\n  abline(a=0,b=1,lty=2,col=8)\n}\n\n\n## roc curve and fitted distributions\npred = predict.glm(glm.fit,newdata = Default, type=\"response\")\ndefault = y\nroc(p=pred, y=Default$default, bty=\"n\", main=\"in-sample\")\n# our 1/5 rule cutoff\npoints(x= 1-mean((pred&lt;.2)[default==0]), \n       y=mean((pred&gt;.2)[default==1]), \n       cex=1.5, pch=20, col='red') \n## a standard `max prob' (p=.5) rule\npoints(x= 1-mean((pred&lt;.5)[default==0]), \n       y=mean((pred&gt;.5)[default==1]), \n       cex=1.5, pch=20, col='blue') \nlegend(\"bottomright\",fill=c(\"red\",\"blue\"),\n       legend=c(\"p=1/5\",\"p=1/2\"),bty=\"n\",title=\"cutoff\")\n\n\n\n\n\n\n\n\nLook at other predictors\n\nglm.fit=glm(default~balance+income+student,data=Default,family=binomial)\nsummary(glm.fit)\n\n\nCall:\nglm(formula = default ~ balance + income + student, family = binomial, \n    data = Default)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.09e+01   4.92e-01  -22.08   &lt;2e-16 ***\nbalance      5.74e-03   2.32e-04   24.74   &lt;2e-16 ***\nincome       3.03e-06   8.20e-06    0.37   0.7115    \nstudentYes  -6.47e-01   2.36e-01   -2.74   0.0062 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.5  on 9996  degrees of freedom\nAIC: 1580\n\nNumber of Fisher Scoring iterations: 8\n\n\nStudent is significant!?\nStudent vs Balance\n\nboxplot(balance~student,data=Default, col = Default$student, ylab = \"balance\")\n\n\n\n\n\n\n\n\nLet’s adjust for balance\n\nx2 = data.frame(balance = seq(1000,2500,length.out = 100), student = as.factor(rep(\"No\",100)), income=rep(40,100))\ny1 = predict.glm(glm.fit,newdata = x1, type=\"response\")\ny2 = predict.glm(glm.fit,newdata = x2, type=\"response\")\nplot(x1$balance,y1, type='l', col=\"red\", xlab=\"Balance\", ylab = \"P(Default)\")\nlines(x2$balance,y2, type='l', col=\"black\")\nlegend(\"topleft\",bty=\"n\", legend=c(\"Not Student\", \"Student\"), col=c(\"black\",\"red\"), lwd=2)\n\n\n\n\n\n\n\n\n\n14.2.1 Estimating the Regression Coefficients\nThe coefficients \\(\\beta = (\\beta_0, \\beta_1)\\) can be estimated using maximum likelihood method we discussed when talked about linear regression.\nThe model we derived above, gives us probability of \\(y\\), given \\(x\\) \\[\np(y\\mid x) = \\dfrac{e^{\\beta^Tx}}{1+e^{\\beta^Tx}}\n\\]\nNow the problem is as follows \\[\n\\underset{\\beta}{maximize} \\prod_{i:y_i = 1}p(x_i)\\prod_{j:y_j = 0}(1-p(x_j)).\n\\]\nMaximum likelihood is a very general approach that is used to fit many of the non-linear models.\n\n\n14.2.2 Choosing \\(p\\) and Evaluating Quality of Classifier\nIn logistic regression we use the logistic function to calculate a probability of \\(y = 1\\) \\[\np(y=1\\mid x) = \\dfrac{e^{\\beta^Tx}}{1+e^{\\beta^Tx}}.\n\\] Then, to predict a label we use a rule if \\(y&lt;p\\), predict 0, and predict 1 otherwise. Now we answer the question of how to choose the cut-off value \\(p\\). We show it through an example.\n\nExample 14.4 (Load Default) Assume a bank is using a logistic regression model to predict probability of a loan default and would issue a loan if \\(a = p(y=1) &lt; p\\). Here \\(p\\) is the level of risk bank is willing to take. If bank chooses \\(p=1\\) and gives loans to everyone it is likely to loose a lot of money from defaulted accounts. If it chooses \\(p = 0\\) it will not issue loan to anyone and wont make any money. In order to choose an appropriate \\(p\\), we need to know what are the risks. Assume, bank makes $0.25 on every $1 borrowed in interest in fees and loose the entire amount of $1 if account defaults. This leads to the following pay-off matrix\n\nPay-off matrix for a loan\n\n\n\npayer\ndefaulter\n\n\n\n\nloan\n-0.25\n1\n\n\nno load\n0\n0\n\n\n\nThen, given \\(a = p(y=1)\\), the expected profit is profit = \\(0.25(1-a) - a\\) to maintain a positive profit we need to choose \\[\n0.25(1-a) - a &gt;0 \\iff -1.25a &gt; -0.25 \\iff a &lt; 0.25 /1.25= 0.2\n\\] Thus, by choosing cutoff to be 0.2 or less, we guarantee to make profit on our loans.\nTo evaluate a binary classification predictor, we will use confusion matrix. It is shows numbers of correct predictions by the model (true positives and true negatives) and incorrect ones (false positive and false negatives). Say, we have a model that predicts weather person has a disease or not and we evaluate this model using 200 samples (\\(n=200\\)) with 60 being labeled as 0 (NO) and 140 labeled as 1 (YES) and model predicted correctly 130 YES labeled observations and 50 NOs.\n\n\n\n\nPredicted: YES\nPredicted: NO\n\n\n\n\nActual: YES\nTP = 130\nFN = 10\n\n\nActual: NO\nFP = 10\nTN = 50\n\n\n\nSometimes, it is convenient to used rates rather than absolute counts and we compute\n\n\n\n\nPredicted: YES\nPredicted: NO\n\n\n\n\nActual: YES\nTPR = 130/140\nFNR = 10/140\n\n\nActual: NO\nFPR = 10/60\nTNR = 50/60\n\n\n\nTrue positive rate (TPR) is nothing but the sensitivity and false positive rate (FPR) is the specificity of our predictive model. Accuracy, which is the percent of correct predictions is another metric can be used to evaluate a classifier. \\[\n\\mbox{Accuracy} = \\dfrac{\\mbox{TP + TN}}{n}.\n\\] The error rate is opposite to accuracy \\[\n\\mbox{Error rate}  = 1- \\mbox{Accuracy}\n\\]\nFor a logistic regression, the confusion matrix will be different for different choices of the cut-off values \\(p\\). If we would like to understand the performance of the model for different values of \\(p\\) we can split an ROC curve, which plots pairs of TPR and FPR for different values of \\(p\\). Saw we take a sequence of 11 values \\(p \\in {0, 0.1, 0.2,\\ldots,1}\\) and we evaluate TPR and FPR for those 10 values and plot those pairs on a 2D plot then we will get the ROC curve. A few facts about the ROC curve:\n\nIf we set \\(p=0\\), then any model will always predict NO, this leads to FPR=0 and TPR=0\nIf we set \\(p=1\\) and model always predicts YES, then we get FPR = 1 and TPR = 1\nIf we have an “ideal\" model then for any \\(0&lt;p&lt;1\\) we will have FPR = 0 and TPR = 1.\nA naive model that uses coin flip to classify will have FPR = 1/2 and TPR = 1/2\nAn ROC curve for an model will lie in-between the ideal curve and naive curve. If your model is worse then naive, it is not a good model. And your model cannot be better than an ideal model.\n\n\n\nExample 14.5 (Default) Let’s consider an example. We want to predict default given attributes of the loan applicant. We have 1000 observations of 9 variables\n\ncredit = read.csv(\"../data/credit.csv\")\ncredit$history = factor(credit$history, levels=c(\"A30\",\"A31\",\"A32\",\"A33\",\"A34\"))\nlevels(credit$history) = c(\"good\",\"good\",\"poor\",\"poor\",\"terrible\")\ncredit$foreign &lt;- factor(credit$foreign, levels=c(\"A201\",\"A202\"), labels=c(\"foreign\",\"german\"))\ncredit$rent &lt;- factor(credit$housing==\"A151\")\ncredit$purpose &lt;- factor(credit$purpose, levels=c(\"A40\",\"A41\",\"A42\",\"A43\",\"A44\",\"A45\",\"A46\",\"A47\",\"A48\",\"A49\",\"A410\"))\nlevels(credit$purpose) &lt;- c(\"newcar\",\"usedcar\",rep(\"goods/repair\",4),\"edu\",NA,\"edu\",\"biz\",\"biz\")\n\ncredit &lt;- credit[,c(\"Default\", \"duration\", \"amount\",\n                    \"installment\", \"age\", \"history\",\n                    \"purpose\", \"foreign\", \"rent\")]\nknitr::kable(head(credit))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefault\nduration\namount\ninstallment\nage\nhistory\npurpose\nforeign\nrent\n\n\n\n\n0\n6\n1169\n4\n67\nterrible\ngoods/repair\nforeign\nFALSE\n\n\n1\n48\n5951\n2\n22\npoor\ngoods/repair\nforeign\nFALSE\n\n\n0\n12\n2096\n2\n49\nterrible\nedu\nforeign\nFALSE\n\n\n0\n42\n7882\n2\n45\npoor\ngoods/repair\nforeign\nFALSE\n\n\n1\n24\n4870\n3\n53\npoor\nnewcar\nforeign\nFALSE\n\n\n0\n36\n9055\n2\n35\npoor\nedu\nforeign\nFALSE\n\n\n\n\n\nWe build a logistic regression model using all of the 8 predictors and their interactions\ncredscore = glm(Default~.^2,data=credit,family=binomial)\nThen we plot ROC curve (FPR vs TPR) for different values of \\(p\\) and compare the curve with the naive\n\n\n\n\n\nROC curve for logistic regression model for repearting defualts",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Classification: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "14-logistic.html#multinomial-logistic-regression",
    "href": "14-logistic.html#multinomial-logistic-regression",
    "title": "14  Classification: Logistic Regression",
    "section": "14.3 Multinomial logistic regression",
    "text": "14.3 Multinomial logistic regression\nSoftmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: \\(y_i \\in \\{0,1\\}\\) . We used such a classifier to distinguish between two kinds of hand-written digits. Softmax regression allows us to handle \\(y_i \\in \\{1,\\ldots ,K\\}\\) where \\(K\\) is the number of classes. Our model took the form: \\[\nf(x\\mid \\beta)=\\dfrac{1}{1+\\exp(-\\beta^Tx)}~,\n\\] and the model parameters \\(\\beta\\) were trained to minimize the loss function (negative log-likelihood) \\[\nJ(\\beta) = -\\left[ \\sum_{i=1}^m y_i \\log f(x\\mid \\beta) + (1-y_i) \\log (1-f(x\\mid \\beta)) \\right]\n\\]\nGiven a test input \\(x\\), we want our model to estimate the probability that \\(p(y=k|x)\\) for each value of \\(k=1,\\ldots ,K\\) Thus, our model will output a \\(K\\)-dimensional vector (whose elements sum to 1) giving us our K estimated probabilities. Concretely, our model \\(f(x\\mid \\beta)\\) takes the form: \\[\n\\begin{aligned}\nf(x\\mid \\beta) =\n\\begin{bmatrix}\np(y = 1 | x; \\beta) \\\\\np(y = 2 | x; \\beta) \\\\\n\\vdots \\\\\np(y = K | x; \\beta)\n\\end{bmatrix}\n=\n\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\beta_k^T x) }}\n\\begin{bmatrix}\n\\exp(\\beta_1^{T} x ) \\\\\n\\exp(\\beta_2^{T} x ) \\\\\n\\vdots \\\\\n\\exp(\\beta_k^T x ) \\\\\n\\end{bmatrix}\\end{aligned}\n\\]\nHere \\(\\beta_i \\in R^n, i=1,\\ldots,K\\) are the parameters of our model. Notice that the term \\(1/ \\sum_{j=1}^{K}{\\exp(\\beta_k^T x) }\\) normalizes the distribution, so that it sums to one.\nFor convenience, we will also write \\(\\beta\\) to denote all the parameters of our model. When you implement softmax regression, it is usually convenient to represent \\(\\beta\\) as an \\(n\\)-by-\\(K\\) matrix obtained by concatenating \\(\\beta_1,\\beta_2,\\ldots ,\\beta_K\\) into columns, so that \\[\n\\beta = \\left[\\begin{array}{cccc}| & | & | & | \\\\\n\\beta_1 & \\beta_2 & \\cdots & \\beta_K \\\\\n| & | & | & |\n\\end{array}\\right].\n\\]\nWe now describe the cost function that we’ll use for softmax regression. In the equation below, \\(1\\) is the indicator function, so that \\(1\\)(a true statement)=1, and \\(1\\)(a false statement)=0. For example, 1(2+3 &gt; 4) evaluates to 1; whereas 1(1+1 == 5) evaluates to 0. Our cost function will be: \\[\n\\begin{aligned}\nJ(\\beta) = - \\left[ \\sum_{i=1}^{m} \\sum_{k=1}^{K}  1\\left\\{y_i = k\\right\\} \\log \\frac{\\exp(\\beta_k^T x_i)}{\\sum_{j=1}^K \\exp(\\beta_k^T x_i)}\\right]\n\\end{aligned}\n\\]\nNotice that this generalizes the logistic regression cost function, which could also have been written: \\[\n\\begin{aligned}\nJ(\\beta) &= - \\left[ \\sum_{i=1}^m   (1-y_i) \\log (1-f(x\\mid \\beta)) + y_i \\log f(x\\mid \\beta) \\right] \\\\\n&= - \\left[ \\sum_{i=1}^{m} \\sum_{k=0}^{1} 1\\left\\{y_i = k\\right\\} \\log p(y_i = k | x_i ; \\beta) \\right]\\end{aligned}\n\\] The softmax cost function is similar, except that we now sum over the \\(K\\) different possible values of the class label. Note also that in softmax regression, we have that \\[\np(y_i = k | x_i ; \\beta) = \\frac{\\exp(\\beta_k^T x_i)}{\\sum_{j=1}^K \\exp(\\beta_k^T x_i) }.\n\\]\nSoftmax regression has an unusual property that it has a redundant set of parameters. To explain what this means, suppose we take each of our parameter vectors \\(\\beta_j\\), and subtract some fixed vector \\(\\psi\\). Our model now estimates the class label probabilities as \\[\n\\begin{aligned}\np(y_i = k | x_i ; \\beta)\n&= \\frac{\\exp((\\beta_k-\\psi)^T x_i)}{\\sum_{j=1}^K \\exp( (\\beta_j-\\psi)^T x_i)}  \\\\\n&= \\frac{\\exp(\\beta_k^T x_i) \\exp(-\\psi^T x_i)}{\\sum_{j=1}^K \\exp(\\beta_k^T x_i) \\exp(-\\psi^T x_i)} \\\\\n&= \\frac{\\exp(\\beta_k^T x_i)}{\\sum_{j=1}^K \\exp(\\beta_k^T x_i)}.\\end{aligned}\n\\] In other words, subtracting \\(\\psi\\) does not affect our model’ predictions at all! This shows that softmax regression’s parameters are redundant. More formally, we say that our softmax model is overparameterized, meaning that for any model we might fit to the data, there are multiple parameter settings that give rise to exactly the same model function \\(f(x \\mid \\beta)\\) mapping from inputs \\(x\\) to the predictions.\nFurther, if the cost function \\(J(\\beta)\\) is minimized by some setting of the parameters \\((\\beta_1,\\ldots,\\beta_K)\\), then it is also minimized by \\((\\beta_1-\\psi,\\ldots,\\beta_K-\\psi)\\) for any value of \\(\\psi\\). Thus, the minimizer of \\(J(\\beta)\\) is not unique. Interestingly, \\(J(\\beta)\\) is still convex, and thus gradient descent will not run into local optima problems. But the Hessian is singular/non-invertible, which causes a straightforward implementation of Newton’s method to run into numerical problems. We can just set \\(\\psi\\) to \\(\\beta_i\\) and remove \\(\\beta_i\\).\n\nExample 14.6 (LinkedIn Study) How to Become an Executive(Irwin 2016; Gan and Fritzler 2016)?\nLogistic regression was used to analyze the career paths of about \\(459,000\\) LinkedIn members who worked at a top 10 consultancy between 1990 and 2010 and became a VP, CXO, or partner at a company with at least 200 employees. About \\(64,000\\) members reached this milestone, \\(\\hat{p} = 0.1394\\), conditional on making it into the database. The goals of the analysis were the following\n\nLook at their profiles – educational background, gender, work experience, and career transitions.\nBuild a predictive model of the probability of becoming an executive\nProvide a tool for analysis of “what if” scenarios. For example, if you are to get a master’s degree, how your jobs perspectives change because of that.\n\nLet’s build a logistic regression model with \\(8\\) key features (a.k.a. covariates): \\[\n\\log\\left ( \\frac{p}{1-p} \\right ) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_8x_8\n\\]\n\n\\(p\\): Probability of “success” – reach VP/CXO/Partner seniority at a company with at least 200 employees.\nFeatures to predict the “success” probability: \\(x_i (i=1,2,\\ldots,8)\\).\n\n\n\nVariable\nParameters\n\n\n\n\n\\(x_1\\)\nMetro region: whether a member has worked in one of the top 10 largest cities in the U.S. or globally.\n\n\n\\(x_2\\)\nGender: Inferred from member names: ‘male’, or ‘female’\n\n\n\\(x_3\\)\nGraduate education type: whether a member has an MBA from a top U.S. program / a non-top program / a top non-U.S. program / another advanced degree\n\n\n\\(x_4\\)\nUndergraduate education type: whether a member has attended a school from the U.S. News national university rankings / a top 10 liberal arts college /a top 10 non-U.S. school\n\n\n\\(x_5\\)\nCompany count: # different companies in which a member has worked\n\n\n\\(x_6\\)\nFunction count: # different job functions in which a member has worked\n\n\n\\(x_7\\)\nIndustry sector count: # different industries in which a member has worked\n\n\n\\(x_8\\)\nYears of experience: # years of work experience, including years in consulting, for a member.\n\n\n\n\nThe following estimated \\(\\hat\\beta\\)s of features were obtained. With a sample size of 456,000 thy are measured rather accurately. Recall, given each location/education choice in the “Choice and Impact” is a unit change in the feature.\n\nLocation: Metro region: 0.28\nPersonal: Gender(Male): 0.31\nEducation: Graduate education type: 1.16, Undergraduate education type: 0.22\nWork Experience: Company count: 0.14, Function count: 0.26, Industry sector count: -0.22, Years of experience: 0.09\n\nHere are three main findings\n\nWorking across job functions, like marketing or finance, is good. Each additional job function provides a boost that, on average, is equal to three years of work experience. Switching industries has a slight negative impact. Learning curve? lost relationships?\nMBAs are worth the investment. But pedigree matters. Top five program equivalent to \\(13\\) years of work experience!!!\nLocation matters. For example, NYC helps.\n\nWe can also personalize the prediction for predict future possible future executives. For example, Person A (p=6%): Male in Tulsa, Oklahoma, Undergraduate degree, 1 job function for 3 companies in 3 industries, 15-year experience.\nPerson B (p=15%): Male in London, Undergraduate degree from top international school, Non-MBA Master, 2 different job functions for 2 companies in 2 industries, 15-year experience.\nPerson C (p=63%): Female in New York City, Top undergraduate program, Top MBA program, 4 different job functions for 4 companies in 1 industry, 15-year experience.\nLet’s re-design Person B.\nPerson B (p=15%): Male in London, Undergraduate degree from top international school, Non-MBA Master, 2 different job functions for 2 companies in 2 industries, 15-year experience.\n\nWork in one industry rather than two. Increase \\(3\\)%\nUndergrad from top \\(10\\) US program rather than top international school. \\(3\\)%\nWorked for \\(4\\) companies rather than \\(2\\). Another \\(4\\)%\nMove from London to NYC. \\(4\\)%\nFour job functions rather than two. \\(8\\)%. A \\(1.5\\)x effect.\nWorked for \\(10\\) more years. \\(15\\)%. A \\(2\\)X effect.\n\nChoices and Impact (Person B) are shown below",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Classification: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "14-logistic.html#imbalanced-data",
    "href": "14-logistic.html#imbalanced-data",
    "title": "14  Classification: Logistic Regression",
    "section": "14.4 Imbalanced Data",
    "text": "14.4 Imbalanced Data\nOften, you have much more observations with a specific label, such a sample is called imbalanced. You should avoid using accuracy as a metric to choose a model. Say, you have a binary classification problem with 95% of samples labeled as 1. Then a naive classifier that assigns label 1 for each input will be 95% accurate. An ROC curve and area under the curve (AUC) metric derived from it should be used. Alternatively, you can use F1 sore, which combines precision and recall \\[\nF1 = 2\\dfrac{\\mathrm{precision} \\times \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}\n\\]\nA modeler should consider synthetically generating more samples of class with small number of observation, e.g. bootstrap or using a generative model or subsampling observations with major label if data set is large enough.\n\n14.4.1 Kernel Trick\nKernel trick is a method of using a linear classifier to solve a non-linear problem. The idea is to map the data into a higher dimensional space, where it becomes linearly separable. The kernel trick is to use a kernel function \\(K(x_i,x_j)\\) to calculate the inner product of two vectors in the higher dimensional space without explicitly calculating the mapping \\(\\phi(x_i)\\) and \\(\\phi(x_j)\\). The kernel function is defined as \\(K(x_i,x_j) = \\phi(x_i)^T\\phi(x_j)\\). The most popular kernel functions are polynomial kernel \\(K(x_i,x_j) = (x_i^Tx_j)^d\\) and Gaussian kernel \\(K(x_i,x_j) = \\exp(-\\gamma||x_i-x_j||^2)\\). The kernel trick is used in Support Vector Machines (SVM) and Gaussian Processes (GP).\n\n\nCode\ngencircledata = function(numSamples,radius,noise) {\n    d = matrix(0,ncol = 3, nrow = numSamples); # matrix to store our generated data\n    # Generate positive points inside the circle.\n    for (i in 1:(numSamples/2) ) {\n    r = runif(1,0, radius * 0.4);\n    angle = runif(1,0, 2 * pi);\n    x = r * sin(angle);\n    y = r * cos(angle);\n    noiseX = runif(1,-radius, radius) * noise;\n    noiseY = runif(1,-radius, radius) * noise;\n    d[i,] = c(0,x,y)\n    }\n\n    # Generate negative points outside the circle.\n    for (i in (numSamples/2+1):numSamples ) {\n    r = runif(1,radius * 0.8, radius);\n    angle = runif(1,0, 2 * pi);\n    x = r * sin(angle);\n    y = r * cos(angle);\n    noiseX = runif(1,-radius, radius) * noise;\n    noiseY = runif(1,-radius, radius) * noise;\n    d[i,] = c(1,x,y)\n    }\n    colnames(d) = c(\"label\", \"x1\", \"x2\")\n    return(d)\n}\n\n\n\nd = gencircledata(numSamples=200, radius=1, noise=0.001)\nplot(d[,2],d[,3], col=d[,1]+1, pch=19, xlab=\"x\", ylab=\"y\")\n\n\n\n\n\n\n\nFigure 14.1\n\n\n\n\n\nThe data on the left in Figure Figure 14.1 is clearly not linearly separable. However, if we map it to a three-dimensional space using the transformation: \\[\n\\begin{aligned}\n\\phi: R^{2} & \\longrightarrow R^{3} \\\\\n\\left(x_{1}, x_{2}\\right) & \\longmapsto\\left(z_{1}, z_{2}, z_{3}\\right)=\\left(x_{1}^{2}, \\sqrt{2} x_{1} x_{2}, x_{2}^{2}\\right),\n\\end{aligned}\n\\] and attempt to linearly separate the transformed data, the decision boundaries become hyperplanes in \\(R^{3}\\), expressed as \\(\\omega^{T} z + b = 0\\). In terms of the original variables \\(x\\), these boundaries take the form: \\[\n\\omega_{1} x_{1}^{2} + \\omega_{2} \\sqrt{2} x_{1} x_{2} + \\omega_{3} x_{2}^{2} = 0,\n\\] which corresponds to the equation of an ellipse. This demonstrates that we can apply a linear algorithm to transformed data to achieve a non-linear decision boundary with minimal effort.\nNow, consider what the algorithm is actually doing. It relies solely on the Gram matrix \\(K\\) of the data. Once \\(K\\) is computed, the original data can be discarded: \\[\n\\begin{aligned}\nK & = \\left[\\begin{array}{ccc}\nx_{1}^{T} x_{1} & x_{1}^{T} x_{2} & \\cdots \\\\\nx_{2}^{T} x_{1} & \\ddots & \\\\\n\\vdots & &\n\\end{array}\\right]_{n \\times n} = X X^{T}, \\\\\n\\text{where} \\quad X & = \\left[\\begin{array}{c}\nx_{1}^{T} \\\\\n\\vdots \\\\\nx_{n}^{T}\n\\end{array}\\right]_{n \\times d}.\n\\end{aligned}\n\\] Here, \\(X\\), which contains all the data, is referred to as the design matrix.\nWhen we map the data using \\(\\phi\\), the Gram matrix becomes: \\[\nK = \\left[\\begin{array}{ccc}\n\\phi\\left(x_{1}\\right)^{T} \\phi\\left(x_{1}\\right) & \\phi\\left(x_{1}\\right)^{T} \\phi\\left(x_{2}\\right) & \\cdots \\\\\n\\phi\\left(x_{2}\\right)^{T} \\phi\\left(x_{1}\\right) & \\ddots & \\\\\n\\vdots & &\n\\end{array}\\right].\n\\]\nLet us compute these inner products explicitly. For vectors \\(r\\) and \\(s\\) in \\(R^{3}\\) corresponding to \\(a\\) and \\(b\\), respectively: \\[\n\\begin{aligned}\n\\langle r, s \\rangle & = r_{1} s_{1} + r_{2} s_{2} + r_{3} s_{3} \\\\\n& = a_{1}^{2} b_{1}^{2} + 2 a_{1} a_{2} b_{1} b_{2} + a_{2}^{2} b_{2}^{2} \\\\\n& = \\langle a, b \\rangle^{2}.\n\\end{aligned}\n\\]\nThus, instead of explicitly mapping the data via \\(\\phi\\) and then computing the inner product, we can compute it directly in one step, leaving the mapping \\(\\phi\\) implicit. In fact, we do not even need to know \\(\\phi\\) explicitly; all we require is the ability to compute the modified inner product. This modified inner product is called a kernel, denoted \\(K(x, y)\\). The matrix \\(K\\), which contains the kernel values for all pairs of data points, is also referred to as the kernel matrix.\nSince the kernel itself is the primary object of interest, rather than the mapping \\(\\phi\\), we aim to characterize kernels without explicitly relying on \\(\\phi\\). Mercer’s Theorem provides the necessary framework for this characterization.\nLet’s implement it\n\nlibrary(\"scatterplot3d\")\nphi &lt;- function(x1, x2) {\n    z1 &lt;- x1^2\n    z2 &lt;- sqrt(2) * x1 * x2\n    z3 &lt;- x2^2\n    return(cbind(z1, z2, z3))\n}\n\n# Generate sample 2D data (you can replace this with your actual data)\n\n\n# Apply the transformation\ntransformed_data &lt;- phi(d[,2], d[,3])\nscatterplot3d(transformed_data, color = ifelse(d[,1] == 0, \"red\", \"blue\"), pch = 19,\n                xlab = \"z1 (x1^2)\", ylab = \"z2 (sqrt(2) * x1 * x2)\", zlab = \"z3 (x2^2)\",\n                main = \"3D Scatter Plot of Transformed Data\", angle=222, grid=FALSE, box=FALSE)\n\n\n\n\n\n\n\n\n\nExample 14.7 (Formula One) As described in the Artificial Intelligence in Formula 1 article, Formula One teams are increasingly leveraging AI and machine learning to optimize race strategies. The article highlights how teams collect massive amounts of data during races - with 300 sensors per car generating millions of data points over 200-mile races. This data includes critical variables like fuel load, tire degradation, weight effects, and pit stop timing that must be optimized in real-time.\nThe key innovation is moving from pre-race strategy planning to in-race dynamic optimization using cloud computing platforms like AWS. Teams run Monte Carlo simulations of all cars and traffic situations to continuously update their strategy recommendations. This allows them to make optimal decisions about when to pit, which tires to use, and how to manage fuel consumption based on real-time race conditions rather than static pre-race plans.\nThe article emphasizes that the best strategies can vary dramatically from moment to moment during a race, making real-time AI-powered decision making crucial for competitive advantage. Teams are limited to 60 data scientists, so they must rely heavily on automated machine learning systems to process the vast amounts of sensor data and generate actionable insights during races.\nThe CNBC article highlights how Formula One championships are increasingly being determined by technological innovation rather than just driver skill. F1 success depends heavily on advanced data analytics, machine learning algorithms, and real-time computing capabilities. Key technological factors driving F1 success include real-time data processing where teams process millions of data points from hundreds of sensors per car during races. AI-powered strategy optimization uses machine learning algorithms to continuously analyze race conditions and recommend optimal pit stop timing, tire choices, and fuel management. Cloud computing infrastructure allows teams to rely on platforms like AWS to run complex simulations and data analysis during races. Predictive modeling employs advanced algorithms to predict tire degradation, fuel consumption, and competitor behavior. Simulation capabilities enable teams to run thousands of Monte Carlo simulations to optimize race strategies.\nThe technological arms race in Formula One has led to significant regulatory challenges. To maintain competitive balance and prevent larger teams from gaining insurmountable advantages through unlimited technological investment, F1 has implemented strict caps on the number of engineers and data scientists that teams can employ. Currently, teams are limited to 60 data scientists and engineers, which forces them to be highly strategic about their technological investments and resource allocation. This cap creates an interesting dynamic where teams must balance the need for sophisticated AI and machine learning capabilities with the constraint of limited human resources. As a result, teams are increasingly turning to automated systems and cloud-based solutions to maximize their technological capabilities within these constraints. The cap also levels the playing field somewhat, ensuring that success depends more on the efficiency and innovation of the technology rather than simply having more engineers and data scientists than competitors.\n\n\n\n\n\nGan, Link, and Alan Fritzler. 2016. “How to Become an Executive.”\n\n\nIrwin, Neil. 2016. “How to Become a C.E.O.? The Quickest Path Is a Winding One.” The New York Times, September.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Classification: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "15-rct.html",
    "href": "15-rct.html",
    "title": "15  Randomized Controlled Trials",
    "section": "",
    "text": "15.1 The Question of Causation\nFlorence Nightingale (1820 - 1910) was a heroine of the Crimean War, Patron Saint of Nurses, admirer of Quetelet, and champion of the statistical, study of society. She can be called the mother of observational studies. To her every piece of legislation was an experiment in the laboratory of society deserving study and demanding evaluation. Nightingale recognized the importance of collecting accurate and reliable data to understand healthcare outcomes. She developed standardized methods for collecting data on hospital admissions, deaths, causes of death, and other relevant factors. This systematic data collection allowed for more rigorous and reliable analysis of healthcare practices and their impact on patient outcomes. During the Crimean War (1853-1856), she collected and analyzed data on mortality rates among soldiers. She created statistical diagrams, such as the famous polar area diagram or “coxcomb,” to illustrate the causes of mortality. These visual representations helped to convey complex information in a clear and understandable way. Nightingale’s observations and statistical analyses led her to emphasize the importance of sanitation and hygiene in healthcare settings. She advocated for improvements in cleanliness, ventilation, and sanitation in hospitals, recognizing the impact of these factors on the health and well-being of patients. Beyond the battlefield, Nightingale continued her work in public health. She used statistical evidence to advocate for healthcare reforms and improvements in public health infrastructure. Her efforts played a crucial role in shaping public health policies and practices.\nThe work of Nightingale would be nowadays called an observational study. An observational study is a research design where researchers observe and collect data on existing groups of people or phenomena without intervening or manipulating any variables. Unlike randomized controlled trials, researchers do not assign participants to different groups and do not directly influence the outcome.\nGeorge Washington (1732-1799) on the other hand has made an enormous fortune in farming and one of the distinguishing features of his farming practices was the use of what we nowadays call a controlled experiment. He was deeply interested in improving agricultural techniques and conducted numerous experiments at his Mount Vernon estate. One of his most notable experiments involved dividing his land into plots and testing different crop rotations and fertilization methods. Washington recognized the importance of sustainable agriculture and the detrimental effects of monoculture (growing the same crop year after year) on soil fertility. He observed how tobacco, his primary cash crop at the time, depleted the soil nutrients, leading to diminishing yields. To address this issue and improve the long-term health of his land, he began experimenting with crop rotation and soil management techniques.\nWashington divided his land into several plots, each receiving different treatments. He experimented with various crop rotations, including wheat-fallow, wheat-rye-fallow, and corn-wheat-fallow. These rotations aimed to prevent soil depletion and promote its natural restoration by planting nitrogen-fixing crops like rye and clover. He also tested different fertilizer applications on various plots. He used manure, compost, and even imported materials like gypsum and marl to improve soil fertility and crop yields.\nWashington meticulously documented his experiments in his agricultural diaries. He recorded planting dates, yields, weather conditions, and observations on crop growth and soil health. This meticulous record-keeping allowed him to analyze the effectiveness of different treatments and compare their impact on crop yields and soil quality.\nWashington’s experiments yielded valuable insights into sustainable agricultural practices. He discovered that crop rotation and fertilization improved soil health and increased crop yields over time. He abandoned tobacco as his primary crop and shifted towards wheat, which was less soil-depleting and offered a more stable income source.\nThe historic trades staff at Mount Vernon have recreated Washington’s experiment at the Pioneer Farm, using the same plot layout, crops, and fertilization methods described in his diaries. This allows visitors to learn about his innovative farming techniques and their impact on the land. Figure 15.1 shows the plot layout at the Pioneer Farm.\nGeorge Washington’s commitment to experimentation and innovation made him a pioneer in American agriculture. His plot-based experiments demonstrated the effectiveness of crop rotation and soil management in promoting sustainable farming practices. His work continues to inspire farmers today and serves as a valuable resource for understanding agricultural history and best practices.\nLater, at the turn of the 20th century, Ronald Fisher (1890 - 1962) developed the theory of experimental design which allowed for controlled experiments, known as randomized controlled trials (RCT). Fisher’s work laid the foundation for modern experimental design and analysis, providing a rigorous statistical framework for conducting randomized controlled trials. His contributions to experimental design and ANOVA were crucial in establishing the importance of randomized trials in research. He emphasized the importance of randomization and control groups in experimental design, recognizing their crucial role in establishing causal relationships.\nThe modern randomized controlled trial (RCT) in medicine is most often attributed to Sir Austin Bradford Hill. In 1948, Hill published a landmark paper titled “Streptomycin Treatment of Pulmonary Tuberculosis” in the British Medical Journal, which described the first fully randomized, double-blind clinical trial. This study is considered a turning point in the history of medical research and established the RCT as the gold standard for evaluating the effectiveness of medical treatments.\nRandomized trials and observational studies are two distinct approaches to gathering and analyzing data in research studies. Here’s a breakdown of their key differences:\nIf you happen to have a choice between randomized trials and observational data (often you do not have that choice), which one should you choose? Here are a few things to consider:\nUltimately, both randomized trials and observational data play crucial roles in research. Combining these two approaches can provide a more comprehensive understanding of the relationship between interventions and outcomes.\nRothmstead t-rations split/pop designs\nRandomized controlled trials (RCTs) and field experiments are considered the gold standard for establishing causation because they allow researchers to isolate the effect of a specific intervention or treatment from other confounding factors. The main principle of RCTs and field experiments is randomization, which ensures that the treatment and control groups are similar in all respects except for the treatment. This allows researchers to attribute any differences in outcomes between the two groups to the treatment, rather than to other factors. Randomization helps to control for confounding variables, which are factors that are associated with both the treatment and the outcome variable. By randomly assigning participants to groups, researchers can ensure that any confounding variables are evenly distributed between the groups. The control group serves as a baseline for comparison. It is a group that is not exposed to the treatment or intervention being studied. By comparing the outcomes of the treatment group and the control group, researchers can isolate the effect of the treatment. Any differences in the outcomes between the two groups can be attributed to the treatment, rather than to other factors.\nFor many years, the main area of applications of randomized trials were medicine and agriculture. In medicine, randomized trials are used to test the effectiveness of new drugs and treatments. In agriculture, randomized trials are used to test the effectiveness of new fertilizers, pesticides, and other agricultural inputs. However, with the rise of internet, randomized trials have become increasingly popular for testing the effectiveness of online interventions, such as email campaigns, website designs, and social media ads. However, then applied to user experience and marketing, randomized trials are often called A/B tests. The idea of A/B testing is the same: randomly assign users to different versions of a website or an email campaign and compare the outcomes. However, the level or “rigor” of designing the experiment is often lower than in medicine or agriculture. There are less strict rules about ethics, randomization, sample size, and statistical analysis. For example, randomization sometimes completely ignored in A/B testing. Instead of assigning users randomly to groups, they are divided into groups based on factors like time of day, location, or browsing history. This can introduce bias into the results, as the groups may not be comparable. As a result, A/B testing is cheap and quick to conduct, as it can be done online without the need for IRB approval or recruitment of participants. Participants most of the times do not even know that they are participating in an A/B experiment. Futhermore, in A/B testing the primarily focused on measuring the comparative performance of variations without necessarily establishing a causal relationship. It answers questions like, “Which version of our web page leads to more clicks?”\nYet another area where RCTs are becoming popular are the economic studies. For example, randomized trials are used to test the effectiveness of educational interventions, such as tutoring programs and online courses. In recent years, randomized trials have been used to study a wide range of other phenomena, including education, economics, and public policy.\nWhile RCTs and field experiments are powerful tools for establishing causation, they are not always feasible or ethical. In some cases, observational studies may be the best way to study a particular phenomenon. However, even in observational studies, researchers can use techniques such as matching and instrumental variables to try to control for confounding variables. It is important to remember that even RCTs and field experiments cannot definitively prove causation. However, they provide the strongest evidence possible for a causal relationship between a treatment or intervention and an outcome.\nDiane Lambert was the original statistitian who promoted the ides of proper statistical usage of observational data among internet companies. She has presented on how to detect selection bias in data streams drawn from transaction logs, ad-systems, etc.; diagnostics to judge when bias overwhelms signal; and practical fixes such as propensity-score weighting, doubly-robust estimators, post-stratification and simulation so that credible causal conclusions can still be drawn from field/observational data. For example, at JSM 2011 (Miami) she co-authored the session “The Effectiveness of Display Ads,” contrasting large-scale field experiments with observational analyses of ad-click logs.\nThe advent of digital data has fundamentally transformed the practice of statistics, shifting the field from a discipline focused on small, carefully collected samples to one that must grapple with massive, often messy datasets generated as byproducts of digital systems. In the pre-digital era, statisticians worked primarily with structured, purposefully collected data through surveys, experiments, and clinical trials, where the sample size was a critical constraint and every observation was expensive to obtain. Today, organizations routinely collect terabytes of data from web traffic, sensor networks, financial transactions, and social media interactions, creating what some have called a “data deluge.” This shift has necessitated new statistical approaches that can handle high-dimensional data, complex dependencies, and the computational challenges of scale. Machine learning algorithms, once considered separate from traditional statistics, have become essential tools for extracting patterns from these vast datasets. However, this transition has also introduced new challenges: the need to distinguish correlation from causation in observational data, the importance of addressing selection bias in non-random samples, and the ethical considerations of privacy and algorithmic fairness. The field has evolved to embrace both the opportunities presented by big data—such as the ability to detect subtle patterns and make real-time predictions—and the responsibility to develop robust methods that can provide reliable insights despite the inherent noise and complexity of digital data sources.\nMarkets give signals in a form of betting odds, e.g. polymarket uses collective thought to produce a signal. It often happens that the collectife though outperforms the best experts.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Randomized Controlled Trials</span>"
    ]
  },
  {
    "objectID": "15-rct.html#bart-for-causal-inference",
    "href": "15-rct.html#bart-for-causal-inference",
    "title": "15  Randomized Controlled Trials",
    "section": "15.2 BART For Causal Inference",
    "text": "15.2 BART For Causal Inference\nEstimating the causal effect of an intervention, such as a new drug, a marketing campaign, or a public policy, is a central goal across science and industry. While the gold standard for causal inference is the Randomized Controlled Trial (RCT), it is often infeasible, unethical, or too expensive to conduct. Researchers must therefore turn to observational data, where the assignment of treatment is not controlled by the investigator. This introduces a fundamental challenge: individuals who receive the treatment may be systematically different from those who do not, a problem known as confounding. Separating the true effect of the treatment from these pre-existing differences is the primary task of causal inference from observational data.\nTo formalize causal questions, we rely on the Rubin Causal Model (RCM), also known as the potential outcomes framework. For a binary treatment \\(Z\\) (where \\(Z_i=1\\) if individual \\(i\\) receives the treatment and \\(Z_i=0\\) otherwise), we posit that each individual \\(i\\) has two potential outcomes: * \\(Y_i(1)\\): The outcome that would be observed if individual \\(i\\) were exposed to the treatment. * \\(Y_i(0)\\): The outcome that would be observed if individual \\(i\\) were exposed to the control (no treatment).\nThis framework leads directly to what Holland (1986) termed the “fundamental problem of causal inference”: for any given individual, we can only ever observe one of these two potential outcomes. The outcome we do not see is the counterfactual. Causal inference can thus be viewed as a missing data problem, where the goal is to estimate the values of the unobserved potential outcomes.\nFrom this foundation, we can define several key causal quantities, or estimands:\n\nIndividual Treatment Effect (ITE): The effect for a single individual, defined as \\[\\tau_i = Y_i(1) - Y_i(0).\\] This is typically unobservable.\nAverage Treatment Effect (ATE): The average effect across the entire population, \\[\\tau_{ATE} = E[Y(1) - Y(0)].\\] This is often the primary estimand of interest for broad policy questions.\nAverage Treatment Effect on the Treated (ATT): The average effect for those who actually received the treatment, \\[\\tau_{ATT} = E[Y(1) - Y(0) | Z=1].\\]\nConditional Average Treatment Effect (CATE): The average effect for a subpopulation defined by a set of covariates \\(X=x\\), \\[\\tau(x) = E[Y(1) - Y(0) | X=x].\\] Understanding the CATE allows for the exploration of treatment effect heterogeneity.\n\nTo estimate these causal estimands from observational data, we must rely on a set of critical, untestable assumptions that connect the observed data to the unobserved potential outcomes. These are known as identification assumptions.\n\nStable Unit Treatment Value Assumption (SUTVA): This assumption has two parts. First, it assumes there is no interference between units, meaning one individual’s treatment status does not affect another’s outcome. Second, it assumes there are no hidden variations of the treatment; the treatment assigned to one individual is the same as the treatment assigned to any other.\nIgnorability (or Unconfoundedness): This is the most crucial assumption. It states that, conditional on a set of observed pre-treatment covariates \\(X\\), treatment assignment \\(Z\\) is independent of the potential outcomes: \\[(Y(0), Y(1)) \\perp Z | X\\]. In essence, it assumes that we have measured all the common causes of both treatment selection and the outcome. If this holds, then within any stratum defined by the covariates \\(X\\), the treatment assignment is “as-if” random.\nPositivity (or Overlap/Common Support): This assumption requires that for any set of covariate values \\(x\\) present in the population, there is a non-zero probability of being in either the treatment or the control group: \\(0 &lt; P(Z=1 | X=x) &lt; 1\\). This ensures that we can find both treated and control individuals with similar characteristics, making comparison meaningful and avoiding extrapolation to regions with no data.\n\nTo demonstrate the application of Bayesian methods to this challenge, we use the famous Lalonde dataset, a canonical benchmark in the causal inference literature. The dataset addresses a real-world policy question: evaluating the effectiveness of the National Supported Work (NSW) Demonstration, a federally funded job training program implemented in the US from 1975-1979. The program was designed to help individuals facing significant social and economic barriers (e.g., former drug addicts, ex-convicts, high school dropouts) improve their labor market prospects. The treatment (\\(treat\\)) is participation in this program, and the primary outcome (\\(re78\\)) is the individual’s real earnings in 1978, after the program.\nThe historical importance of this dataset stems from Robert Lalonde’s 1986 paper, which delivered a powerful critique of the non-experimental methods used at the time. Lalonde started with data from an actual RCT, which provided an unbiased estimate of the program’s effect. He then took the treated group from the experiment but replaced the experimental control group with a non-experimental comparison group drawn from large public surveys—the Panel Study of Income Dynamics (PSID) and the Current Population Survey (CPS). He showed that the standard econometric models of the era failed to replicate the experimental benchmark when applied to this new, confounded dataset, casting serious doubt on their reliability for policy evaluation. Our task is to see if a modern, flexible Bayesian method—Bayesian Additive Regression Trees (BART)—can succeed where these earlier methods failed.\nThe challenge posed by the Lalonde dataset becomes immediately apparent when we examine the pre-treatment characteristics of the treated group versus the non-experimental control group. A naive comparison of their 1978 earnings would be deeply misleading because the groups were profoundly different before the program even began. Table 15.1 illustrates this imbalance for key covariates, including age, education, race, marital status, and earnings in the years prior to the intervention (1974 and 1975).\nThe Standardized Mean Difference (SMD) provides a scale-free measure of the difference between the group means. A common rule of thumb suggests that an absolute SMD greater than 0.1 indicates a potentially meaningful imbalance. As the table shows, the groups differ substantially on nearly every measured characteristic. The treated individuals were younger, less educated, more likely to be from minority groups, and had drastically lower earnings in the years before the program. This severe selection bias is precisely what makes the Lalonde dataset such a difficult and important test case for causal inference methods. Any credible method must be able to adjust for these vast pre-existing differences to isolate the true causal effect of the job training program.\n\n\n\nTable 15.1: Covariate Balance in the Lalonde Non-Experimental Dataset. Note: Data corresponds to the widely used Dehejia and Wahba (1999) sample of the Lalonde dataset. Standardized Mean Difference is calculated as the difference in means divided by the pooled standard deviation.\n\n\n\n\n\nCovariate\nTreated Mean\nControl Mean\nStd. Mean Diff.\n\n\n\n\nAge (years)\n25.82\n28.04\n-0.31\n\n\nEducation (years)\n10.35\n10.23\n0.06\n\n\nBlack (indicator)\n0.84\n0.20\n1.84\n\n\nHispanic (indicator)\n0.06\n0.14\n-0.32\n\n\nMarried (indicator)\n0.19\n0.51\n-0.81\n\n\nNo Degree (indicator)\n0.71\n0.60\n0.25\n\n\nEarnings 1974\n2095.57\n5630.71\n-0.63\n\n\nEarnings 1975\n1532.06\n5205.52\n-0.65\n\n\n\n\n\n\nTo address the challenge of confounding, we need a method that can flexibly model the relationship between the outcome, the treatment, and the many covariates shown to be imbalanced. Bayesian Additive Regression Trees (BART) is a powerful non-parametric machine learning algorithm that is exceptionally well-suited for this task.It combines the predictive power of ensemble methods with a rigorous Bayesian framework for regularization and uncertainty quantification.\nAt its core, BART models the expected value of an outcome \\(Y\\) as a sum of many individual regression trees. For a set of predictors \\(x\\), the model is:\n\\[Y = \\sum_{j=1}^{m} g(x; T_j, M_j) + \\epsilon, \\quad \\text{where} \\quad \\epsilon \\sim N(0, \\sigma^2)\\]\nHere, \\(m\\) is the number of trees in the ensemble (typically around 200), and each function \\(g(x; T_j, M_j)\\) represents a single regression tree. The structure of the tree is denoted by \\(T_j\\), and \\(M_j\\) is the set of parameter values in its terminal nodes (or leaves).\nCrucially, each individual tree is designed to be a “weak learner”. It is kept shallow and simple, meaning it explains only a small fraction of the variation in the outcome. The final, powerful prediction comes from summing up the contributions of all these simple components. This sum-of-trees structure allows BART to automatically capture very complex relationships, including high-order interactions and non-linearities, without the user needing to specify them in advance. For example, an interaction between age and education is implicitly modeled if a tree splits on education within a branch that has already been split on age. This flexibility is a major advantage in observational studies where the true functional form of the relationship between the outcome and the confounders is unknown.\nIn most machine learning algorithms, overfitting is controlled through techniques like cross-validation or complexity penalties. BART, being a fully Bayesian method, achieves this through a carefully specified set of regularization priors. These priors are designed to keep each tree simple and prevent any single tree from dominating the overall fit.\nThe key priors are:\n\nPrior on Tree Structure: This prior strongly encourages shallow trees. It is defined by a rule governing the probability that a node at a certain depth \\(d\\) will be split further. This probability is typically modeled as \\[p(T_j) = \\alpha(1+d)^{-\\beta},\\] where \\(\\alpha \\in (0,1)\\) and \\(\\beta \\ge 0\\) are hyperparameters. Setting \\(\\beta\\) to a value like 2 ensures that the probability of splitting decreases rapidly with depth, keeping the trees small.\nPrior on Terminal Node Parameters: After the response variable \\(Y\\) is centered and scaled, the values \\(\\mu_{jk}\\) in the terminal nodes of each tree are given a Normal prior, \\[\n\\mu_{jk} \\sim N(0, \\sigma_{\\mu}^2).\n\\] This prior shrinks the predictions within each leaf towards zero. Because the final prediction is a sum over \\(m\\) trees, this shrinkage ensures that the contribution of each individual tree is small.\nPrior on Error Variance: The residual variance \\(\\sigma^2\\) is typically given a conjugate Inverse-Gamma prior. This prior is usually chosen to be weakly informative, allowing the data to dominate the posterior estimate of the noise level, but it still constrains the variance to be reasonable.\n\nTogether, these priors act as a sophisticated regularization mechanism that allows BART to fit complex functions while being highly resistant to overfitting.\nBART models are fit using a Markov chain Monte Carlo (MCMC) algorithm, specifically a form of Gibbs sampler known as Bayesian backfitting. The algorithm does not find a single “best” model. Instead, it generates thousands of samples from the joint posterior distribution of all model parameters: \\(p(T_1,\\ldots,T_m, M_1,\\ldots,M_m, \\sigma | Y, X)\\).\nThe fitting process works iteratively :\n\nInitialize all \\(m\\) trees and \\(\\sigma\\).\nFor each tree \\(j\\) from 1 to \\(m\\):\n\nCalculate the “partial residual” by subtracting the predictions of all other trees from the outcome: \\[R_j = Y - \\sum_{k \\neq j} g(x; T_k, M_k)\\].\nDraw a new tree structure \\(T_j\\) and its leaf parameters \\(M_j\\) from their posterior distribution conditional on this partial residual, \\[p(T_j, M_j | R_j, \\sigma).\\]\n\nAfter iterating through all trees, draw a new value for \\(\\sigma\\) from its posterior conditional on the current residuals.\nRepeat steps 2 and 3 for thousands of iterations.\n\nThe output of this process is not one set of trees, but a collection of (e.g., 5000) sets of trees, where each set represents a plausible regression function drawn from the posterior distribution. This collection of draws is the key to quantifying uncertainty in a Bayesian way.\nThe power of BART for causal inference lies in how it leverages the full posterior distribution to estimate counterfactuals. The strategy aligns perfectly with the Bayesian view of causal inference as a missing data problem, as articulated by Rubin (1978).\nThe standard approach for causal inference with BART is to model the outcome \\(Y\\) as a function of both the covariates \\(X\\) and the treatment indicator \\(Z\\). The model learns a single, flexible response surface:\n\\[E[Y | X, Z] = f(X, Z)\\]\nHere, the treatment \\(Z\\) is included as if it were “just another covariate” in the set of predictors fed to the BART algorithm. The model is free to discover how the effect of \\(Z\\) varies with \\(X\\) through the tree-splitting process. The Conditional Average Treatment Effect (CATE) is then simply the difference in the predictions from this learned function:\n\\[\\tau(x) = f(x, Z=1) - f(x, Z=0)\\]\nThe core of the estimation process is a predictive step that is repeated for each draw from the MCMC sampler. Suppose the MCMC algorithm has produced \\(S\\) posterior draws of the function \\(f\\). For each draw \\(s = 1,\\ldots, S\\):\n\nWe take the full dataset of \\(n\\) individuals with their observed covariates \\(X\\).\nWe create two hypothetical, or counterfactual, datasets:\n\nTreated World: The observed covariates \\(X\\) for all \\(n\\) individuals, but with the treatment indicator set to \\(Z=1\\) for everyone.\nControl World: The observed covariates \\(X\\) for all \\(n\\) individuals, but with the treatment indicator set to \\(Z=0\\) for everyone.\n\nUsing the fitted BART model corresponding to posterior draw \\(s\\) (i.e., \\(f^{(s)}\\)), we predict the outcome for every individual under both scenarios. This gives us a full set of posterior predictive draws for the potential outcomes: \\(\\tilde{Y}_i(1)^{(s)}\\) and \\(\\tilde{Y}_i(0)^{(s)}\\) for each individual \\(i\\).\n\nThis process is a direct implementation of the missing data analogy. For an individual \\(i\\) who was actually treated (\\(Z_i=1\\)), their observed outcome \\(Y_i\\) is their potential outcome \\(Y_i(1)\\). The BART model provides a posterior predictive draw for their missing counterfactual outcome, \\(\\tilde{Y}_i(0)^{(s)}\\). Conversely, for a control subject, we use the model to predict their missing \\(\\tilde{Y}_i(1)^{(s)}\\).\nOnce we have the posterior draws of the potential outcomes for every individual at each MCMC iteration, we can compute a posterior draw for any causal estimand of interest. For example, at each iteration \\(s\\):\n\nITE draw: \\[\\tau_i^{(s)} = \\tilde{Y}_i(1)^{(s)} - \\tilde{Y}_i(0)^{(s)}\\]\nATE draw: \\[\\tau_{ATE}^{(s)} = \\frac{1}{n} \\sum_{i=1}^{n} \\tau_i^{(s)}\\]\n\nBy collecting these values across all \\(S\\) MCMC iterations, we obtain \\[\\{\\tau_{ATE}^{(1)}, \\tau_{ATE}^{(2)},\\ldots, \\tau_{ATE}^{(S)}\\}.\\] This set is a Monte Carlo approximation of the entire posterior distribution of the Average Treatment Effect.\nThis is a profoundly powerful result. Instead of a single point estimate and a standard error, the Bayesian approach yields a full probability distribution for the unknown causal effect. From this posterior distribution, we can easily calculate a posterior mean (our best point estimate) and a 95% credible interval. Unlike a frequentist confidence interval, the Bayesian credible interval has a direct and intuitive probabilistic interpretation: given our data and model, there is a 95% probability that the true value of the ATE lies within this range. This propagation of uncertainty from the model parameters all the way to the final causal estimate is a hallmark of the Bayesian approach.\nWe now apply this framework to the Lalonde dataset to estimate the causal effect of the NSW job training program on 1978 earnings.\nThe analysis is streamlined by using the bartCause package in R, which is specifically designed for causal inference with BART. The package provides a wrapper around the core dbarts implementation, simplifying the process of fitting the model and generating counterfactuals. A typical function call would look like this:\n\n# Load the package and data\nlibrary(bartCause)\ndata(lalonde)\n\n# Define confounders\nconfounders &lt;- c('age', 'educ', 'black', 'hisp', 'married', 'nodegr', 're74', 're75')\n\n# Fit the BART model\nfit &lt;- bartc(\n  response = lalonde$re78,\n  treatment = lalonde$treat,\n  confounders = lalonde[, confounders],\n  estimand = \"ate\",\n  commonSup.rule = \"sd\" # Rule to handle poor overlap\n)\n\nIn this call, we specify the outcome (re78), the binary treatment (treat), and the matrix of pre-treatment confounders. We set estimand = ate to target the Average Treatment Effect\nBefore interpreting the causal estimates, it is essential to perform MCMC diagnostics to ensure the algorithm has converged to a stable posterior distribution. The bartCause package provides plotting functions for this purpose. Trace plots for key parameters, such as the posterior draws of the ATE and the residual standard deviation (\\(\\sigma\\)), should be examined. These plots should show the chains mixing well and exploring a consistent region of the parameter space, without long-term drifts or stuck periods, indicating that the sampler has converged.\nThe primary result can be obtained by calling summary(fit). This provides the posterior mean of the ATE, which serves as our point estimate, along with a 95% credible interval. For a richer view, we can plot the entire posterior distribution of the ATE, which visualizes our uncertainty about the treatment effect.\nThe true power of this result is seen when placed in the context of other estimates, as shown in Table 15.2. The naive difference in means between the treated and control groups in the non-experimental data is large and negative, a direct consequence of the severe confounding. The experimental benchmark from the original RCT for this subset of treated individuals is an earnings gain of approximately $886. The BART estimate, after adjusting for the observed confounders, is remarkably close to this benchmark. This result demonstrates that a flexible, non-parametric Bayesian model like BART can successfully overcome the severe selection bias that plagued earlier econometric methods, effectively “solving” the problem posed by\n\n\n\nTable 15.2: Comparison of ATE Estimates for the NSW Program. Note: Estimates are for the non-experimental Lalonde sample (treated units from NSW, control units from PSID). The experimental benchmark is the difference-in-means estimate from the randomized trial for the same treated units. Uncertainty for BART is the posterior standard deviation\n\n\n\n\n\n\n\n\n\n\nMethod\nATE Estimate\nUncertainty (Std. Dev. / Interval)\n\n\n\n\nExperimental Benchmark\n886.3\n-277.37\n\n\nNaive Difference-in-Means\n-8492.24\n-633.91\n\n\nPropensity Score Matching\n1079.13\n-158.59\n\n\nDouble Machine Learning\n370.94\n-394.68\n\n\nCausal BART\n818.79\n-184.46\n\n\n\n\n\n\nWhile the ATE provides a useful summary, it can mask important variations in how the treatment affects different people. A policy might be beneficial on average but ineffective or even harmful for certain subgroups. A key advantage of BART is its ability to move beyond the average and explore this Heterogeneous Treatment Effect (HTE), which is critical for developing more targeted and effective policies.\nEstimating HTE allows us to answer questions like: “For whom does this program work best?” or “Are there individuals for whom the program is detrimental?” In settings with limited resources, this information is vital for allocating the intervention to those most likely to benefit. The flexibility of BART, which does not assume a constant treatment effect, makes it an ideal tool for this task.\nBecause BART provides a posterior predictive distribution of potential outcomes for every individual in the dataset, we can estimate an Individual Conditional Average Treatment Effect (ICATE) for each person. By plotting a histogram of the posterior means of these ICATEs, we can visualize the distribution of effects across the sample. This reveals whether the effect is consistent for everyone or if there is substantial variation, with some individuals benefiting much more than others.\nTo understand what drives this heterogeneity, we can examine how the estimated CATE varies as a function of key pre-treatment covariates. These relationships are often visualized using partial dependence plots. For the Lalonde data, such analyses have revealed that the effect of the job training program is not constant but varies non-linearly with characteristics like age and pre-treatment income (re74). For instance, the program’s benefit might increase with age up to a certain point and then decline, or it might be most effective for individuals with low-to-moderate prior earnings but less so for those with very low or higher earnings. These are nuanced, data-driven insights that would be completely missed by a standard linear regression model that only estimates a single average effect.\nA subtle but important issue can arise when using flexible regularized models like BART for causal inference in the presence of strong confounding, as is the case here. The regularization priors, which are designed to prevent overfitting, can shrink the estimated effects of the confounders towards zero. Because the treatment Z is highly correlated with these confounders, the model may mistakenly attribute some of the effect of the confounders to the treatment, leading to a bias known as Regularization-Induced Confounding (RIC).\nA powerful solution, proposed by Hahn, Murray, and Carvalho (2020), is to first estimate the propensity score, \\(\\pi(x) = P(Z=1|X)\\), which is the probability of receiving treatment given the covariates X. This score serves as a one-dimensional summary of all confounding information. This estimated propensity score is then included as an additional predictor in the BART outcome model. By providing this confounding summary directly to the model, we help the BART algorithm differentiate between the prognostic effects of the covariates (captured by \\(\\pi(x)\\)) and the causal effect of the treatment Z, thereby mitigating RIC. This “ps-BART” approach is considered state-of-the-art and is easily implemented in the bartCause package by setting the argument p.scoreAsCovariate = TRUE\n\n15.2.1 BART versus Propensity Score Matching (PSM)\nBART is one of several methods for causal inference from observational data. It is instructive to compare its philosophy with that of another widely used technique: Propensity Score Matching (PSM). BART and PSM represent two different philosophies for tackling confounding.Propensity Score Matching (PSM): This approach focuses on the design of the study. The goal is to use the observed data to construct a new sample in which the treatment and control groups are balanced on their observed covariates, thereby mimicking the properties of an RCT. The propensity score is the central tool used to achieve this balance. The analysis of the outcome is then performed on this newly created, “balanced” dataset.\nBART focuses on the analysis stage. The goal is to build a highly flexible and accurate predictive model for the outcome that explicitly includes the treatment and confounders, \\(E[Y|X,Z]\\). It uses the full dataset and relies on the model’s ability to correctly adjust for the confounding variables to isolate the causal effect.\nEach approach has its own set of advantages and disadvantages. PSM is often praised for its transparency; one can assess the quality of the covariate balance achieved by the matching procedure before ever looking at the outcome variable, reducing the risk of “p-hacking” or specification searching. However, PSM can be inefficient, as it often requires discarding a significant portion of the control group that does not have good matches in the treated group (i.e., poor overlap). It can also suffer from residual confounding if the matches are not sufficiently close. BART, on the other hand, is highly efficient as it uses all available data. Its main strengths are its flexibility in capturing unknown functional forms and interactions, its ability to easily estimate heterogeneous effects, and its principled framework for uncertainty quantification. Its primary weakness is that it can be perceived as a “black box” if not diagnosed carefully. Its validity, like all modeling approaches, depends on the untestable ignorability assumption, and as discussed, it can be susceptible to regularization-induced confounding if not applied with care.\nIn modern practice, the line between these two philosophies is blurring. It is now common to see them used in conjunction. For example, many practitioners use flexible machine learning models, including BART itself, to estimate the propensity scores used for matching or weighting, which can improve the quality of the covariate balance over simpler logistic regression models. Conversely, the state-of-the-art application of BART for causal inference (ps-BART) incorporates the propensity score directly into the outcome model. This convergence reflects a mature understanding that both balancing the data structure and flexibly modeling the outcome are complementary and powerful tools for robust causal inference.\n\nConceptual Comparison of BART and Propensity Score Matching\n\n\n\n\n\n\n\nFeature\nPropensity Score Matching (PSM)\nBayesian Additive Regression Trees (BART)\n\n\n\n\nPrimary Goal\nCreate balanced treatment/control groups (Design)\nFlexibly model the outcome-covariate relationship (Analysis)\n\n\nUse of Data\nOften discards unmatched units, reducing sample size\nUses all available data\n\n\nConfounding Control\nAchieved by balancing covariates via matching/weighting\nAchieved by conditioning on covariates in a flexible model\n\n\nKey Assumption\nCorrect specification of the propensity score model\nCorrect specification of the outcome model (though BART is very flexible)\n\n\nTreatment Effect\nPrimarily estimates ATT; ATE can be harder to estimate\nEasily estimates ATE, ATT, and CATE/HTE\n\n\nUncertainty\nOften requires bootstrapping for standard errors\nProvides full posterior distributions and credible intervals naturally\n\n\nFlexibility\nLimited by the PS model; main effect is assumed constant after matching\nHighly flexible; automatically models non-linearities and interactions\n\n\n\nThis example shows that BART, a flexible non-parametric method, can successfully adjust for severe confounding and recover a causal estimate that is remarkably close to the experimental benchmark, a feat that eluded many of the methods available when Lalonde first published his critique. It is crucial to remember that BART is not a panacea. Its validity, like that of any non-experimental method, rests on the untestable assumption of ignorability—that we have measured and adjusted for all relevant confounding variables. However, given that assumption, BART offers a suite of powerful advantages that make it a top-tier method in the modern causal inference landscape, a status confirmed by its consistent high performance in data analysis competitions. For the Bayesian statistician, the key takeaways are threefold:\n\nPhilosophical Coherence: BART provides a method for causal inference that is deeply integrated with Bayesian principles. It seamlessly frames the estimation of causal effects as a posterior predictive imputation of missing potential outcomes, propagating all sources of parameter uncertainty into the final result.\nRobustness to Misspecification: By using a flexible sum-of-trees ensemble, BART avoids the need for strong parametric assumptions about the functional form of the relationship between the covariates and the outcome. This provides robust protection against model misspecification bias, which is a major concern in observational studies where these relationships are complex and unknown.\nRichness of Inference: BART naturally yields a full posterior distribution for any causal estimand of interest. This allows for a more complete and intuitive quantification of uncertainty through credible intervals and facilitates the exploration of heterogeneous treatment effects, moving the analysis from a single average number to a nuanced understanding of for whom an intervention works.\n\n\n\n\n\nEnikolopov, Ruben, Vasily Korovkin, Maria Petrova, Konstantin Sonin, and Alexei Zakharov. 2013. “Field Experiment Estimate of Electoral Fraud in Russian Parliamentary Elections.” Proceedings of the National Academy of Sciences 110 (2): 448–52.\n\n\nHahn, P. Richard, Jared S. Murray, and Carlos M. Carvalho. 2020. “Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects (with Discussion).” Bayesian Analysis 15 (3): 965–1056.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Randomized Controlled Trials</span>"
    ]
  },
  {
    "objectID": "16-select.html",
    "href": "16-select.html",
    "title": "16  Model Selection",
    "section": "",
    "text": "16.1 Exploratory Data Analysis\nWhat makes a good model? If the goal is prediction, then the model is as good as its prediction. The easiest way to visualize the quality of the prediction is to plot \\(y\\) vs \\(\\hat y\\). In the case of the linear regression model, the prediction interval is defined by \\[\ns\\sqrt{1+\\frac{1}{n}+\\frac{(x-\\bar x)^2}{\\sum_{i=1}^n(x_i-\\bar x)^2}}\n\\] where \\(s\\) is the standard deviation of the residuals. The prediction interval is the confidence interval for the prediction. The prediction interval is wider than the confidence interval because it includes the uncertainty in the prediction.\nThe bias-variance tradeoff is a fundamental property of statistical models. The bias is the difference between the expected value of the prediction and the true value \\(y-\\hat y\\). The variance is the variance of the prediction. The bias-variance tradeoff says that the bias and variance are inversely related. A model with high bias has low variance and a model with low bias has high variance. The bias-variance tradeoff is a consequence of the fact that the expected value of the prediction is the sum of the bias and the variance.\n\\[\nMSE = E(y-\\hat y)^2 = E(y-\\mathbb{E}(\\hat y))^2 + E(\\mathbb{E}(\\hat y)-\\hat y)^2\n\\] The bias-variance tradeoff is a consequence of the fact that the expected value of the prediction is the sum of the bias and the variance.\nBefore deciding on a parametric model for a dataset. There are several tools that we use to choose the appropriate model. These include\nThe two most common tools for exploratory data analysis are Q-Q plot, scatter plots and bar plots/histograms.\nA Q-Q plot simply compares the quantiles of your data with the quantiles of a theoretical distribution (like normal, exponential, etc.). Quantile is the fraction (or percent) of points below the given value. That is, the \\(i\\)-th quantile is the point \\(x\\) for which \\(i\\)% of the data lies below \\(x\\). On a Q-Q plot, if the two data sets come from a population with the same distribution, we should see the points forming a line that’s roughly straight. More precisely, if the two data sets \\(x\\) and \\(y\\) come from the same distribution, then the points \\((x_{(i)}, y_{(i)})\\) should lie roughly on the line \\(y = x\\). If \\(y\\) comes from a distribution that’s linear in \\(x\\), then the points \\((x_{(i)}, y_{(i)})\\) should lie roughly on a line, but not necessarily on the line \\(y = x\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#exploratory-data-analysis",
    "href": "16-select.html#exploratory-data-analysis",
    "title": "16  Model Selection",
    "section": "",
    "text": "Theoretical assumptions underlying the distribution (our prior knowledge about the data)\nExploratory data analysis\nFormal goodness-of-fit tests\n\n\n\n\nExample 16.1 (Normal Q-Q plot) Figure 16.1 shows the normal Q-Q plot for the Data on birth weights of babies born in a Brisbane hospital on December 18, 1997. The data set contains 44 records. A more detailed description of the data set can be found in UsingR manual.\n\nbabyboom = read.csv(\"../data/babyboom.csv\")\nqqnorm(babyboom$wt)\nqqline(babyboom$wt)\n\n\n\n\n\n\n\nFigure 16.1: Normal Q-Q plot of baby weights\n\n\n\n\n\nVisually, the answer to answer the question “Are Birth Weights Normally Distributed?” is no. We can see that on the left side of the plot the points are below the line. This indicates that the data is skewed to the left. The data is not normally distributed.\nThe Q-Q plots look different if we split the data based on the gender\ng = babyboom %&gt;% filter(gender==\"girl\") %&gt;% pull(wt) \nb = babyboom %&gt;% filter(gender==\"boy\")  %&gt;% pull(wt) \nqqnorm(g); qqline(g)\nqqnorm(b); qqline(b)\n\n\n\n\n\n\nGirls\n\n\n\n\n\n\n\nBoys\n\n\n\n\n\n\nHistogram of baby weights by gender\n\n\n\nHow about the times in hours between births of babies?\n\nhr = ceiling(babyboom$running.time/60)\nBirthsByHour = tabulate(hr)\n# Number of hours with 0, 1, 2, 3, 4 births\nObservedCounts = table(BirthsByHour) \n# Average number of births per hour\nBirthRate=sum(BirthsByHour)/24    \n# Expected counts for Poisson distribution\nExpectedCounts=dpois(0:4,BirthRate)*24    \n# bind into matrix for plotting\nObsExp &lt;- rbind(ObservedCounts,ExpectedCounts) \nbarplot(ObsExp,names=0:4, beside=TRUE,legend=c(\"Observed\",\"Expected\"))\n\n\n\n\n\n\n\n\nWhat about the Q-Q plot?\n\n# birth intervals\nbirthinterval=diff(babyboom$running.time) \n # quantiles of standard exponential distribution (rate=1)   \nexponential.quantiles = qexp(ppoints(43)) \nqqplot(exponential.quantiles, birthinterval)\nlmb=mean(birthinterval)\nlines(exponential.quantiles,exponential.quantiles*lmb) # Overlay a line\n\n\n\n\n\n\n\n\nHere\n\nppoints function computes the sequence of probability points\nqexp function computes the quantiles of the exponential distribution\ndiff function computes the difference between consecutive elements of a vector",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#out-of-sample-performance",
    "href": "16-select.html#out-of-sample-performance",
    "title": "16  Model Selection",
    "section": "16.2 Out of Sample Performance",
    "text": "16.2 Out of Sample Performance\nA parametric model that we choose to fit to data is chosen from a family of functions. Then, we use optimization to find the best model from that family. To find the best model we either minimize empirical loss or maximize the likelihood. We also established, that when \\(y \\sim N(f(x\\mid \\beta),\\sigma^2)\\) then mean squared error loss and negative log-likelihood are the same function. \\[\n\\E{y | x} = f(\\beta^Tx)\n\\]\nFor a regression model, an empirical loss measures a distance between fitted values and measurements and the goal is to minimize it. A typical choice of loss function for regression is \\[\nL (y,\\hat y) =  \\dfrac{1}{n}\\sum_{i=1}^n |y_i -  f(\\beta^Tx_i)|^p\n\\] When \\(p=1\\) we have MAE (mean absolute error), \\(p=2\\) we have MSE (mean squared error).\nFinding an appropriate family of functions is a major problem and is called model selection problem. For example, the choice of input variables to be included in the model is part of the model selection process. Model selection involves determining which predictors, interactions, or transformations should be included in the model to achieve the best balance between complexity and predictive accuracy. In practice, we often encounter several models for the same dataset that perform nearly identically, making the selection process challenging.\nIt is important to note that a good model is not necessarily the one that fits the data perfectly. Overfitting can occur when a model is overly complex, capturing noise rather than the underlying pattern. A good model strikes a balance between fitting the data well and maintaining simplicity to ensure generalizability to new, unseen data. For instance, including too many parameters can lead to a perfect fit when the number of observations equals the number of parameters, but such a model is unlikely to perform well on out-of-sample data.\nThe goal of model selection is not only to achieve a good fit but also to reduce complexity by excluding unnecessary parameters. This process typically involves selecting a model from a relevant class of functions while keeping in mind the trade-offs between bias, variance, and model complexity. Techniques such as cross-validation, information criteria (e.g., AIC, BIC), and regularization methods are commonly used to guide the model selection process.\nThe model selection task is sometimes one of the most consuming parts of the data analysis. Unfortunately, there is no single rule to find the best model. One way to think about the model choice problem as yet another optimization problem, with the goal to find best family of functions that describe the data. With a small number of predictors we can do brute force (check all possible models). For example, with \\(p\\) predictors there are \\(2^p\\) possible models with no interactions. Thus, the number of potential family functions is huge even for modest values of \\(p\\). One cannot consider all transformations and interactions.\nOur goal is to build a model that predicts well for out-of-sample data, e.g. the data that was not used for training. Eventually, we are interested in using our models for prediction and thus, the out of sample performance is the most important metric and should be used to choose the final model. In-sample performance is of little interest when predictive model need to be chosen, as one of the winners of Netflix prize put it, “It’s like predicting how much someone will like a movie, having them watch it and tell you how much they really liked it”. The out-of-sample performance is the final judge of the quality of our model. The goal is to use data to find a pattern that we can exploit. The pattern will be “statistical” in its nature. To uncover the pattern we start with a training dataset, denoted by \\[\nD = (y_i,x_i)_{i=1}^n\n\\] and to test the validity of our mode we use out-of-sample testing dataset \\[\nD^* = (y_j^*, x_j^*)_{j=1}^m,\n\\] where \\(x_i\\) is a set of \\(p\\) predictors ans \\(y_i\\) is response variable.\nA good predictor will “generalize” well and provide low MSE out-of-sample. These are a number of methods/objective functions that we will use to find, \\(\\hat f\\). In a parameter-based style we will find a black box. There are a number of ways to build our black box model. Our goal is to find the map \\(f\\) that approximates the process that generated the data. For example data could be representing some physical observations and our goal is recover the “laws of nature\" that led to those observations. One of the pitfalls is to find a map \\(f\\) that does not generalize. Generalization means that our model actually did learn the”laws of nature\" and not just identified patterns presented in training. The lack of generalization of the model is called over-fitting. It can be demonstrated in one dimension by remembering the fact from calculus that any set of \\(n\\) points can be approximated by a polynomial of degree \\(n\\), e.g we can alway draw a line that connects two points. Thus, in one dimension we can always find a function with zero empirical risk. However, such a function is unlikely to generalize to the observations that were not in our training data. In other words, the empirical risk measure for \\(D^*\\) is likely to be very high. Let us illustrate that in-sample fit can be deceiving.\n\nExample 16.2 (Hard Function) Say we want to approximate the following function \\[\nf(x) = \\dfrac{1}{1+25x^2}.\n\\] This function is simply a ratio of two polynomial functions and we will try to build a liner model to reconstruct this function\n\nx = seq(-2,2,by=0.01)\ny = 1/(1+25*x^2)\n# Approximate with polynomial of degree 1 and 2\nm1 = lm(y~x)\nm2 = lm(y~poly(x,2))\n# Approximate with polynomial of degree 20 and 5\nm20 = lm(y~poly(x,20))\nm5 = lm(y~poly(x,5))\nx = seq(-3,3,by=0.01)\ny = 1/(1+25*x^2)\nplot(x,y,type='l',col='black',lwd=2)\nlines(x,predict(m1,list(x=x)),lwd=2, col=1)\nlines(x,predict(m2,poly(x,2)),lwd=2, col=2)\nlines(x,predict(m5,poly(x,5)),lwd=2, col=3)\nlines(x,predict(m20,poly(x,20)),lwd=2, col=4)\nlegend(\"topright\", legend=c(\"f(x)\",\"m1\",\"m2\",\"m5\",\"m20\"), col=c(\"black\",1:4), lty=1, cex=0.8, bty='n')\n\n\n\n\n\n\n\nFigure 16.2: Runge-Kutta function\n\n\n\n\n\nFigure 16.2 shows the function itself (black line) on the interval \\([-3,3]\\). We used observations of \\(x\\) from the interval \\([-2,2]\\) to train the data (solid line) and from \\([-3,-2) \\cup (2,3]\\) (dotted line) to test the model and measure the out-of-sample performance. We tried four different linear functions to capture the relations. We see that linear model \\(\\hat y = \\beta_0 + \\beta_1 x\\) is not a good model. However, as we increas the degree of the polynomial to 20, the resulting model \\(\\hat y = \\beta_0 + \\beta_1x + \\beta_2 x^2 +\\ldots+\\beta_{20}x^{20}\\) does fit the training data set quite well, but does very poor job on the test data set. Thus, while in-sample performance is good, the out-of sample performance is unsatisfactory. We should not use the degree 20 polynomial function as a predictive model. In practice in-sample out-of-simple loss or classification rates provide us with a metric for providing horse race between different predictors. It is worth mentioning here there should be a penalty for overly complex rules which fits extremely well in sample but perform poorly on out-of-sample data. As Einstein famous said “model should be simple, but not simpler.”\n\nTo a Bayesian, the solution to these decision problems are rather obvious: compute posterior distributions, and then make decisions by maximizing expected utility, where the posterior distribution is used to calculate the expectations. Classical solutions to these problems are different, and use repeated sampling ideas, whereby the performance of a decision rule is judged on its performance if the same decision problem were repeated infinitely. Thus, the decisions are made based on their population properties. One of the main uses of statistical decision theory is to compare different estimators or hypothesis testing procedures. This theory generates many important findings, most notably that many of the common classical estimators are “bad”,in some sense, and that Bayesian estimators are always “good”.\nThese results have major implications for empirical work and practical applications, as they provide a guide for forecasting.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#bias-variance-trade-off",
    "href": "16-select.html#bias-variance-trade-off",
    "title": "16  Model Selection",
    "section": "16.3 Bias-Variance Trade-off",
    "text": "16.3 Bias-Variance Trade-off\nType II MLE: Marginal MLE (MMLE)\nOne approach to find “plug-in” estimates of hyper-parameters (a.k.a. amount of regularisation) is to use the marginal likelihood defined by\n\\[\nm(y | \\lambda) = \\int p(y | \\theta) p(\\theta | \\lambda) d\\theta.\n\\]\nWe then select\n\\[\n\\hat{\\lambda} = \\arg\\max_\\lambda \\; \\log m(y | \\lambda)\n\\]\nEssentially, we have a new objective function for finding the hyper-parameters (tuning parameters).\nWe can add a further regularisation penalty \\(-\\log p(\\lambda)\\) too.\nAs J.W.Tukey stated at the 1972 American Statistical Association meeting:\n\n“A feeling that any gains possible from a complicated procedurelike Stein’s could not be worththe extratroubl”",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#full-bayes",
    "href": "16-select.html#full-bayes",
    "title": "16  Model Selection",
    "section": "16.4 Full Bayes",
    "text": "16.4 Full Bayes\nShould also place a prior on hyper-parameters \\(p(\\lambda)\\). The optimal Bayes estimator under quadratic loss is\n\\[\n\\hat{\\theta}(y) = E(\\theta | y) = E_{\\lambda | y} \\left( E(\\theta | \\lambda, y) \\right).\n\\]\nwhere \\(E_{\\lambda | y}\\) is taken with respect to the marginal posterior\n\\[\np(\\lambda | y) = \\frac{m(y | \\lambda) p(\\lambda)}{m(y)}\n\\]\nThe choice of \\(p(\\lambda)\\) is an issue. Jeffreys, Polson and Scott propose the use of half-Cauchy priors \\(C^+(0,1)\\)-priors.\nFor any predictive model we seek to achieve best possible results, i.e. smallest MSE or misclassification rate. However, a model performance can be different as data used in one training/validation split may produce results dissimilar to another random split. In addition, a model that performed well on the test set may not produce good results given additional data. Sometimes we observe a situation, when a small change in the data leads to large change in the final estimated model, e.g. parameters of the model. These results exemplify the bias/variance tradeoff, where increasing model bias produces large variance in the final results. Similarly, low bias results in low variance, but can also produce an oversimplification of the final model. While Bias/variance concept is depicted below.\n\n\n\n\n\n\nFigure 16.3: Bias-variance trade-off\n\n\n\n\nExample 16.3 (Bias-variance) We demonstrate bias-variance concept using Boston housing example. We fit a model \\(\\mathrm{medv} = f(\\mathrm{lstat})\\). We use polynomial functions to approximate this relation. We fitted twelve polynomial functions with degree \\(1,\\ldots,12\\) ten time. Each time we randomly selected 20% of sample for testing and the rest for training. We estimated in-of-sample performance (bias) and out-of-sample performance by calculating MSE on training and testing sets correspondingly. For each polynomial \\(f\\) we averaged MSE from each of the ten models.\nFigure 16.4 (a) shows bias and variance for our twelve different models. As expected, bias increases while variance increases as model complexity grows. On the other hand out-of-sample MSE is a U-shaped curve. The optimal model is the one that has smallest out-of-sample MSE. In our case it is polynomial of degree 5!\n\n\n\n\n\n\n\n\n\n\n\n(a) Metrics for twelve polynomial functions fitted into Boston housing data set. As model complexity (degree of the polynomial function) increases, model variance increase and bias decreases. Out-of-sample MSE is smallest for 5th degree polynomial function, which is the optimal model in terms of bias-variance trade-off.\n\n\n\n\n\n\n\n\n\n\n\n(b) Optimal complexity model, which is 5th degree polynomial used to predict observations from testing data set. Model predictions (red line) are compared to actual observed values of medv variable (dots)\n\n\n\n\n\n\n\nFigure 16.4: Metrics for 12 models\n\n\n\nLet’s take another, a more formal, look at bias-variance trade-off for a linear regression problem. We are interested in the decomposition of the error \\(\\E{(y-\\hat y)^2}\\) as a function of bias \\(\\E{y-\\hat y}\\) and variance \\(\\Var{\\hat y}\\).\nHere \\(\\hat y = \\hat f_{\\beta}(x)\\) prediction from the model, and \\(y = f(x) + \\epsilon\\) is the true value, which is measured with noise \\(\\Var{\\epsilon} = \\sigma^2\\), \\(f(x)\\) is the true unknown function. The expectation above measures squared error of our model on a random sample \\(x\\). \\[\n\\begin{aligned}\n\\E{(y - \\hat{y})^2}\n& = \\E{y^2 + \\hat{y}^2 - 2 y\\hat{y}} \\\\\n& = \\E{y^2} + \\E{\\hat{y}^2} - \\E{2y\\hat{y}} \\\\\n& = \\Var{y} + \\E{y}^2 + \\Var{\\hat{y}} + \\E{\\hat{y}}^2 - 2f\\E{\\hat{y}} \\\\\n& = \\Var{y} + \\Var{\\hat{y}} + (f^2 - 2f\\E{\\hat{y}} + \\E{\\hat{y}}^2) \\\\\n& = \\Var{y} + \\Var{\\hat{y}} + (f - \\E{\\hat{y}})^2 \\\\\n& = \\sigma^2 + \\Var{\\hat{y}} + \\mathrm{Bias}(\\hat{y})^2\\end{aligned}\n\\] Here we used the following identity: \\(\\Var{X} = \\E{X^2} - \\E{X}^2\\) and the fact that \\(f\\) is deterministic and \\(\\E{\\epsilon} = 0\\), thus \\(\\E{y} = \\E{f(x)+\\epsilon} = f + \\E{\\epsilon} = f\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#cross-validation",
    "href": "16-select.html#cross-validation",
    "title": "16  Model Selection",
    "section": "16.5 Cross-Validation",
    "text": "16.5 Cross-Validation\nIf the data set at-hand is small and we cannot dedicate large enough sample size for testing, simply measuring error on test data set can lead to wrong conclusions. When size of the testing set \\(D^*\\) is small, the estimated out-of-sample performance is of high variance, depending on precisely which observations are included in the test set. On the other hand, when training set \\(D^*\\) is a large fraction of the entire sample available, estimated out-of-sample performance will be underestimated. Why?\nA trivial solution is to perform the training/testing split randomly several times and then use average out-of-sample errors. This procedure has two parameters, the fraction of samples to be selected for testing \\(p\\) and number of estimates to be performed \\(K\\). The resulting algorithm is as follows\nfsz = as.integer(p*n)\nerror = rep(0,K)\nfor (k in 1:K)\n{\n    test_ind = sample(1:n,size = fsz)\n    training = d[-test_ind,]\n    testing  = d[test_ind,]\n    m = lm(y~x, data=training)\n    yhat = predict(m,newdata = testing)\n    error[k] = mean((yhat-testing$y)^2)\n}\nres = mean(error)\nFigure 16.5 shows the process of splitting data set randomly five times.\nCross validation modifies the random splitting approach uses more “disciplined” way to split data set for training and testing. Instead of randomly selecting training data points, CV chooses consecutive observations and thus, each data point is used once for testing. As the random approach, CV helps addressing the high variance issue of out-of-sample performance estimation when data set available is small. Figure 16.6 shows the process of splitting data set five times using cross-validation approach.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.5: Bootstrap\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.6: Cross-validation\n\n\n\n\n\n\n\nTraining set (red) and testing set (green)\n\n\n\n\nExample 16.4 (Simulated) We use simulated data set to demonstrate difference between estimated out-of-sample performance using random 20/80 split, 5-fold cross-validation and random split. We used \\(x=-2,-1.99,-1.98,\\ldots,2\\) and \\(y = 2+3x + \\epsilon, ~ \\epsilon \\sim N(0,\\sqrt{3})\\). We simulated 35 datasets of size 100. For each of the simulated data sets, we fitted a linear model and estimated out-of-sample performance using three different approaches. Figure 16.7 compares empirical distribution of errors estimated from 35 samples.\n\n\n\n\n\n\nFigure 16.7: Empirical comparison of simple split, cross-validation, and bootstrap approaches to estimate out-of sample performance.\n\n\n\nAs we can see the estimated out-of-sample performance by a training set approach is of high variance. While, both cross-validation and bootstrap approaches lead to better estimates, they require model to be fitted 5 times, which can be computationally costly for a complex model. On the other hand, estimate from cross-validation is of lower variance and less bias compared to the bootstrap estimate. Thus, we should prefer cross-validation.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#small-sample-size",
    "href": "16-select.html#small-sample-size",
    "title": "16  Model Selection",
    "section": "16.6 Small Sample Size",
    "text": "16.6 Small Sample Size\nWhen sample size is small and it is not feasible to divide your data into training and validation data sets, an information criterion could be used to assess a model. We can think of information criterion as a metric that “approximates” out-os-sample performance of the model. Akaike’s Information Criterion (AIC) takes the form \\[\n\\mathrm{AIC} = log(\\sigma_k^2) + \\dfrac{n+2k}{n}\n\\] \\[\n\\hat{\\sigma}_k^2 = \\dfrac{SSE_k}{n}\n\\] Here \\(k\\) = number of coefficients in regression model, \\(SSE_k\\) = residual sum of square, \\(\\hat{\\sigma}_k^2\\) = MLE estimator for variance. We do not need to proceed sequentially, each model individually evaluated\nAIC is derived using the Kullback-Leibler information number. It is a ruler to measure the similarity between the statistical model and the true distribution. \\[\nI(g ; f) = E_g\\left(\\log \\left\\{\\dfrac{g(y)}{f(y)}\\right\\}\\right) = \\int_{-\\infty}^{\\infty}\\log \\left\\{\\dfrac{g(y)}{f(y)}\\right\\}g(y)dy.\n\\] Here - \\(I(g ; f) &gt; 0\\) - \\(I(g ; f) = 0 \\iff g(u) = f(y)\\) - \\(f \\rightarrow g\\) as \\(I(g ; f) \\rightarrow 0\\)\nTo estimate \\(I(g ; f)\\), we write \\[\nI(g ; f) = E_g\\left(\\log \\left\\{\\dfrac{g(y)}{f(y)}\\right\\}\\right) = E_g (\\log g(y)) - E_g(\\log f(y))\n\\] Only the second term is important in evaluating the statistical model \\(f(y)\\). Thus we need to estimate \\(E_g(\\log f(y))\\). Given sample \\(z_1,...,z_n\\), and estimated parameters \\(\\hat{\\theta}\\) a naive estimate is \\[\n\\hat{E}_g(\\log f(y)) =  \\dfrac{1}{n} \\sum_{i=1}^n \\log f(z_i) = \\dfrac{\\ell(\\hat{\\theta})}{n}\n\\] where \\(\\ell(\\hat{\\theta})\\) is the log-likelihood function for model under test.\n\nthis estimate is very biased\ndata used used twice: to get the MLE and second to estimate the integral\nit will favor those model that overfit\n\nAkaike showed that the bias is approximately \\(k/n\\) where \\(k\\) is the number of parameters \\(\\theta\\). Therefore we use \\[\n\\hat{E}_g(\\log f(y)) = \\dfrac{\\ell(\\hat{\\theta})}{n} - \\dfrac{k}{n}\n\\] Which leads to AIC \\[\nAIC = 2n \\hat{E}_g(\\log f(y)) = 2 \\ell(\\hat{\\theta}) - 2k\n\\]\nAkaike’s Information Criterion (AIC) \\[\n\\mathrm{AIC} = \\log(\\sigma_k^2) + \\dfrac{n+2k}{n}\n\\] Controls for balance between model complexity (\\(k\\)) and minimizing variance. The model selection process involve trying different \\(k\\), chose model with smallest AIC.\nA slightly modified version designed for small samples is the bias corrected AIC (AICc). \\[\n\\mathrm{AICc} = \\log(\\hat{\\sigma}_k^2) + \\dfrac{n+k}{n-k-2}\n\\] This criterion should be used for regression models with small samples\nYet, another variation designed for larger datasets is the Bayesian Information Criterion (BIC). \\[\n\\mathrm{BIC} = \\log(\\hat{\\sigma}_k^2) + \\dfrac{k \\log(n)}{n}\n\\] Is is the same as AIC but harsher penalty, this chooses simpler models. It works better for large samples when compared to AICc. The motivation fo BIC is from the posterior distribution over model space. Bayes rule lets you calculate the joint probability of parameter and models as \\[\np(\\theta,M\\mid D) = \\dfrac{p(D\\mid \\theta,M(p(M,\\theta)}{p(D)},~~ p(M\\mid D) = \\int p(\\theta,M\\mid D)d\\theta \\approx n^{p/2}p(D\\mid \\hat \\theta M)p(M).\n\\]\nConsider a problem of predicting mortality rates given pollution and temperature measurements. Let’s plot the data.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.8: Time Series Plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.9: Scatter Plot\n\n\n\n\n\n\nRegression Model 1, which just uses the trend: \\(M_t = \\beta_1 + \\beta_2 t + w_t\\). We fit by calling lm(formula = cmort ~ trend) to get the following coefficients\n                Estimate    Std. Error t value  \n    (Intercept) 3297.6062   276.3132   11.93\n    trend         -1.6249     0.1399  -11.61\nRegression Model 2 regresses to time (trend) and temperature: \\(M_t = \\beta_1 + \\beta_2 t + \\beta_t(T_t - T)+ w_t\\). The R call is lm(formula = cmort ~ trend + temp)\n                Estimate    Std. Error t value \n    (Intercept) 3125.75988  245.48233   12.73 \n    trend         -1.53785    0.12430  -12.37 \n    temp          -0.45792    0.03893  -11.76 \nRegression Model 3, uses trend, temperature and mortality: \\(M_t = \\beta_1 + \\beta_2 t + \\beta_3(T_t - T)+ \\beta_4(T_t - T)^2 + w_t\\). The R call is lm(formula = cmort ~ trend + temp + I(temp^2)\n                Estimate    Std. Error t value \n    (Intercept)  3.038e+03  2.322e+02  13.083 \n    trend       -1.494e+00  1.176e-01 -12.710 \n    temp        -4.808e-01  3.689e-02 -13.031 \n    temp2        2.583e-02  3.287e-03   7.858 \nRegression Model 4 adds temperature squared: \\[M_t = \\beta_1 + \\beta_2 t + \\beta_3(T_t - T)+ \\beta_4(T_t - T)^2 + \\beta_5 P_t+ w_t.\\]\nThe R call is lm(formula = cmort ~ trend + temp +  I(temp^2) + part)\n                Estimate    Std. Error t value \n    (Intercept)  2.831e+03  1.996e+02   14.19 \n    trend       -1.396e+00  1.010e-01  -13.82 \n    temp        -4.725e-01  3.162e-02  -14.94 \n    temp2        2.259e-02  2.827e-03    7.99 \n    part         2.554e-01  1.886e-02   13.54 \nTo choose the model, we look at the information criterion\n\n\n\nModel\n\\(k\\)\nSSE\ndf\nMSE\n\\(R^2\\)\nAIC\nBIC\n\n\n\n\n1\n2\n40,020\n506\n79.0\n.21\n5.38\n5.40\n\n\n2\n3\n31,413\n505\n62.2\n.38\n5.14\n5.17\n\n\n3\n4\n27,985\n504\n55.5\n.45\n5.03\n5.07\n\n\n4\n5\n20,508\n503\n40.8\n.60\n4.72\n4.77\n\n\n\n\\(R^2\\) always decreases with number of covariates (that is what MLE does). Thus, cannot be used as a selection criteria. \\(R^2\\) for out-of-sample data is useful!\nThe message to take home on model selection\n\n\\(R^2\\) is NOT a good metric for model selection\nValue of likelihood function is NOT a god metric\nare intuitive and work very well in practice (you should use those)\nAIC is good for big \\(n/df\\), so it overfits in high dimensions\nShould prefer AICc over AIC\nBIC underfits for large \\(n\\)\nCross-validation is important, we will go over it later",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#regularization",
    "href": "16-select.html#regularization",
    "title": "16  Model Selection",
    "section": "16.7 Regularization",
    "text": "16.7 Regularization\nRegularization is a technique to incorporate some prior knowledge about parameters of the model into the estimation process. Consider an example when regularization allows us to solve a hard problem of filtering noisy traffic data.\n\nExample 16.5 (Traffic) Consider traffic flow speed measured by an in-ground sensor installed on interstate I-55 near Chicago. Speed measurements are noisy and prone to have outliers. Figure 16.10 shows speed measured data, averaged over five minute intervals on one of the weekdays.\n\n\n\n\n\n\nFigure 16.10: Speed profile over 24 hour period on I-55, on October 22, 2013\n\n\n\nSpeed measurements are noisy and prone to have outliers. There are two sources of noise. The first is the measurement noise, caused by inhalant nature of the sensor’s hardware. The second source is due to sampling error, vehicles observed on a specific lane where senor installed might not represent well traffic in other lanes.\nTrend filtering, which is a variation of a well-know Hodrick-Prescott filter. In this case, the trend estimate is the minimizer of the weighted sum objective function \\[\n(1/2) \\sum_{t=1}^{n}(y_t - x_t)^2 + \\lambda \\sum_{t=1}^{n-1}|x_{t-1} - 2x_t + x_{t+1}|,\n\\]\n\n\n\n\n\n\n\n\n\nTrend filter for different penalty\n\n\n\n\n\n\n\nTrend filtering with optimal penalty\n\n\n\n\n\n\nTrend Filtering for Traffic Speed Data",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#ridge-regression",
    "href": "16-select.html#ridge-regression",
    "title": "16  Model Selection",
    "section": "16.8 Ridge Regression",
    "text": "16.8 Ridge Regression\nGauss invented the concept of least squares and developed algorithms to solve the the optimization problem \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad ||y- X\\beta||_2^2\n\\] where \\(\\beta = (\\beta_1 , \\ldots , \\beta_p )\\), we can use linear algebra algorithms, the solution given by \\[\n\\hat{\\beta} = ( X^T X )^{-1} X^T y\n\\] This can be numerically unstable when \\(X^T X\\) is ill-conditioned, and happens when \\(p\\) is large. Ridge regression addresses this problem by adding an extra term to the \\(X^TX\\) matrix \\[\n\\hat{\\beta}_{\\text{ridge}} = ( X^T X + \\lambda I )^{-1} X^T y.\n\\] The corresponding optimization problem is \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad ||y- X\\beta||_2^2   + \\lambda||\\beta||_2^2.\n\\] We can think of the constrain is of a budget on the size of \\(\\beta\\). Ridge reguralization was first proposed in solving inverse problems to “discover” physical laws from observations and the norm of the \\(\\beta\\) vector would usually represent amount of energy required, and the 2-norm penalty term allows to find find a balance between data fidelity and solution simplicity. The regularization term acts to constrain the solution space, preventing it from reaching high-energy (overly complex) states, most of the times nature chooses the path of least resistance, thus minimal energy solutions are practical.\nThe we choose \\(\\lambda\\) over a regularisation path. The penalty in ridge regression forces coefficients \\(\\beta\\) to be close to 0. Penalty is large for large values and very small for small ones. Tuning parameter \\(\\lambda\\) controls trade-off between how well model fits the data and how small \\(\\beta\\)s are. Different values of \\(\\lambda\\) will lead to different models. We select \\(\\lambda\\) using cross validation.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#kernel-view",
    "href": "16-select.html#kernel-view",
    "title": "16  Model Selection",
    "section": "16.9 Kernel view",
    "text": "16.9 Kernel view\nAnother interesting view stems from what is called the push-through matrix identity: \\[\n(aI + UV)^{-1}U = U(aI + VU)^{-1}\n\\] for \\(a\\), \\(U\\), \\(V\\) such that the products are well-defined and the inverses exist. We can obtain this from \\(U(aI + VU) = (aI + UV)U\\), followed by multiplication by \\((aI + UV)^{-1}\\) on the left and the right. Applying the identity above to the ridge regression solution with \\(a = \\lambda\\), \\(U = X^T\\), and \\(V = X\\), we obtain an alternative form for the ridge solution: \\[\n\\hat{\\beta} = X^T (XX^T + \\lambda I)^{-1} Y.\n\\] This is often referred to as the kernel form of the ridge estimator. From this, we can see that the ridge fit can be expressed as \\[\nX\\hat{\\beta} = XX^T (XX^T + \\lambda I)^{-1} Y.\n\\] What does this remind you of? This is precisely \\(K(K + \\lambda I)^{-1}Y\\) where \\(K = XX^T\\), which, recall, is the fit from RKHS regression with a linear kernel \\(k(x, z) = x^T z\\). Therefore, we can think of RKHS regression as generalizing ridge regression by replacing the standard linear inner product with a general kernel. (Indeed, RKHS regression is often called kernel ridge regression.) \\[\n(aI + UV)^{-1}U = U(aI + VU)^{-1}\n\\] for \\(a\\), \\(U\\), \\(V\\) such that the products are well-defined and the inverses exist. We can obtain this from \\(U(aI + VU) = (aI + UV)U\\), followed by multiplication by \\((aI + UV)^{-1}\\) on the left and the right. Applying the identity above to the ridge regression solution with \\(a = \\lambda\\), \\(U = X^T\\), and \\(V = X\\), we obtain an alternative form for the ridge solution: \\[\n\\hat{\\beta} = X^T (XX^T + \\lambda I)^{-1} Y.\n\\] This is often referred to as the kernel form of the ridge estimator. From this, we can see that the ridge fit can be expressed as \\[\nX\\hat{\\beta} = XX^T (XX^T + \\lambda I)^{-1} Y.\n\\] What does this remind you of? This is precisely \\(K(K + \\lambda I)^{-1}Y\\) where \\(K = XX^T\\), which, recall, is the fit from RKHS regression with a linear kernel \\(k(x, z) = x^T z\\). Therefore, we can think of RKHS regression as generalizing ridge regression by replacing the standard linear inner product with a general kernel. (Indeed, RKHS regression is often called kernel ridge regression.)\n\nExample 16.6 (Shrinkage) Consider a simulated data with \\(n=50\\), \\(p=30\\), and \\(\\sigma^2=1\\). The true model is linear with \\(10\\) large coefficients between \\(0.5\\) and \\(1\\).\nOur approximators \\(\\hat f_{\\beta}\\) is a linear regression. We can empirically calculate the bias by calculating the empirical squared loss \\(1/n||y -\\hat y||_2^2\\) and variance can be empirically calculated as \\(1/n\\sum  (\\bar{\\hat{y}} - \\hat y_i)\\)\nBias squared \\(\\mathrm{Bias}(\\hat{y})^2=0.006\\) and variance \\(\\Var{\\hat{y}} =0.627\\). Thus, the prediction error = \\(1 + 0.006 + 0.627 = 1.633\\)\nWe’ll do better by shrinking the coefficients to reduce the variance. Let’s estimate, how big a gain will we get with Ridge?\n\n\n\nTrue model coefficients\n\n\nNow we see the accuracy of the model as a function of \\(\\lambda\\)\n\n\n\nPrediction error as a function of \\(\\lambda\\)\n\n\nRidge Regression At best: Bias squared \\(=0.077\\) and variance \\(=0.402\\).\nPrediction error = \\(1 + 0.077 + 0.403 = 1.48\\)\n\n\n\nRidge\n\n\n\nThe additional term \\(\\lambda||\\beta||_2^2\\) in the optimization problem is called the regularization term. There are several ways to regularize an optimization problem. All of those techniques were developed in the middle of last century and were applied to solve problems of fitting physics models into observed data, those frequently arise in physics and engineering applications. Here are a few examples of such regularization techniques.\nIvanov regularization \\[\n\\underset{x \\in \\mathbb{R^n}}{\\mathrm{minimize}}\\quad ||y - X\\beta||_2^2~~~~ \\mbox{s.t.}~~||\\beta||_l \\le k\n\\]\nMorozov regularization \\[\n\\underset{x \\in \\mathbb{R^n}}{\\mathrm{minimize}}\\quad ||\\beta||_l~~~~ \\mbox{s.t.}~~ ||y - X\\beta||_2^2 \\le \\tau\n\\] Here \\(\\tau\\) reflects the so called noise level, i.e. an estimate of the error which is made during the measurement of \\(b\\).\nTikhonov regularization \\[\n\\underset{\\beta\\in \\mathbb{R^n}}{\\mathrm{minimize}}\\quad ||y - X\\beta||_2^2 + \\lambda||\\beta||_l\n\\] - Tikhonov regularization with \\(l=1\\) is lasso - Tikhonov regularization with \\(l=2\\) is ridge regression - lasso + ridge = elastic net",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#ell_1-regularization-lasso",
    "href": "16-select.html#ell_1-regularization-lasso",
    "title": "16  Model Selection",
    "section": "16.10 \\(\\ell_1\\) Regularization (LASSO)",
    "text": "16.10 \\(\\ell_1\\) Regularization (LASSO)\nThe Least Absolute Shrinkage and Selection Operator (LASSO) uses \\(\\ell_1\\) norm penalty and in case of linear regression leads to the following optimization problem \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad ||y- X\\beta||_2^2   + \\lambda||\\beta||_1\n\\]\nIn one dimensional case solves the following optimization problem \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad  \\frac{1}{2} (y-\\beta)^2 + \\lambda | \\beta |\n\\] The solution is given by the soft-thresholding operator defined by \\[\n\\hat{\\beta} = \\mathrm{soft} (y; \\lambda) = ( y - \\lambda ~\\mathrm{sgn}(y) )_+.\n\\] Here sgn is the sign function and \\(( x )_+ = \\max (x,0)\\). To demonstrate how this solution is derived, we can define a slack variable \\(z = | \\beta |\\) and solve the joint constrained optimisation problem which is differentiable.\nGraphically, the soft-thresholding operator is\n\n\n\nSoft-threshold operator.\n\n\nLASSO has a nice feature that it forces some of the \\(\\hat{\\beta}\\)’s to zero. It is an automatic variable selection! Finding optimal solution is computationally fast, it is a convex optimisation problem, though, it is non-smooth. As in ridge regression, we still have to pick \\(\\lambda\\) via cross-validation. Visually the process can be represented using regularization path graph, as in the following example Example: We model prostate cancer using LASSO\n\n\n\n\n\n\n\n\n\nMSE.\n\n\n\n\n\n\n\nPath\n\n\n\n\n\n\nMSE and Regularization path for Prostate Cancer data using LASSO\n\n\n\nNow with ridge regression\n\n\n\n\n\n\n\n\n\nMSE\n\n\n\n\n\n\n\nPath\n\n\n\n\n\n\nMSE and Regularization path for Prostate Cancer data using Ridge\n\n\n\n\nExample 16.7 (Horse race prediction using logistic regression) We use the run.csv data from Kaggle (https://www.kaggle.com/gdaley/hkracing). Thhis dataset contains the condition of horse races in Hong Kong, including race course, distance, track condition and dividends paid. We want to use individual variables to predict the chance of winning of a horse. For the simplicity of computation, we only consider horses with id \\(\\leq 500\\), and train the model with \\(\\ell_1\\)-regularized logistic regression.\nAnd we include lengths_behind, horse_age, horse_country, horse_type, horse_rating, horse_gear, declared_weight, actual_weight, draw, win_odds, place_odds as predicting variables in our model.\nSince most of the variables, such as country, gear, type, are categorical, after spanning them into binary indictors, we have more than 800 columns in the design matrix.\nWe try two logistic regression model. The first one includes win_odds given by the gambling company. The second one does not include the win_odds and we use win_odds to test the power of our model. We tune both models with a 10-fold cross-validation to find the best penalty parameter \\(\\lambda\\).\nIn this model, we fit the logistic regression with full dataset. The best \\(\\lambda\\) we find is \\(5.699782e-06\\).\n\n\n\n\n\n\n\n\n\nNumber of variables vs lambda\n\n\n\n\n\n\n\nCoefficient Ranking\n\n\n\n\n\n\nLogistic regression for full data\n\n\n\nIn this model, we randomly partition the dataset into training(70%) and testing(30%) parts. We fit the logistic regression with training dataset. The best \\(\\lambda\\) we find is \\(4.792637e-06\\).\n\n\n\n\n\n\n\n\n\nNumber of variables vs lambda\n\n\n\n\n\n\n\nCoefficient Ranking\n\n\n\n\n\n\nLogistic regression for test data\n\n\n\nThe out-of-sample mean squared error for win_odds is 0.0668.\n\nElastic Net combines Ridge and Lasso and chooses coefficients \\(\\beta_1,\\ldots,\\beta_p\\) for the input variables by minimizing the sum-of-squared residuals plus a penalty of the form \\[\n\\lambda||\\beta||_1 + \\alpha||\\beta||_2^2.\n\\]",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#bayesian-model-selection",
    "href": "16-select.html#bayesian-model-selection",
    "title": "16  Model Selection",
    "section": "16.11 Bayesian Model Selection",
    "text": "16.11 Bayesian Model Selection\nWhen analyzing data, we deal with three types of quantities\n\n\\(X\\) = observed variables\n\\(Y\\) = hidden variable\n\\(\\theta\\) = parameters of the model that describes the relation between \\(X\\) and \\(Y\\).\n\nA probabilistic models of interest are the joint probability distribution \\(p(D,\\theta)\\) (called a generative model) and \\(P(Y,\\theta \\mid X)\\) (discriminative model). Discriminative models are easier to build and are more frequently used in practice. Generative model requires modeling a distribution over the set of observed variables, which makes our model more complicated. Text analysis provides an illustrative example. The task of identifying a topic of an article can be solved using discriminative distribution. The problem of generating a new article requires generative model.\nWhile performing data analysis using learning algorithms, we perform two tasks, namely training and inference which are summarized in the table below\n\n\n\nStep\nGiven\nHidden\nWhat to find\n\n\n\n\nTraining\n\\(D = (X,Y) = \\{x_i,y_i\\}_{i=1}^n\\)\n\\(\\theta\\)\n\\(p(\\theta \\mid D)\\)\n\n\nPrediction\n\\(x_{\\text{new}}\\)\n\\(y_{\\text{new}}\\)\n\\(p(y_{\\text{new}}  \\mid  x_{\\text{new}}, D)\\)\n\n\n\nThe training can be performed via the Bayes rule \\[\np(\\theta \\mid D) = \\dfrac{p(Y \\mid \\theta,X)p(\\theta)}{\\int p(Y \\mid \\theta,X)p(\\theta)d\\theta}.\n\\] Now to perform the second step (prediction), we calculate \\[\np(y_{\\text{new}}  \\mid  x_{\\text{new}}, D) = \\int p(y_{\\text{new}}  \\mid  x_{\\text{new}},\\theta)p(\\theta \\mid D)d\\theta\n\\] Thus, full Bayesian inference requires calculating two integrals, which might be difficult. We mentioned earlier that MAP allows us to avoid those calculations by approximating the posterior with \\[\np(\\theta \\mid D) \\approx \\delta(\\theta_{\\text{MAP}}),~~\\theta_{\\text{MAP}} \\in \\argmax_{\\theta}p(\\theta \\mid D)\n\\] To calculate \\(\\theta_{\\text{MAP}}\\), we do not need to know the normalizing constant for calculating posterior, since the solution of optimization problem does not depend on this constant. Further, the second integral for inference becomes degenerate and get approximated by \\[\np(y_{\\text{new}}  \\mid  x_{\\text{new}}, D) = \\int p(y_{\\text{new}}  \\mid  x_{\\text{new}},\\theta)p(\\theta \\mid D)d\\theta \\approx p(y_{\\text{new}}  \\mid  x_{\\text{new}},\\theta_{\\text{MAP}}).\n\\]\nNow we consider a case, when we have several candidate density functions for performing the prediction \\[\np_1(Y,\\theta  \\mid  X), ~~p_2(Y,\\theta \\mid X),\\ldots\n\\] How do we choose the better model? We can choose the model with highest evidence value (due to David MacKay) \\[\nj = \\argmax_j p_j(Y  \\mid  X) = \\argmax_j \\int p_j(Y  \\mid  X,\\theta)p(\\theta)d\\theta.\n\\] Note, formally instead of \\(p(\\theta)\\) we need to write \\(p(\\theta \\mid X)\\), however since \\(\\theta\\) does not depend on \\(X\\) we omit it.\n\n\n\nModel Selection\n\n\nCan you think of how the prior \\(p(\\theta)\\), posterior \\(p(\\theta \\mid D)\\) and the evidence \\(p(Y \\mid X)\\) distributions will look like? Which model is the best? Which model will have the highest \\(\\theta_{\\text{MAP}}\\)?\n\nExample 16.8 (Racial discrimination) Say we want to analyze racial discrimination by the US courts. We have three variables:\n\nMurderer: \\(m \\in {0,1}\\) (black/white)\nVictim: \\(v \\in \\{0,1\\}\\) (black/white)\nVerdict: \\(d \\in \\{0,1\\}\\) (prison/death penalty)\n\nSay we have the data\n\n\n\nm\nv\nd\nn\n\n\n\n\n0\n0\n0\n132\n\n\n0\n0\n1\n19\n\n\n0\n1\n0\n9\n\n\n0\n1\n1\n0\n\n\n1\n0\n0\n52\n\n\n1\n0\n1\n11\n\n\n1\n1\n0\n97\n\n\n1\n1\n1\n6\n\n\n\nWe would like to establish a causal relations between the race and verdict variables. For this, we consider several models\n\n\\(p(d \\mid m,v) = p(d) = \\theta\\)\n\\(p(d \\mid m,v) = p(d \\mid v)\\); \\(p(d \\mid v=0) = \\alpha, ~p(d \\mid v=1)=\\beta\\)\n\\(p(d \\mid v,m) = p(d \\mid m)\\); \\(p(d \\mid m=1) = \\gamma,~p(d \\mid m=1) = \\delta\\)\n\\(p(d|v,m)\\) cannot be reduced, and\n\n\n\n\n\\(p(d=1 \\mid m,v)\\)\n\\(m=0\\)\n\\(m=1\\)\n\n\n\n\n\\(v=0\\)\n\\(\\tau\\)\n\\(\\chi\\)\n\n\n\\(v=1\\)\n\\(\\nu\\)\n\\(\\zeta\\)\n\n\n\n\nWe calculate which model describes data the best, we calculate the evidences. We need to describe the discriminative model \\[\np(Y ,\\theta \\mid X) = p(Y \\mid X,\\theta)p(\\theta \\mid X)\n\\] Here \\(X\\) is the number of cases, and \\(Y\\) is the number of death penalties. We use uninformative prior \\(\\theta \\sim U[0,1]\\). To specify the likelihood, we use Binomial distribution \\[\nY \\mid X,\\theta \\sim B(X,\\theta),~~B(Y \\mid X,\\theta) = C_Y^Xp^Y(1-\\theta)^{X-Y}\n\\] We assume \\(p(\\theta)\\sim Uniform\\). Now lets calculate the evidence \\[\np(Y, \\theta \\mid X) = \\int p(Y  \\mid  X,\\theta)p(\\theta)d\\theta\n\\] for each of the four models\n\n\\(p(Y \\mid X) = \\int B(19 \\mid 151,\\theta)B(0 \\mid 9,\\theta)B(11 \\mid 63,\\theta)B(6 \\mid 103,\\theta)d\\theta\\) \\(\\propto \\int_0^{1} \\theta^{36}(1-\\theta)^{290}d\\theta = B(37,291) = 2.8\\times 10^{-51}\\)\n\\(p(Y \\mid X) = \\int\\int B(19 \\mid 151,\\alpha)B(0 \\mid 9,\\beta)B(11 \\mid 63,\\alpha)B(6 \\mid 103,\\beta)d\\alpha d\\beta \\propto 4.7\\times 10^{-51}\\)\n\\(p(d \\mid v,m) = p(d \\mid m)=\\int\\int B(19 \\mid 151,\\gamma)B(0 \\mid 9,\\gamma)B(11 \\mid 63,\\delta)B(6 \\mid 103,\\delta)d\\gamma d\\delta \\propto 0.27\\times10^{-51}\\)\n\\(p(d \\mid v,m) = \\int\\int\\int\\int B(19 \\mid 151,\\tau)B(0 \\mid 9,\\nu)B(11 \\mid 63,\\chi)B(6 \\mid 103,\\zeta)d\\tau d\\nu d\\chi d\\zeta \\propto 0.18\\times10^{-51}\\)\n\nThe last model is too complex, it can explain any relations in the data and this, has the lowest evidence score! However, if we are to use ML estimates, the fourth model will have the highest likelihood. Bayesian approach allows to avoid over-fitting! You can also see that this data set contains the Simpson’s paradox. Check it! A related problem is Bertrand’s gold box problem.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#bayesian-ell_0-regularization",
    "href": "16-select.html#bayesian-ell_0-regularization",
    "title": "16  Model Selection",
    "section": "16.12 Bayesian \\(\\ell_0\\) regularization",
    "text": "16.12 Bayesian \\(\\ell_0\\) regularization\nBayesian \\(\\ell_0\\) regularization is an attractive solution for high dimensional variable selection as it directly penalizes the number of predictors. The caveat is the need to search over all possible model combinations, as a full solution requires enumeration over all possible models which is NP-hard. The gold standard for Bayesian variable selection are spike-and-slab priors, or Bernoulli-Gaussian mixtures.\nConsider a standard Gaussian linear regression model, where \\(X = [X_1, \\ldots, X_p] \\in \\mathbb{R}^{n \\times p}\\) is a design matrix, \\(\\beta = (\\beta_1, \\ldots, \\beta_p)^T \\in \\mathbb{R}^p\\) is a \\(p\\)-dimensional coefficient vector, and \\(e\\) is an \\(n\\)-dimensional independent Gaussian noise. After centralizing \\(y\\) and all columns of \\(X\\), we ignore the intercept term in the design matrix \\(X\\) as well as \\(\\beta\\), and we can write\n\\[\ny = X\\beta + e, \\quad \\text{where } e \\sim N(0, \\sigma_e^2 I_n)\n\\tag{16.1}\\]\nTo specify a prior distribution \\(p(\\beta)\\), we impose a sparsity assumption on \\(\\beta\\), where only a small portion of all \\(\\beta_i\\)’s are non-zero. In other words, \\(\\|\\beta\\|_0 = k \\ll p\\), where \\(\\|\\beta\\|_0 := \\#\\{i : \\beta_i \\neq 0\\}\\), the cardinality of the support of \\(\\beta\\), also known as the \\(\\ell_0\\) (pseudo)norm of \\(\\beta\\). A multivariate Gaussian prior (\\(l_2\\) norm) leads to poor sparsity properties in this situation. (See, for example, Polson and Scott (2011).)\nSparsity-inducing prior distributions for \\(\\beta\\) can be constructed to impose sparsity. The gold standard is a spike-and-slab prior (Jeffreys 1998; Mitchell and Beauchamp 1988; George and and McCulloch 1993). Under these assumptions, each \\(\\beta_i\\) exchangeably follows a mixture prior consisting of \\(\\delta_0\\), a point mass at \\(0\\), and a Gaussian distribution centered at zero. Hence we write\n\\[\n\\beta_i | \\theta, \\sigma_\\beta^2 \\sim (1-\\theta)\\delta_0 + \\theta N(0, \\sigma_\\beta^2)\n\\tag{16.2}\\]\nHere \\(\\theta \\in (0, 1)\\) controls the overall sparsity in \\(\\beta\\) and \\(\\sigma_\\beta^2\\) accommodates non-zero signals. This family is termed as the Bernoulli-Gaussian mixture model in the signal processing community.\nA useful re-parameterization: the parameters \\(\\beta\\) are given by two independent random variable vectors \\(\\gamma = (\\gamma_1, \\ldots, \\gamma_p)\\) and \\(\\alpha = (\\alpha_1, \\ldots, \\alpha_p)\\) such that \\(\\beta_i = \\gamma_i \\alpha_i\\), with probabilistic structure\n\\[\n\\begin{array}{rcl}\n\\gamma_i|\\theta & \\sim & \\text{Bernoulli}(\\theta) \\\\\n\\alpha_i | \\sigma_\\beta^2 &\\sim & N(0, \\sigma_\\beta^2)\n\\end{array}\n\\tag{16.3}\\]\nSince \\(\\gamma_i\\) and \\(\\alpha_i\\) are independent, the joint prior density becomes\n\\[\np(\\gamma_i, \\alpha_i | \\theta, \\sigma_\\beta^2) = \\theta^{\\gamma_i}(1-\\theta)^{1-\\gamma_i}\\frac{1}{\\sqrt{2\\pi}\\sigma_\\beta}\\exp\\left\\{-\\frac{\\alpha_i^2}{2\\sigma_\\beta^2}\\right\\}, \\quad 1 \\leq i \\leq p\n\\]\nThe indicator \\(\\gamma_i \\in \\{0, 1\\}\\) can be viewed as a dummy variable to indicate whether \\(\\beta_i\\) is included in the model .\nLet \\(S = \\{i: \\gamma_i = 1\\} \\subseteq \\{1, \\ldots, p\\}\\) be the “active set” of \\(\\gamma\\), and \\(\\|\\gamma\\|_0 = \\sum_{i=1}^p \\gamma_i\\) be its cardinality. The joint prior on the vector \\(\\{\\gamma, \\alpha\\}\\) then factorizes as\n\\[\n\\begin{array}{rcl}\np(\\gamma, \\alpha | \\theta, \\sigma_\\beta^2) &=& \\prod_{i=1}^p p(\\alpha_i, \\gamma_i | \\theta, \\sigma_\\beta^2) \\\\\n&=& \\theta^{\\|\\gamma\\|_0} (1-\\theta)^{p-\\|\\gamma\\|_0} (2\\pi\\sigma_\\beta^2)^{-p/2} \\exp\\left\\{-\\frac{1}{2\\sigma_\\beta^2} \\sum_{i=1}^p \\alpha_i^2\\right\\}\n\\end{array}\n\\]\nLet \\(X_\\gamma := [X_i]_{i \\in S}\\) be the set of “active explanatory variables” and \\(\\alpha_\\gamma := (\\alpha_i)_{i \\in S}\\) be their corresponding coefficients. We can write \\(X\\beta = X_\\gamma \\alpha_\\gamma\\). The likelihood can be expressed in terms of \\(\\gamma\\), \\(\\alpha\\) as\n\\[\np(y | \\gamma, \\alpha, \\theta, \\sigma_e^2) = (2\\pi\\sigma_e^2)^{-n/2} \\exp\\left\\{ -\\frac{1}{2\\sigma_e^2} \\|y - X_\\gamma \\alpha_\\gamma\\|_2^2 \\right\\}\n\\]\nUnder this re-parameterization by \\(\\{\\gamma, \\alpha\\}\\), the posterior is given by\n\\[\n\\begin{array}{rcl}\np(\\gamma, \\alpha | \\theta, \\sigma_\\beta^2, \\sigma_e^2, y) &\\propto& p(\\gamma, \\alpha | \\theta, \\sigma_\\beta^2) p(y | \\gamma, \\alpha, \\theta, \\sigma_e^2) \\\\\n&\\propto& \\exp\\left\\{ -\\frac{1}{2\\sigma_e^2} \\|y - X_\\gamma \\alpha_\\gamma\\|_2^2 - \\frac{1}{2\\sigma_\\beta^2} \\|\\alpha\\|_2^2 - \\log\\left(\\frac{1-\\theta}{\\theta}\\right) \\|\\gamma\\|_0 \\right\\}\n\\end{array}\n\\]\nOur goal then is to find the regularized maximum a posterior (MAP) estimator\n\\[\n\\arg\\max_{\\gamma, \\alpha} p(\\gamma, \\alpha | \\theta, \\sigma_\\beta^2, \\sigma_e^2, y)\n\\]\nBy construction, \\(\\gamma \\in \\{0, 1\\}^p\\) will directly perform variable selection. Spike-and-slab priors, on the other hand, will sample the full posterior and calculate the posterior probability of variable inclusion.\nFinding the MAP estimator is equivalent to minimizing over \\(\\{\\gamma, \\alpha\\}\\) the regularized least squares objective function:\n\\[\n\\min_{\\gamma, \\alpha} \\|y - X_\\gamma \\alpha_\\gamma\\|_2^2 + \\frac{\\sigma_e^2}{\\sigma_\\beta^2} \\|\\alpha\\|_2^2 + 2\\sigma_e^2 \\log\\left(\\frac{1-\\theta}{\\theta}\\right) \\|\\gamma\\|_0\n\\tag{16.4}\\]\nThis objective possesses several interesting properties:\n\nThe first term is essentially the least squares loss function.\nThe second term looks like a ridge regression penalty and has connection with the signal-to-noise ratio (SNR) \\(\\sigma_\\beta^2/\\sigma_e^2\\). Smaller SNR will be more likely to shrink the estimates towards \\(0\\). If \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the prior uncertainty on the size of non-zero coefficients is much larger than the noise level, that is, the SNR is sufficiently large, this term can be ignored. This is a common assumption in spike-and-slab framework in that people usually want \\(\\sigma_\\beta \\to \\infty\\) or to be “sufficiently large” in order to avoid imposing harsh shrinkage to non-zero signals.\nIf we further assume that \\(\\theta &lt; 1/2\\), meaning that the coefficients are known to be sparse a priori, then \\(\\log((1-\\theta)/\\theta) &gt; 0\\), and the third term can be seen as an \\(\\ell_0\\) regularization.\n\nTherefore, our Bayesian objective inference is connected to \\(\\ell_0\\)-regularized least squares, which we summarize in the following proposition.\n\nProposition 1 (Spike-and-slab MAP & \\(\\ell_0\\) regularization)\nFor some \\(\\lambda &gt; 0\\), assuming \\(\\theta &lt; 1/2\\), \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the Bayesian MAP estimate defined by (Equation 16.4) is equivalent to the \\(\\ell_0\\) regularized least squares objective, for some \\(\\lambda &gt; 0\\),\n\\[\n\\min_{\\beta} \\frac{1}{2} \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_0\n\\tag{16.5}\\]\n\n\nProof. Proof. First, assuming that\n\\[\n\\theta &lt; 1/2, \\quad \\sigma_\\beta^2 \\gg \\sigma_e^2, \\quad \\frac{\\sigma_e^2}{\\sigma_\\beta^2} \\|\\alpha\\|_2^2 \\to 0\n\\]\ngives us an objective function of the form\n\\[\n\\min_{\\gamma, \\alpha} \\frac{1}{2} \\|y - X_\\gamma \\alpha_\\gamma\\|_2^2 + \\lambda \\|\\gamma\\|_0, \\quad \\lambda := \\sigma_e^2 \\log\\left(\\frac{1-\\theta}{\\theta}\\right) &gt; 0\n\\tag{16.6}\\]\nEquation 16.6 can be seen as a variable selection version of Equation 16.5. The interesting fact is that Equation 16.5 and Equation 16.6 are equivalent. To show this, we need only to check that the optimal solution to Equation 16.5 corresponds to a feasible solution to Equation 16.6 and vice versa. This is explained as follows.\nOn the one hand, assuming \\(\\hat\\beta\\) is an optimal solution to Equation 16.5, then we can correspondingly define \\(\\hat\\gamma_i := I\\{\\hat\\beta_i \\neq 0\\}\\), \\(\\hat\\alpha_i := \\hat\\beta_i\\), such that \\(\\{\\hat\\gamma, \\hat\\alpha\\}\\) is feasible to (Equation 16.6) and gives the same objective value as \\(\\hat\\beta\\) gives (Equation 16.5).\nOn the other hand, assuming \\(\\{\\hat\\gamma, \\hat\\alpha\\}\\) is optimal to (Equation 16.6), implies that we must have all of the elements in \\(\\hat\\alpha_\\gamma\\) non-zero, otherwise a new \\(\\tilde\\gamma_i := I\\{\\hat\\alpha_i \\neq 0\\}\\) will give a lower objective value of (Equation 16.6). As a result, if we define \\(\\hat\\beta_i := \\hat\\gamma_i \\hat\\alpha_i\\), \\(\\hat\\beta\\) will be feasible to Equation 16.5 and gives the same objective value as \\(\\{\\hat\\gamma, \\hat\\alpha\\}\\) gives Equation 16.6.\nCombining both arguments shows that the two problems Equation 16.5 and Equation 16.6 are equivalent. Hence we can use results from non-convex optimization literature to find Bayes MAP estimators.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#survey",
    "href": "16-select.html#survey",
    "title": "16  Model Selection",
    "section": "16.13 Computing the \\(\\ell_0\\)-regularized regression solution",
    "text": "16.13 Computing the \\(\\ell_0\\)-regularized regression solution\nWe now turn to the problem of computation. \\(\\ell_0\\)-regularized least squares (Equation 16.5) is closely related to the best subset selection in linear regression as follows.\n\\[\n\\begin{array}{rl}\n\\min_{\\beta} & \\frac{1}{2}\\|y - X\\beta\\|_2^2 \\\\\n\\text{s.t.} & \\|\\beta\\|_0 \\leq k\n\\end{array}\n\\tag{16.7}\\]\nThe \\(\\ell_0\\)-regularized least squares (Equation 16.5) can be seen as (Equation 16.7)’s Lagrangian form. However, due to high non-convexity of the \\(\\ell_0\\)-norm, (Equation 16.5) and (Equation 16.7) are connected but not equivalent. In particular, for any given \\(\\lambda \\geq 0\\), there exists an integer \\(k \\geq 0\\), such that (Equation 16.5) and (Equation 16.7) have the same global minimizer \\(\\hat\\beta\\). However, it’s not true the other way around. It’s possible, even common, that for a given \\(k\\), we cannot find a \\(\\lambda \\geq 0\\), such that the solutions to (Equation 16.7) and (Equation 16.5) are the same.\nIndeed, for \\(k \\in \\{1, 2, \\ldots, p\\}\\), let \\(\\hat\\beta_k\\) be respective optimal solutions to (Equation 16.7) and \\(f_k\\) respective optimal objective values, and so \\(f_1 \\geq f_2 \\geq \\cdots \\geq f_p\\). If we want a solution \\(\\hat\\beta_\\lambda\\) to (Equation 16.5) with \\(\\|\\hat\\beta_\\lambda\\|_0 = k\\), we need to find a \\(\\lambda\\) such that\n\\[\n\\max_{i &gt; k} \\{f_k - f_i\\} \\leq \\lambda \\leq \\min_{j &lt; k} \\{f_j - f_k\\}\n\\]\nwith the caveat that such \\(\\lambda\\) need not exist.\nBoth problems involve discrete optimization and have thus been seen as intractable for large-scale data sets. As a result, in the past, \\(\\ell_0\\) norm is usually replaced by its convex relaxation \\(l_1\\) norm to facilitate computation. However, it’s widely known that the solutions of \\(\\ell_0\\) norm problems provide superior variable selection and prediction performance compared with their \\(l_1\\) convex relaxation such as Lasso. It is known that the solution to the \\(\\ell_0\\)-regularized least squares should be better than Lasso in terms of variable selection especially when we have a design matrix \\(X\\) that has high collinearity among its columns.\nBertsimas, King, and Mazumder (2016) introduced a first-order algorithm to provide a stationary solution \\(\\beta^*\\) to a class of generalized \\(\\ell_0\\)-constrained optimization problem, with convex \\(g\\):\n\\[\n\\begin{array}{rl}\n\\min_{\\beta} & g(\\beta) \\\\\n\\text{s.t.} & \\|\\beta\\|_0 \\leq k\n\\end{array}\n\\tag{16.8}\\]\nLet \\(L\\) be the Lipschitz constant for \\(\\nabla g\\) such that \\(\\forall \\beta_1, \\beta_2\\), \\(\\|\\nabla g(\\beta_1) - \\nabla g(\\beta_2)\\| \\leq L \\|\\beta_1 - \\beta_2\\|\\). Their “Algorithm 1” is as follows.\n\nInitialize \\(\\beta^0\\) such that \\(\\|\\beta^0\\|_0 \\leq k\\).\nFor \\(t \\geq 0\\), obtain \\(\\beta^{t+1}\\) as\n\n\\[\n\\beta^{t+1} = H_k\\left(\\beta^t - \\frac{1}{L} \\nabla g(\\beta^t)\\right)\n\\tag{16.9}\\]\nuntil convergence to \\(\\beta^*\\).\nwhere the operator \\(H_k(\\cdot)\\) is to keep the largest \\(k\\) elements of a vector as the same, whilst to set all else to zero. It can also be called the hard thresholding at the \\(k\\)th largest element. In the least squares setting when \\(g(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2\\), \\(\\nabla g\\) and \\(L\\) are easy to compute. Bertsimas, King, and Mazumder (2016) then uses the stationary solution \\(\\beta^*\\) obtained by the aforementioned algorithm (Equation 16.9) as a warm start for their mixed integer optimization (MIO) scheme to produce a “provably optimal solution” to the best subset selection problem (Equation 16.7).\nIt’s worth pointing out that the key iteration step (Equation 16.9) is connected to the proximal gradient descent (PGD) algorithm many have used to solve the \\(\\ell_0\\)-regularized least squares (Equation 16.5), as well as other non-convex regularization problems. PGD methods solve a general class of problems such as\n\\[\n\\min_{\\beta} g(\\beta) + \\lambda \\phi(\\beta)\n\\tag{16.10}\\]\nwhere \\(g\\) is the same as in (Equation 16.8), and \\(\\phi\\), usually non-convex, is a regularization term. In this framework, in order to obtain a stationary solution \\(\\beta^*\\), the key iteration step is\n\\[\n\\beta^{t+1} = \\mathrm{prox}_{\\lambda\\phi}\\left(\\beta^t - \\frac{1}{L} \\nabla g(\\beta^t)\\right)\n\\tag{16.11}\\]\nwhere \\(\\beta^t - \\frac{1}{L} \\nabla g(\\beta^t)\\) can be seen as a gradient descent step for \\(g\\) and \\(\\mathrm{prox}_{\\lambda\\phi}\\) is the proximal operator for \\(\\lambda\\phi\\). In \\(\\ell_0\\)-regularized least squares, \\(\\lambda\\phi(\\cdot) = \\lambda\\|\\cdot\\|_0\\), and its proximal operator \\(\\mathrm{prox}_{\\lambda\\|\\cdot\\|_0}\\) is just the hard thresholding at \\(\\lambda\\). That is, \\(\\mathrm{prox}_{\\lambda\\|\\cdot\\|_0}\\) is to keep the same all elements no less than \\(\\lambda\\), whilst to set all else to zero. As a result, the similarity between (Equation 16.9) and (Equation 16.11) are quite obvious.\n###Single best replacement (SBR) algorithm {#sbr}\nThe single best replacement (SBR) algorithm, provides a solution to the variable selection regularization (Equation 16.6). Since (Equation 16.6) and the \\(\\ell_0\\)-regularized least squares (Equation 16.5) are equivalent, SBR also provides a practical way to give a sufficiently good local optimal solution to the NP-hard \\(\\ell_0\\) regularization.\nTake a look at the objective (Equation 16.6). For any given variable selection indicator \\(\\gamma\\), we have an active set \\(S = \\{i: \\gamma_i = 1\\}\\), based on which the minimizer \\(\\hat\\alpha_\\gamma\\) of (Equation 16.6) has a closed form. \\(\\hat\\alpha_\\gamma\\) will set every coefficient outside \\(S\\) to zero, and regress \\(y\\) on \\(X_\\gamma\\), the variables inside \\(S\\). Therefore, the minimization of the objective function can be determined by \\(\\gamma\\) or \\(S\\) alone. Accordingly, the objective function (Equation 16.6) can be rewritten as follows.\n\\[\n\\min_{S} f_{SBR}(S) = \\frac{1}{2} \\|y - X_S \\beta_S\\|_2^2 + \\lambda |S|\n\\] {#obj-sbr}\nSBR thus tries to minimize \\(f_{SBR}(S)\\) via choosing the optimal \\(\\hat S\\).\nThe algorithm works as follows. Suppose we start with an initial \\(S\\), usually the empty set. At each iteration, SBR aims to find a “single change of \\(S\\)”, that is, a single removal from or adding to \\(S\\) of one element, such that this single change decreases \\(f_{SBR}(S)\\) the most. SBR stops when no such change is available, or in other words, any single change of \\(\\gamma\\) or \\(S\\) will only give the same or larger objective value. Therefore, intuitively SBR stops at a local optimum of \\(f_{SBR}(S)\\).\nSBR is essentially a stepwise greedy variable selection algorithm. At each iteration, both adding and removal are allowed, so this algorithm is one example of the “forward-backward” stepwise procedures. It’s provable that with this feature the algorithm “can escape from some [undesirable] local minimizers” of \\(f_{SBR}(S)\\). Therefore, SBR can solve the \\(\\ell_0\\)-regularized least squares in a sub-optimal way, providing a satisfactory balance between efficiency and accuracy.\nWe now write out the algorithm more formally. For any currently chosen active set \\(S\\), define a single replacement \\(S \\cdot i, i \\in \\{1, \\ldots, p\\}\\) as \\(S\\) adding or removing a single element \\(i\\):\n\\[\nS \\cdot i :=\n\\begin{cases}\nS \\cup \\{i\\}, & i \\notin S \\\\\nS \\setminus \\{i\\}, & i \\in S\n\\end{cases}\n\\]\nThen we compare the objective value at current \\(S\\) with all of its single replacements \\(S \\cdot i\\), and choose the best one. SBR proceeds as follows:\n\nStep 0: Initialize \\(S_0\\). Usually, \\(S_0 = \\emptyset\\). Compute \\(f_{SBR}(S_0)\\). Set \\(k = 1\\).\nStep \\(k\\): For every \\(i \\in \\{1, \\ldots, p\\}\\), compute \\(f_{SBR}(S_{k-1} \\cdot i)\\). Obtain the single best replacement \\(j := \\arg\\min_{i} f_{SBR}(S_{k-1} \\cdot i)\\).\n\nIf \\(f_{SBR}(S_{k-1} \\cdot j) \\geq f_{SBR}(S_{k-1})\\), stop. Report \\(\\hat S = S_{k-1}\\) as the solution.\nOtherwise, set \\(S_k = S_{k-1} \\cdot j\\), \\(k = k+1\\), and repeat step \\(k\\).\n\n\nIt can be shown that SBR always stops within finite steps. With the output \\(\\hat S\\), the locally optimal solution to the \\(\\ell_0\\)-regularized least squares \\(\\hat\\beta\\) is just the coefficients of \\(y\\) regressed on \\(X_{\\hat S}\\) and zero elsewhere. In order to include both forward and backward steps in the variable selection process, the algorithm needs to compute \\(f_{SBR}(S_{k-1} \\cdot i)\\) for every \\(i\\) at every step. Because it involves a one-column update of current design matrix \\(X_{S_{k-1}}\\), this computation can be made very efficient by using the Cholesky decomposition, without explicitly calculating \\(p\\) linear regressions at each step . An R package implementation of the algorithm is available upon request.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#double-descent",
    "href": "16-select.html#double-descent",
    "title": "16  Model Selection",
    "section": "16.14 Double Descent",
    "text": "16.14 Double Descent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDouble Descent Stylized",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#polya-gamma",
    "href": "16-select.html#polya-gamma",
    "title": "16  Model Selection",
    "section": "16.15 Polya-Gamma",
    "text": "16.15 Polya-Gamma\nBayesian inference for logistic regression has long been recognized as a computationally challenging problem due to the analytically inconvenient form of the binomial likelihood function(Polson, Scott, and Windle 2013). While the probit model enjoys simple latent-variable methods for posterior sampling, the logistic model has historically required more complex approaches involving multiple layers of auxiliary variables or approximations(Polson, Scott, and Windle 2013). The breakthrough work of Polson, Scott, and Windle (2013) introduced a revolutionary data-augmentation strategy using a novel class of distributions called Pólya-Gamma distributions, which enables simple and exact Gibbs sampling for Bayesian logistic regression(Polson, Scott, and Windle 2013).\nThis methodology represents a significant advancement in Bayesian computation, providing a direct analog to the Albert and Chib (1993) method for probit regression while maintaining both exactness and simplicity(Polson, Scott, and Windle 2013). The approach has proven particularly valuable for complex hierarchical models where traditional Metropolis-Hastings samplers are difficult to tune and implement effectively(Polson, Scott, and Windle 2013).\n\n\n\n\n\n\nKey Innovation\n\n\n\nThe Pólya-Gamma methodology provides exact Gibbs sampling for Bayesian logistic regression, eliminating the need for complex Metropolis-Hastings tuning while maintaining theoretical guarantees.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#the-pólya-gamma-distribution",
    "href": "16-select.html#the-pólya-gamma-distribution",
    "title": "16  Model Selection",
    "section": "16.16 The Pólya-Gamma Distribution",
    "text": "16.16 The Pólya-Gamma Distribution\nThe Pólya-Gamma distribution, denoted as PG(b,c), is carefully constructed as a subset of infinite convolutions of gamma distributions(Polson, Scott, and Windle 2013). A random variable X follows a Pólya-Gamma distribution with parameters b &gt; 0 and c ∈ ℝ if:\n\\[X \\stackrel{d}{=} \\frac{1}{2\\pi^2} \\sum_{k=1}^{\\infty} \\frac{g_k}{(k-1/2)^2 + c^2/(4\\pi^2)}\\]\nwhere \\(g_k \\sim \\text{Ga}(b,1)\\) are independent gamma random variables, and \\(\\stackrel{d}{=}\\) indicates equality in distribution(Polson, Scott, and Windle 2013).\nThe Pólya-Gamma family exhibits several remarkable properties that make it ideal for data augmentation:\n\nLaplace Transform: For \\(\\omega \\sim \\text{PG}(b,0)\\), the Laplace transform is \\(E\\{\\exp(-\\omega t)\\} = \\cosh^{-b}(\\sqrt{t}/2)\\)(Polson, Scott, and Windle 2013)\nExponential Tilting: The general PG(b,c) distribution arises through exponential tilting of the PG(b,0) density:\n\n\\[p(x|b,c) = \\frac{\\exp(-c^2x/2)p(x|b,0)}{E[\\exp(-c^2\\omega/2)]}\\]\nwhere the expectation is taken with respect to PG(b,0)(Polson, Scott, and Windle 2013)\n\nConvolution Property: The family is closed under convolution for random variates with the same tilting parameter(Polson, Scott, and Windle 2013)\nKnown Moments: All finite moments are available in closed form, with the expectation given by:\n\n\\[E(\\omega) = \\frac{b}{2c}\\tanh(c/2) = \\frac{b}{2c}\\frac{e^c-1}{1+e^c}\\]\n\n\n\n\n\n\nComputational Advantage\n\n\n\nThe known moments and convolution properties make the Pólya-Gamma distribution computationally tractable and theoretically well-behaved.\n\n\n\n16.16.1 The Data-Augmentation Strategy\nThe core of the Pólya-Gamma methodology rests on a fundamental integral identity that represents binomial likelihoods as mixtures of Gaussians(Polson, Scott, and Windle 2013). The key theorem states:\n\nTheorem 1: For b &gt; 0 and a ∈ ℝ, the following integral identity holds:\n\\[\\frac{(e^\\psi)^a}{(1+e^\\psi)^b} = 2^{-b}e^{\\kappa\\psi} \\int_0^{\\infty} e^{-\\omega\\psi^2/2} p(\\omega) d\\omega\\]\nwhere \\(\\kappa = a - b/2\\), and \\(p(\\omega)\\) is the density of \\(\\omega \\sim \\text{PG}(b,0)\\)(Polson, Scott, and Windle 2013).\nMoreover, the conditional distribution \\(p(\\omega|\\psi)\\) is also in the Pólya-Gamma class: \\((\\omega|\\psi) \\sim \\text{PG}(b,\\psi)\\)(Polson, Scott, and Windle 2013).\n\n\n\n16.16.2 Gibbs Sampling Algorithm\nThis integral identity leads directly to a simple two-step Gibbs sampler for Bayesian logistic regression(Polson, Scott, and Windle 2013). For a dataset with observations \\(y_i \\sim \\text{Binom}(n_i, 1/(1+e^{-\\psi_i}))\\) where \\(\\psi_i = x_i^T\\beta\\), and a Gaussian prior \\(\\beta \\sim N(b,B)\\), the algorithm iterates:\n\nSample auxiliary variables: \\((\\omega_i|\\beta) \\sim \\text{PG}(n_i, x_i^T\\beta)\\) for each observation\nSample parameters: \\((\\beta|y,\\omega) \\sim N(m_\\omega, V_\\omega)\\) where:\n\n\\(V_\\omega = (X^T\\Omega X + B^{-1})^{-1}\\)\n\\(m_\\omega = V_\\omega(X^T\\kappa + B^{-1}b)\\)\n\\(\\kappa = (y_1-n_1/2, \\ldots, y_n-n_n/2)\\)\n\\(\\Omega = \\text{diag}(\\omega_1, \\ldots, \\omega_n)\\)\n\n\nThis approach requires only Gaussian draws for the main parameters and Pólya-Gamma draws for a single layer of latent variables, making it significantly simpler than previous methods(Polson, Scott, and Windle 2013).\n\n\n16.16.3 The PG(1,z) Sampler\nThe practical success of the Pólya-Gamma method depends on efficient simulation of Pólya-Gamma random variables(Polson, Scott, and Windle 2013). The authors developed a sophisticated accept-reject sampler based on the alternating-series method of Devroye (1986)(Devroye 1986). For the fundamental PG(1,c) case, the sampler:\n\nUses exponential and inverse-Gaussian draws as proposals\nAchieves acceptance probability uniformly bounded below at 0.99919\nRequires no tuning for optimal performance\nEvaluates acceptance using iterative partial sums\n\n\n\n16.16.4 General PG(b,z) Sampling\nFor integer values of b, PG(b,z) random variables are generated by summing b independent PG(1,z) draws, exploiting the convolution property(Polson, Scott, and Windle 2013). This approach maintains efficiency for moderate values of b, though computational cost scales linearly with the total number of counts in negative binomial applications(Polson, Scott, and Windle 2013).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#implementation-with-bayeslogit-package",
    "href": "16-select.html#implementation-with-bayeslogit-package",
    "title": "16  Model Selection",
    "section": "16.17 Implementation with BayesLogit Package",
    "text": "16.17 Implementation with BayesLogit Package\n\n16.17.1 Package Overview\nThe BayesLogit package provides efficient tools for sampling from the Pólya-Gamma distribution(Windle 2023). The current version (2.1) focuses on core functionality: sampling from the Pólya-Gamma distribution through the rpg() function and its variants(Windle 2023).\n\n\n16.17.2 Core Functions\nThe package offers several sampling methods:\n\nrpg(): Main function that automatically selects the best method\nrpg.devroye(): Devroye-like method for integer h values\nrpg.gamma(): Sum of gammas method (slower but works for all parameters)\nrpg.sp(): Saddlepoint approximation method\n\n\n\n16.17.3 Installation and Basic Usage\n\n# Install from CRAN\ninstall.packages(\"BayesLogit\")\nlibrary(BayesLogit)\n\n# Basic usage examples\n# Sample from PG(1, 0)\nsamples1 &lt;- rpg(1000, h=1, z=0)\n\n# Sample with tilting parameter\nsamples2 &lt;- rpg(1000, h=1, z=2.5)\n\n# Multiple shape parameters\nh_values &lt;- c(1, 2, 3)\nz_values &lt;- c(1, 2, 3)\nsamples3 &lt;- rpg(100, h=h_values, z=z_values)\n\n\n\n16.17.4 Implementing Bayesian Logistic Regression\nHere’s a complete implementation of Bayesian logistic regression using the Pólya-Gamma methodology:\n\n# Bayesian Logistic Regression with Pólya-Gamma Data Augmentation\nbayesian_logit_pg &lt;- function(y, X, n_iter=5000, burn_in=1000) {\n  n &lt;- length(y)\n  p &lt;- ncol(X)\n  \n  # Prior specification (weakly informative)\n  beta_prior_mean &lt;- rep(0, p)\n  beta_prior_prec &lt;- diag(0.01, p)  # Precision matrix\n  \n  # Storage for samples\n  beta_samples &lt;- matrix(0, n_iter, p)\n  omega_samples &lt;- matrix(0, n_iter, n)\n  \n  # Initialize\n  beta &lt;- rep(0, p)\n  \n  for(iter in 1:n_iter) {\n    # Step 1: Sample omega (auxiliary variables)\n    psi &lt;- X %*% beta\n    omega &lt;- rpg(n, h=1, z=psi)\n    \n    # Step 2: Sample beta (regression coefficients)\n    # Posterior precision and mean\n    V_omega &lt;- solve(t(X) %*% diag(omega) %*% X + beta_prior_prec)\n    kappa &lt;- y - 0.5\n    m_omega &lt;- V_omega %*% (t(X) %*% kappa + beta_prior_prec %*% beta_prior_mean)\n    \n    # Sample from multivariate normal\n    beta &lt;- mvrnorm(1, m_omega, V_omega)\n    \n    # Store samples\n    beta_samples[iter, ] &lt;- beta\n    omega_samples[iter, ] &lt;- omega\n  }\n  \n  # Return samples after burn-in\n  list(\n    beta = beta_samples[(burn_in+1):n_iter, ],\n    omega = omega_samples[(burn_in+1):n_iter, ],\n    n_samples = n_iter - burn_in\n  )\n}\n\n# Example usage with simulated data\nset.seed(123)\nn &lt;- 100\nX &lt;- cbind(1, matrix(rnorm(n*2), n, 2))  # Intercept + 2 predictors\nbeta_true &lt;- c(-0.5, 1.2, -0.8)\nlogits &lt;- X %*% beta_true\nprobs &lt;- 1/(1 + exp(-logits))\ny &lt;- rbinom(n, 1, probs)\n\n# Fit model\nresults &lt;- bayesian_logit_pg(y, X, n_iter=3000, burn_in=500)\n\n# Posterior summaries\nposterior_means &lt;- colMeans(results$beta)\nposterior_sds &lt;- apply(results$beta, 2, sd)\n\nComputational Advantages\nExtensive benchmarking studies demonstrate the superior performance of the Pólya-Gamma method across various scenarios(Polson, Scott, and Windle 2013):\n\nSimple logistic models: Competitive with well-tuned Metropolis-Hastings samplers\nHierarchical models: Significantly outperforms alternative methods\nMixed models: Provides substantial efficiency gains over traditional approaches\nSpatial models: Shows dramatic improvements for Gaussian process spatial models\n\nTheoretical Guarantees\nThe Pólya-Gamma Gibbs sampler enjoys strong theoretical properties(Polson, Scott, and Windle 2013):\n\nUniform ergodicity: Proven by Choi and Hobert (2013), guaranteeing convergence and central limit theorems for Monte Carlo averages(Polson, Scott, and Windle 2013)\nNo tuning required: Unlike Metropolis-Hastings methods, the sampler requires no manual tuning\nExact sampling: Produces draws from the correct posterior distribution without approximation\n\n\n\n\n\n\n\nImportant Note\n\n\n\nThe theoretical guarantees hold under standard regularity conditions, and the method requires proper prior specification for optimal performance.\n\n\nBeyond Binary Logistic Regression\nThe Pólya-Gamma methodology extends naturally to various related models(Polson, Scott, and Windle 2013):\n\nNegative binomial regression: Direct application using the same data-augmentation scheme\nMultinomial logistic models: Extended through partial difference of random utility models(Windle, Polson, and Scott 2014)\nMixed effects models: Seamless incorporation of random effects structures\nSpatial models: Efficient inference for spatial count data models\n\n\n\n16.17.5 Modern Applications\nRecent developments have expanded the methodology’s applicability[Windle, Polson, and Scott (2014)](Zhang, Datta, and Banerjee 2018):\n\nGaussian process classification: Scalable variational approaches using Pólya-Gamma augmentation\nDeep learning: Integration with neural network architectures for Bayesian deep learning\nState-space models: Application to dynamic binary time series models\n\nThe Pólya-Gamma methodology represents a fundamental advancement in Bayesian computation for logistic models, combining theoretical elegance with practical efficiency(Polson, Scott, and Windle 2013). Its introduction of the Pólya-Gamma distribution class and the associated data-augmentation strategy has enabled routine application of Bayesian methods to complex hierarchical models that were previously computationally prohibitive(Polson, Scott, and Windle 2013).\nThe BayesLogit package provides researchers and practitioners with efficient, well-tested implementations of these methods(Windle 2023). The combination of exact inference, computational efficiency, and theoretical guarantees makes the Pólya-Gamma approach the method of choice for Bayesian logistic regression in most practical applications(Polson, Scott, and Windle 2013).\nAs computational demands continue to grow and models become increasingly complex, the Pólya-Gamma methodology’s advantages become even more pronounced, establishing it as an essential tool in the modern Bayesian statistician’s toolkit (Tiao (2019)). Ongoing research continues to extend the Pólya-Gamma methodology to new domains, including high-dimensional settings, nonparametric models, and integration with modern machine learning frameworks.\n\n\n\n\nBertsimas, Dimitris, Angela King, and Rahul Mazumder. 2016. “Best Subset Selection via a Modern Optimization Lens.” The Annals of Statistics 44 (2): 813–52.\n\n\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation. Springer Science & Business Media.\n\n\nGeorge, Edward I., and Robert E. and McCulloch. 1993. “Variable Selection via Gibbs Sampling.” Journal of the American Statistical Association 88 (423): 881–89.\n\n\nJeffreys, Harold. 1998. Theory of Probability. Third Edition, Third Edition. Oxford Classic Texts in the Physical Sciences. Oxford, New York: Oxford University Press.\n\n\nMitchell, T. J., and J. J. Beauchamp. 1988. “Bayesian Variable Selection in Linear Regression.” Journal of the American Statistical Association 83 (404): 1023–32. https://www.jstor.org/stable/2290129.\n\n\nPolson, Nicholas G., and James G. Scott. 2011. “Shrink Globally, Act Locally: Sparse Bayesian Regularization and Prediction.” In Bayesian Statistics 9, edited by José M. Bernardo, M. J. Bayarri, James O. Berger, A. P. Dawid, David Heckerman, Adrian F. M. Smith, and Mike West, 0. Oxford University Press.\n\n\nPolson, Nicholas G., James G. Scott, and Jesse Windle. 2013. “Bayesian Inference for Logistic Models Using Pólya–Gamma Latent Variables.” Journal of the American Statistical Association 108 (504): 1339–49.\n\n\nTiao, Louis. 2019. “Pólya-Gamma Bayesian Logistic Regression.” Blog post.\n\n\nWindle, Jesse. 2023. “BayesLogit: Bayesian Logistic Regression.” R package version 2.1.\n\n\nWindle, Jesse, Nicholas G. Polson, and James G. Scott. 2014. “Sampling Polya-Gamma Random Variates: Alternate and Approximate Techniques.” arXiv. https://arxiv.org/abs/1405.0506.\n\n\nZhang, Yichi, Anirban Datta, and Sudipto Banerjee. 2018. “Scalable Gaussian Process Classification with Pólya-Gamma Data Augmentation.” arXiv Preprint arXiv:1802.06383. https://arxiv.org/abs/1802.06383.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "17-tree.html",
    "href": "17-tree.html",
    "title": "17  Tree Models",
    "section": "",
    "text": "17.1 Finding Good Bayes Predictors\nWe’ve used decision trees before to describe the decision-making process as a sequence of actions and conditions. In this section, we’ll use decision trees to make predictions. You can think of a prediction as a decision task, where you need to decide which value of \\(y\\) to use for a given \\(x\\). Similar to decision tree predictive tree model is a nested sequence of if-else statements that map any input data point \\(x\\) to a predicted output \\(y\\). Each if-else statement checks a feature of \\(x\\) and sends the data left or right along the tree branch. At the end of the branch, a single value of \\(y\\) is predicted.\nFigure 17.1 shows a decision tree for predicting a chess piece given a four-dimension input vector that describes the types of moves available to the piece. The tree is a sequence of nested if-else statements that check the values of the input vector. The tree has six leaves, one for each of the chess pieces and has a depth of four. The tree is a predictive model that maps a four-dimensional input vector to a single output categorical value with six possible values.\nThe prediction algorithm is simple. Start at the root node and move down the tree until you reach a leaf node. The process of building a tree, given a set of training data, is more complicated and has three main components:\nThe splitting process is the most important part of the tree-building process. At each step the splitting process need to decide on the feature index \\(j\\) to be used for splitting and the location of the split. For binary variable there is only one possible split location, but for continuous variables there are many possible split locations. The goal is to find the split that creates the most homogeneous subsets. In the case of regression trees, the best split is the one that minimizes the sum of squared errors. In the case of classification trees, the best split is the one that minimizes the Gini impurity. The Gini impurity is a measure of how homogeneous the subsets are.\nThe widely used CART algorithm uses the table \\(X \\in \\mathbb{R}^{n\\times p}\\) during the splitting process. It loops over every element \\(x_{ij}\\) and finds the best split \\(x_{ij}\\) that creates the most homogeneous subsets. A split creates two subsets the left subset \\(X_L = \\{x  \\mid x_j &lt;  x_{ij}\\}\\) and the right subset \\(X_R = \\{x \\mid x_j \\ge  x_{ij}\\}\\).\nTo measure how homogeneous the sets are, tn case of regression, we calculate \\(\\hat y_L = \\mathrm{Average}(y_i), i \\in I_L\\) and \\(\\hat y_R = \\mathrm{Average}(y_i), i \\in I_R\\), where \\(I_L\\) and \\(I_R\\) are indenes of observed inputs \\(x_i\\) in the left and right subsets. The best split is the one that minimizes the sum of squared errors. \\[\n\\text{SSE} = \\sum_{i\\in I_L}^n (y_i - \\hat{y}_L)^2 + \\sum_{i\\in I_R}^n (y_i - \\hat{y}_R)^2\n\\]\nLet’s start with a quick demo and look at the data. We’ll use the Hitters data set from the ISLR package. The data set contains information about Major League Baseball players. The goal is to predict the salary of a player based on his performance and experience. The data set contains 322 observations and 20 variables. The three variable we are to analyze are:\nThe question we are interested in is, does experience and performance effect the salary of a baseball player?\nSeems like there is a relation between number of years, number of hits and the salary. Left bottom corner of the input space is mostly occupied by the low salary players. We will use the CART algorithm to find the optimal splits\nWe use tree function that has a similar syntax to the lm. Then we cal the prune.tree to find the best tree with 3 leaves (terminal nodes). Each terminal node corresponds to a region\nThe prediction is rather straightforward. The tree divides the predictor space-that is, the set of possible values for \\(x_1, x_2, \\ldots, x_p\\) - into \\(J\\) distinct and non-overlapping boxes, \\(R_1,R_2,...,R_J\\). For every observation that falls into the region \\(R_j\\), we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_j\\).\n\\[\nf(x) = \\sum_{j=1}^J\\bar{y}_jI(x \\in R_j)\n\\]\n\\[\n\\bar{y}_j = \\text{Average}(y_i \\mid x_i \\in R_j)\n\\]\nTge overall goal of building a tree is to find find regions that lead to minima of the total Residual Sum of Squares (RSS) \\[\n\\mathrm{RSS} = \\sum_{j=1}^J\\sum_{i \\in R_j}(y_i - \\bar{y}_j)^2 \\rightarrow \\mathrm{minimize}\n\\]\nUnfortunately, it is computationally infeasible (NP-hard problem) to consider every possible partition of the feature space into \\(J\\) boxes. We can find a good approximate solution, using top-down approach (the CART algorithm). As mentioned earlier at each iteatoin we decide on: which variable \\(j\\) to split and split point \\(s\\). \\[\nR_1(j, s) = \\{x\\mid x_j &lt; s\\} \\mbox{ and } R_2(j, s) = \\{x\\mid x_j \\ge s\\},\n\\] thus, we seek to minimize (in case of regression tree) \\[\n\\min_{j,s}\\left[ \\sum_{i:x_i\\in  R_1}(y_i - \\bar{y}_1)^2 + \\sum_{i:x_i  \\in R_2}(y_i - \\bar{y}_2)^2\\right]\n\\] As a result, every observed input point belongs to a single region.\nNow let’s discuss how many regions we should have. At one extreme end, we can have \\(n\\) regions, one for each observation. Then the tree model will work similar to the one-nearest neighbor model. At the other end, we can have one big region for the entire input space and then every prediction will be the same (average across observed \\(y\\)’s). Both models can be used but usually the best one is in the middle. The number of regions (branches) controls the complexity of the model. We need to find a good size on the variance-bias scale. A smaller tree with fewer splits (that is, fewer regions \\(R_1,...,R_J\\)) might lead to lower variance and better interpretation at the cost of a little bias.\nHow do we construct a tree with a “manageable” number of branches? This is accomplished through the steps of forward tree construction and backward pruning. The forward step is a greedy algorithm that begins with a single region and divides it into two. This procedure is repeated until a certain stopping criterion is met. A practical method is to continue building the tree until the Residual Sum of Squares (RSS) plateaus. However, this method can be myopic as an initially unproductive split might be followed by a highly beneficial one, leading to a significant decrease in RSS in subsequent iterations. A more effective strategy is to grow an extensive tree \\(T_0\\), and then trim it down to obtain a subtree. The size of the subtree can be determined using cross-validation. However, be aware that the number of subtrees can be exponential!\nInstead of considering all possible sub-trees, we will do cost complexity pruning - also known as weakest link pruning. We consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\). For each value of \\(\\alpha\\) there corresponds a subtree \\(T \\subset T_0\\) such that minimizes \\[\n\\sum_{m=1}^{|T|}\\sum_{i:x_i\\in R_m}(y_i - \\bar{y}_m)^2 + \\alpha |T|\n\\] The parameter \\(\\alpha\\) balances the complexity of the subtree and its adherence to the training data. When we increment \\(\\alpha\\) starting from zero, branches are predictably and sequentially pruned from the tree, making it straightforward to acquire the entire series of subtrees as a function of \\(\\alpha\\). We determine the optimal value \\(\\hat \\alpha\\) through cross-validation. Afterward, we refer back to the complete data set and extract the subtree that corresponds to \\(\\hat \\alpha\\).\nLet’s return back to the Baseball example\nLet’s find the best tree\nSize of 3 seems good!\nBayesian methods tackle the problem of good predictive performance in a number of ways. The goal is to find a good predictive MSE \\(E_{Y,\\hat{Y}}(\\Vert\\hat{Y} - Y \\Vert^2)\\). First, Stein shrinkage (a.k.a regularization with an \\(\\ell_2\\) norm) has long been known to provide good mean squared error properties in estimation, namely \\(E(||\\hat{\\theta} - \\theta||^2)\\) as well. These gains translate into predictive performance (in an iid setting) for \\(E(||\\hat{Y}-Y||^2)\\). One of the main issues is how to tune the amount of regularisation (a.k.a prior hyper-parameters). Stein’s unbiased estimator of risk provides a simple empirical rule to address this problem, as does cross-validation. From a Bayes perspective, the marginal likelihood (and full marginal posterior) provides a natural method for hyper-parameter tuning. The issue is computational tractability and scalability. The posterior for \\((W,b)\\) is extremely high dimensional and multimodal and posterior MAP provides good predictors \\(\\hat{Y}(X)\\).\nBayes conditional averaging can also perform well in high dimensional regression and classification problems. High dimensionality brings with it the curse of dimensionality and it is instructive to understand why certain kernel can perform badly.\nAdaptive Kernel predictors (a.k.a. smart conditional averager) are of the form\n\\[\n\\hat{Y}(X) = \\sum_{r=1}^R K_r ( X_i , X ) \\hat{Y}_r (X)\n\\]\nHere \\(\\hat{Y}_r(X)\\) is a deep predictor with its own trained parameters. For tree models, the kernel \\(K_r( X_i , X)\\) is a cylindrical region \\(R_r\\) (open box set). Figure Figure 17.2 illustrates the implied kernels for trees (cylindrical sets) and random forests. Not too many points will be neighbors in high dimensional input space.\nConstructing the regions is fundamental to reduce the curse of dimensionality. It is useful to imagine a very large dataset, e.g. 100k images and think about how a new image’s input coordinates, \\(X\\), are “neighbors” to data point in the training set. Our predictor will then be a smart conditional average of the observed outputs, \\(Y\\), for our neighbors. When \\(p\\) is large, spheres (\\(L^2\\) balls or Gaussian kernels) are terrible: either no points or all points are “neighbors” of the new input variable. Trees are good as not too many “neighbors”.\nTo illustrate the problem further, Figure Figure 17.3 below shows the 2D image of 1000 uniform samples from a 50-dimensional ball \\(B_{50}\\). The image is calculated as \\(w^T Y\\), where \\(w = (1,1,0,\\ldots,0)\\) and \\(Y \\sim U(B_{50})\\). Samples are centered around the equators and none of the samples fall close to the boundary of the set.\nAs dimensionality of the space grows, the variance of the marginal distribution goes to zero. We can empirically see it from Figure Figure 17.4, which shows histogram of 1D image of uniform sample from balls of different dimensionality, i.e. \\(e_1^T Y\\), where \\(e_1 = (1,0,\\ldots,0)\\).\nSimilar central limit results were known to Maxwell who showed that random variable \\(w^TY\\) is close to standard normal, when \\(Y \\sim U(B_p)\\), \\(p\\) is large, and \\(w\\) is a unit vector (lies on the boundary of the ball). For the history of this fact, see Diaconis and Freedman (1987). More general results in this direction were obtained in Klartag (2007). Further, Milman and Schechtman (2009) presents many analytical and geometrical results for finite dimensional normed spaces, as the dimension grows to infinity.\nDeep learning improves on this by performing a sequence of GLM-like transformations, effectively DL learns a distributed partition of the input space. Specifically, suppose that we have \\(K\\) partitions. Then the DL predictor takes the form of a weighted average or soft-max of the weighted average in case of classification of observations in this partition. Given a new high dimensional input \\(X_{\\mathrm{new}}\\), many deep learners are an average of learners obtained by our hyper-plane decomposition. Generically, we have\n\\[\n\\hat{Y}(X) = \\sum_{k \\in K} w_k(X)\\hat{Y}_k(X)\n\\]\nwhere \\(w_k\\) are the weights learned in region \\(K\\), and \\(w_k(X)\\) is an indicator of the region with appropriate weighting given the training data. Where \\(w_k\\) is a weight which also indicates which partition the new \\(X_{new}\\) lies in.\nThe partitioning of the input space by a deep learner is similar to the one performed by decision trees and partition-based models such as CART, MARS, RandomForests, BART. However, trees are more local in the regions that they use to construct their estimators within a region. Each neuron in deep learning model corresponds to a manifold that divides the input space. In case of ReLU activation function \\(f(x) = \\max(0,x)\\) the manifold is simply a hyperplane and neuron gets activated when the new observation is on the “right” side of this hyperplane, the activation amount is equal to how far from the boundary the given point is. For example in two dimensions, three neurons with ReLU activation functions will divide the space into seven regions, as shown on Figure Figure 17.5.\nThe key difference then between tree-based architecture and neural network based models is the way hyper-planes are combined. Figure Figure 17.6 shows the comparison of space decomposition by hyperplanes as performed by a tree-based and neural network architectures. We compare a neural network with two layers (bottom row) with tree model trained with CART algorithm (top row). The network architecture used is:\n\\[\nY =  \\mathrm{softmax}(w^0Z^2 + b^0)\\\\\nZ^2 =  \\tanh(w^2Z^1 + b^2)\\\\\nZ^1 =  \\tanh(w^1X + b^1)\n\\]\nThe weight matrices for simple data \\(W^1, W^2 \\in \\mathbb{R}^{2 \\times 2}\\), for circle data \\(W^1 \\in \\mathbb{R}^{2 \\times 2}\\) and \\(W^2 \\in \\mathbb{R}^{3 \\times 2}\\), for spiral data we have \\(W^1 \\in \\mathbb{R}^{2 \\times 2}\\) and \\(W^2 \\in \\mathbb{R}^{4 \\times 2}\\). In our notations, we assume that the activation function is applied pointwise at each layer. An advantage of deep architectures is that the number of hyper-planes grow exponentially with the number of layers. The key property of an activation function (link) is \\(f(0) = 0\\) and it has zero value in certain regions. For example, hinge or rectified learner \\(\\max(x,0)\\), box car (differences in Heaviside) functions are very common. As compared to a logistic regression, rather than using \\(\\mathrm{softmax}(1/(1+e^{-x}))\\) in deep learning \\(\\tanh\\) is typically used for training.\nFormally, a Bayesian probabilistic approach (if computationally feasible) knows how to optimally weight predictors via a model averaging approach:\n\\[\n\\hat{Y}(X) = \\sum_{r=1}^R w_k \\hat{Y}_k(X)\n\\]\nwhere \\(\\hat{Y}_k(x) = E(Y \\mid X_k)\\). Such rules can achieve great out-of-sample performance. Amit, Blanchard, and Wilder (2000) discuss the striking success of multiple randomized classifiers. Using a simple set of binary local features, one classification tree can achieve 5% error on the NIST data base with 100,000 training data points. On the other hand 100 trees, trained under one hour, when aggregated yield an error rate under 7%. We believe that this stems from the fact that a sample from a very rich and diverse set of classifiers produces on average weakly dependent classifiers conditional on class. A Bayesian model of weak dependence is exchangeability.\nThe use of pooling (a.k.a. averaging) of multiple predictors is commonplace in machine learning. Ensemble methods (a.k.a. some form clever conditional averaging) is prevalent in high dimensions. One reason for these procedures is that it is relatively easy to find unbiased predictors, the caveat that they have large variances due to the dimensionality. The following result on exchangeability (Kingman, 1975) shows that we can simply use the \\(1/N\\)-rule and average to reduce risk. Specifically, suppose that we have \\(K\\) exchangeable, \\(\\mathbb{E} ( \\hat{Y}_i ) = \\mathbb{E} ( \\hat{Y}_{\\pi(i)} )\\), predictors\n\\[\n\\hat{Y} = ( \\hat{Y}_1 , \\ldots , \\hat{Y}_K )\n\\]\nFind \\(w\\) to attain \\(\\operatorname{argmin}_W E l( Y , w^T \\hat{Y} )\\) where \\(l\\) convex in the second argument;\n\\[\nE l( Y , w^T \\hat{Y} )  = \\frac{1}{K!} \\sum_\\pi E l( Y , w^T \\hat{Y} ) \\geq  E l \\left ( Y , \\frac{1}{K!} \\sum_\\pi w_\\pi^T \\hat{Y} )\\right ) =  E l \\left ( Y , (1/K) \\iota^T \\hat{Y} \\right )\n\\]\nwhere \\(\\iota = ( 1 , \\ldots ,1 )\\). Hence, the randomized multiple predictor with weights \\(w = (1/K)\\iota\\) provides close to optimal Bayes predictive performance. We now turn to algorithmic issues.\nAn alternative approach is to perform Bayesian model selection. Here we calculate the optimal Bayes weight for each predictor in accordance with Bayes Rule.\nWe formalize the gains in Classification Risk with the following discussion.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "17-tree.html#finding-good-bayes-predictors",
    "href": "17-tree.html#finding-good-bayes-predictors",
    "title": "17  Tree Models",
    "section": "",
    "text": "Figure 17.2\n\n\n\n\n\n\n\n\n\n\n\nFigure 17.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 17.4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 17.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 17.6",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "17-tree.html#ensemble-averaging-and-1n-rule",
    "href": "17-tree.html#ensemble-averaging-and-1n-rule",
    "title": "17  Tree Models",
    "section": "17.2 Ensemble Averaging and \\(1/N\\) Rule",
    "text": "17.2 Ensemble Averaging and \\(1/N\\) Rule\nIn high dimensions, when I have large number of predictive models that generate uncorrelated predictions, the optimal approach to generate a prediction is to average out predictions from those individual models/ weak predictors. This is called the \\(1/N\\) rule. The variance in the prediction is reduced by a factor of \\(N\\) when we average out \\(N\\) uncorrelated predictions. \\[\n\\mbox{Var} \\left ( \\frac{1}{N} \\sum_{i=1}^N \\hat y_i \\right ) = \\frac{1}{N^2} \\mbox{Var} \\left ( \\hat y_i \\right ) + \\frac{2}{N^2} \\sum_{i \\neq j} \\mbox{Cov} \\left ( \\hat y_i, \\hat y_j \\right )\n\\] In high dimensions it relatively easy to find uncorrelated predictors and those techniques prove to lead to a winning solution in many machine learning competitions. The \\(1/N\\) rule is optimal due to exchangeability of the weak predictors, see Polson, Sokolov, et al. (2017)\n\n\n17.2.1 Classification variance decomposition\nThe famous result is due to Cover who proved that k-nearest neighbors are at most twice the bayes risk.\nAmit, Blanchard, and Wilder (2000) use the population conditional probability distribution of a point \\(X\\) given \\(Y=c\\), denoted by \\(P_{c}\\), and the associated conditional expectation and variance operators will be denoted \\(E_{c}\\) and \\(V a r_{c}\\). Define the vectors of average aggregates conditional on class \\(c\\) as \\[\n\\begin{equation*}\nM_{c}(d)=E_{c}\\left[H_{\\mathbf{Q}}(X, d)\\right]=E\\left[H_{\\mathbf{Q}}(X, d) \\mid Y=c\\right] \\tag{8}\n\\end{equation*}\n\\]\nfor \\(d=1, \\ldots, K\\). The average conditional margin (ACM) for class \\(c\\) is defined as\n\\[\n\\begin{equation*}\n\\theta_{c}=\\min _{d \\neq c}\\left(M_{c}(c)-M_{c}(d)\\right) \\tag{9}\n\\end{equation*}\n\\]\nWe assume that \\(\\theta_{c}&gt;0\\). This assumption is very weak since it involves only the average over the population of class \\(c\\). It is quite natural since one would not expect good classification results when it is violated. Indeed as shown below it is satisfied in all cases.\nGiven that \\(\\theta_{c}&gt;0\\), the error rate for class \\(c\\) depends on the extent to which the aggregate classifier \\(H_{\\mathbf{Q}}(X, d)\\) is concentrated around \\(M_{c}(d)\\) for each \\(d=1, \\ldots, K\\). The simplest measure of concentration is the variance of \\(H_{\\mathbf{Q}}(X, d)\\) with respect to the distribution \\(P_{c}\\). Using Chebyshev’s inequality we write a coarse bound on the misclassification probability with respect to \\(P_{c}\\) as follows.\n\\[\n\\begin{align*}\nP_{c}\\left(C_{\\mathbf{Q}}(X) \\neq c\\right) \\leq & P_{c}\\left(H_{\\mathbf{Q}}(X, c)&lt;M_{c}(c)-\\theta_{c} / 2\\right) \\\\\n& +\\sum_{d \\neq c} P_{c}\\left(H_{\\mathbf{Q}}(X, d)&gt;M_{c}(d)+\\theta_{c} / 2\\right) \\\\\n\\leq & \\sum_{d=1}^{K} P_{c}\\left(\\left|H_{\\mathbf{Q}}(X, d)-M_{c}(d)\\right|&gt;\\theta_{c} / 2\\right) \\\\\n\\leq & \\frac{4}{\\theta_{c}^{2}} \\sum_{d=1}^{K} \\operatorname{Var}_{c}\\left[H_{\\mathbf{Q}}(X, d)\\right] . \\tag{10}\n\\end{align*}\n\\]\nOf course Chebyshev’s inequality is coarse and will not give very sharp results in itself, be we state it here as a landmark pointing to the relative importance of margin and variance, and to the tradeoff between the two quantities.\nWe rewrite each of the variance terms of the last equation as\n\\[\n\\begin{align*}\n\\operatorname{Var}_{c}\\left[E_{\\mathbf{Q}} h(X, d)\\right] & =E_{c}\\left[E_{\\mathbf{Q}} h(X, d)\\right]^{2}-\\left[E_{c} E_{\\mathbf{Q}} h(X, d)\\right]^{2} \\\\\n& =E_{\\mathbf{Q} \\otimes \\mathbf{Q}} E_{c}\\left[h_{1}(X, d) h_{2}(X, d)\\right]-E_{\\mathbf{Q} \\otimes \\mathbf{Q}}\\left[E_{c}\\left[h_{1}(X, d)\\right] E_{c}\\left[h_{2}(X, d)\\right]\\right] \\\\\n& =E_{\\mathbf{Q} \\otimes \\mathbf{Q}} \\operatorname{Cov}_{c}\\left[h_{1}(X, d), h_{2}(X, d)\\right] \\doteq \\gamma_{c, d} \\tag{11}\n\\end{align*}\n\\]\nwhere the notation \\(E_{\\mathbf{Q} \\otimes \\mathbf{Q}}\\) means that \\(h_{1}, h_{2}\\) are two classifiers sampled independently from the distribution \\(\\mathbf{Q}\\). We can therefore interpret this variance term as the conditional covariance of two classifiers independently sampled from \\(\\mathbf{Q}\\). We call this quantity the average conditional covariance (ACC). Even if \\(\\mathbf{Q}\\) is a discrete distribution, such as that provided by a particular run of \\(N\\) classifiers, when it is supported on a moderate number of classifiers, it is dominated by the conditional covariances of which there are order \\(N^{2}\\), and not the conditional variances of which there are order \\(N\\).\n\n\n17.2.2 Conditional and unconditional dependence\nIt should be emphasized that two classifiers, provided that they achieve reasonable classification rate (that is, better than just picking a class at random) will not be unconditionally independent. If we do not know the class label of a point, and vector \\(\\left(h_{1}(X, i)\\right)_{i}\\) is large at class \\(c\\), then we actually change our expectations regarding \\(\\left(h_{2}(X, i)\\right)_{i}\\). On the other hand if we were given in advance the class label \\(Y\\), then knowing \\(h_{1}(X)\\) would hardly affect our guess about \\(h_{2}(X)\\). This is the motivation behind the notion of weak conditional dependence.\nThis is in contrast to the measure of dependence introduced in Dietterich (1998), which involves the unconditional covariance. The \\(\\kappa\\) statistic used there is\n\\[\n\\kappa\\left(h_{1}, h_{2}\\right)=\\frac{\\sum_{d} \\operatorname{Cov}\\left[h_{1}(X, d), h_{2}(X, d)\\right]}{1-\\sum_{d} E h_{1}(X, d) E h_{2}(X, d)},\n\\]\nA simple decomposition of the numerator yields: \\(\\operatorname{Cov}\\left[h_{1}(X, d), h_{2}(X, d)\\right]=E \\operatorname{Cov}\\left[h_{1}(X, d), h_{2}(X, d) \\mid Y\\right]+\\operatorname{Cov}\\left[E\\left[h_{1}(X, d) \\mid Y\\right], E\\left[h_{2}(X, d) \\mid Y\\right]\\right]\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "17-tree.html#classification-trees",
    "href": "17-tree.html#classification-trees",
    "title": "17  Tree Models",
    "section": "17.3 Classification Trees",
    "text": "17.3 Classification Trees\nA classification tree operates much like a regression tree. The prediction is made based on the “majority vote”, which means selecting the class that appears most frequently within the region. The process of developing a classification tree is largely the same as that of a regression tree, involving recursive binary splitting. However, instead of using the Residual Sum of Squares (RSS), we use the classification error rate, which is the proportion of observations in that region that do not belong to the most prevalent class.\nWe start by introducing s0me notations \\[\np_{mk} = \\dfrac{1}{N_m}\\sum_{x_i \\in R_m} I(y_i=k),\n\\] which is proportion of observations of class \\(k\\) in region \\(m\\).\nThe classification then done as follows \\[\np_m = \\max_k p_{mk},~~~ E_m = 1-p_m\n\\] i.e the most frequent observation in region \\(m\\)\nThen classification is done as follows \\[\nP(y=k) = \\sum_{j=1}^J p_j I(x \\in R_j)\n\\]\nAn alternative method to evaluate the quality of a split in a classification tree is through the use of the Gini Index or Cross-Entropy. Let’s consider a scenario where we have an equal number of observations in each class, say 400 in each.\nNow, suppose we create a tree that results in two regions: one with a distribution of (300,100) and the other with (100,300). This means that in the first region, 300 observations belong to one class and 100 to the other, and vice versa in the second region.\nConsider another scenario where we have a different tree that results in two regions with distributions of (200,400) and (200,0).\nIn both cases, the misclassification rate is 0.25, meaning that 25% of the observations are incorrectly classified. However, the second tree is more desirable. Why is that? The second tree has a region with no misclassifications at all (200,0), which means it’s perfectly classifying all observations in that region. This is an ideal situation in classification problems. On the other hand, the first tree, despite having the same overall misclassification rate, doesn’t have any region where all observations are correctly classified.\nThis illustrates that while the misclassification rate is a useful metric, it doesn’t always capture the full picture. Other metrics like the Gini Index or Cross-Entropy can provide a more nuanced view of the quality of a split, taking into account not just the overall error rate, but also the distribution of errors across different regions.\nAnother way to measure the quality of the split is to use the Gini Index and Cross-Entropy Say, I have 400 observations in each class (400,400). I create a tree with two region: (300,100) and (100,300). Say I have another tree: (200,400) and (200,0). In both cases misclassification rate is 0.25. The later tree is preferable. We prefer to have more “pure nodes” and Gini index does a better job.\nThe Gini index: \\[\nG_m = \\sum_{k=1}^K p_{mk}(1-p_{mk})\n\\] It measures a variance across the \\(K\\) classes. It takes on a small value if all of the \\(p_{mk}\\)’s are close to zero or one\nAn alternative to the Gini index is cross-entropy (a.k.a deviance), given by \\[\nD_m = -\\sum_{k=1}^Kp_{mk}\\log p_{mk}\n\\] It is near zero if the \\(p_mk\\)’s are all near zero or near one. Gini index and the cross-entropy led to similar results.\nNow we apply the tree model to the Boston housing dataset.\n\nlibrary(MASS); data(Boston); attach(Boston)\nhead(Boston)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nblack\nlstat\nmedv\n\n\n\n\n0.01\n18\n2.3\n0\n0.54\n6.6\n65\n4.1\n1\n296\n15\n397\n5.0\n24\n\n\n0.03\n0\n7.1\n0\n0.47\n6.4\n79\n5.0\n2\n242\n18\n397\n9.1\n22\n\n\n0.03\n0\n7.1\n0\n0.47\n7.2\n61\n5.0\n2\n242\n18\n393\n4.0\n35\n\n\n0.03\n0\n2.2\n0\n0.46\n7.0\n46\n6.1\n3\n222\n19\n395\n2.9\n33\n\n\n0.07\n0\n2.2\n0\n0.46\n7.2\n54\n6.1\n3\n222\n19\n397\n5.3\n36\n\n\n0.03\n0\n2.2\n0\n0.46\n6.4\n59\n6.1\n3\n222\n19\n394\n5.2\n29\n\n\n\n\n\n\nFirst we build a big tree\n\ntemp = tree(medv~lstat,data=Boston,mindev=.0001)\nlength(unique(temp$where)) # first big tree size\n\n 73\n\n\nThen prune it down to one with 7 leaves\n\nboston.tree=prune.tree(temp,best=7)\nlength(unique(boston.tree$where)) # pruned tree size\n\n 7\n\n\ntext(boston.tree,col=\"blue\",label=c(\"yval\"),cex=.8)\nboston.fit = predict(boston.tree) #get training fitted values\nplot(Boston$lstat,Boston$medv,cex=.5,pch=16) #plot data\noo=order(Boston$lstat)\nlines(Boston$lstat[oo],boston.fit[oo],col='red',lwd=3) #step function fit\ncvals=c(9.725,4.65,3.325,5.495,16.085,19.9) #cutpoints from tree\nfor(i in 1:length(cvals)) abline(v=cvals[i],col='magenta',lty=2) #cutpoints\n\n\n\n\n\n\n\n\n\n\nPick off dis,lstat,medv\n\ndf2=Boston[,c(8,13,14)] \nprint(names(df2))\n\n \"dis\"   \"lstat\" \"medv\" \n\n\nBuild the big tree\n\ntemp = tree(medv~.,df2,mindev=.0001)\nlength(unique(temp$where)) #\n\n 74\n\n\nThen prune it down to one with 7 leaves\n\nboston.tree=prune.tree(temp,best=7)\n\nplot(boston.tree,type=\"u\")# plot tree and partition in x.\ntext(boston.tree,col=\"blue\",label=c(\"yval\"),cex=.8)\npartition.tree(boston.tree)\n\n\n\n\n\n\n\n\n\n\nGet predictions on 2d grid\n\npv=seq(from=.01,to=.99,by=.05)\nx1q = quantile(df2$lstat,probs=pv)\nx2q = quantile(df2$dis,probs=pv)\nxx = expand.grid(x1q,x2q) #matrix with two columns using all combinations of x1q and x2q\ndfpred = data.frame(dis=xx[,2],lstat=xx[,1])\nlmedpred = predict(boston.tree,dfpred)\n\nMake perspective plot\n\npersp(x1q,x2q,matrix(lmedpred,ncol=length(x2q),byrow=T),\n      theta=150,xlab='dis',ylab='lstat',zlab='medv',\n      zlim=c(min(df2$medv),1.1*max(df2$medv)))\n\n\n\n\n\n\n\n\nAdvantages of Decision Trees:\nDecision trees are incredibly intuitive and simple to explain. They can be even more straightforward to understand than linear regression models. Some theorists argue that decision trees mimic human decision-making processes more accurately than other regression and classification methods we’ve discussed in previous chapters. Decision trees can be visually represented, making them easily interpretable, even for those without a deep understanding of the underlying mechanics, particularly when the trees are not overly complex. Decision trees can effortlessly manage qualitative predictors, eliminating the need to create dummy variables.\nDisadvantages of Decision Trees:\nLarge trees can exhibit high variance. This means that a minor change in the data can lead to a significant change in the final estimated tree, making the model unstable. Conversely, small trees, while more stable, may not be powerful predictors as they might oversimplify the problem. It can be challenging to find a balance between bias and variance when using decision trees. A model with too much bias oversimplifies the problem and performs poorly, while a model with too much variance overfits the data and may not generalize well to unseen data.\nThere are several techniques used to address the issue of overfitting in decision trees. We considered the pruning technique which reduces the complexity of the final classifier, and hence improve predictive accuracy by reducing overfitting. Two other methods are random forests and boosting. Random Forests is an ensemble method where multiple decision trees are created and their predictions are averaged (for regression) or majority voting is done (for classification). Boosting is another ensemble technique where trees are built sequentially such that each subsequent tree aims to reduce the bias of the combined classifier.\nIn the bagging approach, we treat the sample as if it were the population and then take iid draws. That is, you sample with replacement so that you can get the same original sample value more than once in a bootstrap sample.\nTo Bootsrap Aggregate (Bag) we:\n\nTake \\(B\\) bootstrap samples from the training data, each of the same size as the training data.\nFit a large tree to each bootstrap sample (we know how to do this fast!). This will give us \\(B\\) trees.\nCombine the results from each of the B trees to get an overall prediction.\n\nWhen the target variable \\(y\\) is numeric, the bagging process is straightforward. The final prediction is simply the average of the predictions from each of the \\(B\\) trees. However, when \\(y\\) is categorical, the process of combining results from different trees is less straightforward. One common approach is to use a voting system. In this system, each tree in the ensemble makes a prediction for a given input \\(x\\). The predicted category that receives the most votes (out of \\(B\\) total votes) is chosen as the final prediction. Another approach is to average the predicted probabilities \\(\\hat p\\) from each tree. This method can provide a more nuanced prediction, especially in cases where the voting results are close.\nDespite the potential benefits of averaging predicted probabilities, most software implementations of bagging for decision trees use the voting method. This is likely due to its simplicity and intuitive appeal. However, the best method to use can depend on the specific characteristics of the problem at hand.\nThe simple idea behind every ensemble modes is that variance of the average is lowe than variance of individual. Say we have \\(B\\) models \\(f_1(x),\\ldots,f_B(x)\\) then we combine those \\[\nf_{avg}(x) = \\dfrac{1}{B}\\sum_{b=1}^Bf_b(x)\n\\] Combining models helps fighting overfilling. On the negative side, it is harder to interpret those ensembles\nLet’s experiment with the number of trees in the model\n\nlibrary(randomForest)\nn = nrow(Boston)\nntreev = c(10,500,5000)\nfmat = matrix(0,n,3)\nfor(i in 1:3) {\n  rffit = randomForest(medv~lstat,data=Boston,ntree=ntreev[i],maxnodes=15)\n  fmat[,i] = predict(rffit)\n  print(mean((fmat[,i] - Boston$medv)^2, na.rm = TRUE))\n}\n\n 31\n 29\n 29\n\n\nLet’s plot the results\noo = order(Boston$lstat)\nfor(i in 1:3) {\n  plot(Boston$lstat,Boston$medv,xlab='lstat',ylab='medv',pch=16)\n  lines(Boston$lstat[oo],fmat[oo,i],col=i+1,lwd=3)\n  title(main=paste('bagging ntrees = ',ntreev[i]))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith 10 trees our fit is too jumbly.\nWith 1,000 and 5,000 trees the fit is not bad and very similar.\nNote that although our method is based multiple trees (average over) so we no longer have a simple step function!!\n\nRandom Forest\nIn the bagging technique, models can become correlated, which prevents the achievement of a \\(1/n\\) reduction in variance. This happens because most, if not all, of the trees will use the most influential predictor in the top split. As a result, bagged trees tend to look very similar to each other.\nRandom Forests, on the other hand, introduce an element of randomness that helps to decorrelate the trees, making the ensemble more robust and improving prediction accuracy. This randomness comes into play when considering a split in a tree. Instead of considering all \\(p\\) predictors for a split, a random sample of \\(m\\) predictors is chosen as split candidates. This subset of predictors is different for each split, which means that different trees are likely to use different predictors in the top split, leading to a more diverse set of trees.\nThe number of predictors considered at each split, \\(m\\), is typically chosen to be the square root of the total number of predictors, \\(p\\). This choice is a rule of thumb that often works well in practice, but it can be tuned based on the specific characteristics of the dataset.\nBy decorrelating the trees, Random Forests can often achieve better performance than bagging, especially when there’s a small number of very strong predictors in the dataset. In such cases, bagging can end up with an ensemble of very similar trees that all rely heavily on these strong predictors, while Random Forests can leverage the other, weaker predictors more effectively.\nOve of the “interpretation” tools that comes with ensemble models is importance rank: total amount that the deviance (loss) is decreased due to splits over a given predictor, averaged over all tree\n\nrf.boston = randomForest(medv~.,data=Boston,mtry=4,importance=TRUE,ntree=50)\nvarImpPlot(rf.boston,pch=21,bg=\"lightblue\",main=\"\")\n\n\n\n\n\n\n\n\n\nrf.boston = randomForest(medv~.,data=Boston,mtry=6,ntree=50, maxnodes=50)\nyhat.rf = predict(rf.boston,newdata=Boston)\noo=order(Boston$lstat)\nplot(Boston$lstat[oo],Boston$medv[oo],pch=21,bg=\"grey\", xlab=\"lstat\", ylab=\"medv\") #plot data\nlines(Boston$lstat[oo],yhat.rf[oo],col='red',lwd=3) #step function fit\n\n\n\n\n\n\n\n\nBoosting\nBoosting, like Random Forests, is a method that combines multiple trees to create a more powerful predictive model. However, the approach it takes is quite distinct.\nHere’s how Boosting works:\n\nInitially, a single decision tree is fitted to the data.\nThis initial tree is intentionally made weak, meaning it doesn’t perfectly fit the data.\nWe then examine the residuals, which represent the portion of the target variable \\(y\\) not explained by the weak tree.\nA new tree is then fitted to these residuals, essentially trying to predict the error of the first tree.\nThis new tree is also “weakened” or “shrunk”. The prediction from this tree is then added to the prediction of the first tree.\nThis process is repeated iteratively. In each iteration, a new tree is fitted to the residuals of the current ensemble of trees, shrunk, and then added to the ensemble.\nThe final model is the sum of all these “shrunk” trees. The key idea behind Boosting is to iteratively improve the model by focusing on the parts of the data that the current model is not explaining well (the residuals). Each new tree is trying to correct the mistakes of the ensemble of previous trees. By adding together many weak models (the shrunk trees), Boosting can often achieve a strong overall model.\n\nPick a loss function \\(L\\) that reflects setting; e.g., for continuous \\(y\\), could take \\(L(y_i , \\theta_i ) = (y_i - \\theta_i )^2\\) Want to solve \\[\\mathrm{minimize}_{\\beta \\in R^M} \\sum_{i=1}^n L \\left(y_i, \\sum_{j=1}^M \\beta_j \\cdot T_j(x_i)\\right)\\]\n\nIndexes all trees of a fixed size (e.g., depth = 5), so \\(M\\) is huge\nSpace is simply too big to optimize\nGradient boosting: basically a version of gradient descent that is forced to work with trees\nFirst think of optimization as \\(\\min_\\theta f (\\theta)\\), over predicted values \\(\\theta\\) (subject to \\(\\theta\\) coming from trees)\n\n\n\n\n\n\n\n\n\n\n\nSet \\(f_1(x)=0\\) (constant predictor) and \\(r_i=y_i\\)\nFor \\(b=1,2,\\ldots,B\\)\n\nFit a tree \\(f_b\\) with \\(d\\) splits to the training set \\((X,r)\\)\nUpdate the model \\[f(x) = f(x) +\\lambda f_b(x)\\]\nUpdate the residuals \\[r_i=r_i - \\lambda f_b(x)\\]\n\nHere are some boosting fits where we vary the number of trees, but fix the depth at 2 (suitable with 1 x) and shrinkage = \\(\\lambda\\) at .2.\n\nlibrary(gbm)\nboost.boston=gbm(medv~.,data=Boston,distribution=\"gaussian\",n.trees=5000,interaction.depth=4)\nyhat.boost=predict(boost.boston,newdata=Boston,n.trees=5000)\nmean((yhat.boost-Boston$medv)^2)\n\n 4e-04\n\n\n\nsummary(boost.boston, plotit=FALSE)\n\n\n\n\n\n\nvar\nrel.inf\n\n\n\n\nlstat\nlstat\n36.32\n\n\nrm\nrm\n30.98\n\n\ndis\ndis\n7.63\n\n\ncrim\ncrim\n5.09\n\n\nnox\nnox\n4.63\n\n\nage\nage\n4.50\n\n\nblack\nblack\n3.45\n\n\nptratio\nptratio\n3.11\n\n\ntax\ntax\n1.74\n\n\nrad\nrad\n1.17\n\n\nindus\nindus\n0.87\n\n\nchas\nchas\n0.39\n\n\nzn\nzn\n0.13\n\n\n\n\n\n\nplot(boost.boston,i=\"rm\")\nplot(boost.boston,i=\"lstat\")\n\n\n\n\n\n\n\n\n\n\nAdvantages of Boosting over Random Forests:\n\nPerformance: Boosting, in many cases, provides better predictive accuracy than Random Forests. By focusing on the residuals or mistakes, Boosting can incrementally improve model performance.\nModel Interpretability: While both methods are not as interpretable as a single decision tree, Boosting models can sometimes be more interpretable than Random Forests, especially when the number of weak learners (trees) is small.\n\nDisadvantages of Boosting compared to Random Forests:\n\nComputation Time and Complexity: Boosting can be more computationally intensive than Random Forests. This is because trees are built sequentially in Boosting, while in Random Forests, they are built independently and can be parallelized.\nOverfitting: Boosting can overfit the training data if the number of trees is too large, or if the trees are too complex. This is less of a problem with Random Forests, which are less prone to overfitting due to the randomness injected into the tree building process.\nOutliers: Boosting can be sensitive to outliers since it tries to correct the mistakes of the predecessors. On the other hand, Random Forests are more robust to outliers.\nNoise: Boosting can overemphasize instances that are hard to classify and can overfit to noise, whereas Random Forests are more robust to noise.\n\nRemember, the choice between Boosting and Random Forests (or any other model) should be guided by the specific requirements of your task, including the nature of your data, the computational resources available, and the trade-off between interpretability and predictive accuracy.\n\n\n\n\nAmit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. “Multiple Randomized Classifiers: MRCL.”\n\n\nDiaconis, Persi, and David Freedman. 1987. “A Dozen de Finetti-style Results in Search of a Theory.” In Annales de l’IHP Probabilités Et Statistiques, 23:397–423.\n\n\nKlartag, Bo’az. 2007. “A Central Limit Theorem for Convex Sets.” Inventiones Mathematicae 168 (1): 91–131.\n\n\nMilman, Vitali D, and Gideon Schechtman. 2009. Asymptotic Theory of Finite Dimensional Normed Spaces: Isoperimetric Inequalities in Riemannian Manifolds. Vol. 1200. Springer.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep Learning: A Bayesian Perspective.” Bayesian Analysis 12 (4): 1275–1304.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "18-forecasting.html",
    "href": "18-forecasting.html",
    "title": "18  Forecasting",
    "section": "",
    "text": "18.1 Structural time series models\nTime series data are everywhere, but time series modeling is a fairly specialized area within statistics and data science. This post describes the bsts software package, which makes it easy to fit some fairly sophisticated time series models with just a few lines of R code.\nTime series data appear in a surprising number of applications, ranging from business, to the physical and social sciences, to health, medicine, and engineering. Forecasting (e.g. next month’s sales) is common in problems involving time series data, but explanatory models (e.g. finding drivers of sales) are also important. Time series data are having something of a moment in the tech blogs right now, with Facebook announcing their “Prophet” system for time series forecasting (Sean J. Taylor and Ben Letham (2017)), and Google posting about its forecasting system in this blog (Eric Tassone and Farzan Rohani (2017)).\nThis post summarizes the bsts R package, a tool for fitting Bayesian structural time series models. These are a widely useful class of time series models, known in various literatures as “structural time series,” “state space models,” “Kalman filter models,” and “dynamic linear models,” among others. Though the models need not be fit using Bayesian methods, they have a Bayesian flavor and the bsts package was built to use Bayesian posterior sampling.\nThe bsts package is open source. You can download it from CRAN with the R command install.packages(\"bsts\"). It shares some features with Facebook and Google systems, but it was written with different goals in mind. The other systems were written to do “forecasting at scale,” a phrase that means something different in time series problems than in other corners of data science. The Google and Facebook systems focus on forecasting daily data into the distant future. The “scale” in question comes from having many time series to forecast, not from any particular time series being extraordinarily long. The bottleneck in both cases is the lack of analyst attention, so the systems aim to automate analysis as much as possible. The Facebook system accomplishes this using regularized regression, while the Google system works by averaging a large ensemble of forecasts. Both systems focus on daily data, and derive much of their efficiency through the careful treatment of holidays.\nThere are aspects of bsts which can be similarly automated, and a specifically configured version of bsts is a powerful member of the Google ensemble. However, bsts can also be configured for specific tasks by an analyst who knows whether the goal is short term or long term forecasting, whether or not the data are likely to contain one or more seasonal effects, and whether the goal is actually to fit an explanatory model, and not primarily to do forecasting at all.\nThe workhorse behind bsts is the structural time series model. These models are briefly described in the section Structural time series models. Then the software is introduced through a series of extended examples that focus on a few of the more advanced features of bsts. Example 1: Nowcasting includes descriptions of the local linear trend and seasonal state models, as well as spike and slab priors for regressions with large numbers of predictors. Example 2: Long term forecasting describes a situation where the local level and local linear trend models would be inappropriate. It offers a semilocal linear trend model as an alternative. Example 3: Recession modeling describes an model where the response variable is non-Gaussian. The goal in Example 3 is not to predict the future, but to control for serial dependence in an explanatory model that seeks to identify relevant predictor variables. A final section concludes with a discussion of other features in the package which we won’t have space (maybe “time” is a better word) to explore with fully fleshed out examples.\nA structural time series model is defined by two equations. The observation equation relates the observed data \\(y_t\\) to a vector of latent variables \\(\\alpha_t\\) known as the “state.” \\[\ny_t = Z_t^T\\alpha_t + \\epsilon_t.\n\\]\nThe transition equation describes how the latent state evolves through time. \\[\n\\alpha_{t+1} = T_t \\alpha_t + R_t \\eta_t.\n\\]\nThe error terms \\(\\epsilon_t\\) and \\(\\eta_t\\) are Gaussian and independent of everything else. The arrays \\(Z_t\\) , \\(T_t\\) and \\(R_t\\) are structural parameters. They may contain parameters in the statistical sense, but often they simply contain strategically placed 0’s and 1’s indicating which bits of \\(\\alpha_t\\) are relevant for a particular computation. An example will hopefully make things clearer.\nThe simplest useful model is the “local level model,” in which the vector \\(\\alpha_t\\) is just a scalar \\(\\mu_t\\). The local level model is a random walk observed in noise. \\[\\begin{align*}\ny_t = &\\mu_t + \\epsilon_t\\\\\n\\mu_{t+1} = &\\mu_t + \\eta_t.\n\\end{align*}\\] Here \\(\\alpha_t=\\mu_t\\) , and \\(Z_t\\) , \\(T_t\\), and \\(R_t\\) all collapse to the scalar value 1. Similar to Bayesian hierarchical models for nested data, the local level model is a compromise between two extremes. The compromise is determined by variances of \\(\\epsilon_t \\sim N(0,\\sigma^2)\\) and \\(\\eta_t \\sim N(0,\\tau^2)\\). If \\(\\tau^2=0\\) then \\(\\mu_t\\) is a constant, so the data are IID Gaussian noise. In that case the best estimator of \\(y_{t+1}\\) is the mean of \\(y_1,\\ldots,y_t\\). Conversely, if \\(\\sigma^2=0\\) then the data follow a random walk, in which case the best estimator of \\(y_{t+1}\\) is \\(y_t\\). Notice that in one case the estimator depends on all past data (weighted equally) while in the other it depends only on the most recent data point, giving past data zero weight. If both variances are positive then the optimal estimator of \\(y_{t+1}\\) winds up being “exponential smoothing,” where past data are forgotten at an exponential rate determined by the ratio of the two variances. Also notice that while the state in this model is Markov (i.e. it only depends on the previous state), the dependence among the observed data extends to the beginning of the series.\nFigure 18.1: Apple Adjusted Closing Price\n\n\n\n\n\n\n\n\n\n\n\nFigure 18.2: Apple Adjusted Closing Price\nIn the example above, one of the plots shows the price of the Apple stock from ‘2021-01-01’, to = 2022-12-31. The other plot is a sequence generated from a random walk model fitted to the Apple price data. Can you spot which one is which?\nStructural time series models are useful because they are flexible and modular. The analyst chooses the structure of \\(\\alpha_t\\) based on things like whether short or long term predictions are more important, whether the data contains seasonal effects, and whether and how regressors are to be included. Many of these models are standard, and can be fit using a variety of tools, such as the StructTS function distributed with base R or one of several R packages for fitting these models (with the dlm package (Petris (2010), Campagnoli, Petrone, and Petris (2009)) deserving special mention). The bsts package handles all the standard cases, but it also includes several useful extensions, described in the next few sections through a series of examples. Each example includes a mathematical description of the model and example bsts code showing how to work with the model using the bsts software. To keep things short, details about prior assumptions are largely avoided.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "18-forecasting.html#structural-time-series-models",
    "href": "18-forecasting.html#structural-time-series-models",
    "title": "18  Forecasting",
    "section": "",
    "text": "Example 18.1 (Nowcasting) S. Scott and Varian (2014) and S. L. Scott and Varian (2015) used structural time series models to show how Google search data can be used to improve short term forecasts (“nowcasts”) of economic time series. Figure below shows the motivating data set from S. Scott and Varian (2014), which is also included with the bsts package. The data consist of the weekly initial claims for unemployment insurance in the US, as reported by the US Federal Reserve. Like many official statistics they are released with delay and subject to revision. At the end of the week, the economic activity determining these numbers has taken place, but the official numbers are not published until several days later. For economic decisions based on these and similar numbers, it would help to have an early forecast of the current week’s number as of the close of the week. Thus the output of this analysis is truly a “nowcast” of data that has already happened rather than a “forecast” of data that will happen in the future.\n\nlibrary(bsts)     # load the bsts package\ndata(iclaims)     # bring the initial.claims data into scope\nplot(initial.claims$iclaimsNSA, lwd=2, ylab=\"Unemployment claims (thousand)\")\n\n\n\n\n\n\n\nFigure 18.3: Weekly initial claims for unemployment in the US.\n\n\n\n\n\nThere are two sources of information about the current value \\(y_t\\) in the initial claims series: past values \\(y_{t-\\tau}\\) describing the time series behavior of the series, and contemporaneous predictors \\(x_t\\) from a data source which is correlated with \\(y_t\\) , but which is available without the delay exhibited by \\(y_t\\) . The time series structure shows an obvious trend (in which the financial and housing crises in 2008 - 2009 are apparent) as well as a strong annual seasonal pattern. The external data source explored by Scott and Varian was search data from Google trends with search queries such as “how to file for unemployment” having obvious relevance.\nScott and Varian modeled the data using a structural time series with three state components:\n\ntrend \\(\\mu_t\\)\nseasonal pattern \\(\\tau_t\\)\n\nregression component \\(\\beta^Tx_t\\).\n\nThe model is \\[\\begin{align*}\ny_t = & \\mu_t + \\tau_t + \\beta^T x_t + \\epsilon_t\\\\\n\\mu_{t+1} = &\\mu_t + \\delta_t + \\eta_{0t}\\\\\n\\delta_{t+1} = &\\delta_t + \\eta_{1t}\\\\\n\\tau_{t+1} = &-\\sum_{s = 1}^{S-1}\\tau_{t} + \\eta_{2t}.\n\\end{align*}\\]\nThe trend component looks similar to the local level model above, but it has an extra term \\(\\delta_t\\) . Notice that \\(\\delta_t\\) is the amount of extra \\(\\mu\\) you can expect as \\(t\\rightarrow t+1\\), so it can be interpreted as the slope of the local linear trend. Slopes normally multiply some \\(x\\) variable, but in this case \\(x=\\Delta t\\), which omitted from the equation because it is always 1. The slope evolves according to a random walk, which makes the trend an integrated random walk with an extra drift term. The local linear trend is a better model than the local level model if you think the time series is trending in a particular direction and you want future forecasts to reflect a continued increase (or decrease) seen in recent observations. Whereas the local level model bases forecasts around the average value of recent observations, the local linear trend model adds in recent upward or downward slopes as well. As with most statistical models, the extra flexibility comes at the price of extra volatility.\nThe best way to understand the seasonal component \\(\\tau_t\\) is in terms of a regression with seasonal dummy variables. Suppose you had quarterly data, so that \\(S=4\\). You might include the annual seasonal cycle using 3 dummy variables, with one left out as a baseline. Alternatively, you could include all four dummy variables but constrain their coefficients to sum to zero. The seasonal state model takes the latter approach, but the constraint is that the \\(S\\) most recent seasonal effects must sum to zero in expectation. This allows the seasonal pattern to slowly evolve. Scott and Varian described the annual cycle in the weekly initial claims data using a seasonal state component with \\(S=52\\). Of course weeks don’t neatly divide years, but given the small number of years for which Google data are available the occasional one-period seasonal discontinuity was deemed unimportant.\nLet’s ignore the regression component for now and fit a bsts model with just the trend and seasonal components.\n\nss &lt;- AddLocalLinearTrend(list(), initial.claims$iclaimsNSA)\nss &lt;- AddSeasonal(ss, initial.claims$iclaimsNSA, nseasons = 52)\nmodel1 &lt;- bsts(initial.claims$iclaimsNSA,state.specification = ss,niter = 1000)\n\nThe first thing to do when fitting a bsts model is to specify the contents of the latent state vector \\(\\alpha_t\\). The bsts package offers a library of state models, which are included by adding them to a state specification (which is just a list with a particular format). The call to AddLocalLinearTrend above adds a local linear trend state component to an empty state specification (the list() in its first argument). The call to AddSeasonal adds a seasonal state component with 52 seasons to the state specification created on the previous line. The state vector \\(\\alpha_t\\) is formed by concatenating the state from each state model. Similarly, the vector \\(Z_t\\) is formed by concatenating the \\(Z\\) vectors from the two state models, while the matrices \\(T_t\\) and \\(R_t\\) are combined in block-diagonal fashion.\nThe state specification is passed as an argument to bsts, along with the data and the desired number of MCMC iterations. The model is fit using an MCMC algorithm, which in this example takes about 20 seconds to produce 1000 MCMC iterations. The returned object is a list (with class attribute bsts). You can see its contents by typing\n\nnames(model1)\n\n \"sigma.obs\"                  \"sigma.trend.level\"         \n \"sigma.trend.slope\"          \"sigma.seasonal.52\"         \n \"final.state\"                \"state.contributions\"       \n \"one.step.prediction.errors\" \"log.likelihood\"            \n \"has.regression\"             \"state.specification\"       \n \"prior\"                      \"timestamp.info\"            \n \"model.options\"              \"family\"                    \n \"niter\"                      \"original.series\"           \n\n\nThe first few elements contain the MCMC draws of the model parameters. Most of the other elements are data structures needed by various S3 methods (plot, print, predict, etc.) that can be used with the returned object. MCMC output is stored in vectors (for scalar parameters) or arrays (for vector or matrix parameters) where the first index in the array corresponds to MCMC iteration number, and the remaining indices correspond to dimension of the deviate being drawn.\nMost users won’t need to look inside the returned bsts object because standard tasks like plotting and prediction are available through familiar S3 methods. For example, there are several plot methods available.\n\npar(mar=c(4,4,2,0))\nplot(model1)\nplot(model1, \"components\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Components\n\n\n\n\n\n\n\nFigure 18.4: Structural time series model for unemployment claims\n\n\n\n\nThe Figure 18.4 (a) above shows the Posterior distribution of model state. Blue circles are actual data points. The Figure 18.4 (b) shows the individual state components. The plot looks fuzzy because it is showing the marginal posterior distribution at each time point.\nThe default plot method plots the posterior distribution of the conditional mean \\(Z_t^T\\alpha_t\\) given the full data \\(y=y_1,\\ldots,y_T\\). Other plot methods can be accessed by passing a string to the plot function. For example, to see the contributions of the individual state components, pass the string “components” as a second argument, as shown above. Figure below shows the output of these two plotting functions. You can get a list of all available plots by passing the string help as the second argument.\nTo predict future values there is a predict method. For example, to predict the next 12 time points you would use the following commands.\n\npar(mar=c(4,4,0,0))\npred1 &lt;- predict(model1, horizon = 12)\nplot(pred1, plot.original = 156)\n\n\n\n\n\n\n\n\nThe output of predict is an object of class bsts.prediction, which has its own plot method. The plot.original = 156 argument says to plot the prediction along with the last 156 time points (3 years) of the original series.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "18-forecasting.html#regression-with-spike-and-slab-priors",
    "href": "18-forecasting.html#regression-with-spike-and-slab-priors",
    "title": "18  Forecasting",
    "section": "18.2 Regression with spike and slab priors",
    "text": "18.2 Regression with spike and slab priors\nNow let’s add a regression component to the model described above, so that we can use Google search data to improve the forecast. The bsts package only includes 10 search terms with the initial claims data set, to keep the package size small, but S. Scott and Varian (2014) considered examples with several hundred predictor variables. When faced with large numbers of potential predictors it is important to have a prior distribution that induces sparsity. A spike and slab prior is a natural way to express a prior belief that most of the regression coefficients are exactly zero.\nA spike and slab prior is a prior on a set of regression coefficients that assigns each coefficient a positive probability of being zero. Upon observing data, Bayes’ theorem updates the inclusion probability of each coefficient. When sampling from the posterior distribution of a regression model under a spike and slab prior, many of the simulated regression coefficients will be exactly zero. This is unlike the “lasso” prior (the Laplace, or double-exponential distribution), which yields MAP estimates at zero but where posterior simulations will be all nonzero. You can read about the mathematical details of spike and slab priors in S. Scott and Varian (2014).\nWhen fitting bsts models that contain a regression component, extra arguments captured by ... are passed to the SpikeSlabPrior function from the BoomSpikeSlab package. This allows the analyst to adjust the default prior settings for the regression component from the bsts function call. To include a regression component in a bsts model, simply pass a model formula as the first argument.\n\n# Fit a `bsts` model with expected model size 1, the default.\nmodel2 &lt;- bsts(iclaimsNSA ~ .,state.specification = ss,niter = 1000,data = initial.claims)\n# Fit a `bsts` model with expected model size 5, to include more coefficients.\nmodel3 &lt;- bsts(iclaimsNSA ~ .,state.specification = ss,niter = 1000,data = initial.claims,expected.model.size = 5)  # Passed to SpikeSlabPrior.\n\nTo examine the output you can use the same plotting functions as before. For example, to see the contribution of each state component you can type\n\npar(mar=c(4,4,3,0))\nplot(model2, \"comp\")\n\n\n\n\n\n\n\n\nIt produces the contribution of each state component to the initial claims data, assuming a regression component with default prior. Compare to the previous model. The regression component is explaining a substantial amount of variation in the initial claims series.\nThere are also plotting functions that you can use to visualize the regression coefficients. The following commands plot posterior inclusion probabilities for predictors in the “initial claims” nowcasting example assuming an expected model size of 1 and 5.\npar(mar=c(4,0,0,0))\nplot(model2, \"coef\")\nplot(model3, \"coef\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Full\n\n\n\n\n\n\n\n\n\n\n\n(b) Sparse\n\n\n\n\n\n\n\nFigure 18.5: Variable Importance\n\n\n\nThe search term “unemployment office” shows up with high probability in both models. Increasing the expected model size from 1 (the default) to 5 allows other variables into the model, though “Idaho unemployment” is the only one that shows up with high probability.\nThose probabilities are calculated from the histogram of the samples of each \\(\\beta\\) calculated by the estimation algorithm (MCMC)\npar(mar=c(4,4,0,0))\n# unemployment.office\nhist(model3$coefficients[,10], breaks = 40, main=\"\",xlab=\"unemployment.office\", col=\"lightblue\")\n# pennsylvania.unemployment\nhist(model3$coefficients[,3], breaks = 40, main = \"\", xlab=\"pennsylvania.unemployment\", col=\"lightblue\")\nhist(model2$coefficients[,3], breaks = 40, main = \"\", xlab=\"pennsylvania.unemployment\", col=\"lightblue\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Sparse\n\n\n\n\n\n\n\n\n\n\n\n(b) Sparse\n\n\n\n\n\n\n\n\n\n\n\n(c) Full\n\n\n\n\n\n\n\nFigure 18.6: Sample from the distribution over two beta parameters",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "18-forecasting.html#model-diagnostics-did-the-google-data-help",
    "href": "18-forecasting.html#model-diagnostics-did-the-google-data-help",
    "title": "18  Forecasting",
    "section": "18.3 Model diagnostics: Did the Google data help?",
    "text": "18.3 Model diagnostics: Did the Google data help?\nAs part of the model fitting process, the algorithm generates the one-step-ahead prediction errors \\(y_t - E(y_t | Y_{t-1}, \\theta)\\), where \\(Y_{t-1}=y_1,\\ldots,y_{t-1}\\), and the vector of model parameters \\(\\theta\\) is fixed at its current value in the MCMC algorithm. The one-step-ahead prediction errors can be obtained from the bsts model by calling bsts.prediction.errors(model1).\nThe one step prediction errors are a useful diagnostic for comparing several bsts models that have been fit to the same data. They are used to implement the function CompareBstsModels, which is called as shown below.\n\nCompareBstsModels(list(\"Model 1\" = model1,\n               \"Model 2\" = model2,\n               \"Model 3\" = model3),\n          colors = c(\"black\", \"red\", \"blue\"))\n\n\n\n\nComparison of Errors for the three models.\n\n\n\n\nThe bottom panel shows the original series. The top panel shows the cumulative total of the mean absolute one step prediction errors for each model. The final time point in the top plot is proportional to the mean absolute prediction error for each model, but plotting the errors as a cumulative total lets you see particular spots where each model encountered trouble, rather than just giving a single number describing each model’s predictive accuracy. This figure shows that the Google data help explain the large spike near 2009, where model 1 accumulates errors at an accelerated rate, but models 2 and 3 continue accumulating errors at about the same rate they had been before. The fact that the lines for models 2 and 3 overlap in this figure means that the additional predictors allowed by the relaxed prior used to fit model 3 do not yield additional predictive accuracy.\n\nExample 18.2 (Long term forecasting) A common question about bsts is “which trend model should I use?” To answer that question it helps to know a bit about the different models that the bsts software package provides, and what each model implies. In the local level model the state evolves according to a random walk: \\[\n\\mu_{t+1}=\\mu_t+\\eta_t.\n\\] If you place your eye at time 0 and ask what happens at time \\(t\\) , you find that \\(\\mu_t \\sim N(\\mu_0,t\\sigma^2\\eta)\\). The variance continues to grow with \\(t\\), all the way to \\(t=\\infty\\). The local linear trend is even more volatile. When forecasting far into the future the flexibility provided by these models becomes a double edged sword, as local flexibility in the near term translates into extreme variance in the long term.\nAn alternative is to replace the random walk with a stationary AR process. For example \\[\n\\mu_{t+1}=\\rho\\mu_t+\\eta_t,\n\\]\nwith \\(\\eta_t \\sim N(0,\\sigma^2\\eta)\\) and \\(|\\rho|&lt;1\\). This model has stationary distribution \\[\n\\mu_{\\infty} \\sim N\\left(0,\\dfrac{\\sigma^2_{\\eta}}{1-\\rho^2}\\right),\n\\] which means that uncertainty grows to a finite asymptote, rather than infinity, in the distant future. bsts offers autoregressive state models through the functions AddAr, when you want to specify a certain number of lags, and AddAutoAr when you want the software to choose the important lags for you.\nA hybrid model modifies the local linear trend model by replacing the random walk on the slope with a stationary AR(1) process, while keeping the random walk for the level of the process. The bsts package refers to this is the “semilocal linear trend” model. \\[\\begin{align*}\n\\mu_{t+1}=&    \\mu_t+\\delta_t+\\eta_{0t}\\\\\n\\delta_{t+1}=& D+\\rho(\\delta_t-D)+\\eta_{1t}\n\\end{align*}\\] The \\(D\\) parameter is the long run slope of the trend component, to which \\(\\delta_t\\) will eventually revert. However \\(\\delta_t\\) can have short term autoregressive deviations from the long term trend, with memory determined by \\(\\rho\\). Values of \\(\\rho\\) close to 1 will lead to long deviations from \\(D\\). To see the impact this can have on long term forecasts, consider the time series of daily closing values for the S&P 500 stock market index over the last 5 years, shown below.\n\nGSPC = read.csv(\"../data/GSPC.csv\")\nGSPC = xts(GSPC, order.by = as.Date(rownames(GSPC), \"%Y-%m-%d\"))\nknitr::kable(head(GSPC))\nplot(GSPC$GSPC.Adjusted, main=\"\")\n\n\n\n\nDaily closing values for the S&P 500 stock market index\n\n\n\n\nConsider two forecasts of the daily values of this series for the next 360 days. The first assumes the local linear trend model. The second assumes the semilocal linear trend.\n\nsp500 = GSPC$GSPC.Adjusted\nss1 &lt;- AddLocalLinearTrend(list(), sp500)\nmodel1 &lt;- bsts(sp500, state.specification = ss1, niter = 1000)\nss2 &lt;- AddSemilocalLinearTrend(list(), sp500)\nmodel2 &lt;- bsts(sp500, state.specification = ss2, niter = 1000)\n\nFigure below shows long term forecasts of the S&P 500 closing values under the (left) local linear trend and (right) semilocal linear trend state models.\nload(\"../data/timeseries/model12-sp500.RData\")\npar(mar=c(4,4,0,1))\npred1 &lt;- predict(model1, horizon = 360)\npred2 &lt;- predict(model2, horizon = 360)\nplot(pred2, plot.original = 360, ylim = range(pred1))\nplot(pred1, plot.original = 360, ylim = range(pred1))\n\n\n\n\n\n\n\n\n\n\n\n(a) Semi-local trend\n\n\n\n\n\n\n\n\n\n\n\n(b) Local trend\n\n\n\n\n\n\n\nFigure 18.7: S&P 500 Prediction\n\n\n\nNot only the forecast expectations from the two models are different, but the forecast errors from the local linear trend model are implausibly wide, including a small but nonzero probability that the S&P 500 index could close near zero in the next 360 days. The error bars from the semilocal linear trend model are far more plausible, and more closely match the uncertainty observed over the life of the series thus far.\n\n\nExample 18.3 (Recession modeling using non-Gaussian data) Although we have largely skipped details about how the bsts software fits models, the Gaussian error assumptions in the observation and transition equations are important for the model fitting process. Part of that process involves running data through the Kalman filter, which assumes Gaussian errors in both the state and transition equations. In many settings where Gaussian errors are obviously inappropriate, such as for binary or small count data, one can introduce latent variables that give the model a conditionally Gaussian representation. Well known “data augmentation” methods exist for probit regression (Albert (1993)) and models with student-T errors (Rubin (2015)). Somewhat more complex methods exist for logistic regression (Frühwirth-Schnatter and Frühwirth (2007), Held and Holmes (2006), Gramacy and Polson (2012)) and Poisson regression (Frühwirth-Schnatter et al. (2008)). Additional methods exist for quantile regression (Benoit and Van den Poel (2012)), support vector machines (Polson and Scott (2011)), and multinomial logit regression (Frühwirth-Schnatter and Frühwirth (2010)). These are not currently provided by the bsts package, but they might be added in the future.\nTo see how non-Gaussian errors can be useful, consider the analysis done by Berge, Sinha, and Smolyansky (2016) who used Bayesian model averaging (BMA) to investigate which of several economic indicators would best predict the presence or absence of a recession. We will focus on their nowcasting example, which models the probability of a recession at the same time point as the predictor variables. Berge, Sinha, and Smolyansky (2016) also analyzed the data with the predictors at several lags.\nThe model used in Berge, Sinha, and Smolyansky (2016) was a probit regression, with Bayesian model averaging used to determine which predictors should be included. The response variable was the the presence or absence of a recession (as determined by NBER).\n\ndat &lt;- read.csv(\"../data/timeseries/rec_data_20160613.csv\")\nrec = ts(dat$nber, start=c(1973, 1), end=c(2016, 5), frequency=12)\nplot(rec, type='l', col='blue', ylab=\"Recession\")\n\n\n\n\nRecession periods identified by NBER\n\n\n\n\nThe BMA done by Berge, Sinha, and Smolyansky (2016) is essentially the same as fitting a logistic regression under a spike-and-slab prior with the prior inclusion probability of each predictor set to 1/2 . That analysis can be run using the BoomSpikeSlab R package (S. L. Scott (2022)), which is similar to bsts, but with only a regression component and no time series.\nThe logistic regression model is highly predictive, but it ignores serial dependence in the data. To capture serial dependence, consider the following dynamic logistic regression model with a local level trend model. \\[\\begin{align*}\n\\mathrm{logit}(p_t)= &  \\mu_t+\\beta^Tx_t\\\\\n\\mu_{t+1}= &            \\mu_t+\\eta_t\n\\end{align*}\\] Here \\(p_t\\) is the probability of a recession at time \\(t\\) ,and \\(x_t\\) is the set of economic indicators used by Berge, Sinha, and Smolyansky (2016) in their analysis. The variables are listed in the table below\n\n\n\n\n\n\n\n\nVariable\nDefinition/notes\nTransformation\n\n\n\n\nFinancial Variables\n\n\n\n\nSlope of yield curve\n10-year Treasury less 3-month yield\n\n\n\nCurvature of yield curve\n2 x 2-year minus 3-month and 10-year\n\n\n\nGZ index\nGilchrist and Zakrajsek (AER, 2012)\n\n\n\nTED spread\n3-month ED less 3-month Treasury yield\n\n\n\nBBB corporate spread\nBBB less 10-year Treasury yield\n\n\n\nS 500, 1-month return\n\n1-month log diff.\n\n\nS 500, 3-month return\n\n3-month log diff.\n\n\nTrade-weighted dollar\n\n3-month log diff.\n\n\nVIX\nCBOE and extended following Bloom\n\n\n\nMacroeconomic Indicators\n\n\n\n\nReal personal consumption expend.\n\n3-month log diff.\n\n\nReal disposable personal income\n\n3-month log diff.\n\n\nIndustrial production\n\n3-month log diff.\n\n\nHousing permits\n\n3-month log diff.\n\n\nNonfarm payroll employment\n\n3-month log diff.\n\n\nInitial claims\n4-week moving average\n3-month log diff.\n\n\nWeekly hours, manufacturing\n\n3-month log diff.\n\n\nPurchasing managers index\n\n3-month log dif\n\n\n\nFirst, we prepare the data by shifting it by \\(h\\), which is the forecast horison.\n\nh=0\n# predict h months ahead\ny.h &lt;- dat$nber[-(1:h)]\nhh &lt;- length(dat$nber) - h\ndat.h &lt;- dat[1:hh,-1]\n# h=0 is a special case\nif(h==0) y.h   &lt;- dat$nber\nif(h==0) dat.h &lt;- dat[,-1]\n\nTo fit this model, we can issue the commands shown below.\n\n# Because 'y' is 0/1 and the state is on the logit scale the default prior\n# assumed by AddLocalLevel won't work here, so we need to explicitly set the\n# priors for the variance of the state innovation errors and the initial value\n# of the state at time 0.  The 'SdPrior' and 'NormalPrior' functions used to\n# define these priors are part of the Boom package.  See R help for\n# documentation.  Note the truncated support for the standard deviation of the\n# random walk increments in the local level model.\n# A more complex model\nss &lt;- AddLocalLevel(list(),y.h,\n                    sigma.prior = SdPrior(sigma.guess = .1,\n                                          sample.size = 1,\n                                          upper.limit = 1),\n                    initial.state.prior = NormalPrior(0, 5))\n# Tell bsts that the observation equation should be a logistic regression by\n# passing the 'family = \"logit\"' argument.\nts.model &lt;- bsts(y.h ~ ., ss, data = dat.h, niter = 20000,family = \"logit\", expected.model.size = 10)\n\nNot let’s plot the results\npar(mar=c(4,4,0,0))\nplot(ts.model,\"coef\")\nplot(ts.model)\nlines(y.h, lwd=3,col=\"blue\")\n\n\n\n\n\n\n\n\n\n\n\nplot(ts.model,\"predictors\")\n\n\n\n\n\n\n\n\nNotice, the distribution of \\(p_t\\), it is moving to very large values during a recession, and to very small values outside of a recession. This effect captures the strong serial dependence in the recession data. Recessions are rare, but once they occur they tend to persist. Assuming independent time points is therefore unrealistic, and it substantially overstates the amount of information available to identify logistic regression coefficients.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "18-forecasting.html#final-remarks-on-structural-models",
    "href": "18-forecasting.html#final-remarks-on-structural-models",
    "title": "18  Forecasting",
    "section": "18.4 Final Remarks on Structural Models",
    "text": "18.4 Final Remarks on Structural Models\nThe preceding examples have shown that the bsts software package can handle several nonstandard, but useful, time series applications. These include the ability to handle large numbers of contemporaneous predictors with spike and slab priors, the presence of trend models suitable for long term forecasting, and the ability to handle non-Gaussian data. We have run out of space, but bsts can do much more.\nFor starters there are other state models you can use. Bsts has elementary support for holidays. It knows about 18 US holidays, and has capacity to add more, including holidays that occur on the same date each year, holidays that occur on a fixed weekday of a fixed month (e.g. 3rd Tuesday in February, or last Monday in November). The model for each holiday is a simple random walk, but look for future versions to have improved holiday support via Bayesian shrinkage.\nBsts offers support for multiple seasonalities. For example, if you have several weeks of hourly data then you will have an hour-of-day effect as well as a day-of-week effect. You can model these using a single seasonal effect with 168 seasons (which would allow for different hourly effects on weekends and weekdays), or you can assume additive seasonal patterns using the season.duration argument to AddSeasonal,\nss &lt;- AddSeasonal(ss, y, nseasons = 24)\nss &lt;- AddSeasonal(ss, y, nseasons = 7, season.duration = 24)\nThe latter specifies that each daily effect should remain constant for 24 hours. For modeling physical phenomena, bsts also offers trigonometric seasonal effects, which are sine and cosine waves with time varying coefficients. You obtain these by calling AddTrig. Time varying effects are available for arbitrary regressions with small numbers of predictor variables through a call to AddDynamicRegression.\nIn addition to the trend models discussed so far, the function AddStudentLocalLinearTrend gives a version of the local linear trend model that assumes student-t errors instead of Gaussian errors. This is a useful state model for short term predictions when the mean of the time series exhibits occasional dramatic jumps. Student-t errors can be introduced into the observation equation by passing the family = \"student\" argument to the bsts function call. Allowing for heavy tailed errors in the observation equation makes the model robust against individual outliers, while heavy tails in the state model provides robustness against sudden persistent shifts in level or slope. This can lead to tighter prediction limits than Gaussian models when modeling data that have been polluted by outliers. The observation equation can also be set to a Poisson model for small count data if desired.\nFinally, the most recent update to bsts supports data with multiple observations at each time stamp. The Gaussian version of the model is \\[\\begin{align*}\ny_{it} = &\\beta^T x_{it} + Z_t^T\\alpha_t + \\epsilon_{it}\\\\\n\\alpha_{t+1} = & T_t \\alpha_t + R_t \\eta_t,\n\\end{align*}\\] which is best understood as a regression model with a time varying intercept.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "18-forecasting.html#algorithms",
    "href": "18-forecasting.html#algorithms",
    "title": "18  Forecasting",
    "section": "18.5 Algorithms",
    "text": "18.5 Algorithms\nThe classic filtering and prediction algorithms for linear and Gaussian systems are described in Kalman (1960) and Kalman and Bucy (1961). Early work on discrete recursions for hidden Markov models are in Baum, Petrie, Soules and Weiss (1970) who use an EM-type algorithm, Viterbi (1967) who provides a modal state filter estimate and recursions developed in Lindgren (1978). While these can be used to evaluate the marginal likelihood for the parameters they are computationally too intensive to solve the filtering and learning, Lindgren (1978). Scott (2002) provides a review of FFBS algorithms for discrete HMMs.\nMarkov chain Monte Carlo (MCMC) algorithms for parameter learning and nonlinear non-Gaussian state space models were developed by Carlin, Polson and Stoffer (1992). For linear and Gaussian systems, Carter and Kohn (1994) provide the filter forward and backwards sample (FFBS) algorithm to draw the block of hidden states. Scott (2008) and Fruhwirth-Schnatter (2006) for a mixture of normals approximation for the multinomial logit. West and Harrison (1997) has conditionally conjugate prior where parameters can be marginalized out of the updating equations.\n\n18.5.1 Kalman Filtering\nThe Normal/ Normal Bayesian learning model provides the basis for shrinkage estimation of multiple means and the basis of the Kalman filter for dynamically tracking a path of an object.\nThe Kalman filter is arguable the most common application of Bayesian inference. The Kalman filter assumes a linear and Gaussian state-space model: \\[\ny_{t}=x_{t}+\\sigma\\varepsilon_{t}^{y}\\text{ and }x_{t}=x_{t-1}+\\sigma\n_{x}\\varepsilon_{t}^{x},\n\\] where \\(\\varepsilon_{t}^{y}\\) and \\(\\varepsilon_{t}^{x}\\) are i.i.d. standard normal and \\(\\sigma\\) and \\(\\sigma_{x}\\) are known. The observation equation posits that the observed data, \\(y_{t}\\), consists of the random-walk latent state, \\(x_{t}\\), that is polluted by noise, \\(\\sigma\\varepsilon_{t}^{y}\\). Further, \\(\\sigma_{x}/\\sigma\\) is the “signal-to-noise” ratio, measures the information content of the signal. As \\(\\sigma\\) increases relatively to \\(\\sigma_{x}\\), the observations become noisier and less informative. The model is initialized via a prior distribution over \\(x_{0}\\), which is for analytical tractability must be normally distributed, \\(x_{0}\\sim\\mathcal{N}\\left( \\mu_{0},\\sigma_{0}^{2}\\right)\\).\nThe posterior distribution solves the filtering problem and is defined recursively via Bayes rule: \\[\np\\left(  x_{t+1} \\mid y^{t+1}\\right)  =\\frac{p\\left(  y_{t+1} \\mid x_{t+1}\\right)\np\\left(  x_{t+1} \\mid y^{t}\\right)  }{p\\left(  y_{t+1} \\mid y^{t}\\right)  }\\propto\np\\left(  y_{t+1} \\mid x_{t+1}\\right)  p\\left(  x_{t+1} \\mid y^{t}\\right)  \\text{.}%\n\\] and the likelihood function, \\(p\\left( y_{t+1} \\mid x_{t+1}\\right)\\). The predictive distribution summarizes all of the information about \\(x_{t+1}\\) based on lagged observations. The likelihood function summarizes the new information in \\(y_{t+1}\\) about \\(x_{t+1}\\).\nThe Kalman filter relies on an inductive argument: assume that \\(p\\left( x_{t} \\mid y^{t}\\right) \\sim\\mathcal{N}\\left( \\mu_{t},\\sigma_{t}^{2}\\right)\\) and then verify that \\(p\\left( x_{t+1} \\mid y^{t+1}\\right) \\sim\\mathcal{N}\\left( \\mu_{t+1},\\sigma_{t+1}^{2}\\right)\\) with analytical expressions for the hyperparameters. To verify, note that since \\(p\\left(x_{t} \\mid y^{t}\\right) \\sim\\mathcal{N}\\left( \\mu_{t},\\sigma_{t}^{2}\\right)\\), \\(x_{t}=\\mu_{t}+\\sigma_{t}\\eta_{t}\\) for some standard normal \\(\\eta_{t}\\). Substituting into the state evolution, the predictive is \\(x_{t+1} \\mid y^{t}\\sim\\mathcal{N}\\left( \\mu_{t},\\sigma_{t}^{2}+\\sigma_{x}^{2}\\right)\\). Since \\(p\\left( y_{t+1} \\mid x_{t+1}\\right) \\sim\\mathcal{N}\\left(x_{t+1},\\sigma^{2}\\right)\\), the posterior is \\[\\begin{align*}\np\\left(  x_{t+1} \\mid y^{t+1}\\right)   &  \\propto p\\left(  y_{t+1} \\mid x_{t+1}\\right)\np\\left(  x_{t+1} \\mid y^{t}\\right)  \\propto\\exp\\left[  -\\frac{1}{2}\\left( \\frac{\\left(  y_{t+1}-x_{t+1}\\right)  ^{2}}{\\sigma^{2}}+\\frac{\\left( x_{t+1}-\\mu_{t}\\right)  ^{2}}{\\sigma_{t}^{2}+\\sigma_{x}^{2}}\\right)  \\right]\n\\\\\n&  \\propto\\exp\\left(  -\\frac{1}{2}\\frac{\\left(  x_{t+1}-\\mu_{t+1}\\right)\n^{2}}{\\sigma_{t+1}^{2}}\\right)\n\\end{align*}\\] where \\(\\mu_{t+1}\\) and \\(\\sigma_{t+1}^{2}\\) are computed by completing the square: \\[\\begin{equation}\n\\frac{\\mu_{t+1}}{\\sigma_{t+1}^{2}}=\\frac{y_{t+1}}{\\sigma^{2}}+\\frac{\\mu_{t}%\n}{\\sigma_{t}^{2}+\\sigma_{x}^{2}}\\text{ and }\\frac{1}{\\sigma_{t+1}^{2}}%\n=\\frac{1}{\\sigma^{2}}+\\frac{1}{\\sigma_{t}^{2}+\\sigma_{x}^{2}}\\text{.}\\nonumber\n\\end{equation}\\] Here, inference on \\(x_{t}\\) is merely running the Kalman filter, that is, sequential computing \\(\\mu_{t}\\) and \\(\\sigma_{t}^{2}\\), which are state sufficient statistics.\nThe Kalman filter provides an excellent example of the mechanics of Bayesian inference: given a prior and likelihood, compute the posterior distribution. In this setting, it is hard to imagine an more intuitive or alternative approach. The same approach applied to learning fixed static parameters. In this case, \\(y_{t}=\\mu+\\sigma\\varepsilon_{t},\\) where \\(\\mu\\sim\\mathcal{N}\\left( \\mu_{0},\\sigma_{0}^{2}\\right)\\) is the initial distribution. Using the same arguments as above, it is easy to show that \\(p\\left(\\mu \\mid y^{t+1}\\right) \\sim\\mathcal{N}\\left( \\mu_{t+1},\\sigma_{t+1}^{2}\\right)\\), where \\[\\begin{align*}\n\\frac{\\mu_{t+1}}{\\sigma_{t+1}^{2}}  &  =\\left(  \\frac{y_{t+1}}{\\sigma^{2}}+\\frac{\\mu_{t}}{\\sigma_{t}^{2}}\\right)  =\\frac{\\left(  t+1\\right) \\overline{y}_{t+1}}{\\sigma^{2}}+\\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\text{,}\\\\\n\\frac{1}{\\sigma_{t+1}^{2}}  &  =\\frac{1}{\\sigma^{2}}+\\frac{1}{\\sigma_{t}^{2}}=\\frac{\\left(  t+1\\right)  }{\\sigma^{2}}+\\frac{1}{\\sigma_{0}^{2}}\\text{,}\n\\end{align*}\\] and \\(\\overline{y}_{t}=t^{-1}\\sum_{t=1}^{t}y_{t}\\).\nNow, given this example, the same statements can be posed as in the state variable learning problem: it is hard to think of a more intuitive or alternative approach for sequential learning. In this case, researchers often have different feelings about assuming a prior distribution over the state variable and a parameter. In the state filtering problem, it is difficult to separate the prior distribution and the likelihood. In fact, one could view the initial distribution over \\(x_{0}\\), the linear evolution for the state variable, and the Gaussian errors as the “prior” distribution.\nNow consider,linear multivariate Gaussian state space model: \\[\\begin{align*}\ny_{t} &  =F_{t}x_{t}+\\varepsilon_{t} \\; \\; \\text{where} \\; \\; \\varepsilon_{t}\\sim\\mathcal{N}\\left( 0,\\Sigma_{t}\\right) \\\\\nx_{t} &  =G_{t}x_{t-1}+\\varepsilon_{t}^{x} \\; \\; \\text{where} \\; \\; \\varepsilon_{t}^x\\sim\\mathcal{N}\\left( 0,\\Sigma_{t}^x\\right)\n\\end{align*}\\] where we allow for heteroscedascity in the error variance-covariance matrices. We complete the model specification with a normal prior on the initial starting condition \\(x_{0}\\sim\\mathcal{N}\\left(  \\mu_{0},\\Sigma_{0}\\right)\\). It is important to recognize that \\(\\varepsilon_{t}\\) and \\(\\varepsilon_{t}^{x}\\) need only be conditionally normal. There are a number of distributions of interest \\[\\begin{align*}\n\\text{Filtering}  &  :p\\left(  x_{t}|y^{t}\\right)  \\text{ }t=1,...,T\\\\\n\\text{Forecasting}  &  :p\\left(  x_{t+1}|y^{t}\\right)  \\text{ } t=1,...,T\\\\\n\\text{Smoothing}  &  :p\\left(  x_{t}|y^{t+1}\\right)  \\text{ }t=1,...,T \\\\\n\\text{Prediction}  &  :p\\left(  y_{t+1}|y^{t}\\right)  \\text{ }t=1,...,T\n\\end{align*}\\] For known parameters with linearity and Gaussianity we have the following Kalman filter recursions for calculation these distributions.\nThe fundamental filtering relationship is based on the fact that the filtering distribution is of the form \\[\np(x_{t}|y^{t})\\sim\\mathcal{N}\\left(  \\mu_{t|t},\\Sigma_{t|t}\\right)\n\\;\\;\\mathrm{and}\\;\\;p(x_{t+1}|y^{t+1})\\sim\\mathcal{N}\\left(  \\mu\n_{t+1|t+1},\\Sigma_{t+1|t+1}\\right)\n\\] where \\((\\mu_{t+1|t+1},\\Sigma_{t+1|t+1})\\) are related to \\((\\mu_{t|t}%\n,\\Sigma_{t|t})\\) via the Kalman filter recursions. In the following, it is sometimes useful to write this as \\[\np(x_{t}|y^{t})\\sim\\mathcal{N}\\left(  \\mu_{t|t},\\Sigma_{t|t}\\right) \\; \\Rightarrow \\; x_{t}=\\mu_{t|t}+\\Sigma_{t|t}^{\\frac{1}{2}}\\widehat{\\varepsilon}_{t}%\n\\] where \\(\\widehat{\\varepsilon}_{t} \\sim \\mathcal{N}(0,1)\\). Before we derive the filtering recursions and characterize the state filtering distribution we first find the forecasting distribution. The predictive or forecast distribution is defined as follows.\nPredictive Distribution, \\(p( x_{t+1} | y^{t} )\\).\nThe key distributions in Bayes rule are the predictive and the conditional state posterior given by \\[\np( x_{t+1}|y^{t} )\\sim\\mathcal{N}\\left(  \\mu_{t+1|t},\\Sigma_{t+1|t}\\right)\n\\] To compute the predictive or forecasting distribution note that:\n\\[\np\\left(  x_{t+1}|y^{t}\\right)  =p\\left(  G_{t+1}x_{t}+\\varepsilon_{t+1}%\n^{x}|y^{t}\\right)  \\sim\\mathcal{N}\\left(  \\mu_{t+1|t},\\Sigma_{t+1|t} \\right)\n\\] where the predictive moments are\n\\[\\begin{align*}\n\\mu_{t+1|t}  &  =G_{t+1}\\mu_{t}\\\\\n\\Sigma_{t+1|t}  &  =G_{t+1}\\Sigma_{t}G_{t+1}^{\\prime}+\\Sigma_{t+1}^{x}.\n\\end{align*}\\] We now state and derive the main Kalman filtering recursions for linear Gaussian models with known parameters.\nFiltering Distribution The classic Kalman filter characterisation of the state filtering distributionp(x_{t+1}|y^{t+1})$ and moment recursions are given by \\[\np(x_{t+1}|y^{t+1})\\sim\\mathcal{N}\\left(  \\mu_{t+1|t+1},\\Sigma_{t+1|t+1}\\right)\n\\] The updated posterior means and variances are defined by \\[\\begin{align*}\n\\mu_{t+1|t+1} &  =\\mu_{t+1|t}+K_{t+1}e_{t+1}\\\\\n\\Sigma_{t+1|t+1} &  =(I-K_{t+1}F_{t+1})\\Sigma_{t+1|t}%\n\\end{align*}\\] where the Kalman gain \\(K_{t+1}\\) matrix and innovations vector \\(e_{t+1}\\) are \\[\\begin{align*}\nK_{t+1}  &  = \\Sigma_{t+1|t} F_{t+1}^{\\prime}\\left(  F_{t+1} \\Sigma_{t+1|t}\nF_{t+1}^{\\prime}+ \\Sigma_{t+1} \\right)  ^{-1}\\\\\ne_{t+1}  &  = y_{t+1} - F_{t+1} \\mu_{t+1|t}%\n\\end{align*}\\]\nTo prove this result we use the predictive distribution and an application of Bayes rule which implies that\n\\[\\begin{align*}\np\\left(  x_{t+1}|y^{t+1}\\right)   & = p\\left(  x_{t+1}|y_{t+1}%\n,y^{t}\\right) \\\\\n&  = \\frac{ p\\left(  y_{t+1}|x_{t+1}\\right)  p\\left(  x_{t+1}|y^{t}\\right)}{  p\\left(  y_{t+1}|y^t\\right) }\n\\text{.}%\n\\end{align*}\\] Under the normality assumption, the likelihood term is \\[\np( y_{t+1} | x_{t+1} ) = ( 2 \\pi )^{-\\frac{p}{2}} | \\Sigma_{t+1} |^{-\\frac{1}{2}}\n  \\exp \\left ( - \\frac{1}{2} ( y_{t+1} - F_{t+1} x_{t+1} )^{\\prime}\\Sigma_{t+1}^{-1} ( y_{t+1} - F_{t+1}\nx_{t+1} ) \\right )\n\\] Combining with the exponent term from the state predicitive distribution, then gives an exponent for the filtering distribution of the form \\[\n( y_{t+1} - F_{t+1} x_{t+1} )^{\\prime}\\Sigma_{t+1}^{-1} ( y_{t+1} - F_{t+1}\nx_{t+1} ) +( x_{t+1} - \\mu_{t+1|t} )^{\\prime}\\Sigma_{t+1|t}^{-1} ( x_{t+1} -\n\\mu_{t+1|t} )\n\\] Now we define the de-meaned state and innovations vectors,\n\\[\n\\tilde{x}_{t+1} = x_{t+1} - \\mu_{t+1|t} \\; \\text{and} \\; e_{t+1} = y_{t+1} - F_{t+1} \\mu_{t+1|t}\n\\] Using the usual completing the square trick we can re-write the exponent as \\[\n( e_{t+1} - F_{t+1} \\tilde{x}_{t+1} )^{\\prime}\\Sigma_{t+1}^{-1} ( e_{t+1} -\nF_{t+1} \\tilde{x}_{t+1} ) + \\tilde{x}_{t+1}^{\\prime}\\Sigma_{t+1|t}^{-1}\n\\tilde{x}_{t+1}\n\\] The sums of squares can be decomposed further as \\[\n\\tilde{x}_{t+1}^{\\prime}\\left(  F_{t+1}^{\\prime}\\Sigma_{t+1}^{\\prime}+\n\\Sigma_{t+1|t}^{-1} \\right)  \\tilde{x}_{t+1} + 2 \\tilde{x}_{t+1}^{\\prime\n}\\left(  F_{t+1}^{\\prime}\\Sigma_{t+1} e_{t+1} \\right)  + e_{t+1}^{\\prime\n}\\Sigma_{t+1}^{-1} e_{t+1}\n\\] The exponent is then a quadratic form implying that the vector \\(\\tilde{x}_{t+1}^{\\prime}\\) is normal distributed with the appropriate mean and variance-covariance matrix. The definitions are given by \\[\n\\Sigma_{t+1|t+1} F_{t+1}^{\\prime}\\Sigma_{t+1} e_{t+1} \\; \\; \\mathrm{and} \\; \\;\n\\Sigma_{t+1|t+1} = \\left(  F_{t+1}^{\\prime}\\Sigma_{t+1}^{-1} F_{t+1} +\n\\Sigma_{t+1|t}^{-1} \\right)  ^{-1}\n\\] respectively. Hence, we obtain the identity \\[\n\\Sigma_{t+1|t+1} = \\left(  F_{t+1}^{\\prime}\\Sigma_{t+1}^{\\prime}F_{t+1} +\n\\Sigma_{t+1|t}^{-1} \\right)  ^{-1} = ( I - K_{t+1} F_{t+1} ) \\Sigma_{t+1|t}\n\\] where \\(K_{t+1} = \\Sigma_{t+1|t} F_{t+1}^{\\prime}\\left(  F_{t+1} \\Sigma_{t+1|t}^{-1} F_{t+1}^{\\prime}+ \\Sigma_{t+1} \\right)^{-1}\\) is the Kalman gain matrix.\nThe mean of the \\(\\tilde{x}_{t+1}^{\\prime} = x_{t+1} - \\mu_{t+1|t}\\) distribution is then \\(K_{t+1}e_{t+1}\\). Un de-meaning the vector, we have \\(x_{t+1} = \\tilde{x}_{t+1} + \\mu_{t+1|t} =K_{t+1}e_{t+1}\\) leads to the following distributional result \\[\np( x_{t+1}|y^{t+1}) \\sim\\mathcal{N}\\left(  \\mu_{t+1|t+1},\\Sigma_{t+1|t+1}%\n\\right)  ,\n\\] The moments for the next filtering distribution are given by the classic recursions \\[\\begin{align*}\n\\mu_{t+1|t+1}  &  =\\mu_{t+1|t}+K_{t+1}\\left(  y_{t+1}-F_{t+1}\\mu_{t+1|t}\n\\right) \\\\\n\\Sigma_{t+1|t+1}  &  =\\left(  I-K_{t+1}F_{t+1}\\right)  \\Sigma_{t+1|t}\\text{.}%\n\\end{align*}\\] There are two other distributions to compute: the data predictive \\(p( y_{t+1} | y^t )\\) and the state smoothing distribution \\(p( x_t | y^{t+1} )\\). These are derived as follows.\nThe data predictive \\(p(y_{t+1}|y^t)\\) is determined from the observation equation and the state predictive distribution as follows \\[\\begin{align*}\ny_{t+1}  & =F_{t+1}x_{t+1}+\\varepsilon_{t+1} \\; \\text{with} \\;   \\varepsilon_{t+1}\\sim\\mathcal{N}\\left( 0,\\Sigma_{t+1}\\right) \\\\\np( x_{t+1} | y^t ) & \\sim \\mathcal{N} \\left ( \\mu_{t+1|t} , \\Sigma_{t+1|t} \\right )  \n\\end{align*}\\] Then substituting we have a predictive distribution for the next observation of the form \\[\np( y_{t+1}|y^{t}) \\sim\\mathcal{N}\\left(  F_{t+1}\\mu_{t+1|t},F_{t+1}%\n\\Sigma_{t+1|t} F_{t+1}+\\Sigma_{t+1} \\right)  \\text{.}%\n\\] The state smoothing distribution \\(p(x_t | y^{t+1} )\\) is determined from the joint distribution, \\(p( x_{t} , x_{t+1} | y^{t} )\\) as follows. First, factorise this joint distribution as \\[\np( x_{t+1} , x_{t} | y^{t} ) = p( x_{t+1} | x_{t} ) p( x_{t} | y^{t} )\n\\] Then calculate the conditional posterior distribution, \\(p( x_{t+1} | x_{t} , y^{t+1} )\\) by Bayes rule as \\[\np(x_{t+1}|x_{t},y^{t+1}) = \\frac{p\\left(  y_{t+1}|x_{t+1}\\right)\np(x_{t+1}|x_{t})p(x_{t}|y^{t})}{p(x_{t+1}|y^{t})}%\n\\] Now, we can view the system as having two observations on \\(x_{t+1}\\), namely \\[\\begin{align*}\nx_{t+1} &  =F_{t+1}x_{t}+\\Sigma_{t+1}^{x}\\epsilon_{t+1}^{x}\\\\\nx_{t} &  =\\mu_{t|t}+\\Sigma_{t|t}^{\\frac{1}{2}}\\hat{\\epsilon}_{t}%\n\\end{align*}\\] where the errors \\(\\epsilon_{t+1}^{x},\\hat{\\epsilon}_{t}\\) are independent.\nThis leads to a joint posterior with an exponent that is proportional to \\[\n(x_{t+1}-G_{t+1}x_{t})^{\\prime}\\left(  \\Sigma\n_{t+1}^{x}\\right)  ^{-1}(x_{t+1}-G_{t+1}x_{t})- (x_{t}-\\mu\n_{t|t})^{\\prime}\\Sigma_{t|t}^{-1}(x_{t}-\\mu_{t|t})\n\\] The first term comes from the state evolution and the second from the current filtering posterior. Completing the square gives \\[\n(x_{t+1}-G_{t+1}x_{t})^{\\prime}\\left(  \\Sigma_{t+1}^{x}\\right)  ^{-1}%\n(x_{t+1}-G_{t+1}x_{t})+(x_{t}-\\mu_{t|t})^{\\prime}\\Sigma_{t|t}^{-1}(x_{t}%\n-\\mu_{t|t})\n\\] \\[\n=(x_{t+1}-\\mu_{t+1|t})^{\\prime}\\Sigma_{t|t}^{-1}(x_{t+1}-\\mu_{t+1|t}%\n)+(x_{t}-\\mu_{t|t+1})^{\\prime}\\Sigma_{t|t+1}^{-1}(x_{t}-\\mu_{t|t+1})\n\\] which leads to the smoothed state moments \\[\\begin{align*}\n\\mu_{t|t+1} &  =\\Sigma_{t|t+1}\\left(  \\Sigma_{t|t}\\mu_{t|t}+F_{t+1}^{\\prime\n}\\left(  \\Sigma_{t+1}^{x}\\right)  ^{-1}x_{t+1}\\right)  \\\\\n\\Sigma_{t|t+1} &  =F_{t+1}^{\\prime}\\left(  \\Sigma_{t+1}^{x}\\right)\n^{-1}F_{t+1}+\\Sigma_{t|t}^{-1}%\n\\end{align*}\\] The Kalman filter recursions then follow by induction.\n\nExample 18.4 (Kalman Filter for Robot Localization) The Kalman filter is a powerful tool for estimating the state of a system, given noisy observations. It is used in a wide range of applications, from tracking the position of a robot to estimating the state of a financial market. The Kalman filter is particularly useful when the state of the system is not directly observable, and must be inferred from noisy measurements.\nOften KF is used for localization problem: given noisy measurements about the position of a robot and the motion model of the robot, the Kalman filter can estimate the true position of the robot. The Kalman filter is a recursive algorithm that estimates the state of a system at each time step, based on the state estimate from the previous time step and a new observation. We will use the language of state-space models in this example and will use the notation \\(x_t\\) to denote the state of the system at time \\(t\\) (parameter we are trying to estimate), and \\(y_t\\) to denote the observation at time \\(t\\) (observed data). The state-space model is given by \\[\n\\begin{aligned}\nx_{t+1} & = A x_t + w,\\quad w \\sim N(0,Q)\\\\\ny_t &=G x_t + \\nu, \\quad \\nu \\sim N(0,R)\\\\\nx_0 & \\sim N(\\hat{x}_0, \\Sigma_0),\n\\end{aligned}\n\\] where \\(A\\) is the state transition matrix, \\(G\\) is the observation matrix, \\(w\\) is the process noise, and \\(\\nu\\) is the observation noise. The process noise and observation noise are assumed to be independent and normally distributed with zero mean and covariance matrices \\(Q\\) and \\(R\\), respectively. The initial state \\(x_0\\) is assumed to be normally distributed with mean \\(\\hat{x}_0\\) and covariance matrix \\(\\Sigma_0\\). The Kalman filter provides a recursive algorithm for estimating the state of the system at each time step, based on the state estimate from the previous time step and a new observation. The state estimate is normal with mean \\(\\hat{x}_t\\) and the covariance matrix \\(\\Sigma_t\\). The Kalman filter equations are given by \\[\n\\begin{aligned}\n\\hat{x}_{t+1} &= A \\hat{x}_t + K_{t} (y_t - G \\hat{x}_t) \\\\\nK_{t} &= A \\Sigma_t G^T (G \\Sigma_t G^T + R)^{-1}\\\\\n\\Sigma_{t+1} &= A \\Sigma_t A^T - K_{t} G \\Sigma_t A^T + Q\n\\end{aligned}\n\\] Kalman filter performs a multivariate normal-normal update using \\(N(A \\hat{x}_t,,A \\Sigma_t A^T)\\) as prior and \\(N(y_t, G \\Sigma_t G^T + R)\\) as likelihood. The posterior distribution is \\(N(\\hat{x}_{t+1}, \\Sigma_{t+1})\\). Matrix \\(K_{t}\\) is called the Kalman gain and provides a weight on the residual between observed and prior \\(y_t - G \\hat{x}_t\\) in the update.\nAssume our robot starts at \\(\\hat x_0 = (0.2,-0.2)\\) (x-y Cartesian coordinates) and initial covariance is \\[\n\\Sigma_0 = \\begin{bmatrix} 0.4 & 0.3 \\\\ 0.3 & 0.45 \\end{bmatrix}.\n\\] The prior distribution of the robot’s position can be visualized in R with a contour plot.\n\nlibrary(mnormt)\nxhat &lt;- c(0.2, -0.2)\nSigma &lt;- matrix(c(0.4, 0.3, \n                  0.3, 0.45), ncol=2)\nx1 &lt;- seq(-2, 4,length=151)\nx2 &lt;- seq(-4, 2,length=151)\nf &lt;- function(x1,x2, mean=xhat, varcov=Sigma) \n  dmnorm(cbind(x1,x2), mean,varcov)\nz &lt;- outer(x1,x2, f)\nmycols &lt;- topo.colors(100,0.5)\nimage(x1,x2,z, col=mycols, main=\"Prior density\",\n      xlab=expression('x'[1]), ylab=expression('x'[2]))\ncontour(x1,x2,z, add=TRUE)\npoints(0.2, -0.2, pch=19)\ntext(0.1, -0.2, labels = expression(hat(x)), adj = 1)\n\n\n\n\n\n\n\n\nNow I get readings from GPS \\(y_0 = (2.3, -1.9)\\) and I know from the manufacturer that the GPS has a covariance matrix of \\(R = 0.5\\Sigma_0\\). We assume the measurement matrix \\(G\\) to be identity matrix, thus \\[\ny_t = Gx_t + \\nu_t = x_t + \\nu, \\quad \\nu \\sim N(0, R).\n\\]\n\nR &lt;- 0.5 * Sigma\nz2 &lt;- outer(x1,x2, f, mean=c(2.3, -1.9), varcov=R)\nimage(x1, x2, z2, col=mycols, main=\"Sensor density\")\ncontour(x1, x2, z2, add=TRUE)\npoints(2.3, -1.9, pch=19)\ntext(2.2, -1.9, labels = \"y\", adj = 1)\ncontour(x1, x2,z, add=TRUE)\npoints(0.2, -0.2, pch=19)\ntext(0.1, -0.2, labels = expression(hat(x)), adj = 1)\n\n\n\n\n\n\n\n\nNow we combine our initial guess about the location \\(x_0\\) with the measure noisy location data \\(y_0\\) to obtain posterior distribution of the location of the robot \\(p(x\\mid \\hat x_0, \\Sigma,R) = N(x\\mid \\hat x_f, \\Sigma_f)\\) \\[\n\\begin{aligned}\n\\hat{x}_f & = (\\Sigma^{-1} + R^{-1})^{-1} (\\Sigma^{-1} \\hat{x} + R^{-1} y) \\\\\n\\Sigma_f & = (\\Sigma^{-1} + R^{-1})^{-1}\n\\end{aligned}\n\\] Using the matrix inversion identity \\[\n\\begin{aligned}\n(A^{-1} + B^{-1})^{-1} & = A - A (A + B)^{-1}A = A (A + B)^{-1} B\n\\end{aligned}\n\\] I can write the above as: \\[\n\\begin{aligned}\n\\hat{x}_f & = (\\Sigma - \\Sigma (\\Sigma + R)^{-1}\\Sigma)(\\Sigma^{-1} \\hat{x} + R^{-1}y)\\\\\n& =\\hat{x} - \\Sigma (\\Sigma + R)^{-1} \\hat{x} + \\Sigma R^{-1} y -\\Sigma (\\Sigma + R)^{-1}\\Sigma R^{-1}y\\\\\n& = \\hat{x} + \\Sigma (\\Sigma + R)^{-1} (y - \\hat{x})\\\\\n& = (1.667, -1.333)\\\\\n\\Sigma_f &= \\Sigma - \\Sigma (\\Sigma + R)^{-1} \\Sigma\\\\\n&=\\left[\\begin{array}{lll}\n0.133 & 0.10\\\\\n0.100 & 0.15\n\\end{array}\n\\right]\n\\end{aligned}\n\\] In the more general case when \\(G\\) is not the identity matrix I have \\[\n\\begin{aligned}\n\\hat{x}_f & = \\hat{x} + \\Sigma G^T (G \\Sigma G^T + R)^{-1} (y - G \\hat{x})\\\\\n\\Sigma_f &= \\Sigma - \\Sigma G^T (G \\Sigma G^T + R)^{-1} G \\Sigma\n\\end{aligned}\n\\]\n\nG &lt;- diag(2)\ny &lt;- c(2.4, -1.9)\nxhatf &lt;- xhat + Sigma %*% t(G) %*% solve(G %*% Sigma %*% t(G) + R) %*% (y - G %*% xhat)\nSigmaf &lt;- Sigma - Sigma %*% t(G) %*% solve(G %*% Sigma %*% t(G) + R) %*% G %*% Sigma\nz3 &lt;- outer(x1, x2, f, mean=c(xhatf), varcov=Sigmaf)\nimage(x1, x2, z3, col=mycols,\n      xlab=expression('x'[1]), ylab=expression('x'[2]),\n      main=\"Filtered density\")\ncontour(x1, x2, z3, add=TRUE)\npoints(xhatf[1], xhatf[2], pch=19)\ntext(xhatf[1]-0.1, xhatf[2],\n     labels = expression(hat(x)[f]), adj = 1)\nlb &lt;- adjustcolor(\"black\", alpha=0.5)\ncontour(x1, x2, z, add=TRUE, col=lb)\npoints(0.2, -0.2, pch=19, col=lb)\ntext(0.1, -0.2, labels = expression(hat(x)), adj = 1, col=lb)\ncontour(x1, x2, z2, add=TRUE, col=lb)\npoints(2.3, -1.9, pch=19, col=lb)\ntext(2.2, -1.9,labels = \"y\", adj = 1, col=lb)\n\n\n\n\n\n\n\n\nNow I assume my robot moves according to the following model \\[\nx_t = A x_{t-1} + w_t, \\quad w_t \\sim N(0, Q)\n\\] with \\[\n\\begin{split}\nA = \\left( \\begin{array}{cc}\n1.2 & 0.0 \\\\\n0.0 & -0.2\n\\end{array} \\right),\n\\qquad\nQ = 0.3 \\Sigma\n\\end{split}\n\\] Then the next location is normally distributed with the parameters \\[\n\\begin{split}\nA= \\left(\\begin{array}{cc}\n1.2 & 0.0 \\\\\n0.0 & -0.2\n\\end{array}\\right),\n\\qquad\nQ = 0.3 \\Sigma\n\\end{split}\n\\] Here \\(K = A \\Sigma G^T (G \\Sigma G^T + R)^{-1}\\) is so-called Kalman gain matrix.\n\nA &lt;- matrix(c(1.2, 0,\n              0, -0.2), ncol=2)\nQ &lt;- 0.3 * Sigma\nK &lt;- A %*% Sigma %*% t(G) %*% solve(G%*% Sigma %*% t(G) + R)\nxhatnew &lt;- A %*% xhat + K %*% (y - G %*% xhat)\nSigmanew &lt;- A %*% Sigma %*% t(A) - K %*% G %*% Sigma %*% t(A) + Q\nz4 &lt;- outer(x1,x2, f, mean=c(xhatnew), varcov=Sigmanew)\nimage(x1, x2, z4, col=mycols,\n      xlab=expression('x'[1]), ylab=expression('x'[2]),\n      main=\"Predictive density\")\ncontour(x1, x2, z4, add=TRUE)\npoints(xhatnew[1], xhatnew[2], pch=19)\ntext(xhatnew[1]-0.1, xhatnew[2],\n     labels = expression(hat(x)[new]), adj = 1)\ncontour(x1, x2, z3, add=TRUE, col=lb)\npoints(xhatf[1], xhatf[2], pch=19, col=lb)\ntext(xhatf[1]-0.1, xhatf[2], col=lb, \n     labels = expression(hat(x)[f]), adj = 1)\ncontour(x1, x2, z, add=TRUE, col=lb)\npoints(0.2, -0.2, pch=19, col=lb)\ntext(0.1, -0.2, labels = expression(hat(x)), adj = 1, col=lb)\ncontour(x1, x2, z2, add=TRUE, col=lb)\npoints(2.3, -1.9, pch=19, col=lb)\ntext(2.2, -1.9,labels = \"y\", adj = 1, col=lb)\n\n\n\n\n\n\n\n\n\nForward filtering and Backwards Sampling\nThe Kalman filtering recursions lead to a fully recursive algorithm for characterizing \\(p( x| y)\\) known as FFBS (Forward filtering and Backwards Sampling). This provides the counterpart to the Baum-Welch algorithm developed earlier for HMMs. The details are as follows. The first step is to factorize the joint posterior distribution of the states via \\[\\begin{align*}\np\\left(  x|y^{T}\\right)   &  =p\\left(  x_{T}|y^{T}\\right)  \\prod_{t=1}%\n^{T-1}p\\left(  x_{t}|x_{t+1},...,x_{T},y^{T}\\right)  \\\\\n&  =p\\left(  x_{T}|y^{T}\\right)  \\prod_{t=1}^{T-1}p\\left(  x_{t}|x_{t+1}%\n,y^{t}\\right) \\label{ffbs-1}\n\\end{align*}\\] where we have used the fact that \\(p\\left(  x_{t}|x_{t+1},...,x_{T},y^{T}\\right)  =p\\left(  x_{t}|x_{t+1},y^{T}\\right)\\) by the Markov property (conditional on \\(x_{t+1}\\), \\(x_{t}\\) is independent of all \\(x_{t+2}\\), etc.).\nThis forms the FFBS algorithm: forward-filtering, backward sampling algorithm for generating a block sample from \\(p\\left(  x|y^{T}\\right)\\). Filter forward using the Kalman recursions and obtain a sample from \\(p( x_T|y^T)\\) and then backwards sample using \\(p\\left(x_{t}|x_{t+1},y^{t}\\right)\\) to generate a block draw of \\(x\\). In what follows, we will often write \\[\np\\left(  x|y^{T}\\right)  \\sim FFBS\n\\] to denote that the FFBS algorithm can be used to generate a block draw.\nBackwards Sampling.\nThe distribution of the final state given the data history \\(p( x_{T} | y^{T} )\\) is given by the Kalman filter \\[\np( x_{T}|y^{T}) \\sim\\mathcal{N}\\left(  \\mu_{T},\\Sigma_{T}\\right)\n\\] where \\(( \\mu_{T} , \\Sigma_{T} )\\) are computed via the Kalman filter recursions. The second distribution comes from the factorization of \\(p( x_{t}, x_{t+1} | y^{t} )\\) in the derivation of the Kalman filtering recursions. Hence, the conditional state filtering distribution given \\(( x_{t+1} , y^t )\\) is \\[\np( x_{t} | x_{t+1} , y^{t} ) \\sim\\mathcal{N}\\left(  \\mu_{t|t+1} ,\n\\Sigma_{t|t+1} \\right)\n\\] where the one-step back smoothed moments are \\[\\begin{align*}\n\\mu_{t|t+1}  &  = \\Sigma_{t|t+1} \\left(  \\Sigma_{t|t} \\mu_{t|t} +\nF_{t+1}^{\\prime}\\left(  \\Sigma^{x}_{t+1} \\right)  ^{-1} x_{t+1} \\right) \\\\\n\\Sigma_{t|t+1}  &  = F_{t+1}^{\\prime}\\left(  \\Sigma^{x}_{t+1} \\right)  ^{-1}\nF_{t+1} + \\Sigma_{t|t}^{-1}%\n\\end{align*}\\] as computed above. Then we can sequentially sample from this distribution.\n\n\n18.5.2 HMM: Hidden Markov Models\nThe algorithms described in this section were originally developed by Baum and Welch Baum et al. (1970) and Viterbi (1967). Baum developed original trading algorithms for Renissance Technology which later became a multi-billion dollar hedge fund. The algorithms are now widely used in many applications, including speech recognition, bioinformatics, and finance. The algorithms are also known as the Baum-Welch and Viterbi algorithms, respectively. The algorithms are used to estimate the parameters of a Hidden Markov Model (HMM) from a sequence of observations. The HMM is a statistical model that describes a system that is assumed to be a Markov process with unobserved (hidden) states. The HMM is widely used in many applications, including speech recognition, bioinformatics, and finance. The algorithms are also known as the Baum-Welch and Viterbi algorithms, respectively. The algorithms are used to estimate the parameters of a Hidden Markov Model (HMM) from a sequence of observations. The HMM is a statistical model that describes a system that is assumed to be a Markov process with unobserved (hidden) states.\nViterbi on the other hand was one of the founders of the what is now known as Qualcomm, a company that is now a multi-billion dollar semiconductor and telecommunications equipment company. Viterbi’s algorithm is used to find the most likely sequence of hidden states in a Hidden Markov Model (HMM) given a sequence of observations. The algorithm is widely used in many applications, including speech recognition, bioinformatics, and finance. The algorithm is also known as the Viterbi algorithm.\nBaum-Welch (1970) and Viterbi (1967) the two famous discrete HMM algorithms.\nConsider a model with a Hidden Chain or regime-switching variable\n\\[\ny_{t} = \\mu\\left( x_{t}\\right) + \\sigma\\left( x_{t}\\right) \\varepsilon _{t}\\text{.}\n\\]\nSuppose that \\(x_{t}\\) is a finite state Markov chain with a time-homogeneous transition matrix P with entries \\(\\left\\{ p_{ij}\\right\\}\\) which are given by\n\\[\np_{ij} = P\\left( x_{t} = i | x_{t-1} = j, \\theta\\right) \\text{.}\n\\]\nWe define the marginal filtering and smoothing distributions\n\\[\np_{i}^{t,t} = P\\left( x_{t} = i | \\theta, y^{t}\\right) \\text{ and } p_{i}^{t,T} = P\\left( x_{t} = i | \\theta, y^{T}\\right)\n\\]\nand the corresponding joint filtering and smoothing matrices:\n\\[\np_{ij}^{t,t} = P\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{t}\\right) \\text{ and } p_{ij}^{t,T} = P\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{T}\\right) \\text{.}\n\\]\nThe key to the algorithm is that we are just going to track the joint matrices, and then peel-off marginals from the rows and columns.\n\nForward-filtering\nTo derive the forward equations\n\\[\n\\begin{aligned}\np_{ij}^{t,t} & = P\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{t}\\right) \\propto p\\left( y_{t}, x_{t-1} = i, x_{t} = j | \\theta, y^{t-1}\\right) \\\\\n& \\propto p\\left( y_{t} | x_{t} = j, \\theta\\right) p\\left( x_{t} = j | x_{t-1} = i, \\theta\\right) p\\left( x_{t-1} = i | \\theta, y^{t-1}\\right) \\\\\n& \\propto p\\left( y_{t} | x_{t} = j, \\theta\\right) p_{ij}\\left( p_{i}^{t-1,t-1}\\right) \\text{.}\n\\end{aligned}\n\\]\nThis shows how to compute today’s filtering distribution given the likelihood. The advantage of this is that it only requires matrix multiplication.\n\n\nBackward-sampling\nThe result of the forward-filtering is the final observation \\(p_{ij}^{T,T}\\). Like in the previous section, we can then filter in reverse to compute \\(p_{ij}^{t,T}\\), which is required for the MCMC algorithm. We have that\n\\[\n\\begin{aligned}\np_{ij}^{t,T} & = p\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{T}\\right) \\\\\n& \\propto p\\left( x_{t-1} = i | x_{t} = j, \\theta, y^{T}\\right) p\\left( x_{t} = j | \\theta, y^{T}\\right) \\\\\n& \\propto p\\left( x_{t-1} = i | x_{t} = j, \\theta, y^{t}\\right) p\\left( x_{t} = j | \\theta, y^{T}\\right) \\\\\n& \\propto \\frac{p\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{t}\\right)}{p\\left( x_{t} = j | \\theta, y^{t}\\right)} p\\left( x_{t} = j | \\theta, y^{T}\\right) \\\\\n& \\propto p_{ij}^{t,t} \\frac{p_{j}^{t,T}}{p_{j}^{t,t}} \\text{.}\n\\end{aligned}\n\\]\nIn deriving this, we have used the fact that\n\\[\np\\left( x_{t-1} = i | x_{t} = j, \\theta, y^{T}\\right) \\propto p\\left( x_{t-1} = i | x_{t} = j, \\theta , y^{t}\\right)\n\\]\nbecause conditional time \\(t\\) information, the past transition is independent of the future. This is the discrete-state version of the FFBS algorithm.\n\n\nSmoothing: Forwards and Backwards\nLet \\(y^T = \\{y_1, \\dots, y_T\\}\\) be a sequence of random variables where the conditional distribution\n\\[\np(y^T | x^T) = p (y_1 | x_1 ) \\prod_{t=2}^T p(y_t | x_t, y_{t-1})\n\\]\nwhere we suppress the dependence of the mixture components on a parameter \\(\\theta\\). The full smoothing distribution can be written\n\\[\np( x | y ) = p( x_T | y^T ) \\prod_{t=1}^{T-1} p( x_t | x_{t+1} , \\theta , y_{t+1} )\n\\]\nSuppose \\(\\{x_t\\}\\) follows a finite state Markov chain with initial distribution \\(\\pi_0\\) and transition probabilities\n\\[\nQ_t(r,s) = Pr(x_t = s | x_{t-1} = r) \\; \\; \\mathrm{and} \\; \\; P_t ( t, r ,s) = Pr(x_{t-1} = r, x_t = s | y_1^t)\n\\]\nwhich we will compute sequentially. Let the current filtering distribution of the state be given by, for \\(t &gt; 0\\),\n\\[\np_t(s) = Pr(x_t = s | y^t) \\; \\; \\mathrm{and} \\; \\; A_t(x_{t-1}, x_t, y_t) = p(x_{t-1}, x_t, y_t | y^{t-1})\n\\]\nBy definition,\n\\[\nA_t(r,s,y_t) = \\frac{p_{t-1}(r) Q_t(r,s)}{p(y_t | y_{t-1})}.\n\\tag{18.1}\\]\nwhere the marginal likelihood is given by\n\\[\np(y_t | y_{1}^{t-1}) = \\sum_{r,s} A_t(r,s,y_t)\n\\]\nThe filtered transition distribution\n\\[\np_{trs} = A_t(r,s,y_t) / p(y_t | y^{t-1})\n\\]\nThe forward-backward recursions are used to efficiently compute the observed data likelihood\n\\[\np(y) = \\sum_{ x } p(y | x)p( x )\n\\]\nwhere \\(x = ( x_1 , \\ldots , x_T )\\). We also need the posterior distribution \\(p( x | y )\\) of the latent Markov chain given observed data. The recursions consist of a forward step that computes the distribution of the \\(t\\)’th transition given all the data up to time \\(t\\), and a backward recursion that updates each distribution to condition on all observed data.\nThe forward recursion operates on the set of transition distributions, represented by a sequence of matrices \\(P_t = ( p_{trs} )\\). Computing\n\\[\np_t(s) = \\sum_{r}p_{trs}\n\\]\nsets up the next step in the recursion. The recursion is initialized by replacing \\(p\\) with \\(p^0\\) in equation Equation 18.1.\nThe observed data log-likelihood can be computed as\n\\[\n\\log p(y) = \\log p(y_1) + \\sum_{t=2}^T \\log p(y_t | y^{t-1})\n\\]\nWith appropriate use of logarithms, \\(p\\) and \\(p(y_t | y^{t-1})\\) need only ever be evaluated on the log scale.\nThe stochastic version of the backward recursion simulates from\n\\[\np(x | y).\n\\]\nBegin with the factorization\n\\[\n\\label{eq:bkwd} p(x | y) = p(x_T | y^T) \\prod_{t=1}^{T-1} p(x_{t} | x_{t+1}^T, y).\n\\]\nThen notice that, given \\(x_{t+1}\\), \\(x_t\\) is conditionally independent of \\(y_{t+1}^T\\) and all later \\(x\\)’s. Thus\n\\[\np(x_t | x_{t+1}, y) = P(x_t = r | x_{t+1} = s, y^{t+1}) \\propto p_{t+1rs}\n\\]\nTherefore, if one samples \\((x_{t-1}, x_t)\\) from the discrete bivariate distribution given by \\(P_t\\) and then repeatedly samples \\(x_t\\) from a multinomial distribution proportional to column \\(x_{t+1}\\) of \\(P_{t+1}\\) then \\(x=(x_1, \\ldots , x_T)\\) is a draw from \\(p(x|y)\\).\n\n\n\n18.5.3 Mixture Kalman filter\nWe can also introduce a \\(\\lambda_t\\) state variable and consider a system\n\\[\n\\begin{aligned}\ny_t & = F_{\\lambda_t} x_t + D_{\\lambda_t} \\epsilon_t  \\\\\nx_t & = G_{\\lambda_t} x_{t-1} + B_{\\lambda_t} v_t\n\\end{aligned}\n\\]\nThe Kalman filter gives moments of the state filtering distribution\n\\[\nx_t | \\lambda^t , y^t \\sim \\mathcal{N} \\left ( \\mu_{t|t} , \\Sigma_{t|t} \\right )\n\\]\nHere we assume that the iid auxiliary state variable shocks \\(\\lambda_t \\sim p( \\lambda_t )\\).\nFirst we marginalize over the state variable \\(x_t\\). Then we can track the sufficient statistics for the hidden \\(z_t\\) variable dynamically in time, namely \\(\\left ( z_{t|t} , S_t \\right )\\), just the Kalman filter moments, in the conditionally Gaussian and discrete cases Lindgren (1978). Then we re-sample \\(( z_{t|t} , S_t  , \\theta )^{(i)}\\) particles.\n\n\n18.5.4 Regime Switching Models\nThe general form of a continuous-time regime switching model is\n\\[\ndy_{t}=\\mu\\left(  \\theta,x_{t},y_{t}\\right)  dt+\\sigma\\left(  \\theta\n,x_{t},y_{t}\\right)  d B_{t}%\n\\]\nwhere \\(x_{t}\\) takes values in a discrete space with transition matrix \\(P_{ij}\\left(  t\\right)\\) with parameters \\(\\theta=\\left(  \\theta_{1},\\ldots,\\theta_{J}\\right)\\). Common specifications assume the drift and diffusion coefficients are parametric functions and the parameters switch over time. In this case, it is common to write the model as\n\\[\ndy_{t}=\\mu\\left(  \\theta_{x_{t}},y_{t}\\right)  dt+\\sigma\\left(  \\theta_{x_{t}%\n},y_{t}\\right)  d B_{t}\\text{.}%\n\\]\nScott (2002) provides a fast MCMC algorithm for state filtering by adapting the FFBS algorithm. Time discretized the model is:\n\\[\ny_{t}=\\mu\\left(  \\theta_{x_{t}},y_{t-1}\\right)  +\\sigma\\left(  \\theta_{x_{t}%\n},y_{t-1}\\right)  \\varepsilon_{t}\\text{.}%\n\\]\nNote that we use the standard notation from discrete-time models where the time index on the Markov state is equal to the current observation. The discrete-time transition probabilities are\n\\[\np_{ij}=P\\left(  x_{t}=i|x_{t-1}=j\\right)\n\\]\nand we assume, apriori, that the transition functions are time and state invariant. The joint likelihood is given by\n\\[\np\\left(  y| x,\\theta\\right)  = \\prod_{t=1}^{T} p\\left(  y_{t}|y_{t-1}%\n,x_{t-1},\\theta\\right)\\]\nwhere \\(p\\left(  y_{t}|y_{t-1},x_{t-1},\\theta\\right)  =N\\left(  \\mu\\left(\n\\theta_{x_{t-1}},y_{t-1}\\right)  ,\\sigma^{2}\\left(  \\theta_{x_{t-1}}%\n,y_{t-1}\\right)  \\right)\\).\nClifford-Hammersley implies that the complete conditionals are given by \\(p\\left(  \\theta| x ,s , y \\right)\\), \\(p\\left(  s | x , \\theta, y \\right)\\), and \\(p\\left(  x | s ,\\theta, y \\right)\\). Conditional on the states and the transition probabilities, updating the parameters is straightforward. Conditional on the states, the transition matrix has a Dirchlet distribution, and updating this is also straightforward. To update the states use FFBS.\nAn important component of regime switching models is the prior distribution. Regime switching models (and most mixture models) are not formally identified. For example, in all regime switching models, there is a labeling problem: there is no unique way to identify the states. A common approach to overcome this identification issue is to order the parameters.\n\nChangepoint Problems\nSmith (1975) introduced the single changepoint problem from a Bayesian perspective. Consider a sequence of random variables \\(y_{1} , \\ldots, y_{T}\\) which has a change-point at time \\(\\tau\\) in the sense that\n\\[\ny_t | \\theta_{k} \\sim    \\left\\{\n\\begin{array}[c]{l}\np( y | \\theta_{1} ) \\; \\; \\; \\; \\; \\; \\mathrm{for} \\; \\; 1 \\leq i \\leq\\tau \\\\\np( y | \\theta_{2} ) \\; \\; \\; \\; \\; \\; \\mathrm{for} \\; \\; \\tau+1 \\leq\ni \\leq T\n\\end{array}  \\right\\}\\]\nThis can be rewritten as a state space model\n\\[\ny_t = \\theta_{ x_t } + \\sigma_{ x_t } \\epsilon_t\n\\]\nwhere \\(x_t\\) has a Markov transition evolutiuon.\nAn idea that appears to be under-exploited is that of “model reparametrisation”. The multiple change-point problem which is computationally expensive if approached directly has a natural model reparametrisation exists however that makes the implementation of MCMC methods straightforward (see Chib (1998)). Specifically, suppose that the data generating process \\(y^{T} = \\{ y_{1} , \\ldots, y_{T} \\}\\) is given by a sequence of conditionals \\(f ( y_{t} | y^{t-1} , \\theta_{k} )\\) for parameters \\(\\theta_{k}\\) that change at un-known change-points \\(\\{ \\tau_{1} , \\ldots,\n\\tau_{k} \\}\\).\nThe model parameterization is based on using a hidden Markov state space model with a vector of latent variables \\(s_{t}\\) where \\(s_{t}\n= k\\) indicates that \\(y_{t}\\) is drawn from \\(p ( y_{t} | y^{t-1} ,\n\\theta_{k} )\\). Let the prior distribution on the \\(s_{t}\\)’s have transition matrix where \\(p_{ij} = P \\left(  s_{t} = j | s_{t-1} = i\n\\right)\\) is the probability of jumping regimes. With this model parameterization the \\(k\\)th change occurs at \\(\\tau_{k}\\) if \\(s_{\n\\tau_{k} } = k\\) and \\(s_{ \\tau_{k} + 1 } = k + 1\\). The reparameterisation automatically enforces the order constraints on the change-points and is it very easy to perform MCMC analysis on the posterior distribution. This provides a more efficient strategy for posterior computation. MCMC analysis of the \\(s_{t}\\)’s is straightforward and the posterior for the \\(\\tau_{k}\\)’s can be obtained by inverting the definition above. Hence\n\\[\np( \\tau = t | y ) = p( x_t =1 | y )\n\\]\nThe alternative is single state updating conditional on \\(\\tau\\) which is slow for finding the multiple-changeponts.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "18-forecasting.html#particle-learning-for-general-mixture-models",
    "href": "18-forecasting.html#particle-learning-for-general-mixture-models",
    "title": "18  Forecasting",
    "section": "18.6 Particle Learning for General Mixture Models",
    "text": "18.6 Particle Learning for General Mixture Models\nParticle learning (PL) offers a powerful and flexible approach for sequential inference in general mixture models. Unlike traditional MCMC methods, which require repeated passes over the entire dataset and can be computationally demanding, particle learning operates in an online fashion. This means it can efficiently update inference as new data arrives, making it particularly well-suited for large or high-dimensional datasets and real-time applications.\nThe particle learning framework is designed to efficiently and sequentially learn from a broad class of mixture models. At its core, the approach models data as arising from a mixture distribution:\n\\[\nf(z) = \\int k(z;\\theta) d G(\\theta)\n\\]\nwhere \\(G\\) is a discrete mixing measure and \\(k(z;\\theta)\\) is a kernel parameterized by \\(\\theta\\). The generality of this formulation allows PL to be applied to a wide variety of models, including finite mixture models, Dirichlet process mixtures, Indian buffet processes, and probit stick-breaking models. This flexibility is a significant advantage, as it enables practitioners to use PL across many settings without needing to redesign the inference algorithm for each new model structure.\nIn addition to its generality, particle learning provides an alternative to MCMC for tasks such as online model fitting, marginal likelihood estimation, and posterior cluster allocation. Its sequential nature makes it particularly attractive for streaming data and scenarios where computational resources are limited.\nA general mixture model can be described by three components: a likelihood, a transition equation for latent allocations, and a prior over parameters. Specifically,\n\nThe likelihood is given by \\(p(y_{t+1} | k_{t+1}, \\theta)\\), representing the probability of the next observation given the current allocation and parameters.\nThe transition equation \\(p(k_{t+1} | k^t, \\theta)\\) governs how the latent allocation variables evolve over time, where \\(k^t = \\{k_1, ···, k_t\\}\\) denotes the history of allocations.\nThe parameter prior \\(p(\\theta)\\) encodes prior beliefs about the mixture component parameters.\n\nThis structure can be expressed in a state-space form:\n\\[\n\\begin{align}\ny_{t+1} &= f(k_{t+1}, \\theta) \\\\\nk_{t+1} &= g(k^t, \\theta)\n\\end{align}\n\\]\nwhere the first equation is the observation model and the second describes the evolution of the latent allocation states.\nThe mixture modeling framework described above is closely related to hidden Markov models (HMMs). In this context, the observed data \\(y_t\\) are assumed to be generated from a mixture, with allocation variables \\(k_t\\) determining which mixture component is responsible for each observation. The parameters \\(\\theta_{k_t}\\) for each component are drawn from the mixing measure \\(G\\). This structure allows for both standard mixture models, where each observation is assigned to a single component, and more general latent feature models, where multivariate allocation variables \\(k_t\\) allow an observation to be associated with multiple components simultaneously.\nA central concept in particle learning is the essential state vector \\(\\mathcal{Z}_t\\), which is tracked over time. This vector is constructed to be sufficient for sequential inference, meaning that it contains all the information needed to compute the posterior predictive distribution for new data, update the state as new observations arrive, and learn about the underlying parameters:\n\nPosterior predictive: \\(p(y_{t+1} | \\mathcal{Z}_t)\\)\nPosterior updating: \\(p(\\mathcal{Z}_{t+1} | \\mathcal{Z}_t, y_{t+1})\\)\nParameter learning: \\(p(\\theta | \\mathcal{Z}_{t+1})\\)\n\n\n18.6.1 The Particle Learning Algorithm\nParticle learning approximates the posterior distribution \\(p(\\mathcal{Z}_t | y^t)\\) with a set of equally weighted particles \\(\\{\\mathcal{Z}_t^{(i)}\\}_{i=1}^N\\). When a new observation \\(y_{t+1}\\) becomes available, the algorithm proceeds in two main steps:\n\nResample: The current set of particles is resampled with weights proportional to the predictive likelihood \\(p(y_{t+1} | \\mathcal{Z}_t^{(i)})\\). This step focuses computational effort on the most plausible states given the new data.\nPropagate: Each resampled particle is then propagated forward by sampling from the transition distribution \\(p(\\mathcal{Z}_{t+1} | \\mathcal{Z}_t^{(i)}, y_{t+1})\\), thus updating the state to incorporate the new observation.\n\nThis two-step process is grounded in Bayes’ theorem, where the resampling step corresponds to updating the posterior with the new data, and the propagation step advances the state according to the model dynamics. After these steps, the set of particles provides an updated approximation to the posterior \\(p(\\mathcal{Z}_{t+1} | y^{t+1})\\).\nOne important distinction between particle learning and standard particle filtering methods is that the essential state vector $_t $does not necessarily need to include the full history of allocation variables \\(k^t\\) to be sufficient for inference. This makes PL both more efficient and more flexible than many existing particle filtering approaches for mixture models. Furthermore, the order of resampling and propagation steps is reversed compared to standard filters, which helps mitigate particle degeneracy and improves performance in mixture modeling contexts.\nParticle learning also provides an efficient mechanism for sampling from the full posterior distribution of the allocation vector \\(p(k^t | y^t)\\). This is achieved using a backwards uncertainty update, which allows for the recovery of smoothed samples of the allocation history. For each particle, and for each time step in reverse order, the allocation variable \\(k_r\\) is sampled with probability proportional to the product of the likelihood and the prior for that allocation, given the state vector. This results in an algorithm with computational complexity linear in the number of particles, making it practical even for large datasets.\nThe particle learning framework is applicable to a wide range of density estimation problems involving mixtures of the form\n\\[\nf(y;G) = \\int k(y ; \\theta) dG(\\theta)\n\\]\nThere are many possible choices for the prior on the mixing measure \\(G\\). Common examples include finite mixture models, which use a finite number of components; Dirichlet process mixtures, which allow for an infinite number of components via a stick-breaking construction; beta two-parameter processes; and kernel stick-breaking processes. Each of these priors offers different modeling flexibility and computational properties, and the choice depends on the specific application and desired level of model complexity.\nIn some cases, it is useful to consider a collapsed state-space model, where the predictive distribution for a new observation is expressed as an expectation over the mixing measure \\(G\\) given the current state vector:\n\\[\n\\mathbb{E}[f(y_{t+1};G) | \\mathcal{Z}_t] = \\int k(y_{t+1};\\theta) d \\mathbb{E}[G(\\theta) | \\mathcal{Z}_t]\n\\]\nIf \\(t\\) observations have been allocated to \\(m_t\\) mixture components, the posterior expectation of \\(G\\) can be written as a weighted sum of the base measure and point masses at the component parameters. The predictive density then combines contributions from both new and existing components, weighted according to their posterior probabilities.\nParticle learning offers a versatile and efficient framework for sequential inference in general mixture models. By representing the posterior with a set of particles and updating these particles as new data arrives, PL enables real-time model fitting, efficient posterior allocation, and flexible density estimation across a wide range of mixture modeling scenarios. Its ability to handle both finite and infinite mixture models, as well as latent feature models, makes it a valuable tool for modern statistical analysis.\n\nExample 18.5 (Particle Learning for Poisson Mixture Models) We will implement Particle Learning (PL) for a finite mixture of Poisson distributions based on the example from Carvalho et al. (2010). This example follows Algorithm 1 for finite mixture models from Section 2.1 of the paper.\nWe generate data from a mixture of two Poisson distributions (\\(\\lambda_1=2\\) with weight 0.7, \\(\\lambda_2\\)=10 with weight 0.3).\n\nset.seed(8) # Ovi\n# Simulate data (500 observations)\nn_obs &lt;- 500\ntrue_z &lt;- sample(1:2, n_obs, replace=TRUE, prob=c(0.7, 0.3))\ny &lt;- ifelse(true_z == 1, rpois(n_obs, 2), rpois(n_obs, 10))\n\n\n\nCode\n# Plot two empirical density plots for each mixture component. Put them in one plot \nplot(density(y[true_z == 1]), xlab = \"y\", col = \"blue\", lwd = 2, xlim = c(0, 15), ylim = c(0, 0.3), main=\"\")\nlines(density(y[true_z == 2]), col = \"red\", lwd = 2)\nlines(density(y),xlab = \"y\", col = \"purple\", lwd = 2, xlim = c(0, 15), ylim = c(0, 0.3))\nlegend(\"topright\", legend=c(\"Component 1 (Lambda=2)\", \"Component 2 (Lambda=10)\", \"Mixture Density\"), col=c(\"blue\", \"red\", \"purple\"), lwd=2)\n\n\n\n\n\n\n\n\n\nThe code below implements the Particle Learning algorithm using the following steps: 1 Particle Initialization: - Each particle tracks sufficient statistics: - s: Sum of observations per component - n: Count of observations per component 2. PL Algorithm: - Resample: Particles are weighted by the posterior predictive probability of the next observation - Propagate: For each particle: - Compute component allocation probabilities - Sample component assignment - Update sufficient statistics - Learn: Store posterior estimates of \\(\\lambda\\) parameters and mixture weights\nThe key features of this implementaiton is the use of posterior predictive uses Poisson-Gamma conjugacy and allocation of probabilities by combining prior weights and likelihood.\n\n\nCode\n# Model parameters\nm &lt;- 2  # Number of components\nalpha &lt;- c(1, 1)  # Gamma prior shape parameters\nbeta &lt;- c(1, 1)   # Gamma prior rate parameters\ngamma &lt;- c(1, 1)  # Dirichlet prior parameters\nn_particles &lt;- 1000  # Number of particles\n\n# Initialize particles\nparticles &lt;- lapply(1:n_particles, function(i) {\n  list(s = c(0, 0),    # Sufficient statistics (sum of y in each component)\n       n = c(0, 0))    # Counts per component\n})\n\n# Store posterior samples\nposterior_lambda &lt;- matrix(0, nrow = n_obs, ncol = m)\nposterior_weights &lt;- matrix(0, nrow = n_obs, ncol = m)\n\n# Particle Learning algorithm\nfor (t in 1:n_obs) {\n  y_t &lt;- y[t]\n  log_weights &lt;- numeric(n_particles)\n  \n  # 1. Compute weights using posterior predictive\n  for (i in 1:n_particles) {\n    total_obs &lt;- sum(particles[[i]]$n)\n    weight_components &lt;- (particles[[i]]$n + gamma) / (total_obs + sum(gamma))\n    \n    # Predictive for each component (Poisson-Gamma)\n    pred_prob &lt;- sapply(1:m, function(j) {\n      shape_post &lt;- alpha[j] + particles[[i]]$s[j]\n      rate_post &lt;- beta[j] + particles[[i]]$n[j]\n      exp(dpois(y_t, shape_post/rate_post, log = TRUE) +\n            dgamma(shape_post/rate_post, shape_post, rate_post, log = TRUE))\n    })\n    \n    log_weights[i] &lt;- log(sum(weight_components * pred_prob))\n  }\n  \n  # Normalize weights\n  max_logw &lt;- max(log_weights)\n  weights &lt;- exp(log_weights - max_logw)\n  weights &lt;- weights / sum(weights)\n  \n  # 2. Resample particles\n  idx &lt;- sample(1:n_particles, size = n_particles, replace = TRUE, prob = weights)\n  particles &lt;- particles[idx]\n  \n  # 3. Propagate state\n  for (i in 1:n_particles) {\n    # Compute allocation probabilities\n    alloc_probs &lt;- sapply(1:m, function(j) {\n      shape_post &lt;- alpha[j] + particles[[i]]$s[j]\n      rate_post &lt;- beta[j] + particles[[i]]$n[j]\n      log_prior &lt;- log(particles[[i]]$n[j] + gamma[j]) - log(sum(particles[[i]]$n) + sum(gamma))\n      log_lik &lt;- dpois(y_t, shape_post/rate_post, log = TRUE)\n      exp(log_prior + log_lik)\n    })\n    alloc_probs &lt;- alloc_probs / sum(alloc_probs)\n    \n    # Sample component allocation\n    k &lt;- sample(1:m, size = 1, prob = alloc_probs)\n    \n    # Update sufficient statistics\n    particles[[i]]$s[k] &lt;- particles[[i]]$s[k] + y_t\n    particles[[i]]$n[k] &lt;- particles[[i]]$n[k] + 1\n  }\n  \n  # 4. Learn parameters (store posterior means)\n  lambda_samples &lt;- t(sapply(particles, function(p) {\n    (alpha + p$s) / (beta + p$n)\n  }))\n  weight_samples &lt;- t(sapply(particles, function(p) {\n    (gamma + p$n) / sum(gamma + p$n)\n  }))\n  \n  posterior_lambda[t,] &lt;- colMeans(lambda_samples)\n  posterior_weights[t,] &lt;- colMeans(weight_samples)\n}\n\n\nNow we are ready to plot the results\n\n\nCode\nlibrary(ggplot2)\n# Convert posterior estimates to data frames for ggplot\nposterior_df &lt;- data.frame(\n  Observation = 1:n_obs,\n  Lambda1 = posterior_lambda[, 1],\n  Lambda2 = posterior_lambda[, 2],\n  Weight1 = posterior_weights[, 1],\n  Weight2 = posterior_weights[, 2]\n)\n# Plot Lambda parameters\nggplot(posterior_df, aes(x = Observation)) +\n  geom_line(aes(y = Lambda1, color = \"Lambda1\"), size = 1) +\n  geom_line(aes(y = Lambda2, color = \"Lambda2\"), size = 1) +\n  geom_hline(yintercept = 2, linetype = \"dashed\", color = \"blue\") +\n  geom_hline(yintercept = 10, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Posterior Estimates: Lambda Parameters\",\n       x = \"Observation\", y = \"Lambda\") +\n  scale_color_manual(values = c(\"black\", \"red\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n# Plot Weights\nggplot(posterior_df, aes(x = Observation)) +\n  geom_line(aes(y = Weight1, color = \"Weight1\"), size = 1) +\n  geom_line(aes(y = Weight2, color = \"Weight2\"), size = 1) +\n  geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"blue\") +\n  geom_hline(yintercept = 0.3, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Posterior Estimates: Mixture Weights\",\n       x = \"Observation\", y = \"Weight\") +\n  scale_color_manual(values = c(\"black\", \"red\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nPosterior Estimates: Lambda Parameters\n\n\n\n\n\n\n\nPosterior Estimates: Mixture Weights\n\n\n\n\n\nThe first plot shows the posterior estimates of \\(\\lambda\\) parameters converging to true values (2 and 10). The second plot shows mixture weights converging to true weights (0.7 and 0.3). Convergence typically occurs after 100-200 observations. Particle degeneracy is mitigated through systematic resampling\nAdvantages of PL for Mixtures:\n\nSequential Updating: Processes observations one-at-a-time\nEfficiency: Only tracks sufficient statistics, not full history\nFlexibility: Easily extends to other mixture types (DP, IBP, etc.)\nReal-time Inference: Posterior updates after each observation\n\nThis implementation demonstrates PL’s ability to handle finite mixtures, but the same framework extends to infinite mixtures (DP mixtures) and other general mixture models described in the paper by modifying the propagation and resampling steps.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "18-forecasting.html#modern-era-forecasting",
    "href": "18-forecasting.html#modern-era-forecasting",
    "title": "18  Forecasting",
    "section": "18.7 Modern Era Forecasting",
    "text": "18.7 Modern Era Forecasting\nA recent post by the Amazon Science group Amazon (2021) describes the evolution of the time series algorithms used for forecasting from 2007 to 2021. Figure below shows the entire evolution of the algorithms.\n\n\n\n\n\nThey went from standard textbook time series forecasting methods to make predictions to the quantile-based transformer models. The main problem of the traditional TS models is that they assume stationary. A stationary time series is one whose properties do not depend on the time at which the series is observed. For exmaple, a white noise series is stationary - it does not matter when you observe it, it should look much the same at any point in time.\n\nyt = rnorm(100)\nplot(yt,type='l')\n\n\n\n\n\n\n\n\nIn other words, all the coefficients of a time series model do not change over time. We know how to deal with trends and seasonality quite well. Thus, those types of non-stationary are not an issue. Below some of the example of time series data. Although most of those are not stationary, we can model them using traditional techniques (Hyndman and Athanasopoulos (2021)).\n\n\n\n\n\n\n\n\nFigure 18.8: Which of these series are stationary? (a) Dow Jones index on 292 consecutive days; (b) Daily change in the Dow Jones index on 292 consecutive days; (c) Annual number of strikes in the US; (d) Monthly sales of new one-family houses sold in the US; (e) Annual price of a dozen eggs in the US (constant dollars); (f) Monthly total of pigs slaughtered in Victoria, Australia; (g) Annual total of lynx trapped in the McKenzie River district of north-west Canada; (h) Monthly Australian beer production; (i) Monthly Australian electricity production.\n\n\n\n\n\nHowever, when you try to forecast for a time series with no prior history or non-recurrent “jumps”, like recessions, traditional models are unlikely to work well.\nAmazon used a sequence of “patches” to hack the model and to make it produce useful results. All of those reacquired manual feature engineering and led to less transparent and fragile models. One solution is to use random forests.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "18-forecasting.html#quantile-regression-forests.",
    "href": "18-forecasting.html#quantile-regression-forests.",
    "title": "18  Forecasting",
    "section": "18.8 Quantile Regression Forests.",
    "text": "18.8 Quantile Regression Forests.\nMost estimators during prediction return \\(E(Y|X)\\), which can be interpreted as the answer to the question, what is the expected value of your output given the input?\nQuantile methods, return \\(y\\) at \\(q\\) for which \\(F(Y=y|X)=q\\) where \\(q\\) is the percentile and \\(y\\) is the quantile. One quick use-case where this is useful is when there are a number of outliers which can influence the conditional mean. It is sometimes important to obtain estimates at different percentiles, (when grading on a curve is done for instance.)\nNote, Bayesian models return the entire distribution of \\(P(Y|X)\\).\nIt is fairly straightforward to extend a standard decision tree to provide predictions at percentiles. When a decision tree is fit, the trick is to store not only the sufficient statistics of the target at the leaf node such as the mean and variance but also all the target values in the leaf node. At prediction, these are used to compute empirical quantile estimates.\nThe same approach can be extended to Random Forests. To estimate \\(F(Y=y|x)=q\\) each target value in training \\(y\\)s is given a weight. Formally, the weight given to \\(y_j\\) while estimating the quantile is \\[\n\\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\mathbb{1}(y_j \\in L(x))}{\\sum_{i=1}^N \\mathbb{1}(y_i \\in L(x))},\n\\] where \\(L(x)\\) denotes the leaf that \\(x\\) falls into.\nInformally, what it means that for a new unknown sample, we first find the leaf that it falls into at each tree. Then for each \\((X, y)\\) in the training data, a weight is given to \\(y\\) at each tree in the following manner.\n\nIf it is in the same leaf as the new sample, then the weight is the fraction of samples in the same leaf.\nIf not, then the weight is zero.\n\nThese weights for each y are summed up across all trees and averaged. Now since we have an array of target values and an array of weights corresponding to these target values, we can use this to measure empirical quantile estimates.nding to these target values, we can use this to measure empirical quantile estimates.\nMotivated by the success of gradient boositg model for predicting Walmart sales (kaggle (2020)), Januschowski et al. (2022) tries to explain why tree-based methods were so widely used for forecasting.\n\n\n\nJanuschowski et al. (2022)\n\n\n\n\n\n\nAlbert, Jim. 1993. “A Statistical Analysis of Hitting Streaks in Baseball: Comment.” Journal of the American Statistical Association 88 (424): 1184–88. https://www.jstor.org/stable/2291255.\n\n\nAmazon. 2021. “The History of Amazon’s Forecasting Algorithm.” Amazon Science. https://www.amazon.science/latest-news/the-history-of-amazons-forecasting-algorithm.\n\n\nBaum, Leonard E., Ted Petrie, George Soules, and Norman Weiss. 1970. “A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains.” The Annals of Mathematical Statistics 41 (1): 164–71. https://www.jstor.org/stable/2239727.\n\n\nBenoit, Dries F., and Dirk Van den Poel. 2012. “Binary Quantile Regression: A Bayesian Approach Based on the Asymmetric Laplace Distribution.” Journal of Applied Econometrics 27 (7): 1174–88.\n\n\nBerge, Travis, Nitish Sinha, and Michael Smolyansky. 2016. “Which Market Indicators Best Forecast Recessions?” FEDS Notes, August.\n\n\nCampagnoli, Patrizia, Sonia Petrone, and Giovanni Petris. 2009. Dynamic Linear Models with R. New York, NY: Springer.\n\n\nCarvalho, Carlos M, Hedibert F Lopes, Nicholas G Polson, and Matt A Taddy. 2010. “Particle Learning for General Mixtures.” Bayesian Analysis 5 (4): 709–40.\n\n\nChib, Siddhartha. 1998. “Estimation and Comparison of Multiple Change-Point Models.” Journal of Econometrics 86 (2): 221–41.\n\n\nEric Tassone, and Farzan Rohani. 2017. “Our Quest for Robust Time Series Forecasting at Scale.”\n\n\nFrühwirth-Schnatter, Sylvia, and Rudolf Frühwirth. 2007. “Auxiliary Mixture Sampling with Applications to Logistic Models.” Computational Statistics & Data Analysis 51 (April): 3509–28.\n\n\n———. 2010. “Data Augmentation and MCMC for Binary and Multinomial Logit Models.” In Statistical Modelling and Regression Structures: Festschrift in Honour of Ludwig Fahrmeir, 111–32.\n\n\nFrühwirth-Schnatter, Sylvia, Rudolf Frühwirth, Leonhard Held, and Håvard Rue. 2008. “Improved Auxiliary Mixture Sampling for Hierarchical Models of Non-Gaussian Data.” Statistics and Computing 19 (4): 479.\n\n\nGramacy, Robert B., and Nicholas G. Polson. 2012. “Simulation-Based Regularized Logistic Regression.” arXiv. https://arxiv.org/abs/1005.3430.\n\n\nHeld, Leonhard, and Chris C. Holmes. 2006. “Bayesian Auxiliary Variable Models for Binary and Multinomial Regression.” Bayesian Analysis 1 (1): 145–68.\n\n\nHyndman, Rob J., and George Athanasopoulos. 2021. Forecasting: Principles and Practice. 3rd ed. edition. Melbourne, Australia: Otexts.\n\n\nJanuschowski, Tim, Yuyang Wang, Kari Torkkola, Timo Erkkilä, Hilaf Hasson, and Jan Gasthaus. 2022. “Forecasting with Trees.” International Journal of Forecasting, Special Issue: M5 competition, 38 (4): 1473–81.\n\n\nkaggle. 2020. “M5 Forecasting - Accuracy.” https://kaggle.com/competitions/m5-forecasting-accuracy.\n\n\nLindgren, Georg. 1978. “Markov Regime Models for Mixed Distributions and Switching Regressions.” Scandinavian Journal of Statistics 5 (2): 81–91. https://www.jstor.org/stable/4615692.\n\n\nPetris, Giovanni. 2010. “An R Package for Dynamic Linear Models.” Journal of Statistical Software 36 (October): 1–16.\n\n\nPolson, Nicholas, and Steven Scott. 2011. “Data Augmentation for Support Vector Machines.” Bayesian Analysis 6 (March).\n\n\nRubin, Hal S. Stern, John B. Carlin. 2015. Bayesian Data Analysis. 3rd ed. New York: Chapman and Hall/CRC.\n\n\nScott, Steven L. 2022. “BoomSpikeSlab: MCMC for Spike and Slab Regression.”\n\n\nScott, Steven L., and Hal R. Varian. 2015. “Bayesian Variable Selection for Nowcasting Economic Time Series.” In Economic Analysis of the Digital Economy, 119–35. University of Chicago Press.\n\n\nScott, Steven, and Hal Varian. 2014. “Predicting the Present with Bayesian Structural Time Series.” Int. J. Of Mathematical Modelling and Numerical Optimisation 5 (January): 4–23.\n\n\nSean J. Taylor, and Ben Letham. 2017. “Prophet: Forecasting at Scale - Meta Research.” Meta Research. https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/.\n\n\nSmith, A. F. M. 1975. “A Bayesian Approach to Inference about a Change-Point in a Sequence of Random Variables.” Biometrika 62 (2): 407–16. https://www.jstor.org/stable/2335381.\n\n\nViterbi, A. 1967. “Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.” IEEE Transactions on Information Theory 13 (2): 260–69.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "20-theorydl.html",
    "href": "20-theorydl.html",
    "title": "20  Theory of Deep Learning",
    "section": "",
    "text": "20.1 Ridge and Projection Pursuit Regression\nThis chapter explores the theoretical foundations of deep learning through the lens of multivariate function approximation, beginning with ridge functions as fundamental building blocks. Ridge functions, which take the form \\(f(x) = g(w^Tx)\\), represent one of the simplest forms of nonlinear multivariate functions by combining a single linear projection with a univariate nonlinear transformation. Their key geometric property—remaining constant along directions orthogonal to the projection vector \\(w\\)—makes them particularly useful for high-dimensional approximation. The chapter then introduces projection pursuit regression, which approximates complex input-output relationships using linear combinations of ridge functions, demonstrating how these mathematical constructs provide the groundwork for modern deep learning approaches.\nThe chapter culminates with the Kolmogorov Superposition Theorem (KST), a profound result that shows any real-valued continuous function can be represented as a sum of compositions of single-variable functions. This theorem provides a theoretical framework for understanding how complex multivariate functions can be decomposed into simpler, more manageable components—a principle that underlies the architecture of modern neural networks. The discussion raises important questions about the trade-off between computational power and mathematical efficiency in machine learning, challenging whether superior performance can be achieved through mathematically elegant representations rather than brute-force computational approaches.\nTo understand the significance of this trade-off, we consider ridge functions, which represent a fundamental building block in multivariate analysis. Since our ultimate goal is to model arbitrary multivariate functions \\(f\\), we need a way to reduce dimensionality while preserving the ability to capture nonlinear relationships. Ridge functions accomplish this by representing one of the simplest forms of nonlinear multivariate functions, requiring only a single linear projection and a univariate nonlinear transformation. Formally, a ridge function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) takes the form \\(f(x) = g(w^Tx)\\), where \\(g\\) is a univariate function and \\(x,w \\in \\mathbb{R}^n\\). The non-zero vector \\(w\\) is called the direction. The term “ridge” reflects a key geometric property: the function remains constant along any direction orthogonal to \\(w\\). Specifically, for any direction \\(u\\) such that \\(w^Tu = 0\\), we have\n\\[\nf(x+u) = g(w^T(x+u)) = g(w^Tx) = f(x)\n\\]\nThis structural simplicity makes ridge functions particularly useful as building blocks for high-dimensional approximation.\nRidge functions play a central role in high-dimensional statistical analysis. For example, projection pursuit regression approximates input-output relations using a linear combination of ridge functions Friedman and Stuetzle (1981),huber1985proje:\n\\[\n\\phi(x) = \\sum_{i=1}^{p}g_i(w_i^Tx),\n\\]\nwhere both the directions \\(w_i\\) and functions \\(g_i\\) are variables and \\(w_i^Tx\\) are one-dimensional projections of the input vector. The vector \\(w_i^Tx\\) is a projection of the input vector \\(x\\) onto a one-dimensional space and \\(g_i(w_i^Tx)\\) can be though as a feature calculated from data. Diaconis and Shahshahani (1984) use nonlinear functions of linear combinations, laying important groundwork for deep learning.\nThe landscape of modern machine learning has been shaped by the exponential growth in computational power, particularly through advances in GPU technology and frameworks like PyTorch. While Moore’s Law has continued to drive hardware improvements and CUDA algorithms have revolutionized our ability to process vast amounts of internet data, we pose the following question: can we achieve superior performance through mathematically efficient representations of multivariate functions rather than raw computational power?\nA fundamental challenge in machine learning lies in effectively handling high-dimensional input-output relationships. This challenge manifests itself in two distinct but related tasks. First, one task is to construct a “look-up” table (dictionary) for fast search and retrieval of input-output examples. This is an encoding and can be thought of as a data compression problem. Second, and perhaps more importantly, we must develop prediction rules that can generalize beyond these examples to handle arbitrary inputs.\nMore formally, we seek to find a good predictor function \\(f(x)\\) that maps an input \\(x\\) to its output prediction \\(y\\). In practice, the input \\(x\\) is typically a high-dimensional vector:\n\\[\ny = f ( x )  \\; \\; {\\rm where}  \\; \\; x =  ( x_1 , \\ldots , x_d )\n\\]\nGiven a training dataset \\((y_i,x_i)_{i=1}^N\\) of example input-output pairs, our goal is to train a model, i.e. to find the function \\(f\\). The key question is: how do we represent a multivariate function so as to obtain a desirable \\(f\\)?",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "20-theorydl.html#kolmogorov-superposition-theorem-kst",
    "href": "20-theorydl.html#kolmogorov-superposition-theorem-kst",
    "title": "20  Theory of Deep Learning",
    "section": "20.2 Kolmogorov Superposition Theorem (KST)",
    "text": "20.2 Kolmogorov Superposition Theorem (KST)\nKolmogorov demonstrated that any real-valued continuous function \\(f(\\mathbf{x})\\) defined on \\(E^n\\) can be represented as a convolution of two single variable functions:\n\\[\nf(x_1,\\ldots,x_n) = \\sum_{q=1}^{2n+1} g_q\\left(\\phi_q(x_1,\\ldots,x_n)\\right)\n\\]\nwhere \\(g_q\\) are continuous single-variable functions defined on \\(\\phi_q(E^n)\\). Kolmogorov further showed that the \\(\\phi_q\\) functions can be decomposed into sums of single-variable functions:\n\\[\n\\phi_q(x_1,\\ldots,x_n) = \\sum_{i=1}^n \\psi_{q,i}(x_i)\n\\]\nThis result is known as Kolmogorov representation theorem Kolmogorov (1956) and is often written in the following form:\n\\[\nf(x_1,\\ldots,x_n) = \\sum_{q=1}^{2n+1} g_q\\left(\\sum_{i=1}^n \\psi_{q,i}(x_i)\\right)\n\\]\nThe theorem has seen several refinements over time, the inner functions could be Hölder continuous and Lipschitz continuous, though this required modifications to both the outer and inner functions.\nThe inner functions \\(\\Psi_q\\) partition the input space into distinct regions, and the outer function, \\(g\\), must be constructed to provide the correct output values across the regions that the inner function defines. The outer function, \\(g\\), can be determined via a computationally intensive process of averaging. For each input configuration, the inner functions \\(\\Psi_q\\) generate a unique encoding, and \\(g\\) must map this encoding to the appropriate value of \\(f(x)\\). This creates a dictionary-like structure that associates each region with its corresponding output value. Köppen made significant contributions by correcting Sprecher’s original proof of this construction process, with improvements to the computational algorithm later suggested by Actor (2018) and Demb and Sprecher (2021). Braun further enhanced the understanding by providing precise definitions of the shift parameters \\(\\delta_k\\) and characterizing the topological structure induced by \\(\\Psi_q\\).\nA fundamental trade-off in KST exists between function smoothness and dimensionality. The inner functions \\(\\psi_{p,q}\\) can be chosen from two different function spaces, each offering distinct advantages. The first option is to use functions from \\(C^1([0,1])\\), but this limits the network’s ability to handle higher dimensions effectively. The second option is to relax the smoothness requirement to Hölder continuous functions (\\(\\psi_{p,q} \\in \\text{Holder}_\\alpha([0,1])\\)), which satisfy the inequality \\(|\\psi(x) - \\psi(y)| &lt; |x-y|^\\alpha\\). These functions are less smooth, but this “roughness” enables better approximation in higher dimensions.\n\n20.2.1 Kolmogorov-Arnold Networks\nA significant development has been the emergence of Kolmogorov-Arnold Networks (KANs). The key innovation of KANs is their use of learnable functions rather than weights on the network edges. This replaces traditional linear weights with univariate functions, typically parametrized by splines, enhancing both representational capacity and interpretability.\nThere is a practical connection between KST and neural networks by showing that any KAN can be constructed as a 3-layer MLP. Consider a KST in the form of sums of functions, a two layer model:\n\\[\nf( x_1 , \\ldots , x_d ) = f( x) = ( g \\circ \\psi ) (x )\n\\]\nThen KAN not only a superposition of functions but also a particular case of a tree of discrete Urysohn operators:\n\\[\nU(x_1 , \\ldots , x_d ) = \\sum_{j=1}^d g_j (x_j )\n\\]\nThis insight leads to a fast scalable algorithm that avoids back-propagation, applicable to any GAM model, using a projection descent method with a Newton-Kacmarz scheme.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "20-theorydl.html#kolmogorov-generalized-additive-models-k-gam",
    "href": "20-theorydl.html#kolmogorov-generalized-additive-models-k-gam",
    "title": "20  Theory of Deep Learning",
    "section": "20.3 Kolmogorov Generalized Additive Models (K-GAM)",
    "text": "20.3 Kolmogorov Generalized Additive Models (K-GAM)\nRather than using learnable functions as network nodes activations, Polson Sokolov directly use KST representation. This is a 2-layer network with a non-differentiable inner function. The network’s architecture can be expressed as:\n\\[\nf(x_1,\\ldots,x_d) = \\sum_{q=0}^{2d} g_q(z_q)\n\\]\nwhere the inner layer performs an embedding from \\([0,1]^d\\) to \\(\\mathbb{R}^{2d+1}\\) via:\n\\[\nz_q = \\eta_q ( x_1 , \\ldots , x_d ) = \\sum_{p=1}^ d \\lambda_p \\psi  ( x_p + q a )\n\\]\nHere, \\(\\lambda_p = \\sum_{r=1}^\\infty \\gamma^{-(p-1)\\beta(r)}\\) is a \\(p\\)-adic expansion with \\(\\beta(r) = (n^r-1)/(n-1)\\) and \\(\\gamma \\geq d+2\\) with \\(a = (\\gamma(\\gamma-1))^{-1}\\).\nThe Köppen function \\(\\psi\\) is defined through a recursive limit:\n\\[\n\\psi(x) = \\lim_{k \\rightarrow \\infty} \\psi_k\\left(\\sum_{l=1}^{k}i_l\\gamma^{-l}\\right)\n\\]\nwhere each \\(x \\in [0,1]\\) has the representation:\n\\[\nx = \\sum_{l=1}^{\\infty}i_l\\gamma^{-l} = \\lim_{k \\rightarrow \\infty} \\left(\\sum_{l=1}^{k}i_l\\gamma^{-l}\\right)\n\\]\nand \\(\\psi_k\\) is defined recursively as:\n\\[\n\\psi_k =\n\\begin{cases}\n    d, & d \\in D_1\\\\\n    \\psi_{k-1}(d-i_k\\gamma^{-k}) + i_k\\gamma^{-\\beta_n(k)}, & d \\in D_k,k&gt;1,i_k&lt;\\gamma-1\\\\\n    \\frac{1}{2}\\left(\\psi_k(d-\\gamma^{-k}) + \\psi_{k-1}(d+\\gamma^{-k})\\right), & d \\in D_k, k&gt;1, i_k = \\gamma - 1\n\\end{cases}\n\\]\nThe most striking aspect of KST is that it leads to a Generalized Additive Model (GAM) with fixed features that are independent of the target function \\(f\\). These features, determined by the Köppen function, provide universal topological information about the input space, effectively implementing a k-nearest neighbors structure that is inherent to the representation.\nThis leads to the following architecture. Any deep learner can be represented as a GAM with feature engineering (topological information) given by features \\(z_k\\) in the hidden layer:\n\\[\n\\begin{align*}\ny_i &= \\sum_{k=1}^{2n+1} g(z_k)\\\\\nz_k &= \\sum_{j=1}^n \\lambda^k\\psi(x_j + \\epsilon k) + k\n\\end{align*}\n\\]\nwhere \\(\\psi\\) is a single activation function common to all nodes, and \\(g\\) is a single outer function.\nOne approach is to replace each \\(\\phi_j\\) with a single ReLU network \\(g\\):\n\\[\ng(x) = \\sum_{k=1}^K \\beta_k\\text{ReLU}(w_kx + b_k)\n\\]\nwhere \\(K\\) is the number of neurons.\n\n20.3.1 Kernel Smoothing: Interpolation\nThe theory of kernel methods was developed by Fredholm in the context of integral equations Fredholm (1903). The idea is to represent a function as a linear combination of basis functions, which are called kernels.\n\\[\nf(x) = \\int_{a}^{b} K(x,x')  d \\mu (x') dx'  \\; \\; {\\rm where} \\; \\; x = ( x_1 , \\ldots , x_d )\n\\]\nHere, the unknown function \\(f(x)\\) is represented as a linear combination of kernels \\(K(x,x')\\) with unknown coefficients \\(\\phi(x')\\). The kernels are known, and the coefficients are unknown. The coefficients are found by solving the integral equation. The first work in this area was done by Abel who considered equations of the form above.\nNowadays, we call those equations Volterra integral equations of the first kind. Integral equations typically arise in inverse problems. Their significance extends beyond their historical origins, as kernel methods have become instrumental in addressing one of the fundamental challenges in modern mathematics: the curse of dimensionality.\nBartlett Nadaraya (1964) and Watson (1964) proposed the use of kernels to estimate the regression function. The idea is to estimate the regression function \\(f(x)\\) at point \\(x\\) by averaging the values of the response variable \\(y_i\\) at points \\(x_i\\) that are close to \\(x\\). The kernel is used to define the weights.\nThe regression function is estimated as follows:\n\\[\n\\hat{f}(x) = \\sum_{i=1}^n  y_i K(x,x_i)/ \\sum_{i=1}^n K(x,x_i) ,\n\\]\nwhere the kernel weights are normalized.\nBoth Nadaraya and Watson considered the symmetric kernel \\(K(x,x') = K(\\|x'-x\\|_2)\\), where \\(||\\cdot||_2\\) is the Euclidean norm. The most popular kernel of that sort is the Gaussian kernel:\n\\[\nK(x,x') = \\exp\\left( -\\dfrac{\\|x-x'\\|_2^2}{2\\sigma^2}\\right).\n\\]\nAlternatively, the 2-norm can be replaced by the inner-product: \\(K(x,x')  =  \\exp\\left( x^Tx'/2\\sigma^2\\right)\\).\nKernel methods are supported by numerous generalization bounds which often take the form of inequalities that describe the performance limits of kernel-based estimators. A particularly important example is the Bayes risk for \\(k\\)-nearest neighbors (\\(k\\)-NN), which can be expressed in a kernel framework as:\n\\[\n\\hat{f} ( x) =  \\sum_{i=1}^N w_i y_i        \\; {\\rm where} \\; w_i := K( x_i , x ) /  \\sum_{i=1}^N K( x_i ,x )   \n\\]\n\\(k\\)-NN classifiers have been proven to converge to an error rate that is bounded in relation to the Bayes error rate, with the exact relationship depending on the number of classes. For binary classification, the asymptotic error rate of \\(k\\)-NN is at most \\(2R^*(1-R^*)\\), where \\(R^*\\) is the Bayes error rate. This theoretical bound suggests potential for improvement in practice. Cover and Hart proved that interpolated k-NN schemes are consistent estimators, meaning that their performance improves with increasing sample size.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "20-theorydl.html#transformers-as-kernel-smoothing",
    "href": "20-theorydl.html#transformers-as-kernel-smoothing",
    "title": "20  Theory of Deep Learning",
    "section": "20.4 Transformers as Kernel Smoothing",
    "text": "20.4 Transformers as Kernel Smoothing\nBahdanau, Cho, and Bengio (2014) proposed using kernel smoothing for sequence-to-sequence learning. This approach estimates the probability of the next word in the sequence using a so-called context vector, which is a weighted average of the vectors from the input sequence \\(h_j\\):\n\\[\nc_i = \\sum_{j=1}^n \\alpha_{ij} h_j,\n\\]\nwhere \\(\\alpha_{ij}\\) are the weights. The weights are defined by the kernel function:\n\\[\n\\alpha_{ij} = \\dfrac{\\exp\\left( e_{ij}\\right)}{\\sum_{k=1}^n \\exp\\left( e_{ik}\\right)}.\n\\]\nInstead of using a traditional similarity measure like the 2-norm or inner product, the authors used a neural network to define the energy function \\(e_{ij} = a(s_{i-1},h_j)\\). This neural network measures the similarity between the last generated element of the output sequence \\(s_{i-1}\\) and \\(j\\)-th element of the input sequence \\(h_j\\). The resulting context vector is then used to predict the next word in the sequence.\n\n20.4.1 Transformer\nTransformers have since become a main building block for various natural language processing (NLP) tasks and has been extended to other domains as well due to their effectiveness. The transformer architecture is primarily designed to handle sequential data, making it well-suited for tasks such as machine translation, language modeling, text generation, and more. It achieves state-of-the-art performance by leveraging a novel attention mechanism.\nThe idea to use kernel smoothing for sequence to sequence was called “attention”, or cross-attention, by Bahdanau, Cho, and Bengio (2014). When used for self-supervised learning, it is called self-attention. When a sequence is mapped to a matrix \\(M\\), it is called multi-head attention. The concept of self-attention and attention for natural language processing was further developed by Vaswani et al. (2023) who developed a smoothing method that they called the transformer.\nThe transformer architecture revolves around a series of mathematical concepts and operations:\n\nEmbeddings: The input text is converted into vectors using embeddings. Each word (or token) is represented by a unique vector in a high-dimensional space.\nPositional Encoding: Since transformers do not have a sense of sequence order (like RNNs do), positional encodings are added to the embeddings to provide information about the position of each word in the sequence.\nMulti-Head Attention: The core of the transformer model. It enables the model to focus on different parts of the input sequence simultaneously. The attention mechanism is defined as: \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] where \\(Q\\), \\(K\\), and \\(V\\) are query, key, and value matrices respectively.\nQuery (Q), Key (K), and Value (V) Vectors: These are derived from the input embeddings. They represent different aspects of the input.\nScaled Dot-Product Attention: The attention mechanism calculates the dot product of the Query with all Keys, scales these values, and then applies a softmax function to determine the weights of the Values.\nMultiple ‘Heads’: The model does this in parallel multiple times (multi-head), allowing it to capture different features from different representation subspaces.\nLayer Normalization and Residual Connections: After each sub-layer in the encoder and decoder (like multi-head attention or the feed-forward layers), the transformer applies layer normalization and adds the output of the sub-layer to its input (residual connection). This helps in stabilizing the training of deep networks.\nFeed-Forward Neural Networks: Each layer in the transformer contains a fully connected feed-forward network applied to each position separately and identically. It is defined as: \\[ \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2 \\] where \\(W_1\\), \\(W_2\\), \\(b_1\\), and \\(b_2\\) are learnable parameters.\nOutput Linear Layer and Softmax: The decoder’s final output passes through a linear layer followed by a softmax layer. This layer converts the decoder output into predicted next-token probabilities.\nTraining and Loss Function: Transformers are often trained using a variant of Cross-Entropy Loss to compare the predicted output with the actual output.\nMasking: In the decoder, to prevent future tokens from being used in the prediction, a technique called ‘masking’ is applied.\nBackpropagation and Optimization: The model’s parameters are adjusted through backpropagation and optimization algorithms like Adam.\n\nLater, Lin et al. (2017) proposed using similar idea for self-supervised learning, where a sequence of words (sentence) is mapped to a single matrix:\n\\[\nM = AH,\n\\]\nwhere \\(H\\) is the matrix representing an input sequence \\(H = (h_1,\\ldots,h_n)\\) and \\(A\\) is the matrix of weights:\n\\[\nA = \\mathrm{softmax}\\left(W_2\\tanh\\left(W_1H^T\\right)\\right).\n\\]\nThis allows to represent a sequence of words of any length \\(n\\) using a “fixed size” \\(r\\times u\\) matrix \\(M\\), where \\(u\\) is the dimension of a vector that represents an element of a sequence (word embedding) and \\(r\\) is the hyper-parameter that defines the size of the matrix \\(M\\).\nThe main advantage of using smoothing techniques (transformers) is that they are parallelizable. Current language models such as BERT, GPT, and T5 rely on this approach. Further, they also has been applied to computer vision and other domains. Its ability to capture long-range dependencies and its scalability have made it a powerful tool for a wide range of applications. See Tsai et al. (2019) et al for further details.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "20-theorydl.html#application",
    "href": "20-theorydl.html#application",
    "title": "20  Theory of Deep Learning",
    "section": "20.5 Application",
    "text": "20.5 Application\n\n20.5.1 Simulated Data\nWe also apply the K-GAM architecture to a simulated dataset to evaluate its performance on data with known structure and relationships. The dataset contains 100 observations generated from the following function:\n\\[\n\\begin{align*}\n    &y = \\mu(x) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1)\\\\\n         &\\mu(x) = 10\\sin(\\pi x_1 x_2) + 20(x_3-0.5)^2 + 10x_4 + 5x_5.\n\\end{align*}\n\\]\nThe goal is to predict the function \\(y(x)\\) based on the input \\(x\\). The dataset is often used as a benchmark dataset for regression algorithms due to its diverse mix of relationships (linear, quadratic, nonlinear, Gaussian random noise) between the input features and the target function.\nWe use the Köppen function to transform the five-dimensional input into a set of 11 features (\\(2d+1\\)). We then learn the outer function \\(g\\) using a ReLU network. To thoroughly investigate the model’s capabilities, we implement two distinct approaches to learning the outer function. The first approach uses different \\(g\\) functions for each feature, following the original KST formulation. This allows each function to specialize in capturing specific patterns, but might be more difficult to train and has more parameters. The second approach uses a single \\(g\\) function for all features, as proposed by Lorentz (1976), providing a more unified and parameter-efficient representation.\nFor the first model with multiple \\(g_i\\) functions, the dimensions of each \\(g_i\\) are as follows: \\(W^0_i \\in \\mathbb{R}^{16\\times 1}\\) and for \\(j=1,\\ldots,18\\), \\(W^j_i \\in \\mathbb{R}^{16\\times 16}\\).\nThe next architecture, which used only one function \\(g\\) for all features, maintains a similar structure to the multiple \\(g\\) functions approach. The only difference is in the dimensionality of the inner layers: we increased the width from 16 to 200. This increased capacity allows the single function to learn more complex patterns and compensate for the constraint of using just one function instead of multiple specialized ones.\n\n\n20.5.2 Training Rates\nConsider the non-parametric condition regression, \\(y_i= f (x_i) + \\epsilon_i\\) where \\(x_i = ( x_{1i} , \\ldots , x_{di} )\\). We wish to estimate \\(f( x_1 , \\ldots , x_d )\\) where \\(x  = ( x_1 , \\ldots , x_d ) \\in [0,1]^d\\). From a classical risk perspective, define\n\\[\nR ( f , \\hat{f}_N ) = E_{X,Y} \\left ( \\lVert  f - \\hat{f}_N \\rVert^2 \\right )\n\\]\nwhere \\(\\lVert . \\rVert\\) denotes \\(L^2 ( P_X)\\)-norm.\nUnder standard assumptions, we have an optimal minimax rate \\(\\inf_{\\hat{f}} \\sup_f R( f , \\hat{f}_N )\\) of \\(O_p \\left ( N^{- 2 \\beta /( 2 \\beta + d )} \\right )\\) for \\(\\beta\\)-Hölder smooth functions \\(f\\). This rate still depends on the dimension \\(d\\), which can be problematic in high-dimensional settings. By restricting the class of functions, better rates can be obtained, including ones that do not depend on \\(d\\). In this sense, we avoid the curse of dimensionality. Common approaches include considering the class of linear superpositions (a.k.a. ridge functions) and projection pursuit models.\nAnother asymptotic result comes from a posterior concentration property. Here, \\(\\hat{f}_N\\) is constructed as a regularized MAP (maximum a posteriori) estimator, which solves the optimization problem\n\\[\n\\hat{f}_N = \\arg \\min_{ \\hat{f}_N } \\frac{1}{N} \\sum_{i=1}^N ( y_i - \\hat{f}_N ( x_i )^2 + \\phi ( \\hat{f}_N )\n\\]\nwhere \\(\\phi(\\hat{f})\\) is a regularization term. Under appropriate conditions, the ensuing posterior distribution \\(\\Pi(f | x, y)\\) can be shown to concentrate around the true function at the minimax rate (up to a \\(\\log N\\) factor).\nA key result in the deep learning literature provides convergence rates for deep neural networks. Given a training dataset of input-output pairs \\(( x_i , y_i)_{i=1}^N\\) from the model \\(y = f(x) + \\epsilon\\) where \\(f\\) is a deep learner (i.e. superposition of functions\n\\[\nf = g_L \\circ \\ldots g_1 \\circ g_0\n\\]\nwhere each \\(g_i\\) is a \\(\\beta_i\\)-smooth Hölder function with \\(d_i\\) variables, that is \\(| g_i (x) -g_i (y) &lt; | x-y |^\\beta\\).\nThen, the estimator has optimal rate:\n\\[\nO \\left ( \\max_{1\\leq i \\leq L } N^{- 2 \\beta^* /( 2 \\beta^* + d_i ) } \\right )  \\; {\\rm where} \\; \\beta_i^* = \\beta_i \\prod_{l = i+1}^L \\min ( \\beta_l , 1 )\n\\]\nThis result can be applied to various function classes, including generalized additive models of the form\n\\[\nf_0 ( x ) = h \\left ( \\sum_{p=1}^d f_{0,p} (x_p) \\right )\n\\]\nwhere \\(g_0(z) = h(z)\\), \\(g_1 ( x_1 , \\ldots , x_d ) = ( f_{01}(x_1) , \\ldots , f_{0d} (x_d) )\\) and \\(g_2 ( y_1 , \\ldots , y_d ) = \\sum_{i=1}^d y_i\\). In this case, \\(d_1 = d_2 = 1\\), and assuming \\(h\\) is Lipschitz, we get an optimal rate of \\(O(N^{-1/3})\\), which is independent of \\(d\\).\nSchmidt-Hieber (2021) show that deep ReLU networks also have optimal rate of \\(O( N^{-1/3} )\\) for certain function classes. For \\(3\\)-times differentiable (e.g. cubic B-splines ), Coppejans (2004) finds a rate of \\(O( N^{-3/7} ) = O( N^{-3/(2 \\times 3 + 1) } )\\). Igelnik and Parikh (2003) finds a rate \\(O( N^{-1} )\\) for Kolmogorov Spline Networks.\nFinally, it’s worth noting the relationship between expected risk and empirical risk. The expected risk, \\(R\\), is typically bounded by the empirical risk plus a term of order \\(1/\\sqrt{N}\\):\n\\[\nR(y, f^\\star) \\leq \\frac{1}{N} \\sum_{i=1}^N R(y_i, f^\\star(x_i)) + O\\left(\\frac{\\|f\\|}{\\sqrt{N}}\\right)\n\\]\nwhere \\(f^\\star\\) is the minimizer of the expected risk. However, in the case of interpolation, where the model perfectly fits the training data, the empirical risk term becomes zero, leaving only the \\(O(1/\\sqrt{N})\\) term.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "20-theorydl.html#general-latent-feature-model",
    "href": "20-theorydl.html#general-latent-feature-model",
    "title": "20  Theory of Deep Learning",
    "section": "20.6 General latent feature model",
    "text": "20.6 General latent feature model\nGiven a training data-set of input-output pairs \\((\\mathbf{X}_i , \\mathbf{Y}_i )_{i=1}^N\\), the goal is to find a prediction rule for a new output \\(\\mathbf{Y}_*\\) given a new input \\(\\mathbf{X}_*\\). Let \\(\\mathbf{Z}\\) denote latent hidden features that are to be hand-coded or learned from the data and our nonlinear latent feature predictive model takes the form:\n\\[\n\\begin{align}\n\\mathbf{Y}\\mid \\mathbf{Z} & \\sim p(\\mathbf{Y} \\mid \\mathbf{Z} ) \\label{eq:bry}\\\\\n\\mathbf{Z} &=\\phi(\\mathbf{X}) \\label{eq:brf}\n\\end{align}\n\\]\nwhere \\(\\phi(\\cdot)\\) is a data transformation that allows for relations between latent features \\(\\mathbf{Z} = \\phi(\\mathbf{X})\\) and \\(\\mathbf{Y}\\) to be modeled by a well-understood probabilistic model \\(p\\). Typically, \\(\\phi(\\cdot)\\) will perform dimension reduction or dimension expansion and can be learned from data. It is worthwhile to emphasize that the top level of such a model is necessarily stochastic.\nAs pointed out before, the basic problem of machine learning is to learn a predictive rule from observed pairs \\((\\mathbf{X},\\mathbf{Y})\\), \\(\\mathbf{Y}_{\\star}  = F(\\mathbf{X}_{\\star} )\\) where \\(F(\\mathbf{X}_{\\star} ) = \\mathbb{E}\\{\\mathbf{Y} \\mid  \\phi(\\mathbf{X}_{\\star})\\}\\). Even though it is well known that deep learners are universal approximators, it is still an open area of research to understand why deep learners generalize well on out-of-sample predictions. One of the important factors for the success of deep learning approach is the ability to perform a non-linear dimensionality reduction.\nThe purely statistical approach requires full specification of the conditional distribution \\(p(\\mathbf{Y} \\mid \\mathbf{X})\\) and then uses conditional probability via Bayes’ rule to perform inference. However, a fully Bayesian approach is often computationally prohibitive in high-dimensional feature space, without resorting to approximations or foregoing UQ. The alternative approach is to perform a data transformation or reduction on the data input \\(\\mathbf{X}\\), such as performing a PCA or PCR first and then use a statistical approach on the transformed feature space. Deep learning methods can fit into this spectrum by viewing it as a non-linear PCA or PLS N. Polson, Sokolov, and Xu (2021),Malthouse, Mah, and Tamhane (1997). However, it possesses some unique properties not available for shallow models.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "20-theorydl.html#deep-learning-expansions",
    "href": "20-theorydl.html#deep-learning-expansions",
    "title": "20  Theory of Deep Learning",
    "section": "20.7 Deep Learning Expansions",
    "text": "20.7 Deep Learning Expansions\nSimilar to a tree model that finds features (aka tree leaves) via recursive space partitioning, the deep learning model finds the regions by using hyperplanes at the first layer and combinations of hyperplanes in the further layers. The prediction rule is embedded into a parameterized deep learner, a composite of univariate semi-affine functions, denoted by \\(F_{\\mathbf{W}}\\) where \\(\\mathbf{W} = [\\mathbf{W}^{(1)}, \\ldots , \\mathbf{W}^{(L)}]\\) represents the weights of each layer of the network. A deep learner takes the form of a composition of link functions:\n\\[\nF_{\\mathbf{W}} = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1  \\; {\\rm where} \\; f_L = \\sigma_L ( \\mathbf{W}_L \\phi (\\mathbf{X}) + \\mathbf{b}_L),\n\\]\nwhere \\(\\sigma_L(\\cdot)\\) is a univariate link or activation function. Specifically, let $^{(l)} $ denote the \\(l^{th}\\) layer, and so \\(\\mathbf{X} = \\mathbf{Z}^{(0)}\\). The final output is the response \\(\\mathbf{Y}\\), which can be numeric or categorical. A deep prediction rule is then \\(\\hat{\\mathbf{Y}}(\\mathbf{X}) = \\mathbf{W}^{(L)} \\mathbf{Z}^{(L)} + \\mathbf{b}^{(L)}\\) where\n\\[\n\\begin{align*}\n\\mathbf{Z}^{(L)} & = f^{(L)} \\left ( \\mathbf{W}^{(L-1)} \\mathbf{Z}^{(L-1)} + \\mathbf{b}^{(L-1)} \\right ),\\\\\n& \\ldots\\\\\n\\mathbf{Z}^{(2)} & = f^{(2)} \\left ( \\mathbf{W}^{(1)} \\mathbf{Z}^{(1)} + \\mathbf{b}^{(1)} \\right ),\\\\\n\\mathbf{Z}^{(1)} & = f^{(1)} \\left (\\mathbf{W}^{(0)} \\phi(\\mathbf{X}) + \\mathbf{b}^{(0)} \\right ).\n\\end{align*}\n\\]\nIt is often beneficial to replace the original input \\(\\mathbf{X}\\) with the features \\(\\mathbf{Z} = \\phi(\\mathbf{X})\\) of lower dimensionality when developing a predictive model for \\(\\mathbf{Y}\\). For example, in the context of regressions, a lower variance prediction rule can be obtained in lower dimensional space. DL simply uses a composition or superposition of semi-affine filters (aka link functions), leading to a new framework for high-dimensional modeling in Section @ref(sec:merging).\nDeep learning can then be viewed as a feature engineering solution and one of finding nonlinear factors via supervised dimension reduction. A composition of hand-coded characteristics i.e. dimension expanding, with supervised learning of data filters i.e. dimension reduction. Advances in computation allow for massive data and gradients of high-dimensional nonlinear filters. Neural networks can be viewed from two perspectives: either as a flexible link function, as in a generalized linear model, or as a method to achieve dimensionality reduction, similar to sliced inverse regression or sufficient dimensionality reduction.\nOne advantage of depth is that the hierarchical mixture allows the width of a given layer to be manageable. With a single layer (e.g., kernel PCA/SVM) we need exponentially many more basis functions in that layer. Consider kernel PCA with say radial basis functions (RBF) kernels: technically there are infinitely many basis functions, but it cannot handle that many input dimensions. Presumably, a deep neural network allows a richer class of covariances that allows anisotropy and non-stationarity. In the end, this is reflected in the function realizations from a DNN. To see this, consider the deep GP models, which are infinite width limits of DNNs. There is a recursive formula connecting the covariance of layer \\(k\\) to that of layer \\(k+1\\), but no closed form. The covariance function of the final hidden layer is probably very complicated and capable of expressing an arbitrary number of features, even if the covariances in each layer may be simple. The increase in dimensionality happens through the hierarchical mixture, rather than trying to do it all in one layer. From a statistical viewpoint, this is similar to the linear shallow wide projections introduced by Wold (1975/ed) and the sufficient dimension reduction framework of Cook (2007).\nIn the context of unsupervised learning, information in the marginal distribution, \\(p(\\mathbf{X})\\), of the input space is used as opposed to the conditional distribution, \\(p(\\mathbf{X}\\mid \\mathbf{Y})\\). Methods such as PCA (PCA), PCR (PCR), Reduced Rank Regression (RRR), Projection-Pursuit Regression (PPR) all fall into this category and PLS (PLS), Sliced Inverse Regression (SIR) are examples of supervised learning of features, see N. G. Polson, Sokolov, et al. (2017) for further discussion.\nWe first uncover the structure in the predictors relevant for modeling the output \\(\\mathbf{Y}\\). The learned factors are denoted by \\(F(\\phi(\\mathbf{X}))\\) and are constructed as a sequence of input filters. The predictive model is given by a probabilistic model of the form \\(p(\\mathbf{Y} \\mid \\mathbf{X}) \\equiv p(\\mathbf{Y} \\mid F(\\phi(\\mathbf{X})))\\). Here \\(\\phi: \\mathbb{R}^p \\mapsto \\mathbb{R}^c,~c \\gg p\\) initially expands the dimension of the input space by including terms such as interactions, dummy variables (aka one-hot encodings) and other nonlinear features of the input space deemed relevant. Then, \\(F\\) reduces dimension of deep learning by projecting back with a univariate activation function into an affine space (aka regression). This framework also sheds light on how to build deep (skinny) architectures. Given \\(n\\) data points, we split into \\(L = 2^p\\) regions so that there is a fixed sample size within each bin. This process transforms \\(\\mathbf{X}\\) into many interpretable characteristics. This can lead to a huge number of predictors that can be easily dealt within the DL architecture, making the advantage of depth clear.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "20-theorydl.html#sec:dim-exp",
    "href": "20-theorydl.html#sec:dim-exp",
    "title": "20  Theory of Deep Learning",
    "section": "20.8 Dimensionality Expansion",
    "text": "20.8 Dimensionality Expansion\nFirst, we review ‘dimensionality expansions’: data transformation that transforms an input vector \\(\\mathbf{X}\\) into a higher dimensional vector \\(\\phi(\\mathbf{X})\\). One approach is to use hand-coded predictors. This expanded set can include terms such as interactions, dummy variables or nonlinear functional of the original predictors. The goal is to model the joint distribution of outputs and inputs, namely $p( , ()) $, where we allow our stochastic predictors.\nKernel Expansion: The kernel expansion idea is to enlarge the feature space via basis expansion. The basis is expanded using nonlinear transformations of the original inputs: \\[\\phi(\\mathbf{X}) = (\\phi_1(\\mathbf{X}),\\phi_2(\\mathbf{X}),\\ldots,\\phi_M(\\mathbf{X}))\\] so that linear regression \\(\\hat{\\mathbf{Y}} = \\phi(\\mathbf{X})^T\\beta + \\beta_0\\) or generalized linear model can be used to model the input-output relations. Here, the ‘kernel trick’ increases dimensionality, and allows hyperplane separation while avoiding an exponential increase in the computational complexity. The transformation \\(\\phi(\\mathbf{x})\\) is specified via a kernel function \\(K(\\cdot, \\cdot)\\) which calculates the dot product of feature mappings: \\(K(\\mathbf{x},\\mathbf{x}') = \\phi(\\mathbf{x})^T\\phi(\\mathbf{x}').\\) By choosing a feature map \\(\\phi\\), we implicitly choose a kernel function and, conversely, every positive semi-definite kernel matrix corresponds to a feature mapping \\(\\phi\\). For example, when \\(\\mathbf{X}\\in \\mathbb{R}^2\\), choosing \\(K(\\mathbf{X},\\mathbf{X}') = (1+\\mathbf{X}^T\\mathbf{X}')^2\\) is equivalent to expanding the basis to \\(\\phi(\\mathbf{X}) = (1,\\sqrt{2}\\mathbf{X}_1, \\sqrt{2}\\mathbf{X}_1, \\mathbf{X}_1^2,\\mathbf{X}_2^2,\\sqrt{2}\\mathbf{X}_1 \\mathbf{X}_2)\\).\nTree Expansion: Similar to kernels, we can think of trees as a technique for expanding a feature space. Each region in the input space defined by a terminating node of a tree corresponds to a new feature. Then, the predictive rule becomes very simple: identify in which region the new input is and use the average across observations or a majority voting rule from this region to calculate the prediction.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "20-theorydl.html#sec:pca-pcr",
    "href": "20-theorydl.html#sec:pca-pcr",
    "title": "20  Theory of Deep Learning",
    "section": "20.9 Dimensionality Reduction: PCA, PCR and PLS",
    "text": "20.9 Dimensionality Reduction: PCA, PCR and PLS\nGiven input/predictors \\(\\mathbf{X}\\) and response \\(\\mathbf{Y}\\) and associated observed data \\(\\mathbf{X}\\in {\\mathbb{R}}^{n \\times p}\\) and \\(\\mathbf{Y} \\in {\\mathbb{R}}^{n\\times q}\\), the goal is to find data transformations \\((\\mathbf{Y},\\mathbf{X}) \\mapsto \\phi(\\mathbf{Y},\\mathbf{X})\\) so that modeling the transformed data becomes an easier task. In this paper, we consider several types of transformations and model non-linear relations.\nWe start by reviewing the widely used singular value decomposition (SVD) which allows finding linear transformations to identify a lower dimensional representation of either \\(\\mathbf{X}\\), by what is known as principal component analysis (PCA), or, when using both \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), known as partial least squares (PLS). First, start with the SVD decomposition of the input matrix: \\(\\mathbf{X} = \\mathbf{U} \\mathbf{D} \\mathbf{W}^T\\), which is full-rank if \\(n &gt; p\\). Here, $ = (d_1 , , d_p ) $ are the nonzero ordered singular values (\\(d_1 \\ge \\ldots \\ge d_p)\\) . The matrices \\(\\mathbf{U}\\) and \\(\\mathbf{W}\\) are orthogonal matrices of dimensions \\(n\\times p\\) and \\(p \\times p\\) with columns of \\(\\mathbf{U}\\) as the right singular vectors and columns of \\(\\mathbf{W}\\) as the left singular vectors, \\(\\mathbf{W}\\) can be also thought of as the matrix consisting of eigenvectors for \\(\\mathbf{S} = \\mathbf{X}^T \\mathbf{X}\\). We can then transform the original first layer to an orthogonal regression, namely defining \\(\\mathbf{Z} = \\mathbf{U}\\mathbf{D}\\) whose columns are the principal components. For PCR, using \\(\\boldsymbol{\\alpha} = \\mathbf{W}^T \\boldsymbol{\\beta}\\), we arrive at the corresponding OLS estimator \\(\\hat{\\boldsymbol{\\alpha}} = (\\mathbf{Z}^T \\mathbf{Z} )^{-1} \\mathbf{Z}^T \\mathbf{Y} = \\mathbf{D}^{-1} \\mathbf{U}^T \\mathbf{y}\\), and obtain \\(\\hat{y}_{pcr} = \\sum_{j=1}^{K}\\hat{\\alpha}_j \\mathbf{z}_j\\), where \\(\\hat{\\alpha}_j = \\mathbf{z}_j^T \\mathbf{y}/\\mathbf{z}_j^T \\mathbf{z}_j\\), since \\(\\mathbf{z}_j\\)’s are orthogonal, and \\(K \\ll p\\) denotes the reduced dimension that captures a certain percentage of the total variability.\nPCR, as an unsupervised approach to dimension reduction, has a long history in statistics. Specifically, we first center and standardize \\((\\mathbf{Y}, \\mathbf{X})\\), followed by a singular value decomposition of \\(\\mathbf{V}: = \\mathrm{ave} ( \\mathbf{X} \\mathbf{X}^T )  = \\dfrac{1}{n}\\sum_{i=1}^{n}\\mathbf{X}_i\\mathbf{X}_i^T\\) where \\(\\mathrm{ave}(\\cdot)\\) denotes the empirical average. Then, we find the eigenvalues $e_j^2 $ and eigenvectors arranged in non-increasing order, so we can write:\n\\[\n\\mathbf{V} = \\sum_{j=1}^p  e_j^2 {\\mathbf{v}}_j {\\mathbf{v}}_k^T .\n\\]\nThis leads to a sequence of regression models \\((\\hat Y_0, \\ldots, \\hat Y_K)\\) with $Y_0 $ being the overall mean:\n\\[\n\\hat{Y}_L = \\sum_{l=0}^K (\\mathrm{ave} ( \\mathbf{W}_l^T \\mathbf{X} ) / e_l^2 ) \\mathbf{v}_l^T \\mathbf{X}.\n\\]\nTherefore, PCR finds features \\(\\{\\mathbf{Z}_k\\}_{k=0}^K = \\{\\mathbf{v}_k^T \\mathbf{x}\\}_{k=0}^K = \\{\\mathbf{f}_k \\}_{k=0}^K\\).\nPLS and SVD Algorithm: Partial least squares, or PLS, is a related dimension reduction technique similar to PCR that first identifies a lower-dimensional set of features and then fits a linear model on this feature set, but PLS does this in a supervised fashion unlike PCR. In successive steps, PLS finds a reduced dimensional representation of \\(\\mathbf{X}\\) that is relevant for the response \\(\\mathbf{Y}\\).\nPCA and multivariate output: PCA requires us to compute a reduction of multivariate output \\(\\mathbf{Y}\\) using a singular value decomposition of \\(\\mathbf{Y}\\) by finding eigenvectors of \\(\\mathbf{Z} = \\mathrm{ave}(\\mathbf{Y}\\mathbf{Y}^T)\\). Then, the output is a linear combination of the singular vectors\n\\[\n\\mathbf{Y} = \\mathbf{W}_1 \\mathbf{Z}_1 + \\cdots + \\mathbf{W}_k \\mathbf{Z}_k,\n\\]\nwhere the weights \\(\\mathbf{W}_i\\) follow a Gaussian Process, \\(\\mathbf{W}\\sim \\mathrm{GP}(m,K)\\). Hence, the method can be highly non-linear. This method is typically used when input variables come from a designed experiment. If the interpretability of factors is not important, and from a purely predictive point of view, PLS will lead to improved performance.\nIn the light of the above, one can view deep learning models as non-stochastic hierarchical data transformations. The advantage is that we can learn deterministic data transformations before applying a stochastic model. This allows us to establish a connection between a result due to Brillinger (2012) and the use of deep learning models to develop a unified framework for modeling complex high-dimensional data sets. The prediction rule can be viewed as interpolation. In high-dimensional spaces, one can mix-and-match the deterministic and stochastic data transformation rules.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "20-theorydl.html#uncertainty-quantification",
    "href": "20-theorydl.html#uncertainty-quantification",
    "title": "20  Theory of Deep Learning",
    "section": "20.10 Uncertainty Quantification",
    "text": "20.10 Uncertainty Quantification\nOur probabilistic model takes the form \\(\\mathbf{Y} \\mid F \\sim p(\\mathbf{Y} \\mid F )\\), $F = g ( ) $, where \\(\\mathbf{Y}\\) is possibly a multivariate output matrix and \\(\\mathbf{X}\\) is a $n p $ matrix of input variables, and \\(\\mathbf{B} \\mathbf{X}\\) performs dimension reduction. Here \\(g = g_{\\mathbf{W}, \\mathbf{b}}\\) is a deep learner and the parameters \\((\\hat{\\mathbf{W}} , \\hat{\\mathbf{b}} )\\) are estimated using traditional SGD methods. The key result, due to Brillinger (2012) and Naik and Tsai (2000) is that \\(\\hat{\\mathbf{B}}\\) can be estimated consistently, up to a constant of proportionality, using PLS irrespective of the nonlinearity on \\(g\\). Even though Brillinger (2012) assumes that input \\(\\mathbf{X}\\) is Gaussian in order to apply Stein’s lemma, this result generalizes to scale-mixtures of Gaussians. See also Iwata (2001) who provides analytical derivation of the uncertainty intervals for ReLU and Probit nonlinear activation functions.\nThe key insight here is that the lion’s share of the UQ can be done at the top layer that outputs \\(\\mathbf{Y}\\) as the uncertainty in the dimension reduction of the \\(\\mathbf{X}\\) space is much harder to quantify compared to quantifying the uncertainty for the prediction rule. By merging the two cultures, the probabilistic model on the first stage and deep learning on the subsequent stage, for the transformation of input data, we can obtain the best of both worlds.\nGiven a specification of \\(g\\), the constant of proportionality can also be estimated consistently with \\(\\sqrt{n}\\)-asymptotics. Hence, to predict at a new level \\(\\mathbf{X}_{\\star}\\), we can use the predictive distribution to make a forecast and provide uncertainty bounds.\n\\[\n\\begin{align*}\n    \\mathbf{Y}_{\\star}  & \\sim  p \\left ( \\mathbf{Y} \\; \\mid \\;   g_{ \\hat{\\mathbf{W}}, \\hat{\\mathbf{b}}} ( \\hat{\\mathbf{B}}_{\\mathrm{PLS}} \\mathbf{X} )  \\right )   \\\\\n    \\hat{\\mathbf{Y}}_* &  =  \\mathrm{E(\\mathbf{Y}_* \\mid \\mathbf{F}_*)} = \\mathrm{E}_{\\mathbf{B} \\mid \\mathbf{X},\\mathbf{Y}} \\left(g(\\hat{\\mathbf{B}}_{\\mathrm{PLS}}\\mathbf{X}_*)\\right),\n\\end{align*}\n\\tag{20.1}\\]\nwhere \\(\\hat{\\mathbf{B}}_{\\mathrm{PLS}}\\) is given by the left-hand side of Equation 20.1.\nNotice that we can also incorporate uncertainty in the estimation of \\(\\mathbf{B}\\) via the posterior \\(p(\\mathbf{B} \\mid \\mathbf{X},\\mathbf{Y})\\). Furthermore, a result of Iwata (2001) can be used to show that the posterior distribution is asymptotically normal. Hence, we can calculate the expectations analytically for activation functions, such as ReLU. As ReLU is convex, Jensen’s inequality \\(g\\left(\\mathrm{E}(\\mathbf{B} \\mathbf{X})\\right) \\le \\mathrm{E}\\left(g(\\mathbf{B} \\mathbf{X})\\right)\\) shows that ignoring parameter uncertainty leads to under-prediction.\n\n\n\n\nActor, Jonas. 2018. “Computation for the Kolmogorov Superposition Theorem.” {{MS Thesis}}, Rice.\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” arXiv. https://arxiv.org/abs/1409.0473.\n\n\nBrillinger, David R. 2012. “A Generalized Linear Model With ‘Gaussian’ Regressor Variables.” In Selected Works of David Brillinger, edited by Peter Guttorp and David Brillinger, 589–606. Selected Works in Probability and Statistics. New York, NY: Springer.\n\n\nCook, R. Dennis. 2007. “Fisher Lecture: Dimension Reduction in Regression.” Statistical Science, 1–26. https://www.jstor.org/stable/27645799.\n\n\nCoppejans, Mark. 2004. “On Kolmogorov’s Representation of Functions of Several Variables by Functions of One Variable.” Journal of Econometrics 123 (1): 1–31.\n\n\nDemb, Robert, and David Sprecher. 2021. “A Note on Computing with Kolmogorov Superpositions Without Iterations.” Neural Networks 144 (December): 438–42.\n\n\nDiaconis, Persi, and Mehrdad Shahshahani. 1984. “On Nonlinear Functions of Linear Combinations.” SIAM Journal on Scientific and Statistical Computing 5 (1): 175–91.\n\n\nFredholm, Ivar. 1903. “Sur Une Classe d’équations Fonctionnelles.” Acta Mathematica 27 (none): 365–90.\n\n\nFriedman, Jerome H., and Werner Stuetzle. 1981. “Projection Pursuit Regression.” Journal of the American Statistical Association 76 (376): 817–23.\n\n\nIgelnik, B., and N. Parikh. 2003. “Kolmogorov’s Spline Network.” IEEE Transactions on Neural Networks 14 (4): 725–33.\n\n\nIwata, Shigeru. 2001. “Recentered and Rescaled Instrumental Variable Estimation of Tobit and Probit Models with Errors in Variables.” Econometric Reviews 20 (3): 319–35.\n\n\nKolmogorov, AN. 1956. “On the Representation of Continuous Functions of Several Variables as Superpositions of Functions of Smaller Number of Variables.” In Soviet. Math. Dokl, 108:179–82.\n\n\nLin, Zhouhan, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. “A Structured Self-attentive Sentence Embedding.” arXiv. https://arxiv.org/abs/1703.03130.\n\n\nLorentz, George G. 1976. “The 13th Problem of Hilbert.” In Proceedings of Symposia in Pure Mathematics, 28:419–30. American Mathematical Society.\n\n\nMalthouse, Edward, Richard Mah, and Ajit Tamhane. 1997. “Nonlinear Partial Least Squares.” Computers & Chemical Engineering 12 (April): 875–90.\n\n\nNadaraya, E. A. 1964. “On Estimating Regression.” Theory of Probability & Its Applications 9 (1): 141–42.\n\n\nNaik, Prasad, and Chih-Ling Tsai. 2000. “Partial Least Squares Estimator for Single-Index Models.” Journal of the Royal Statistical Society. Series B (Statistical Methodology) 62 (4): 763–71. https://www.jstor.org/stable/2680619.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep Learning: A Bayesian Perspective.” Bayesian Analysis 12 (4): 1275–1304.\n\n\nPolson, Nicholas, Vadim Sokolov, and Jianeng Xu. 2021. “Deep Learning Partial Least Squares.” arXiv Preprint arXiv:2106.14085. https://arxiv.org/abs/2106.14085.\n\n\nSchmidt-Hieber, Johannes. 2021. “The Kolmogorov–Arnold Representation Theorem Revisited.” Neural Networks 137 (May): 119–26.\n\n\nTsai, Yao-Hung Hubert, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. “Transformer Dissection: A Unified Understanding of Transformer’s Attention via the Lens of Kernel.” arXiv. https://arxiv.org/abs/1908.11775.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. “Attention Is All You Need.” arXiv. https://arxiv.org/abs/1706.03762.\n\n\nWatson, Geoffrey S. 1964. “Smooth Regression Analysis.” Sankhyā: The Indian Journal of Statistics, Series A (1961-2002) 26 (4): 359–72. https://www.jstor.org/stable/25049340.\n\n\nWold, Herman. 1975/ed. “Soft Modelling by Latent Variables: The Non-Linear Iterative Partial Least Squares (NIPALS) Approach.” Journal of Applied Probability 12 (S1): 117–42.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "21-nn.html",
    "href": "21-nn.html",
    "title": "21  Neural Networks",
    "section": "",
    "text": "21.1 Introduction\nThe goal of this section paper is to provide an overview of of Deep Learning (DL) for statisticians. To do this, we have discussed the model estimation procedure and demonstrated that DL is an extension of a generalized linear model. One goal of statistics is to build predictive models along with uncertainty and to develop an understanding of the data generating mechanism. Data models are well studied in statistical literature, but often do not provide enough flexibility to learn the input-output relations. Closed box predictive rules, such as trees and neural networks, are more flexible learners. However, in high-dimensional problems, finding good models is challenging, and this is where deep learning methods shine. We can think of deterministic DL model as a transformation of high dimensional input and outputs. Hidden features lie on the transformed space, and are empirically learned as opposed to theoretically specified.\nAlthough DL models have been almost exclusively used for problems of image analysis and natural language processing, more traditional data sets, which arise in finance, science and engineering, such as spatial and temporal data can be efficiently analyzed using deep learning. Thus, DL provides an alternative for applications where traditional statistical techniques apply. There are a number of areas of future research for Statisticians. In particular, uncertainty quantification and model selection, such as architecture design, as well as algorithmic improvements and Bayesian deep learning. We hope this review will make DL models accessible for statisticians.\nIn recent yeats neural networks and deep learning has re-emerged as a dominant technology for natural language processing (NLP), image analysis and reinforcement learning. The majority of applications use feed-forward neural network architectures such as convolutional neural networks and transformers. In this chapter we will focus on the feed-forward neural networks and their applications to regression and classification problems. We will also discuss the computational aspects of deep learning and its implementation in R and Python.\nOur goal is to provide a review of deep learning methods which provide insight into structured high-dimensional data. Rather than using shallow additive architectures common to most statistical models, deep learning uses layers of semi-affine input transformations to provide a predictive rule. Applying these layers of transformations leads to a set of attributes (or, features) to which probabilistic statistical methods can be applied. Thus, the best of both worlds can be achieved: scalable prediction rules fortified with uncertainty quantification, where sparse regularization finds the features.\nDeep learning is one of the widely used machine learning method for analysis of large scale and high-dimensional data sets. Large-scale means that we have many samples (observations) and high dimensional means that each sample is a vector with many entries, usually hundreds and up.\nMachine learning is the engineer’s version of statistical data analysis. Major difference between ML and statistics is that ML focuses on practical aspects, such as computational efficiency and ease of use of techniques. While statistical analysis is more concerned with rigorousness of the analysis and interpretability of the results.\nDeep learning provides a powerful pattern matching tool suitable for many AI applications. Image recognition and text analysis are probably two of the deep learning’s most successful. From a computational perspective, you can think of an image or a text as a high dimensional matrices and vectors, respectively. The problem of recognizing objects in images or translating a text requires designing complex decision boundaries in the high dimensional space of inputs.\nAlthough, image analysis and natural language processing are the applications where deep learning is the dominating approach, more traditional engineering and science applications, such as spatio-temporal and financial analysis is where DL also showed superior performance compared to traditional statistical learning techniques (N. Polson, Sokolov, and Xu 2021; Nicholas G. Polson, Sokolov, et al. 2017; Dixon, Polson, and Sokolov 2019; Sokolov 2017; Bhadra et al. 2021, 2021; Behnia, Karbowski, and Sokolov 2021; Nareklishvili, Polson, and Sokolov 2022, 2023b, 2023a; Nicholas G. Polson and Sokolov 2023; N. Polson and Sokolov 2020)\nThere are several deep learning architectures exist - each has its own uses and purposes. Convolutional Neural Networks (CNN) deal with 2-dimensional input objects, i.e. images and were shown to outperform any other techniques. Recurrent Neural Networks (RNN) were shown the best performance on speech and text analysis tasks.\nIn general, a neural network can be described as follows. Let \\(f_1 , \\ldots , f_L\\) be given univariate activation functions for each of the \\(L\\) layers. Activation functions are nonlinear transformations of weighted data. A semi-affine activation rule is then defined by \\[\nf_l^{W,b} = f_l \\left ( \\sum_{j=1}^{N_l} W_{lj} X_j + b_l \\right ) = f_l ( W_l X_l + b_l )\\,,\n\\] which implicitly needs the specification of the number of hidden units \\(N_l\\). Our deep predictor, given the number of layers \\(L\\), then becomes the composite map\n\\[\n\\hat{Y}(X) = F(X) = \\left ( f_l^{W_1,b_1} \\circ \\ldots \\circ f_L^{W_L,b_L} \\right ) ( X)\\,.\n\\]\nThe fact that DL forms a universal “basis” which we recognize in this formulation dates to Poincare and Hilbert is central. From a practical perspective, given a large enough data set of “test cases”, we can empirically learn an optimal predictor. Similar to a classic basis decomposition, the deep approach uses univariate activation functions to decompose a high dimensional \\(X\\).\nLet \\(Z^{(l)}\\) denote the \\(l\\)th layer, and so \\(X = Z^{(0)}\\). The final output \\(Y\\) can be numeric or categorical. The explicit structure of a deep prediction rule is then \\[\n\\begin{aligned}\n\\hat{Y} (X) & = W^{(L)} Z^{(L)} + b^{(L)} \\\\\nZ^{(1)} & = f^{(1)} \\left ( W^{(0)} X + b^{(0)} \\right ) \\\\\nZ^{(2)} & = f^{(2)} \\left ( W^{(1)} Z^{(1)} + b^{(1)} \\right ) \\\\\n\\ldots  & \\\\\nZ^{(L)} & = f^{(L)} \\left ( W^{(L-1)} Z^{(L-1)} + b^{(L-1)} \\right )\\,.\n\\end{aligned}\n\\] Here \\(W^{(l)}\\) is a weight matrix and \\(b^{(l)}\\) are threshold or activation levels. Designing a good predictor depends crucially on the choice of univariate activation functions \\(f^{(l)}\\). The \\(Z^{(l)}\\) are hidden features which the algorithm will extract.\nPut differently, the deep approach employs hierarchical predictors comprising of a series of \\(L\\) nonlinear transformations applied to \\(X\\). Each of the \\(L\\) transformations is referred to as a layer, where the original input is \\(X\\), the output of the first transformation is the first layer, and so on, with the output \\(\\hat Y\\) as the first layer. The layers \\(1\\) to \\(L\\) are called hidden layers. The number of layers \\(L\\) represents the depth of our routine.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "21-nn.html#motivating-examples",
    "href": "21-nn.html#motivating-examples",
    "title": "21  Neural Networks",
    "section": "21.2 Motivating Examples",
    "text": "21.2 Motivating Examples\nInteraction terms, \\(x_1 x_2\\) and \\((x_1 x_2)^2\\), and max functions, \\(\\max(x_1, x_2)\\) can be expressed as nonlinear functions of semi-affine combinations. Specifically:\n\\[\nx_1x_2 = \\frac{1}{4} ( x_1+x_2 )^2 - \\frac{1}{4} (x_1-x_2)^2\n\\]\n\\[\n\\max(x_1,x_2) = \\frac{1}{2} | x_1+x_2 | + \\frac{1}{2} | x_1-x_2 |\n\\]\n\\[\n(x_1x_2)^2 = \\frac{1}{4} ( x_1+x_2 )^4 + \\frac{7}{4 \\cdot 3^3} (x_1-x_2)^4 - \\frac{1}{2 \\cdot 3^3} ( x_1+ 2 x_2)^4 - \\frac{2^3}{3^3} ( x_1 + \\frac{1}{2} x_2 )^4\n\\]\nDiaconis and Shahshahani (1981) provide further discussion for Projection Pursuit Regression, where the network uses a layered model \\(\\sum_{i=1}^N f ( w_i^\\top X )\\). They provide an ergodic view of composite iterated functions, a precursor to the use of multiple layers of single operators that can model complex multivariate systems.\nDeep ReLU architectures can be viewed as Max-Sum networks via the following simple identity. Define \\(x^+ = \\max(x,0)\\). Let \\(f_x(b) = (x + b)^+\\) where \\(b\\) is an offset. Then \\((x + y^+)^+ = \\max(0, x, x+y)\\). This is generalized in Feller (1971) (p.272) who shows by induction that\n\\[\n( f_{x_1} \\circ \\ldots \\circ f_{x_k} ) (0) = ( x_1 + ( x_2 + \\ldots + ( x_{k-1} + x_k^+ )^+ )^+ = \\max_{1 \\leq j \\leq k} ( x_1 + \\ldots + x_j )^+\n\\]\nA composition or convolution of \\(\\max\\)-layers is then a one layer max-sum network.\nAuto-encoding is an important data reduction technique. An auto-encoder is a deep learning architecture designed to replicate \\(X\\) itself, namely \\(X=Y\\), via a bottleneck structure. This means we select a model \\(F^{W,b}(X)\\) which aims to concentrate the information required to recreate \\(X\\). See Heaton et al (2017) for an application to smart indexing in finance.\nSuppose that we have \\(N\\) input vectors \\(X = \\{ x_1 , \\ldots , x_N \\} \\in \\mathbb{R}^{M\\times N}\\) and \\(N\\) output (or target) vectors \\(\\{ x_1 , \\ldots , x_N \\} \\in \\mathbb{R}^{M\\times N}\\). Setting biases to zero, for the purpose of illustration, and using only one hidden layer (\\(L=2\\)) with \\(K &lt; N\\) factors, gives for \\(j=1, \\ldots, N\\):\n\\[\nY_j(x) = F^m_{W} ( X )_j = \\sum_{k=1}^K W^{jk}_2 f \\left ( \\sum_{i=1}^N W^{ki}_1 x_i \\right ) =  \\sum_{k=1}^K W^{jk}_2 Z_j \\quad \\text{for } Z_j =  f \\left ( \\sum_{i=1}^N W^{ki}_1 x_i \\right )\n\\]\nSince, in an auto-encoder, we fit the model \\(X = F_{W}( X)\\), and train the weights \\(W = (W_1, W_2)\\) with regularization penalty in a\n\\[\n\\mathcal{L} ( W )  =  \\operatorname{argmin}_W \\Vert X - F_W (X) \\Vert^2  + \\lambda \\phi(W)\n\\]\nwith\n\\[\n\\phi(W) = \\sum_{i,j,k} | W^{jk}_1 |^2 +  | W^{ki}_2 |^2.\n\\]\nWriting the DL objective as an augmented Lagrangian (as in ADMM) with a hidden factor \\(Z\\), leads to a two step algorithm: an encoding step (a penalty for \\(Z\\)), and a decoding step for reconstructing the output signal via\n\\[\n\\operatorname{argmin}_{W,Z} \\Vert X - W_2 Z \\Vert^2 + \\lambda \\phi(Z) + \\Vert Z -  f( W_1, X ) \\Vert^2,\n\\]\nwhere the regularization on \\(W_1\\) induces a penalty on \\(Z\\). The last term is the encoder, the first two the decoder.\nIf \\(W_2\\) is estimated from the structure of the training data matrix, then we have a traditional factor model, and the \\(W_1\\) matrix provides the factor loadings. PCA, PLS, SIR fall into this category (see Cook 2007 for further discussion). If \\(W_2\\) is trained based on the pair \\(\\hat{X}=\\{Y,X\\}\\) then we have a sliced inverse regression model. If \\(W_1\\) and \\(W_2\\) are simultaneously estimated based on the training data \\(X\\), then we have a two layer deep learning model.\nAuto-encoding demonstrates that deep learning does not directly model variance-covariance matrix explicitly as the architecture is already in predictive form. Given a hierarchical non-linear combination of deep learners, an implicit variance-covariance matrix exists, but that is not the driver of the algorithm.\nAnother interesting area for future research are long short term memory models (LSTMs). For example, a dynamic one layer auto-encoder for a financial time series \\((Y_t)\\) is a coupled system of the form\n\\[\nY_t = W_x X _t + W_y Y_{t-1} \\quad \\text{and} \\quad \\begin{pmatrix} X_t \\\\ Y_{t-1} \\end{pmatrix} = W Y_t\n\\]\nHere, the state equation encodes and the matrix \\(W\\) decodes the \\(Y_t\\) vector into its history \\(Y_{t-1}\\) and the current state \\(X_t\\).\nWe will start with applying a feed-forward neural network with one hidden layer to a problem of binary classification on a simulated data set. We start by generating a simple dataset shown in Figure below. The data is generated from a mixture of two distributions (Gaussian and truncated Gaussian). The red points are the positive class and the green points are the negative class. The goal is to find a model boundary that discriminates the two classes.\n\n\n\n\n\n\n\n\n\nLet’s try to use a simple logistic regression model to separate the two classes.\n\n# Fit a logistic regression model\nfit = glm(label~x1+x2, data=as.data.frame(d), family=binomial(link='logit'))\n# Plot the training dataset\nplot(d[,2],d[,3], col=d[,1]+2, pch=16, xlab=\"x1\", ylab=\"x2\")\nth = fit$coefficients\n# Plot the decision boundary\nabline(-th[1]/th[3], -th[2]/th[3], col=2)\n\n\n\n\n\n\n\n\nWe can see that a logistic regression could not do it. It uses a single line to separate observations of two classes. We can see that the data is not linearly separable. However, we can use multiple lines to separate the data.\n\nplot(x1~x2, data=d,col=d[,1]+2, pch=16)\n# Plot lines that separate once class (red) from another (green)\nlines(x1, -x1 - 6); text(-4,-3,1)\nlines(x1, -x1 + 6); text(4,3,2)\nlines(x1,  x1 - 6); text(4,-3,3)\nlines(x1,  x1 + 6); text(-3,4,4)\n\n\n\n\n\n\n\n\nNow, we do the same thing as in simple logistic regression and apply logistic function to each of those lines\n\n# Define sigmoid function\nsigmoid  = function(z) exp(z)/(1+exp(z))\n\n# Define hidden layer of our neural network\nfeatures = function(x1,x2) {\n  z1 =  6 + x1 + x2; a1 = sigmoid(z1)\n  z2 =  6 - x1 - x2; a2 = sigmoid(z2)\n  z3 =  6 - x1 + x2; a3 = sigmoid(z3)\n  z4 =  6 + x1 - x2; a4 = sigmoid(z4)\n  return(c(a1,a2,a3,a4))\n}\n\nUsing the matrix notaitons, we have \\[\nz = \\sigma(Wx + b), ~ W = \\begin{bmatrix} 1 & 1 \\\\ -1 & -1 \\\\ -1 & 1 \\\\ 1 & -1 \\end{bmatrix}, ~ b = \\begin{bmatrix} 6 \\\\ 6 \\\\ 6 \\\\ 6 \\end{bmatrix}, ~ \\sigma(z) = \\frac{1}{1+e^{-z}}\n\\]\nThe model shown above is the first layer of our neural network. It takes a two-dimensional input \\(x\\) and produces a four-dimensional output \\(z\\) which is a called a feature vector. The feature vector is then passed to the output layer, which applies simple logistic regression to the feature vector. \\[\n\\hat{y} = \\sigma(w^Tz + b), ~ w = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, ~ b = -3.1, ~ \\sigma(z) = \\frac{1}{1+e^{-z}}\n\\]\nThe output of the output layer is the probability of the positive class.\n\n# Calculate prediction (classification) using our neural network\npredict_prob = function(x){\n  x1 = x[1]; x2 = x[2]\n  z = features(x1,x2)\n  # print(z)\n  mu = sum(z) - 3.1\n  # print(mu)\n  sigmoid(mu)\n}\n\nWe can use our model to do the predictions now\n\n# Predict the probability of the positive class for a given point\npredict_prob(c(0,0))\n\n 0.71\n\npredict_prob(c(0,10))\n\n 0.26\n\n\nThe model generates sensible predictions, let’s plot the decision boundary to see how well it separates the data.\n\nx1 = seq(-11,11,length.out = 100)\nx2 = seq(-11,11,length.out = 100)\ngr = as.matrix(expand.grid(x1,x2));\n\n 10000     2\n\nyhat = apply(gr,1,predict_prob)\n\n 10000\n\nimage(x1,x2,matrix(yhat,ncol = 100), col = heat.colors(20,0.7))\n\n\n\n\n\n\n\n\nHow about a regression model. We will use a one-layer neural network to fit a quadratic function. We simulate noisy data from the following model \\[\ny = 0.5 + 0.3x^2 + \\epsilon, ~ \\epsilon \\sim N(0,0.02^2)\n\\] And use 3 hidden units in the first hidden layer and two units in the second hidden layer. The output layer is a single unit. We will use the hyperbolic tangent (tanh) activation function for all layers. The model is defined as follows\n\nrelu = function(x) max(0,x)\nnn = function(W,f=relu) {\n    b0 = W[1]; w0=W[2:4];b1 = W[5]; w1 = W[6:8]\n    z0 = apply(b0 + outer(x,w0,'*'),1:2,f)\n    yhat = b1 + z0 %*% w1\n    return(list(yhat = yhat[,1],z0=z0))\n}\n\nThe hidden layer has three outputs (neurons) and uses the ReLu activation function. The output linear layer has a single output. Thus, the prediction yhat is generated as a linear model of the feature vector z0. The model has 8 parameters Let’s generate training data and fit the model. We will use the BFGS optimization algorithm to minimize the loss function (negative log-likelihood) of the model.\n\nset.seed(99) #gretzky\nnl  = c(3,2)\nparams = c(0,rnorm(3),0,rnorm(3))\nx = seq(-1,1,0.02)\ny = 0.5 + 0.3*x^2 + rnorm(length(x),0,0.02)\nloss = function(W) sum((nn(W)$yhat - y)^2)\nres = optim(params, loss, method='BFGS')\nres$par\n\n -0.24  1.39 -0.82  0.46  0.50  0.17  0.46  0.39\n\n\nThe Figure 21.1 shows the quadratic function and the neural network model. The solid black line is the neural network model, and the dashed lines are the basis functions. The model fits the data well.\n\no = nn(res$par)\nplot(x,y); lines(x,o$yhat, lwd=2)\nlines(x,0.5+o$z0[,1],col=2, lwd=2, lty=2); lines(x,0.5+o$z0[,2],col=3, lwd=2, lty=2); lines(x,0.5+o$z0[,3],col=4, lwd=2, lty=2)\n\n\n\n\n\n\n\nFigure 21.1: Noisy quadratic function approximated by a neural network with ReLu activation function.\n\n\n\n\n\nLet’s try the \\(\\tanh\\) function\n\nset.seed(8) #gretzky\nparams = c(0,rnorm(3),0,rnorm(3))\nloss = function(W) mean((nn(W,f=tanh)$yhat - y)^2)\nres = optim(params, loss, method='BFGS')\nres$par\n\n -0.98 -0.23  0.83 -1.14  0.84 -0.65  0.59  0.53\n\no = nn(res$par, f=tanh)\nplot(x,y, ylim=c(0.4,0.95)); lines(x,o$yhat, lwd=2);\nlines(x,0.5*o$z0[,1]+0.9, lwd=2, lty=2, col=2); lines(x,0.5*o$z0[,2]+0.9, lwd=2, lty=2, col=3); lines(x,0.5*o$z0[,3]+0.9, lwd=2, lty=2, col=4)\n\n\n\n\nNoisy quadratic function approximated by a neural network with tanh activation function.\n\n\n\n\nNotice that we did not have to explicitly specify that our model need to have a quadratic term, the model learned it from the data. This is the power of deep learning. The model is able to learn the structure of the data from the data itself.\nWe can apply the same approach to the interactions, say the true model for the data as follows \\[\ny = 0.5 + 0.1x_1 + 0.2x_2  + 0.5x_1x_2+ \\epsilon, ~ \\epsilon \\sim N(0,0.02^2)\n\\] We can use the same model as above, but with two input variables. The model will learn the interaction term from the data.\n\nset.seed(99) #ovi\nx1 = seq(-1,1,0.01)\nx2 = x1\ny = 0.5 + 0.1*x1 + 0.2*x2 + 0.5*x1*x2 + rnorm(length(x1),0,0.02)\nlibrary(\"scatterplot3d\")\ns3d = scatterplot3d(x1,x2,y, pch=16)\nx = cbind(x1,x2)\nnn = function(W,f=relu) {\n    b0 = W[1]; w0 = W[2:5]; b1 = W[6]; w1 = W[7:8]\n    w0 = matrix(w0,nrow=2)\n    z0 = apply(b0 + x%*%w0,1:2,f)\n    yhat = b1 + z0 %*% w1\n    return(list(yhat = yhat[,1],z0=z0))\n}\nW = c(0,rnorm(4),0,rnorm(2))\nloss = function(W) sum((nn(W, f=tanh)$yhat - y)^2)\nres = optim(W, fn=loss, method='BFGS')\nres$par\n\n  0.78  0.50 -1.39  0.63 -0.94 -2.06 -2.88  6.78\n\no = nn(res$par, f=tanh)\ns3d$points3d(x1,x2,o$yhat, col=2, type='l', lwd=5)\ns3d$points3d(x1,x2,o$z0[,1], col=3, type='l', lwd=5)\ns3d$points3d(x1,x2,o$z0[,2], col=4, type='l', lwd=5)\n\n\n\n\n\n\n\n\nEffectively, you can think of the neural network as a flexible function approximator, equivalent to a nonparametric regression approach which learns the basis functions from data. The model can learn the structure of the data from the data itself. This is the power of deep learning.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "21-nn.html#activation-functions",
    "href": "21-nn.html#activation-functions",
    "title": "21  Neural Networks",
    "section": "21.3 Activation Functions",
    "text": "21.3 Activation Functions\nThe last output layer of a neural network has sigmoid activation function for binary output variable (classification) and no activation function for continuous output variable regression. The hidden layers can have different activation functions. The most common activation functions are the hyperbolic tangent function and the rectified linear unit (ReLU) function.\nA typical approach is to use the same activation function for all hidden layers. The hyperbolic tangent function is defined as \\[\n\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n\\] Notice, that the hyperbolic tangent function is a scaled version of the sigmoid function, with \\(\\tanh(0) = 0\\). It is a smooth function which is differentiable everywhere. However,\n\n\n\n\n\n\n\n\n\ntanh\n\n\n\n\n\n\n\nHard tanh\n\n\n\n\n\n\n\nsoftplus\n\n\n\n\n\n\n\n\n\nReLU\n\n\n\n\n\n\n\nLeaky ReLU\n\n\n\n\n\n\n\nsigmoid\n\n\n\n\n\nTypically \\(\\tanh\\) is preferred to the sigmoid function because it is zero-centered. The major drawback of sigmoid and \\(\\tanh\\) functions is that they saturate when the input is very large or very small. When we try to learn the weights of the network, the optimisation algorithms makes small steps in the space of the parameters and when the weights are large the small changes won’t effect the values of the layers’ outputs and optimisation will “stagnate”.\nThis means that the gradient of the function is very small, which makes learning slow. The ReLU function is defined as\nThe ReLU function is defined as \\[\n\\text{ReLU}(z) = \\max(0,z)\n\\] The ReLU function is a piecewise linear function which is computationally efficient and easy to optimize. The ReLU function is the most commonly used activation function in deep learning. The ReLU function is not differentiable at \\(z=0\\), but it is differentiable everywhere else. The derivative of the ReLU function is \\[\n\\text{ReLU}'(z) = \\begin{cases} 0 & \\text{if } z &lt; 0 \\\\ 1 & \\text{if } z &gt; 0 \\end{cases}\n\\]\n\n\n\n\nBehnia, Farnaz, Dominik Karbowski, and Vadim Sokolov. 2021. “Deep Generative Models for Vehicle Speed Trajectories.” arXiv Preprint arXiv:2112.08361. https://arxiv.org/abs/2112.08361.\n\n\nBhadra, Anindya, Jyotishka Datta, Nick Polson, Vadim Sokolov, and Jianeng Xu. 2021. “Merging Two Cultures: Deep and Statistical Learning.” arXiv Preprint arXiv:2110.11561. https://arxiv.org/abs/2110.11561.\n\n\nDiaconis, Persi, and Mehrdad Shahshahani. 1981. “Generating a Random Permutation with Random Transpositions.” Probability Theory and Related Fields 57 (2): 159–79.\n\n\nDixon, Matthew F, Nicholas G Polson, and Vadim O Sokolov. 2019. “Deep Learning for Spatio-Temporal Modeling: Dynamic Traffic Flows and High Frequency Trading.” Applied Stochastic Models in Business and Industry 35 (3): 788–807.\n\n\nFeller, William. 1971. An Introduction to Probability Theory and Its Applications. Wiley.\n\n\nNareklishvili, Maria, Nicholas Polson, and Vadim Sokolov. 2022. “Deep Partial Least Squares for Iv Regression.” arXiv Preprint arXiv:2207.02612. https://arxiv.org/abs/2207.02612.\n\n\n———. 2023a. “Generative Causal Inference,” June. https://arxiv.org/abs/2306.16096.\n\n\n———. 2023b. “Feature Selection for Personalized Policy Analysis,” July. https://arxiv.org/abs/2301.00251.\n\n\nPolson, Nicholas G., and Vadim Sokolov. 2023. “Generative AI for Bayesian Computation,” June. https://arxiv.org/abs/2305.14972.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep Learning: A Bayesian Perspective.” Bayesian Analysis 12 (4): 1275–1304.\n\n\nPolson, Nicholas, and Vadim Sokolov. 2020. “Deep Learning: Computational Aspects.” Wiley Interdisciplinary Reviews: Computational Statistics 12 (5): e1500.\n\n\nPolson, Nicholas, Vadim Sokolov, and Jianeng Xu. 2021. “Deep Learning Partial Least Squares.” arXiv Preprint arXiv:2106.14085. https://arxiv.org/abs/2106.14085.\n\n\nSokolov, Vadim. 2017. “Discussion of ‘Deep Learning for Finance: Deep Portfolios’.” Applied Stochastic Models in Business and Industry 33 (1): 16–18.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "22-sgd.html",
    "href": "22-sgd.html",
    "title": "22  Gradient Descent",
    "section": "",
    "text": "22.1 Deep Learning and Least Squares\nTraditional statistical models are estimated by maximizing likelihood and using the least squares algorithm for linear regression and weighted least squares or Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm for generalized linear models.\nThe deep learning model approximates the relation between inputs \\(x\\) and outputs \\(y\\) using a non-linear function \\(f(x,\\theta)\\), where \\(\\theta\\) is a vector of parameters. The goal is to find the optimal value of \\(\\theta\\) that minimizes the expected loss function, given a training data set \\(D = \\{x_i,y_i\\}_{i=1}^n\\). The loss function is a measure of discrepancy between the true value of \\(y\\) and the predicted value \\(f(x,\\theta)\\). The loss function is usually defined as the negative log-likelihood function of the model. \\[\n    l(\\theta) = - \\sum_{i=1}^n \\log p(y_i | x_i, \\theta),\n\\] where \\(p(y_i | x_i, \\theta)\\) is the conditional distribution of \\(y_i\\) given \\(x_i\\) and \\(\\theta\\). Thus, in the case of regression, we have \\[\n  y_i = f(x_i,\\theta) + \\epsilon, ~ \\epsilon \\sim N(0,\\sigma^2),\n\\] Thus, the loss function is \\[\n    l(\\theta) = - \\sum_{i=1}^n \\log p(y_i | x_i, \\theta) = \\sum_{i=1}^n (y_i - f(x_i, \\theta))^2,\n\\]",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "22-sgd.html#regression",
    "href": "22-sgd.html#regression",
    "title": "22  Gradient Descent",
    "section": "22.2 Regression",
    "text": "22.2 Regression\nRegression is simply a neural network which is wide and shallow. The insight of DL is that you use a deep and shallow neural network. Let’s look at a simple example and fit a linear regression model to iris dataset.\n\ndata(iris)\ny = iris$Petal.Length\nloss = function(theta) sum((y - x %*% theta)^2)\ngrad = function(theta) -2 * t(x) %*% (y - x %*% theta)\nres = optim(c(0,0), loss, grad, method=\"BFGS\")\ntheta = res$par\n\nOur loss minimization algorithm finds the following coefficients\n\n\n\n\n\nIntercept (\\(\\theta_1\\))\nPetal.Width (\\(\\theta_2\\))\n\n\n\n\n1.1\n2.2\n\n\n\n\n\nLet’s plot the data and model estimated using gradient descent\n\nplot(x[,2],y,pch=16, xlab=\"Petal.Width\")\nabline(theta[2],theta[1], lwd=3,col=\"red\")\n\n\n\n\n\n\n\n\nLet’s compare it to the standard estimation algorithm\n\nm = lm(Petal.Length~Petal.Width, data=iris)\n\n\n\n\n(Intercept)\nPetal.Width\n\n\n\n\n1.1\n2.2\n\n\n\n\n\nThe values found by gradient descent are very close to the ones found by the standard OLS algorithm.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "22-sgd.html#logistic-regression",
    "href": "22-sgd.html#logistic-regression",
    "title": "22  Gradient Descent",
    "section": "22.3 Logistic Regression",
    "text": "22.3 Logistic Regression\nLogistic regression is a generalized linear model (GLM) with a logit link function, defined as: \\[\n    \\log \\left(\\frac{p}{1-p}\\right) = \\theta_0 + \\theta_1 x_1 + \\ldots + \\theta_p x_p,\n\\] where \\(p\\) is the probability of the positive class. The negative log-likelihood function for logistic regression is a cross-entropy loss \\[\n    l(\\theta) = - \\sum_{i=1}^n \\left[ y_i \\log p_i + (1-y_i) \\log (1-p_i) \\right],\n\\] where \\(p_i = 1/\\left(1 + \\exp(-\\theta_0 - \\theta_1 x_{i1} - \\ldots - \\theta_p x_{ip})\\right)\\). The derivative of the negative log-likelihood function is \\[\n    \\nabla l(\\theta) = - \\sum_{i=1}^n \\left[ y_i - p_i \\right] \\begin{pmatrix} 1 \\\\ x_{i1} \\\\ \\vdots \\\\ x_{ip} \\end{pmatrix}.\n\\] In matrix notations, we have \\[\n    \\nabla l(\\theta) = - X^T (y - p).\n\\] Let’s implement gradient descent algorithm now.\n\ny = ifelse(iris$Species==\"setosa\",1,0)\nx = cbind(rep(1,150),iris$Sepal.Length)\nlrgd  = function(x,y, alpha, n_iter) {\n  theta &lt;- matrix(c(0, 0), nrow = 2, ncol = 1)\n  for (i in 1:n_iter) {\n    # compute gradient\n    p = 1/(1+exp(-x %*% theta))\n    grad &lt;- -t(x) %*% (y - p)\n    # update theta\n    theta &lt;- theta - alpha * grad\n  }\n  return(theta)\n}\ntheta = lrgd(x,y,0.005,20000)\n\nThe gradient descent parameters are\n\n\n\n\n\nIntercept (\\(\\theta_1\\))\nSepal.Length (\\(\\theta_2\\))\n\n\n\n\n28\n-5.2\n\n\n\n\n\nAnd the plot is\n\npar(mar=c(4,4,0,0), bty='n')\nplot(x[,2],y,pch=16, xlab=\"Sepal.Length\")\nlines(x[,2],p,type='p', pch=16,col=\"red\")\n\n\n\n\n\n\n\n\nLet’s compare it to the standard estimation algorithm\n\nglm(y~x-1, family=binomial(link=\"logit\"))\n\n\nCall:  glm(formula = y ~ x - 1, family = binomial(link = \"logit\"))\n\nCoefficients:\n   x1     x2  \n27.83  -5.18  \n\nDegrees of Freedom: 150 Total (i.e. Null);  148 Residual\nNull Deviance:      208 \nResidual Deviance: 72   AIC: 76\n\n\nNow, we demonstrate the gradient descent for estimating a generalized linear model (GLM), namely logistic regression. We will use the iris data set again and try to predict the species of the flower using the petal width as a predictor. We will use the following model \\[\n    \\log \\left(\\frac{p_i}{1-p_i}\\right) = \\theta_0 + \\theta_1 x_i,\n\\] where \\(p_i = P(y_i = 1)\\) is the probability of the flower being of the species \\(y_i = 1\\) (setosa).\nThe negative log-likelihood function for logistic regression model is \\[\n    l(\\theta) = - \\sum_{i=1}^n \\left[ y_i \\log p_i + (1-y_i) \\log (1-p_i) \\right],\n\\] where \\(p_i = 1/\\left(1 + \\exp(-\\theta_0 - \\theta_1 x_i)\\right)\\). The derivative of the negative log-likelihood function is \\[\n    \\nabla l(\\theta) = - \\sum_{i=1}^n \\left[ y_i - p_i \\right] \\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}.\n\\] In matrix notations, we have \\[\n    \\nabla l(\\theta) = - X^T (y - p).\n\\]",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "22-sgd.html#stochastic-gradient-descent",
    "href": "22-sgd.html#stochastic-gradient-descent",
    "title": "22  Gradient Descent",
    "section": "22.4 Stochastic Gradient Descent",
    "text": "22.4 Stochastic Gradient Descent\nStochastic gradient descent (SGD) is a variant of the gradient descent algorithm. The main difference is that instead of computing the gradient over the whole data set, SGD computes the gradient over a randomly selected subset of the data. This allows SGD to be applied to estimate models when data set is too large to fit into memory, which is often the case with the deep learning models. The SGD algorithm replaces the gradient of the negative log-likelihood function with the gradient of the negative log-likelihood function computed over a randomly selected subset of the data \\[\n    \\nabla l(\\theta) \\approx \\dfrac{1}{|B|} \\sum_{i \\in B} \\nabla l(y_i, f(x_i, \\theta)),\n\\] where \\(B \\in \\{1,2,\\ldots,n\\}\\) is the batch samples from the data set. This method can be interpreted as gradient descent using noisy gradients, which are typically called mini-batch gradients with batch size \\(|B|\\).\nThe SGD is based on the idea of stochastic approximation introduced by Robbins and Monro (1951). Stochastic simply replaces \\(F(l)\\) with its Monte Carlo approximation.\nIn a small mini-batch regime, when \\(|B| \\ll n\\) and typically \\(|B| \\in \\{32,64,\\ldots,1024\\}\\) it was shown that SGD converges faster than the standard gradient descent algorithm, it does converge to minimizers of strongly convex functions (negative log-likelihood function from exponential family is strongly convex) (Bottou, Curtis, and Nocedal 2018) and it is more robust to noise in the data (Hardt, Recht, and Singer 2016). Further, it was shown that it can avoid saddle-points, which is often an issue with deep learning log-likelihood functions. In the case of multiple-minima, SGD can find a good solution LeCun et al. (2002), meaning that the out-of-sample performance is often worse when trained with large- batch methods as compared to small-batch methods.\nNow, we implement SGD for logistic regression and compare performance for different batch sizes \n\nlrgd_minibatch  = function(x,y, alpha, n_iter, bs) {\n  theta &lt;- matrix(c(0, 0), nrow = 2, ncol = n_iter+1)\n  n = length(y)\n  for (i in 1:n_iter) {\n    s = ((i-1)*bs+1)%%n\n    e = min(s+bs-1,n)\n    xl = x[s:e,]; yl = y[s:e]\n    p = 1/(1+exp(-xl %*% theta[,i]))\n    grad &lt;- -t(xl) %*% (yl - p)\n    # update theta\n    theta[,i+1] &lt;- theta[,i] - alpha * grad\n  }\n  return(theta)\n}\n\nNow run our SGD algorithm with different batch sizes.\n\nset.seed(92) # kuzy\nind = sample(150)\ny = ifelse(iris$Species==\"setosa\",1,0)[ind] # shuffle data\nx = cbind(rep(1,150),iris$Sepal.Length)[ind,] # shuffle data\nnit=200000\nlr = 0.01\nth1 = lrgd_minibatch(x,y,lr,nit,5)\nth2 = lrgd_minibatch(x,y,lr,nit,15)\nth3 = lrgd_minibatch(x,y,lr,nit,30)\n\n\n\n\n\n\n\n\nWe run it with 2^{5} iterations and the learning rate of 0.01 and plot the values of \\(\\theta_1\\) every 1000 iteration. There are a couple of important points we need to highlight when using SGD. First, we shuffle the data before using it. The reason is that if the data is sorted in any way (e.g. by date or by value of one of the inputs), then data within batches can be highly correlated, which reduces the convergence speed. Shuffling helps avoiding this issue. Second, the larger the batch size, the smaller number of iterations are required for convergence, which is something we would expect. However, in this specific example, from the number of computation point of view, the batch size does not change the number calculations required overall. Let’s look at the same plot, but scale the x-axis according to the amount of computations\n\nplot(ind/1000,th1[1,ind], type='l', ylim=c(0,33), col=1, ylab=expression(theta[1]), xlab=\"Iteration\")\nabline(h=27.83, lty=2)\nlines(ind/1000*3,th2[1,ind], type='l', col=2)\nlines(ind/1000*6,th3[1,ind], type='l', col=3)\nlegend(\"bottomright\", legend=c(5,15,30),col=1:3, lty=1, bty='n',title = \"Batch Size\")\n\n\n\n\n\n\n\n\nThere are several important considerations about choosing the batch size for SGD.\n\nThe larger the batch size, the more memory is required to store the data.\nParallelization is more efficient with larger batch sizes. Modern harware supports parallelization of matrix operations, which is the main operation in SGD. The larger the batch size, the more efficient the parallelization is. Usually there is a sweet spot \\(|B|\\) for the batch size, which is the largest batch size that can fit into the memory or parallelized. Meaning it takes the same amount of time to compute SGD step for batch size \\(1\\) and \\(B\\).\nThird, the larger the batch size, the less noise in the gradient. This means that the larger the batch size, the more accurate the gradient is. However, it was empirically shown that in many applications we should prefer noisier gradients (small batches) to obtain high quality solutions when the objective function (negative log-likelihood) is non-convex (Keskar et al. 2016).\n\nDeep learning estimation problem as well as a large number of statistical problems, can be expressed in the form \\[\n\\min l(x) + \\phi(x).\n\\] In learning \\(l(x)\\) is the negative log-likelihood and \\(\\phi(x)\\) is a penalty function that regularizes the estimate. From the Bayesian perspective, the solution to this problem may be interpreted as a maximum a posteriori \\[p(y\\mid x) \\propto \\exp\\{-l(x)\\}, ~ p(x) \\propto \\exp\\{-\\phi(x)\\}.\\]\nSecond order optimisation algorithms, such as BFGS used for traditional statistical models do not work well for deep learning models. The reason is that the number of parameters a DL model has is large and estimating second order derivatives (Hessian or Fisher information matrix) becomes prohibitive from both computational and memory use standpoints. Instead, first order gradient descent methods are used for estimating parameters of a deep learning models.\nThe problem of parameter estimation (when likelihood belongs to the exponential family) is an optimisation problem\n\\[\n    \\min_{\\theta} l(\\theta) := \\dfrac{1}{n} \\sum_{i=1}^n \\log p(y_i, f(x_i, \\theta))\n\\] where \\(l\\) is the negative log-likelihood of a sample, and \\(\\theta\\) is the vector of parameters. The gradient descent method is an iterative algorithm that starts with an initial guess \\(\\theta^{0}\\) and then updates the parameter vector \\(\\theta\\) at each iteration \\(t\\) as follows: \\[\n    \\theta^{t+1} = \\theta^t - \\alpha_t \\nabla l(\\theta^t).\n\\]\nLet’s demonstrate these algorithms on a simple example of linear regression. We will use the mtcars data set and try to predict the fuel consumption (mpg) \\(y\\) using the number of cylinders (cyl) as a predictor \\(x\\). We will use the following model: \\[\n    y_i = \\theta_0 + \\theta_1 x_i + \\epsilon_i,\n\\] or in matrix form \\[\n    y = X \\theta + \\epsilon,\n\\] where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\), \\(X = [1 ~ x]\\) is the design matrix with first column beign all ones.\nThe negative log-likelihood function for the linear regression model is \\[\n    l(\\theta) = \\sum_{i=1}^n (y_i - \\theta_0 - \\theta_1 x_i)^2.\n\\]\nThe gradient then is \\[\n    \\nabla l(\\theta) = -2 \\sum_{i=1}^n (y_i - \\theta_0 - \\theta_1 x_i) \\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}.\n\\] In matrix form, we have \\[\n    \\nabla l(\\theta) = -2 X^T (y - X \\theta).   \n\\]",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "22-sgd.html#automatic-differentiation-backpropagation",
    "href": "22-sgd.html#automatic-differentiation-backpropagation",
    "title": "22  Gradient Descent",
    "section": "22.5 Automatic Differentiation (Backpropagation)",
    "text": "22.5 Automatic Differentiation (Backpropagation)\nTo calculate the value of the gradient vector, at each step of the optimization process, deep learning libraries require calculations of derivatives. In general, there are three different ways to calculate those derivatives. First, is numerical differentiation, when a gradient is approximated by a finite difference \\(f'(x) = (f(x+h)-f(x))/h\\) and requires two function evaluations. However, the numerical differentiation is not backward stable (Griewank, Kulshreshtha, and Walther 2012), meaning that for a small perturbation in input value \\(x\\), the calculated derivative is not the correct one. Second, is a symbolic differentiation which has been used in symbolic computational frameworks such as Mathematica or Maple for decades. Symbolic differentiation uses a tree form representation of a function and applies chain rule to the tree to calculate the symbolic derivative of a given function. Figure @ref(fig:comp-graph) shows a tree representation of of composition of affine and sigmoid functions (the first layer of our neural network).\n\n\n\nComputational graph of the first layer of our neural network\n\n\nThe advantage of symbolic calculations is that the analytical representation of derivative is available for further analysis. For example, when derivative calculation is in an intermediate step of the analysis. Third way to calculate a derivative is to use automatic differentiation (AD). Similar to symbolic differentiation, AD recursively applies the chain rule and calculates the exact value of derivative and thus avoids the problem of numerical instability. The difference between AD and symbolic differentiation is that AD provides the value of derivative evaluated at a specific point, rather than an analytical representation of the derivative.\nAD does not require analytical specification and can be applied to a function defined by a sequence of algebraic manipulations, logical and transient functions applied to input variables and specified in a computer code. AD can differentiate complex functions which involve IF statements and loops, and AD can be implemented using either forward or backward mode. Consider an example of calculating a derivative of the following function with respect to x.\n\nsigmoid = function(x,b,w){\n  v1 = w*x;\n  v2 = v1 + b\n  v3 = 1/(1+exp(-v2))\n}\n\nIn the forward mode an auxiliary variable, called a dual number, will be added to each line of the code to track the value of the derivative associated with this line. In our example, if we set x=2, w=3, b=52, we get the calculations given in Table below.\n\n\n\n\n\n\n\nFunction calculations\nDerivative calculations\n\n\n\n\n1. v1 = w*x = 6\n1. dv1 = w = 3 (derivative of v1 with respect to x)\n\n\n2. v2 = v1 + b = 11\n2. dv2 = dv1 = 3 (derivative of v2 with respect to x)\n\n\n3. v3 = 1/(1+exp(-v2)) = 0.99\n3. dv3 = eps2*exp(-v2)/(1+exp(-v2))**2  = 5e-05\n\n\n\nVariables dv1,dv2,dv3 correspond to partial (local) derivatives of each intermediate variables v1,v2,v3 with respect to \\(x\\), and are called dual variables. Tracking for dual variables can either be implemented using source code modification tools that add new code for calculating the dual numbers or via operator overloading.\nThe reverse AD also applies chain rule recursively but starts from the outer function, as shown in Table below.\n\n\n\n\n\n\n\nFunction calculations\nDerivative calculations\n\n\n\n\n1. v1 = w*x = 6\n4. dv1dx =w; dv1 = dv2*dv1dx = 3*1.3e-05=5e-05\n\n\n2. v2 = v1 + b = 11\n3. dv2dv1 =1; dv2 = dv3*dv2dv1 = 1.3e-05\n\n\n3. v3 = 1/(1+exp(-v2)) = 0.99\n2. dv3dv2 = exp(-v2)/(1+exp(-v2))**2;\n\n\n4. v4 = v3\n1. dv4=1\n\n\n\nFor DL, derivatives are calculated by applying reverse AD algorithm to a model which is defined as a superposition of functions. A model is defined either using a general purpose language as it is done in PyTorch or through a sequence of function calls defined by framework libraries (e.g. in TensorFlow). Forward AD algorithms calculate the derivative with respect to a single input variable, but reverse AD produces derivatives with respect to all intermediate variables. For models with many parameters, it is much more computationally feasible to perform the reverse AD.\nIn the context of neural networks, the reverse AD algorithms is called back-propagation and was popularized in AI by Rumelhart, Hinton, and Williams (1986). According to Schmidhuber (2015) the first version of what we call today back-propagation was published in 1970 in a master’s thesis Linnainmaa (1970) and was closely related to the work of Ostrovskii, Volin, and Borisov (1971). However, similar techniques rooted in Pontryagin’s maximization principle were discussed in the context of multi-stage control problems Bryson (1961),bryson1969applied}. Dreyfus (1962) applies back-propagation to calculate the first order derivative of a return function to numerically solve a variational problem. Later Dreyfus (1973) used back-propagation to derive an efficient algorithm to solve a minimization problem. The first neural network specific version of back-propagation was proposed in P. Werbos (1974) and an efficient back-propagation algorithm was discussed in P. J. Werbos (1982).\nModern deep learning frameworks fully automate the process of finding derivatives using AD algorithms. For example, PyTorch relies on autograd library which automatically finds gradient using back-propagation algorithm. Here is a small code example using autograd library in jax.\n\nimport jax.numpy as jnp\nfrom jax import grad,jit\nimport pandas as pd\nfrom jax import random\nimport matplotlib.pyplot as plt\n\ndef abline(slope, intercept):\n    \"\"\"Plot a line from slope and intercept\"\"\"\n    axes = plt.gca()\n    x_vals = jnp.array(axes.get_xlim())\n    ylim = axes.get_xlim()\n    y_vals = intercept + slope * x_vals\n    plt.plot(x_vals, y_vals, '-'); plt.ylim(ylim)\n\nd = pd.read_csv('../data/circle.csv').values\nx = d[:, 1:3]; y = d[:, 0]\ndef sigmoid(x):\n    return 1 / (1 + jnp.exp(-x))\ndef predict(x, w1,b1,w2,b2):\n    z = sigmoid(jnp.dot(x, w1)+b1)\n    return sigmoid(jnp.dot(z, w2)+b2)[:,0]\ndef nll(x, y, w1,b1,w2,b2):\n    yhat = predict(x, w1,b1,w2,b2)\n    return -jnp.sum(y * jnp.log(yhat) + (1 - y) * jnp.log(1 - yhat))\n@jit\ndef sgd_step(x, y, w1,b1,w2,b2, lr):\n    grads = grad(nll,argnums=[2,3,4,5])(x, y, w1,b1,w2,b2)\n    return w1 - lr * grads[0],b1 - lr * grads[1],w2 - lr * grads[2],b2 - lr * grads[3]\ndef accuracy(x, y, w1,b1,w2,b2):\n    y_pred = predict(x, w1,b1,w2,b2)\n    return jnp.mean((y_pred &gt; 0.5) == y)\nk = random.PRNGKey(0)\nw1 = 0.1*random.normal(k,(2,4))\nb1 = 0.01*random.normal(k,(4,))\nw2 = 0.1*random.normal(k,(4,1))\nb2 = 0.01*random.normal(k,(1,))\n\nfor i in range(1000):\n    w1,b1,w2,b2 = sgd_step(x,y,w1,b1,w2,b2,0.003)\nprint(accuracy(x,y,w1,b1,w2,b2))\n\nfig, ax = plt.subplots()\nax.scatter(x[:,0], x[:,1], c=['r' if x==1 else 'g' for x in y],s=7); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.xlim(-10,10)\nax.spines['top'].set_visible(False)\n# plt.scatter((x[:,1]*w1[1,0] - b1[0])/w1[0,0], x[:,1])\nabline(w1[1,0]/w1[0,0],b1[0]/w1[0,0])\nabline(w1[1,1]/w1[0,1],b1[1]/w1[0,1])\nabline(w1[1,2]/w1[0,2],b1[2]/w1[0,2])\nabline(w1[1,3]/w1[0,3],b1[3]/w1[0,3])\nplt.show()",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "22-sgd.html#stochastic-gradient-descent-1",
    "href": "22-sgd.html#stochastic-gradient-descent-1",
    "title": "22  Gradient Descent",
    "section": "22.6 Stochastic Gradient Descent",
    "text": "22.6 Stochastic Gradient Descent\nStochastic gradient descent (SGD) is a default standard for minimizing the loss function \\(f(W,b)\\) (maximizing the likelihood) to find the deep learning weights and offsets. SGD simply minimizes the function by taking a negative step along an estimate \\(g^k\\) of the gradient \\(\\nabla f(W^k, b^k)\\) at iteration \\(k\\). The gradients are available via the chain rule applied to the superposition of semi-affine functions. The approximate gradient is estimated by calculating\n\\[\ng^k = \\frac{1}{|E_k|} \\sum_{i \\in E_k} \\nabla \\mathcal{L}_{w,b}( Y_i , \\hat{Y}^k( X_i))\n\\]\nwhere \\(E_k \\subset \\{1,\\ldots,T \\}\\) and \\(|E_k|\\) is the number of elements in \\(E_k\\).\nWhen \\(|E_k| &gt;1\\) the algorithm is called batch SGD and simply SGD otherwise. Typically, the subset \\(E\\) is chosen by going cyclically and picking consecutive elements of \\(\\{1,\\ldots,T \\}\\), \\(E_{k+1} = [E_k \\mod T]+1\\). The direction \\(g^k\\) is calculated using a chain rule (a.k.a. back-propagation) providing an unbiased estimator of \\(\\nabla f(W^k, b^k)\\). Specifically, we have\n\\[\n\\mathrm{E}(g^k) = \\frac{1}{T} \\sum_{i =1}^T \\nabla \\mathcal{L}_{w,b}( Y_i , \\hat{Y}^k( X_i)) = \\nabla f(W^k, b^k)\n\\]\nAt each iteration, the SGD updates the solution\n\\[\n(W,b)^{k+1} = (W,b)^k - t_k g^k\n\\]\nDeep learning algorithms use step size \\(t_k\\) (a.k.a learning rate) that is either kept constant or a simple step size reduction strategy, such as \\(t_k = a\\exp(-kt)\\) is used. The hyper parameters of reduction schedule are usually found empirically from numerical experiments and observations of the loss function progression.\nOne caveat of SGD is that the descent in \\(f\\) is not guaranteed or can be very slow at every iteration. Stochastic Bayesian approaches ought to alleviate these issues. The variance of the gradient estimate \\(g^k\\) can also be near zero as the iterates converge to a solution. To tackle those problems a coordinate descent (CD) and momentum-based modifications can be applied. Alternative directions method of multipliers (ADMM) can also provide a natural alternative and lead to non-linear alternating updates (see Carreira-Perpinán and Wang (2014)).\nThe CD evaluates a single component \\(E_k\\) of the gradient \\(\\nabla f\\) at the current point and then updates the \\(E_k\\)th component of the variable vector in the negative gradient direction. The momentum-based versions of SGD or so-called accelerated algorithms were originally proposed by Nesterov (1983). For more recent discussion, see Nesterov (2013). Momentum adds memory to the search process by combining new gradient information with the previous search directions. Empirically momentum-based methods have been shown a better convergence for deep learning networks Sutskever et al. (2013). The gradient only influences changes in the velocity of the update, which then updates the variable\n\\[\nv^{k+1} = \\mu v^k - t_k g((W,b)^k)\n\\] \\[\n(W,b)^{k+1} = (W,b)^k +v^k\n\\]\nThe hyper-parameter \\(\\mu\\) controls the damping effect on the rate of update of the variables. The physical analogy is the reduction in kinetic energy that allows to “slow down” the movements at the minima. This parameter can also be chosen empirically using cross-validation.\nNesterov’s momentum method (a.k.a. Nesterov acceleration) calculates the gradient at the point predicted by the momentum. One can view this as a look-ahead strategy with updating scheme\n\\[\nv^{k+1} = \\mu v^k - t_k g((W,b)^k +v^k)\n\\] \\[\n(W,b)^{k+1} = (W,b)^k +v^k\n\\]\nAnother popular modification are the AdaGrad methods Zeiler (2012), which adaptively scales each of the learning parameter at each iteration\n\\[\nc^{k+1} = c^k + g((W,b)^k)^2\n\\] \\[\n(W,b)^{k+1} = (W,b)^k - t_k g(W,b)^k)/(\\sqrt{c^{k+1}} - a)\n\\]\nwhere \\(a\\) is usually a small number, e.g. \\(a = 10^{-6}\\) that prevents from dividing by zero. RMSprop takes the AdaGrad idea further and puts more weight on recent values of gradient squared to scale the update direction:\n\\[\nc^{k+1} =  dc^k + (1-d)g((W,b)^k)^2\n\\]\nThe Adam method Kingma and Ba (2014) combines both RMSprop and momentum methods, it leads to the following update equations\n\\[\nv^{k+1} = \\mu v^k - (1-\\mu)t_k g((W,b)^k +v^k)\n\\] \\[\nc^{k+1} = dc^k + (1-d)g((W,b)^k)^2\n\\] \\[\n(W,b)^{k+1} = (W,b)^k - t_k v^{k+1}/(\\sqrt{c^{k+1}} - a)\n\\]\nSecond order methods solve the optimization problem by solving a system of nonlinear equations \\(\\nabla f(W,b) = 0\\) by applying the Newton’s method\n\\[\n(W,b)^+ = (W,b) - \\{ \\nabla^2f(W,b) \\}^{-1}\\nabla f(W,b)\n\\]\nWe can see that SGD simply approximates \\(\\nabla^2f(W,b)\\) by \\(1/t\\). The advantages of a second order method include much faster convergence rates and insensitivity to the conditioning of the problem. In practice, second order methods are rarely used for deep learning applications Dean et al. (2012). The major disadvantage is inability to train model using batches of data as SGD does. Since typical deep learning model relies on large scale data sets, the second order methods become memory and computationally prohibitive at even modest-sized training data sets.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "22-sgd.html#why-robbins-monro",
    "href": "22-sgd.html#why-robbins-monro",
    "title": "22  Gradient Descent",
    "section": "22.7 Why Robbins-Monro?",
    "text": "22.7 Why Robbins-Monro?\nThe Robbins-Monro algorithm was introduced in their seminal 1951 paper “A Stochastic Approximation Method” Robbins and Monro (1951). The paper addressed the problem of finding the root of a function when only noisy observations are available.\nConsider a function \\(M(\\theta)\\) where we want to find \\(\\theta^*\\) such that \\(M(\\theta^*) = \\alpha\\) for some target value \\(\\alpha\\). In the original formulation, \\(M(\\theta)\\) represents the expected value of some random variable \\(Y(\\theta)\\):\n\\[M(\\theta) = \\mathbb{E}[Y(\\theta)] = \\alpha\\]\nThe key insight is that we can only observe noisy realizations \\(y(\\theta)\\) where:\n\\[y(\\theta) = M(\\theta) + \\epsilon(\\theta)\\]\nwhere \\(\\epsilon(\\theta)\\) is a zero-mean random error term.\nThe Robbins-Monro algorithm iteratively updates the estimate \\(\\theta_n\\) using:\n\\[\\theta_{n+1} = \\theta_n - a_n(y(\\theta_n) - \\alpha)\\]\nwhere \\(a_n\\) is a sequence of positive step sizes that must satisfy:\n\\[\\sum_{n=1}^{\\infty} a_n = \\infty \\quad \\text{and} \\quad \\sum_{n=1}^{\\infty} a_n^2 &lt; \\infty\\]\nThese conditions ensure that the algorithm can explore the entire space (first condition) while eventually converging (second condition).\nUnder appropriate conditions on \\(M(\\theta)\\) (monotonicity and boundedness), the algorithm converges almost surely to \\(\\theta^*\\):\n\\[\\lim_{n \\to \\infty} \\theta_n = \\theta^* \\quad \\text{almost surely}\\]\nThe convergence rate depends on the choice of step sizes. For \\(a_n = c/n\\) with \\(c &gt; 0\\), the algorithm achieves optimal convergence rates.\nThis foundational work established the theoretical basis for stochastic approximation methods that are now widely used in machine learning, particularly in stochastic gradient descent and related optimization algorithms.\nInference on estimands can be expressed as the solution to a convex optimization problem. In addition to means, this includes medians, other quantiles, linear and logistic regression coefficients, and many other quantities. Formally, we consider estimands of the form\n\\[\\theta^* = \\arg\\min_{\\theta \\in \\mathbb{R}^p} \\mathbb{E}[\\ell_\\theta(X_i, Y_i)],\\]\nfor a loss function \\(\\ell_\\theta: \\mathcal{X} \\times \\mathcal{Y} \\to \\mathbb{R}\\) that is convex in \\(\\theta \\in \\mathbb{R}^p\\), for some \\(p \\in \\mathbb{N}\\). Throughout, we take the existence of \\(\\theta^*\\) as given. If the minimizer is not unique, our method will return a confidence set guaranteed to contain all minimizers. Under mild conditions, convexity ensures that \\(\\theta^*\\) can also be expressed as the value solving \\[\n\\mathbb{E}[g_\\theta(X_i, Y_i)] = 0.\n\\tag{22.1}\\]\nwhere \\(g_\\theta: \\mathcal{X} \\times \\mathcal{Y} \\to \\mathbb{R}^p\\) is a subgradient of \\(\\ell_\\theta\\) with respect to \\(\\theta\\). We will call convex estimation problems where \\(\\theta^*\\) satisfies Equation 22.1 nondegenerate, and we will later discuss mild conditions that ensure this regularity.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "22-sgd.html#the-em-ecm-and-ecme-algorithms",
    "href": "22-sgd.html#the-em-ecm-and-ecme-algorithms",
    "title": "22  Gradient Descent",
    "section": "22.8 The EM, ECM, and ECME algorithms",
    "text": "22.8 The EM, ECM, and ECME algorithms\nMCMC methods have been used extensively to perform numerical integration. There is also interest in using simulation-based methods to optimise functions. The EM algorithm is an algorithm in a general class of Q-maximisation algorithms that finds a (deterministic) sequence \\(\\{\\theta^{(g)}\\}\\) converging to \\(\\arg\\max_{\\theta \\in \\Theta} Q(\\theta)\\).\nFirst, define a function \\(Q(\\theta,\\phi)\\) such that \\(Q(\\theta) = Q(\\theta,\\theta)\\) and it satisfies a convexity constraint \\(Q(\\theta,\\phi) \\geq Q(\\theta,\\theta)\\). Then define\n\\[\\theta^{(g+1)} = \\arg\\max_{\\theta \\in \\Theta} Q(\\theta,\\theta^{(g)})\\]\nThis satisfies the convexity constraint \\(Q(\\theta,\\theta) \\geq Q(\\theta,\\phi)\\) for any \\(\\phi\\). In order to prove convergence, you get a sequence of inequalities\n\\[Q(\\theta^{(0)},\\theta^{(0)}) \\leq Q(\\theta^{(1)},\\theta^{(0)}) \\leq Q(\\theta^{(1)},\\theta^{(1)}) \\leq \\ldots \\leq Q\\]\nIn many models we have to deal with a latent variable and require estimation where integration is also involved. For example, suppose that we have a triple \\((y,z,\\theta)\\) with joint probability specification \\(p(y,z,\\theta) = p(y|z,\\theta)p(z,\\theta)\\). This can occur in missing data problems and estimation problems in mixture models.\nA standard application of the EM algorithm is to find\n\\[\\arg\\max_{\\theta \\in \\Theta} \\int_z p(y|z,\\theta)p(z|\\theta)dz\\]\nAs we are just finding an optimum, you do not need the prior specification \\(p(\\theta)\\). The EM algorithm finds a sequence of parameter values \\(\\theta^{(g)}\\) by alternating between an expectation and a maximisation step. This still requires the numerical (or analytical) computation of the criteria function \\(Q(\\theta,\\theta^{(g)})\\) described below.\nEM algorithms have been used extensively in mixture models and missing data problems. The EM algorithm uses the particular choice where\n\\[Q(\\theta) = \\log p(y|\\theta) = \\log \\int p(y,z|\\theta)dz\\]\nHere the likelihood has a mixture representation where \\(z\\) is the latent variable (missing data, state variable etc.). This is termed a Q-maximization algorithm with:\n\\[Q(\\theta,\\theta^{(g)}) = \\int \\log p(y|z,\\theta)p(z|\\theta^{(g)},y)dz = \\mathbb{E}_{z|\\theta^{(g)},y} [\\log p(y|z,\\theta)]\\]\nTo implement EM you need to be able to calculate \\(Q(\\theta,\\theta^{(g)})\\) and optimize at each iteration.\nThe EM algorithm and its extensions ECM and ECME are methods of computing maximum likelihood estimates or posterior modes in the presence of missing data. Let the objective function be \\(\\ell(\\theta) = \\log p(\\theta|y) + c(y)\\), where \\(c(y)\\) is a possibly unknown normalizing constant that does not depend on \\(\\beta\\) and \\(y\\) denotes observed data. We have a mixture representation,\n\\[p(\\theta|y) = \\int p(\\theta,z|y)dz = \\int p(\\theta|z,y)p(z|y)dz\\]\nwhere distribution of the latent variables is \\(p(z|\\theta,y) = p(y|\\theta,z)p(z|\\theta)/p(y|\\theta)\\).\nIn some cases the complete data log-posterior is simple enough for \\(\\arg\\max_{\\theta} \\log p(\\theta|z,y)\\) to be computed in closed form. The EM algorithm alternates between the Expectation and Maximization steps for which it is named. The E-step and M-step computes\n\\[Q(\\beta|\\beta^{(g)}) = \\mathbb{E}_{z|\\beta^{(g)},y} [\\log p(y,z|\\beta)] = \\int \\log p(y,z|\\beta)p(z|\\beta^{(g)},y)dz\\]\n\\[\\beta^{(g+1)} = \\arg\\max_{\\beta} Q(\\beta|\\beta^{(g)})\\]\nThis has an important monotonicity property that ensures \\(\\ell(\\beta^{(g)}) \\leq \\ell(\\beta^{(g+1)})\\) for all \\(g\\). In fact, the monotonicity proof given by Dempster et al. (1977) shows that any \\(\\beta\\) with \\(Q(\\beta,\\beta^{(g)}) \\geq Q(\\beta^{(g)},\\beta^{(g)})\\) also satisfies the log-likelihood inequality \\(\\ell(\\beta) \\geq \\ell(\\beta^{(g)})\\).\nIn problems with many parameters the M-step of EM may be difficult. In this case \\(\\theta\\) may be partitioned into components \\((\\theta_1,\\ldots,\\theta_k)\\) in such a way that maximizing \\(\\log p(\\theta_j|\\theta_{-j},z,y)\\) is easy. The ECM algorithm pairs the EM algorithm’s E-step with \\(k\\) conditional maximization (CM) steps, each maximizing \\(Q\\) over one component \\(\\theta_j\\) with each component of \\(\\theta_{-j}\\) fixed at the most recent value. Due to the fact that each CM step increases \\(Q\\), the ECM algorithm retains the monotonicity property. The ECME algorithm replaces some of ECM’s CM steps with maximizations over \\(\\ell\\) instead of \\(Q\\). Liu and Rubin (1994) show that doing so can greatly increase the rate of convergence.\nIn many cases we will have a parameter vector \\(\\theta = (\\beta,\\nu)\\) partitioned into its components and a missing data vector \\(z = (\\lambda,\\omega)\\). Then we compute the \\(Q(\\beta,\\nu|\\beta^{(g)},\\nu^{(g)})\\) objective function and then compute E- and M steps from this to provide an iterative algorithm for updating parameters. To update the hyperparameter \\(\\nu\\) we can maximize the fully data posterior \\(p(\\beta,\\nu|y)\\) with \\(\\beta\\) fixed at \\(\\beta^{(g+1)}\\). The algorithm can be summarized as follows:\n\\[\\beta^{(g+1)} = \\arg\\max_{\\beta} Q(\\beta|\\beta^{(g)},\\nu^{(g)}) \\quad \\text{where} \\quad Q(\\beta|\\beta^{(g)},\\nu^{(g)}) = \\mathbb{E}_{z|\\beta^{(g)},\\nu^{(g)},y} \\log p(y,z|\\beta,\\nu^{(g)})\\]\n\\[\\nu^{(g+1)} = \\arg\\max_{\\nu} \\log p(\\beta^{(g+1)},\\nu|y)\\]\n\nExample 22.1 Simulated Annealing (SA) is a simulation-based approach to finding\n\\[\\hat{\\theta} = \\arg\\max_{\\theta \\in \\Theta} H(\\theta)\\]\n\\[\\pi_J(\\theta) = \\frac{e^{-JH(\\theta)}}{\\int e^{JH(\\theta)}d\\mu(\\theta)}\\]\nwhere \\(J\\) is a temperature parameter. Instead of looking at derivatives and performing gradient-based optimization you can simulate from the sequence of densities. This forms a time-homogeneous Markov chain and under suitable regularity conditions on the relaxation schedule for the temperature we have \\(\\theta^{(g)} \\to \\hat{\\theta}\\). The main caveat is that we need to know the criterion function \\(H(\\theta)\\) to evaluate the Metropolis probability for sampling from the sequence of densities. This is not always available.\nAn interesting generalisation which is appropriate in latent variable mixture models is the following. Suppose that \\(H(\\theta) = \\mathbb{E}_{z|\\theta} \\{H(z,\\theta)\\}\\) is unavailable in closed-form where without loss of generality we assume that \\(H(z,\\theta) \\geq 0\\). In this case we can use latent variable simulated annealing (LVSA) methods. Define a joint probability distribution for \\(z_J = (z_1,\\ldots,z_J)\\) as\n\\[\\pi_J(z_J,\\theta) \\propto \\prod_{j=1}^J H(z_j,\\theta)p(z_j|\\theta)\\mu(\\theta)\\]\nfor some measure \\(\\mu\\) which ensures integrability of the joint. This distribution has the property that its marginal distribution on \\(\\theta\\) is given by\n\\[\\pi_J(\\theta) \\propto \\mathbb{E}_{z|\\theta} \\{H(z,\\theta)\\}^J \\mu(\\theta) = e^{J \\ln H(\\theta)}\\mu(\\theta)\\]\nBy the simulated annealing argument we see that this marginal collapses on the maximum of \\(\\ln H(\\theta)\\). The advantage of this approach is that it is typically straightforward to sample with MCMC from the conditionals\n\\[\\pi_J(z_i|\\theta) \\sim H(z_j,\\theta)p(z_j|\\theta) \\quad \\text{and} \\quad \\pi_J(\\theta|z) \\sim \\prod_{j=1}^J H(z_j,\\theta)p(z_j|\\theta)\\mu(\\theta)\\]\n\n\n\n\n\nBottou, Léon, Frank E Curtis, and Jorge Nocedal. 2018. “Optimization Methods for Large-Scale Machine Learning.” SIAM Review 60 (2): 223–311.\n\n\nBryson, Arthur E. 1961. “A Gradient Method for Optimizing Multi-Stage Allocation Processes.” In Proc. Harvard Univ. Symposium on Digital Computers and Their Applications. Vol. 72.\n\n\nCarreira-Perpinán, Miguel A, and Weiran Wang. 2014. “Distributed Optimization of Deeply Nested Systems.” In AISTATS, 10–19.\n\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, et al. 2012. “Large Scale Distributed Deep Networks.” In Advances in Neural Information Processing Systems, 1223–31.\n\n\nDreyfus, Stuart. 1962. “The Numerical Solution of Variational Problems.” Journal of Mathematical Analysis and Applications 5 (1): 30–45.\n\n\n———. 1973. “The Computational Solution of Optimal Control Problems with Time Lag.” IEEE Transactions on Automatic Control 18 (4): 383–85.\n\n\nGriewank, Andreas, Kshitij Kulshreshtha, and Andrea Walther. 2012. “On the Numerical Stability of Algorithmic Differentiation.” Computing. Archives for Scientific Computing 94 (2-4): 125–49.\n\n\nHardt, Moritz, Ben Recht, and Yoram Singer. 2016. “Train Faster, Generalize Better: Stability of Stochastic Gradient Descent.” In International Conference on Machine Learning, 1225–34. PMLR.\n\n\nKeskar, Nitish Shirish, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. 2016. “On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.” arXiv Preprint arXiv:1609.04836. https://arxiv.org/abs/1609.04836.\n\n\nKingma, Diederik, and Jimmy Ba. 2014. “Adam: A Method for Stochastic Optimization.” arXiv Preprint arXiv:1412.6980. https://arxiv.org/abs/1412.6980.\n\n\nLeCun, Yann, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. 2002. “Efficient Backprop.” In Neural Networks: Tricks of the Trade, 9–50. Springer.\n\n\nLinnainmaa, Seppo. 1970. “The Representation of the Cumulative Rounding Error of an Algorithm as a Taylor Expansion of the Local Rounding Errors.” Master’s Thesis (in Finnish), Univ. Helsinki, 6–7.\n\n\nNesterov, Yurii. 1983. “A Method of Solving a Convex Programming Problem with Convergence Rate O (1/K2).” In Soviet Mathematics Doklady, 27:372–76.\n\n\n———. 2013. Introductory Lectures on Convex Optimization: A Basic Course. Vol. 87. Springer Science & Business Media.\n\n\nOstrovskii, GM, Yu M Volin, and WW Borisov. 1971. “Uber Die Berechnung von Ableitungen.” Wissenschaftliche Zeitschrift Der Technischen Hochschule f Ur Chemie, Leuna-Merseburg 13 (4): 382–84.\n\n\nRobbins, Herbert, and Sutton Monro. 1951. “A Stochastic Approximation Method.” The Annals of Mathematical Statistics 22 (3): 400–407.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533.\n\n\nSchmidhuber, Jürgen. 2015. “Deep Learning in Neural Networks: An Overview.” Neural Networks 61: 85–117.\n\n\nSutskever, Ilya, James Martens, George Dahl, and Geoffrey Hinton. 2013. “On the Importance of Initialization and Momentum in Deep Learning.” In International Conference on Machine Learning, 1139–47.\n\n\nWerbos, Paul. 1974. “Beyond Regression:\" New Tools for Prediction and Analysis in the Behavioral Sciences.” Ph. D. Dissertation, Harvard University.\n\n\nWerbos, Paul J. 1982. “Applications of Advances in Nonlinear Sensitivity Analysis.” In System Modeling and Optimization, 762–70. Springer.\n\n\nZeiler, Matthew D. 2012. “ADADELTA: An Adaptive Learning Rate Method.” arXiv Preprint arXiv:1212.5701. https://arxiv.org/abs/1212.5701.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "24-qnn.html",
    "href": "24-qnn.html",
    "title": "23  Quantile Neural Networks",
    "section": "",
    "text": "23.1 MEU\nGBC Let \\((X,Y) \\sim P_{X,Y}\\) be input-output pairs and \\(P_{X,Y}\\) a joint measure from which we can simulate a training dataset \\((X_i, Y_i)_{i=1}^N \\sim P_{X,Y}\\). Standard prediction techniques for the conditional posterior mean \\(\\hat{X}(Y) = E(X|Y) = f(Y)\\) of the input given the output. To do this, consider the multivariate non-parametric regression \\(X = f(Y) + \\epsilon\\) and provide methods for estimating the conditional mean. Typically estimators, \\(\\hat{f}\\), include KNN and Kernel methods. Recently, deep learners have been proposed and the theoretical properties of superpositions of affine functions (a.k.a. ridge functions) have been provided (see Montanni and Yang, Schmidt-Hieber, Polson and Rockova).\nGenerative methods take this approach one step further. Let \\(Z \\sim P_Z\\) be a base measure for a latent variable, \\(Z\\), typically a standard multivariate normal or vector of uniforms. The goal of generative methods is to characterize the posterior measure \\(P_{X|Y}\\) from the training data \\((X_i, Y_i)_{i=1}^N \\sim P_{X,Y}\\) where \\(N\\) is chosen to be suitably large. A deep learner is used to estimate \\(\\hat{f}\\) via the non-parametric regression \\(X = f(Y, Z)\\). In the case where \\(Z\\) is uniform, this amounts to inverse cdf sampling, namely \\(X = F_{X|Y}^{-1}(U)\\).\nIn general, we characterize the posterior map for any output \\(Y\\). Simply evaluate the network at any \\(Y\\) via the transport map \\[\nX = H(S(Y), \\psi(Z))\n\\] from a new base draw, \\(Z\\). Here \\(\\psi\\) denotes the cosine embedding so that the architecture for the latent variable corresponds to a Fourier approximation with rates of convergence given by \\(O(N^{-\\frac{1}{2}})\\), see Barron (1993). The deep learner is estimated via a quantile NN from the triples \\((X_i, Y_i, Z_i)_{i=1}^N \\sim P_{X,Y} \\times P_Z\\). The ensuing estimator \\(\\hat{H}_N\\) can be thought of as a transport map from the base distribution to the posterior as required.\nSpecifically, the idea of generative methods is straightforward. Let \\(y\\) denote data and \\(\\theta\\) a vector of parameters including any hidden states (a.k.a. latent variables) \\(z\\). First, we generate a “look-up” table of “fake” data \\(\\{y^{(i)}, \\theta^{(i)}\\}_{i=1}^N\\). By simulating a training dataset of outputs and parameters allows us to use deep learning to solve for the inverse map via a supervised learning problem. Generative methods have the advantage of being likelihood-free. For example, our model might be specified by a forward map \\(y^{(i)} = f(\\theta^{(i)})\\) rather than a traditional random draw from a likelihood function \\(y^{(i)} \\sim p(y^{(i)}|\\theta^{(i)})\\).\nGenerative methods have a number of advantages. First, they are density free. Hence they can be applied in a variety of contexts such as computer simulation, economics where traditional methods are computationally harder. Secondly, they naturally extend to decision problems. Third, they exploit the use of deep neural networks such as Quantile NNs. Hence they naturally provide good forecasting tools.\nPosterior uncertainty is solved via the inverse non-parametric regression problem where we predict \\(\\theta^{(i)}\\) from \\(y^{(i)}\\) and \\(\\tau^{(i)}\\) which is an independent base distribution, \\(p(\\tau)\\). The base distribution is typically uniform or a very large dimensional Gaussian vector. Then we need to train a deep neural network, \\(H\\), on \\[\n\\theta^{(i)} = H(S(y^{(i)}), \\tau^{(i)}).\n\\] Here \\(S(y)\\) is a statistic to perform dimension reduction with respect to the signal distribution. Specifying \\(H\\) is the key to the efficiency of the approach. Polson, Ruggeri, and Sokolov (2024) propose the use of quantile neural networks implemented with ReLU activation functions.\nTo extend our generative method to MEU problems, we assume that the utility function \\(U\\) is given. Then we simply draw additional associated utilities \\(U^{(i)}_d \\defeq U(d,\\theta^{(i)})\\) for a given decision \\(d\\) and \\(\\theta^{(i)}\\) draw from above. Then we append the utilities to our training dataset including the baseline distribution \\(\\tau^{(i)}\\) to yield a new training dataset \\[\n\\{U_d^{(i)}, y^{(i)}, \\theta^{(i)}, \\tau^{(i)}\\}_{i=1}^N.\n\\] Now we construct a non-parametric estimator of the form \\[\nU_d^{(i)} = H(S(y^{(i)}), \\theta^{(i)}, \\tau^{(i)}, d),\n\\] Given that the posterior quantiles of the distributional utility, denoted by \\(F^{-1}_{U|d,y}(\\tau)\\) are represented as a quantile neural network, we then use a key identity which shows how to represent any expectation as a marginal over quantiles, namely \\[\nE_{\\theta|y}[U(d, \\theta)] = \\int_0^1 F^{-1}_{U|d,y}(\\tau) d\\tau\n\\] The optimal decision function simply maximizes the expected utility \\[\nd^\\star(y) \\defeq \\arg \\max_d E_{\\theta|y}[U(d, \\theta)]\n\\]\nTo fix notation and to allow for deterministic updates (a.k.a. measures rather than probabilities). Let \\(\\mathcal{Y}\\) denote a locally compact metric space of signals, denoted by \\(y\\), and \\(\\mathcal{B}(\\mathcal{Y})\\) the Borel \\(\\sigma\\)-algebra of \\(\\mathcal{Y}\\). Let \\(\\lambda\\) be a measure on the measurable space of signals \\((\\mathcal{Y}, \\mathcal{B}(\\mathcal{Y}))\\). Let \\(P(dy|\\theta)\\) denote the conditional distribution of signals given the parameters. Let \\(\\Theta\\) denote a locally compact metric space of admissible parameters (a.k.a. hidden states and latent variables \\(z \\in \\mathcal{Z}\\)) and \\(\\mathcal{B}(\\Theta)\\) the Borel \\(\\sigma\\)-algebra of \\(\\Theta\\). Let \\(\\mu\\) be a measure on the measurable space of parameters \\((\\Theta, \\mathcal{B}(\\Theta))\\). Let \\(\\Pi(d\\theta|y)\\) denote the conditional distribution of the parameters given the observed signal \\(y\\) (a.k.a., the posterior distribution). In many cases, \\(\\Pi\\) is absolutely continuous with density \\(\\pi\\) such that \\[\n\\Pi(d\\theta|y) = \\pi(\\theta|y) \\mu(d\\theta).\n\\] Moreover, we will write \\(\\Pi(d\\theta) = \\pi(\\theta) \\mu(d\\theta)\\) for prior density \\(\\pi\\) when available. In the case of likelihood-free models, the output is simply specified by a map (a.k.a. forward equation) \\[\ny = f(\\theta)\n\\] When a likelihood \\(p(y|\\theta)\\) is available w.r.t. the measure \\(\\lambda\\), we write \\[\nP(dy|\\theta) = p(y|\\theta) \\lambda(dy).\n\\] There are a number of advantages of such an approach, primarily the fact that they are density free. They use simulation methods and deep neural networks to invert the prior to posterior map. We build on this framework and show how to incorporate utilities into the generative procedure.\nNoise Outsourcing Theorem If \\((Y, \\Theta)\\) are random variables in a Borel space \\((\\mathcal{Y}, \\Theta)\\) then there exists an r.v. \\(\\tau \\sim U(0,1)\\) which is independent of \\(Y\\) and a function \\(H: [0,1] \\times \\mathcal{Y} \\rightarrow \\Theta\\) such that \\[\n(Y, \\Theta) \\stackrel{a.s.}{=} (Y, H(Y, \\tau))\n\\] Hence the existence of \\(H\\) follows from the noise outsourcing theorem Kallenberg (1997). Moreover, if there is a statistic \\(S(Y)\\) with \\(Y \\independent \\Theta | S(Y)\\), then \\[\n\\Theta|Y \\stackrel{a.s.}{=} H(S(Y), \\tau).\n\\] The role of \\(S(Y)\\) is equivalent to the ABC literature. It performs dimension reduction in \\(n\\), the dimensionality of the signal. Our approach then is to use deep neural network first to calculate the inverse probability map (a.k.a posterior) \\(\\theta \\stackrel{D}{=} F^{-1}_{\\theta|y}(\\bm{\\tau})\\) where \\(\\bm{\\tau}\\) is a vector of uniforms. In the multi-parameter case, we use an RNN or autoregressive structure where we model a vector via a sequence \\((F^{-1}_{\\theta_1}(\\tau_1), F^{-1}_{\\theta_2|\\theta_1}(\\tau_2), \\ldots)\\). A remarkable result due to Brillinger (2012) shows that we can learn \\(S\\) independent of \\(H\\) simply via OLS.\nAs a default choice of network architecture, we will use a ReLU network for the posterior quantile map. The first layer of the network is given by the utility function and hence this is what makes the method different from learning the posterior and then directly using naive Monte Carlo to estimate expected utility. This would be inefficient as quite often the utility function places high weight on region of low-posterior probability representing tail risk.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "24-qnn.html#bayes-rule-for-quantiles",
    "href": "24-qnn.html#bayes-rule-for-quantiles",
    "title": "23  Quantile Neural Networks",
    "section": "23.2 Bayes Rule for Quantiles",
    "text": "23.2 Bayes Rule for Quantiles\nParzen (2004) shows that quantile methods are direct alternatives to density computations. Specifically, given \\(F_{\\theta|y} (u)\\), a non-decreasing and continuous from right function, we define\n\\[Q_{\\theta| y} (u) \\defeq  F^{-1}_{\\theta|y}  ( u ) = \\inf \\left ( \\theta : F_{\\theta|y} (\\theta) \\geq u \\right )\\]\nwhich is non-decreasing, continuous from left.\nParzen (2004) shows the important probabilistic property of quantiles\n\\[\\theta \\stackrel{P}{=} Q_\\theta ( F_\\theta (\\theta ) )\\]\nHence, we can increase the efficiency by ordering the samples of \\(\\theta\\) and the baseline distribution and use monotonicity of the inverse CDF map.\nLet \\(g(y)\\) be non-decreasing and continuous from left with \\(g^{-1} (z ) = \\sup \\left ( y : g(y ) \\leq z \\right )\\). Then, the transformed quantile has a compositional nature, namely\n\\[Q_{ g(Y) } ( u ) = g ( Q (u ))\\]\nHence, quantiles act as superposition (a.k.a. deep Learner).\nThis is best illustrated in the Bayes learning model. We have the following result updating prior to posterior quantiles known as the conditional quantile representation\n\\[Q_{ \\theta | Y=y } ( u ) = Q_\\theta ( s )  \\; \\; {\\rm where} \\; \\;   s = Q_{ F(\\theta) |  Y=y } ( u)\\]\nTo compute \\(s\\), by definition\n\\[u = F_{ F(\\theta ) | Y=y} ( s  ) = P( F (\\theta ) \\leq s | Y=y ) = P( \\theta \\leq Q_\\theta (s ) | Y=y )  = F_{ \\theta | Y=y } ( Q_\\theta ( s ) )\\]\n\n23.2.1 Maximum Expected Utility\nDecision problems are characterized by a utility function \\(U( \\theta , d )\\) defined over parameters, \\(\\theta\\), and decisions, \\(d \\in \\mathcal{D}\\). We will find it useful to define the family of utility random variables indexed by decisions defined by\n\\[U_d \\defeq U( \\theta , d ) \\; \\; {\\rm where} \\; \\; \\theta \\sim \\Pi ( d  \\theta )\\]\nOptimal Bayesian decisions are then defined by the solution to the prior expected utility\n\\[U(d) = E_{\\theta}(U(d,\\theta)) = \\int U(d,\\theta)p(\\theta)d\\theta\\]\n\\[d^\\star = {\\rm arg} \\max_d U(d)\\]\nWhen information in the form of signals \\(y\\) is available, we need to calculate the posterior distribution \\(p( \\theta | y ) = f(y | \\theta ) p( \\theta )/ p(y)\\). Then we have to solve for the optimal a posterior decision rule \\(d^\\star (y)\\) defined by\n\\[d^\\star(y)  = {\\rm arg} \\max_d  \\; \\int U( \\theta , d ) p( \\theta | y ) d \\theta\\]\nwhere expectations are now taken w.r.t. \\(p( \\theta | y)\\) the posterior distribution.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "24-qnn.html#normal-normal-bayes-learning-wang-distortion",
    "href": "24-qnn.html#normal-normal-bayes-learning-wang-distortion",
    "title": "23  Quantile Neural Networks",
    "section": "23.3 Normal-Normal Bayes Learning: Wang Distortion",
    "text": "23.3 Normal-Normal Bayes Learning: Wang Distortion\nFor the purpose of illustration, we consider the normal-normal learning model. We will develop the necessary quantile theory to show how to calculate posteriors and expected utility without resorting to densities. Also, we show a relationship with Wang’s risk distortion measure as the deep learning that needs to be learned.\nSpecifically, we observe the data \\(y = ( y_1,\\ldots,y_n)\\) from the following model\n\\[y_1 , \\ldots , y_n  \\mid \\theta \\sim N(\\theta, \\sigma^2)\\]\n\\[\\theta \\sim N(\\mu,\\alpha^2)\\]\nHence, the summary (sufficient) statistic is \\(S(y) = \\bar y = \\frac{1}{n} \\sum_{i=1}^n y_i\\).\nGiven observed samples \\(y = (y_1,\\ldots,y_n)\\), the posterior is then \\(\\theta \\mid y \\sim N(\\mu_*, \\sigma_*^2)\\) with\n\\[\\mu_* = (\\sigma^2 \\mu + \\alpha^2s) / t, \\quad \\sigma^2_* = \\alpha^2 \\sigma^2 / t\\]\nwhere\n\\[t =  \\sigma^2 + n\\alpha^2 \\; \\; {\\rm and} \\; \\; s(y) = \\sum_{i=1}^{n}y_i\\]\nThe posterior and prior CDFs are then related via the\n\\[1-\\Phi(\\theta, \\mu_*,\\sigma_*) = g(1 - \\Phi(\\theta, \\mu, \\alpha^2))\\]\nwhere \\(\\Phi\\) is the normal distribution function. Here the Wang distortion function defined by\n\\[g(p) = \\Phi\\left(\\lambda_1 \\Phi^{-1}(p) + \\lambda\\right)\\]\nwhere\n\\[\\lambda_1 = \\dfrac{\\alpha}{\\sigma_*} \\; \\; {\\rm and} \\; \\; \\lambda = \\alpha\\lambda_1(s-n\\mu)/t\\]\nThe proof is relatively simple and is as follows\n\\[\\begin{align*}\n    g(1 - \\Phi(\\theta, \\mu, \\alpha^2)) & = g(\\Phi(-\\theta, \\mu, \\alpha^2)) = g\\left(\\Phi\\left(-\\dfrac{\\theta - \\mu}{\\alpha}\\right)\\right)\\\\\n    & = \\Phi\\left(\\lambda_1 \\left(-\\dfrac{\\theta - \\mu}{\\alpha}\\right) + \\lambda\\right) =  1 - \\Phi\\left(\\dfrac{\\theta - (\\mu+ \\alpha\\lambda/\\lambda_1)}{\\alpha/\\lambda_1}\\right)\n\\end{align*}\\]\nThus, the corresponding posterior updated parameters are\n\\[\\sigma_* = \\alpha/\\lambda_1, \\quad \\lambda_1 = \\dfrac{\\alpha}{\\sigma_*}\\]\nand\n\\[\\mu_* = \\mu+ \\alpha\\lambda/\\lambda_1, \\quad \\lambda = \\dfrac{\\lambda_1(\\mu_* - \\mu)}{\\alpha} = \\alpha\\lambda_1(s-n\\mu)/t\\]\nWe now provide an empirical example.\n\n23.3.1 Numerical Example\nConsider the normal-normal model with Prior \\(\\theta \\sim N(0,5)\\) and likelihood \\(y \\sim N(3,10)\\). We generate \\(n=100\\) samples from the likelihood and calculate the posterior distribution.\n\n\n\n\n\n\n\n\n\n\n\n(a) Model for simulated data\n\n\n\n\n\n\n\n\n\n\n\n(b) Distortion Function \\(g\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) 1 - \\(\\Phi\\)\n\n\n\n\n\n\n\nFigure 23.1: Density for prior, likelihood and posterior, distortion function and 1 - \\(\\Phi\\) for the prior and posterior of the normal-normal model.\n\n\n\nThe posterior distribution calculated from the sample is then \\(\\theta \\mid y \\sim N(3.28, 0.98)\\).\nFigure 23.2 shows the Wang distortion function for the normal-normal model. The left panel shows the model for the simulated data, while the middle panel shows the distortion function, the right panel shows the 1 - \\(\\Phi\\) for the prior and posterior of the normal-normal model.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "24-qnn.html#portfolio-learning",
    "href": "24-qnn.html#portfolio-learning",
    "title": "23  Quantile Neural Networks",
    "section": "23.4 Portfolio Learning",
    "text": "23.4 Portfolio Learning\nConsider power utility and log-normal returns (without leverage). For \\(\\omega \\in (0,1)\\)\n\\[U(W) = -e^{-\\gamma W}, ~ W\\mid \\omega \\sim \\mathcal{N}( (1-\\omega) r_f + \\omega R,\\sigma^2)\\]\nLet \\(W = (1-\\omega)r_f + \\omega R\\), with \\(R \\sim N(\\mu,\\sigma^2)\\), Here, \\(U^{-1}\\) exists and \\(r_f\\) is the risk-free rate, \\(\\mu\\) is the mean return and \\(\\tau^2\\) is the variance of the return. Then the expected utility is\n\\[U(\\omega) = E(-e^{\\gamma W}) = \\exp\\left\\{\\gamma E(W) + \\frac{1}{2}\\omega^2Var(W)\\right\\}\\]\nWe have closed-form utility in this case, since it is the moment-generating function of the log-normal. Within the Gen-AI framework, it is easy to add learning or uncertainty on top of \\(\\sigma^2\\) and have a joint posterior distribution \\(p(\\mu, \\sigma^2 \\mid R)\\).\nThus, the closed form solution is\n\\[U(\\omega) = \\exp\\left\\{\\gamma \\left\\{(1-\\omega)r_f + \\omega\\mu\\right\\}\\right\\} \\exp \\left \\{ \\dfrac{1}{2}\\gamma^2\\omega^2\\sigma^2 \\right \\}\\]\nThe optimal Kelly-Brieman-Thorpe-Merton rule is given by\n\\[\\omega^* = (\\mu - r_f)/(\\sigma^2\\gamma)\\]\nNow we reorder the integral in terms of quantiles of the utility function. We assume utility is the random variable and re-order the sum as the expected value of \\(U\\)\n\\[E(U(W)) = \\int_{0}^{1}F_{U(W)}^{-1}(\\tau)d\\tau\\]\nHence, if we can approximate the inverse of the CDF of \\(U(W)\\) with a quantile NN, we can approximate the expected utility and optimize over \\(\\omega\\).\nThe stochastic utility is modeled with a deep neural network, and we write",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "24-qnn.html#bayes-rule-for-quantiles-1",
    "href": "24-qnn.html#bayes-rule-for-quantiles-1",
    "title": "23  Quantile Neural Networks",
    "section": "23.5 Bayes Rule for Quantiles",
    "text": "23.5 Bayes Rule for Quantiles\nParzen (2004) shows that quantile methods are direct alternatives to density computations. Specifically, given \\(F_{\\theta|y} (u)\\), a non-decreasing and continuous from right function, we define\n\\[Q_{\\theta| y} (u) \\defeq  F^{-1}_{\\theta|y}  ( u ) = \\inf \\left ( \\theta : F_{\\theta|y} (\\theta) \\geq u \\right )\\]\nwhich is non-decreasing, continuous from left. Parzen (2004) shows the important probabilistic property of quantiles \\[\n\\theta \\stackrel{P}{=} Q_\\theta ( F_\\theta (\\theta ) )\n\\] Hence, we can increase the efficiency by ordering the samples of \\(\\theta\\) and the baseline distribution and use monotonicity of the inverse CDF map.\nLet $ g(y)$ be non-decreasing and continuous from left with $g^{-1} (z ) = ( y : g(y ) z ) \\(.\nThen,  the transformed quantile has a compositional nature, namely\\)$ Q_{ g(Y) } ( u ) = g ( Q (u )) $$ Hence, quantiles act as superposition (a.k.a. deep Learner).\nThis is best illustrated in the Bayes learning model. We have the following result updating prior to posterior quantiles known as the conditional quantile representation \\[\nQ_{ \\theta | Y=y } ( u ) = Q_\\theta ( s )  \\; \\;\n{\\rm where} \\; \\;   s = Q_{ F(\\theta) |  Y=y } ( u)\n\\] To compute \\(s\\), by definition \\[\nu = F_{ F(\\theta ) | Y=y} ( s  ) = P( F (\\theta ) \\leq s | Y=y )\n= P( \\theta \\leq Q_\\theta (s ) | Y=y )  = F_{ \\theta | Y=y } ( Q_\\theta ( s ) )\n\\]\nDecision problems are characterized by a utility function $ U( , d ) $ defined over parameters, $ $, and decisions, $ d \\(.\nWe will find it  useful to define the family of utility random variables indexed by decisions defined by\\)$ U_d U( , d ) ; ; {} ; ; ( d ) \\[\nOptimal Bayesian decisions \\cite{degroot2005optimal} are then defined by the solution to  the  prior expected utility\n\\[\nU(d) = E_{\\theta}(U(d,\\theta)) = \\int U(d,\\theta)p(\\theta)d\\theta,\n\\]\n\\] d^= {} _d U(d) \\[\nWhen information in the form of signals $y$ is available, we need to calculate the posterior distribution $p( \\theta | y ) = f(y | \\theta ) p( \\theta )/ p(y)$. Then we have to  solve for the optimal  \\emph{a posterior} decision rule $ d^\\star (y) $ defined by\n\\] d^(y) = {} _d ; U( , d ) p( | y ) d $$ where expectations are now taken w.r.t. $ p( | y) $ the posterior distribution.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "24-qnn.html#normal-normal-bayes-learning-wang-distortion-1",
    "href": "24-qnn.html#normal-normal-bayes-learning-wang-distortion-1",
    "title": "23  Quantile Neural Networks",
    "section": "23.6 Normal-Normal Bayes Learning: Wang Distortion",
    "text": "23.6 Normal-Normal Bayes Learning: Wang Distortion\nFor the purpose of illustration, we consider the normal-normal learning model. We will develop the necessary quantile theory to show how to calculate posteriors and expected utility without resorting to densities. Also, we show a relationship with Wang’s risk distortion measure as the deep learning that needs to be learned.\nSpecifically, we observe the data \\(y = (y_1,\\ldots,y_n)\\) from the following model\n\\[y_1 , \\ldots , y_n  \\mid \\theta \\sim N(\\theta, \\sigma^2)\\] \\[\\theta \\sim N(\\mu,\\alpha^2)\\]\nHence, the summary (sufficient) statistic is \\(S(y) = \\bar y = \\frac{1}{n} \\sum_{i=1}^n y_i\\).\nGiven observed samples \\(y = (y_1,\\ldots,y_n)\\), the posterior is then \\(\\theta \\mid y \\sim N(\\mu_*, \\sigma_*^2)\\) with\n\\[\\mu_* = (\\sigma^2 \\mu + \\alpha^2s) / t, \\quad \\sigma^2_* = \\alpha^2 \\sigma^2 / t\\]\nwhere\n\\[t =  \\sigma^2 + n\\alpha^2 \\; \\; {\\rm and} \\; \\; s(y) = \\sum_{i=1}^{n}y_i\\]\nThe posterior and prior CDFs are then related via the\n\\[1-\\Phi(\\theta, \\mu_*,\\sigma_*) = g(1 - \\Phi(\\theta, \\mu, \\alpha^2))\\]\nwhere \\(\\Phi\\) is the normal distribution function. Here the Wang distortion function defined by\n\\[g(p) = \\Phi\\left(\\lambda_1 \\Phi^{-1}(p) + \\lambda\\right)\\]\nwhere\n\\[\\lambda_1 = \\dfrac{\\alpha}{\\sigma_*} \\; \\; {\\rm and} \\; \\;\n\\lambda = \\alpha\\lambda_1(s-n\\mu)/t\\]\nThe proof is relatively simple and is as follows\n\\[g(1 - \\Phi(\\theta, \\mu, \\alpha^2)) = g(\\Phi(-\\theta, \\mu, \\alpha^2)) = g\\left(\\Phi\\left(-\\dfrac{\\theta - \\mu}{\\alpha}\\right)\\right)\\]\n\\[= \\Phi\\left(\\lambda_1 \\left(-\\dfrac{\\theta - \\mu}{\\alpha}\\right) + \\lambda\\right) =  1 - \\Phi\\left(\\dfrac{\\theta - (\\mu+ \\alpha\\lambda/\\lambda_1)}{\\alpha/\\lambda_1}\\right)\\]\nThus, the corresponding posterior updated parameters are\n\\[\\sigma_* = \\alpha/\\lambda_1, \\quad \\lambda_1 = \\dfrac{\\alpha}{\\sigma_*}\\]\nand\n\\[\\mu_* = \\mu+ \\alpha\\lambda/\\lambda_1, \\quad \\lambda = \\dfrac{\\lambda_1(\\mu_* - \\mu)}{\\alpha} = \\alpha\\lambda_1(s-n\\mu)/t\\]\nWe now provide an empirical example.\n\n23.6.1 Numerical Example\nConsider the normal-normal model with Prior \\(\\theta \\sim N(0,5)\\) and likelihood \\(y \\sim N(3,10)\\). We generate \\(n=100\\) samples from the likelihood and calculate the posterior distribution.\n\n\n\n\n\n\n\n\n\n\n\n(a) Model for simulated data\n\n\n\n\n\n\n\n\n\n\n\n(b) Distortion Function \\(g\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) 1 - \\(\\Phi\\)\n\n\n\n\n\n\n\nFigure 23.2: Density for prior, likelihood and posterior, distortion function and 1 - \\(\\Phi\\) for the prior and posterior of the normal-normal model.\n\n\n\nThe posterior distribution calculated from the sample is then \\(\\theta \\mid y \\sim N(3.28, 0.98)\\).\nFigure 23.2 shows the Wang distortion function for the normal-normal model. The left panel shows the model for the simulated data, while the middle panel shows the distortion function, the right panel shows the 1 - \\(\\Phi\\) for the prior and posterior of the normal-normal model.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "24-qnn.html#portfolio-learning-1",
    "href": "24-qnn.html#portfolio-learning-1",
    "title": "23  Quantile Neural Networks",
    "section": "23.7 Portfolio Learning",
    "text": "23.7 Portfolio Learning\nConsider power utility and log-normal returns (without leverage). For \\(\\omega \\in (0,1)\\)\n\\[U(W) = -e^{-\\gamma W}, ~ W\\mid \\omega \\sim \\mathcal{N}( (1-\\omega) r_f + \\omega R,\\sigma^2)\\]\nLet \\(W = (1-\\omega)r_f + \\omega R\\), with \\(R \\sim N(\\mu,\\sigma^2)\\), Here, \\(U^{-1}\\) exists and \\(r_f\\) is the risk-free rate, \\(\\mu\\) is the mean return and \\(\\tau^2\\) is the variance of the return. Then the expected utility is\n\\[U(\\omega) = E(-e^{\\gamma W}) = \\exp\\left\\{\\gamma E(W) + \\frac{1}{2}\\omega^2Var(W)\\right\\}\\]\nWe have closed-form utility in this case, since it is the moment-generating function of the log-normal. Within the Gen-AI framework, it is easy to add learning or uncertainty on top of \\(\\sigma^2\\) and have a joint posterior distribution \\(p(\\mu, \\sigma^2 \\mid R)\\).\nThus, the closed form solution is\n\\[U(\\omega) = \\exp\\left\\{\\gamma \\left\\{(1-\\omega)r_f + \\omega\\mu\\right\\}\\right\\} \\exp \\left \\{ \\dfrac{1}{2}\\gamma^2\\omega^2\\sigma^2 \\right \\}\\]\nThe optimal Kelly-Brieman-Thorpe-Merton rule is given by\n\\[\\omega^* = (\\mu - r_f)/(\\sigma^2\\gamma)\\]\nNow we reorder the integral in terms of quantiles of the utility function. We assume utility is the random variable and re-order the sum as the expected value of \\(U\\)\n\\[E(U(W)) = \\int_{0}^{1}F_{U(W)}^{-1}(\\tau)d\\tau\\]\nHence, if we can approximate the inverse of the CDF of \\(U(W)\\) with a quantile NN, we can approximate the expected utility and optimize over \\(\\omega\\).\nThe stochastic utility is modeled with a deep neural network, and we write\n\\[Z = U(W) \\approx F, ~ W  = U^{-1}(F)\\]\nWe can do optimization by doing the grid for \\(\\omega\\).\n\n23.7.1 Empirical Example\nConsider \\(\\omega \\in (0,1)\\), \\(r_f = 0.05\\), \\(\\mu=0.1\\), \\(\\sigma=0.25\\), \\(\\gamma = 2\\). We have the closed-form fractional Kelly criterion solution\n\\[\\omega^* = \\frac{1}{\\gamma}   \\frac{ \\mu - r_f}{ \\sigma^2} = \\frac{1}{2} \\frac{ 0.1 - 0.05 }{ 0.25^2 } = 0.40\\]\nWe can simulate the expected utility and compare with the closed-form solution.\n\n\n\n\n\n\n\n\n\n\n\n(a) Portfolio tau z\n\n\n\n\n\n\n\n\n\n\n\n(b) Portfolio\n\n\n\n\n\n\n\nFigure 23.3: Line at 0.4 optimum\n\n\n\nAdd code.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "24-qnn.html#learning-quantiles",
    "href": "24-qnn.html#learning-quantiles",
    "title": "23  Quantile Neural Networks",
    "section": "23.8 Learning Quantiles",
    "text": "23.8 Learning Quantiles\nThe 1-Wasserstein distance is the \\(\\ell_1\\) metric on the inverse distribution function. It is also known as earth mover’s distance and can be calculated using order statistics (Levina and Bickel 2001). For quantile functions \\(F^{-1}_U\\) and \\(F^{-1}_V\\) the 1-Wasserstein distance is given by\n\\[\nW_1(F^{-1}_U, F^{-1}_V) = \\int_0^1 |F^{-1}_U(\\tau) - F^{-1}_V(\\tau)| d\\tau\n\\]\nIt can be shown that Wasserstein GAN networks outperform vanilla GAN due to the improved quantile metric. \\(q = F^{-1}_U(\\tau)\\) minimizes the expected quantile loss\n\\[\nE_U[\\rho_{\\tau}(u-q)]\n\\]\nQuantile regression can be shown to minimize the 1-Wasserstein metric. A related loss is the quantile divergence,\n\\[\nq(U,V) = \\int_0^1 \\int_{F^{-1}_U(q)}^{F^{-1}_V(q)} (F_U(\\tau)-q) dq d\\tau\n\\]\nThe quantile regression likelihood function is an asymmetric function that penalizes overestimation errors with weight \\(\\tau\\) and underestimation errors with weight \\(1-\\tau\\). For a given input-output pair \\((x, y)\\), and the quantile function \\(f(x, \\theta)\\), parametrized by \\(\\theta\\), the quantile loss is \\(\\rho_{\\tau}(u) = u(\\tau - I(u &lt; 0))\\), where \\(u = y - f(x)\\). From the implementation point of view, a more convenient form of this function is\n\\[\n\\rho_{\\tau}(u) = \\max(u\\tau, u(\\tau-1))\n\\]\nGiven a training data \\(\\{x_i, y_i\\}_{i=1}^N\\), and given quantile \\(\\tau\\), the loss is\n\\[\nL_{\\tau}(\\theta) = \\sum_{i=1}^N \\rho_{\\tau}(y_i - f(\\tau, x_i, \\theta))\n\\]\nFurther, we empirically found that adding a mean-squared loss to this objective function improves the predictive power of the model, thus the loss function we use is\n\\[\n\\alpha L_{\\tau}(\\theta) + \\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i, \\theta))^2\n\\]\nOne approach to learn the quantile function is to use a set of quantiles \\(0 &lt; \\tau_1 &lt; \\tau_2 &lt; \\ldots &lt; \\tau_K &lt; 1\\) and then learn \\(K\\) quantile functions simultaneously by minimizing\n\\[\nL(\\theta, \\tau_1, \\ldots, \\tau_K) = \\frac{1}{N K} \\sum_{i=1}^N \\sum_{k=1}^K \\rho_{\\tau_k}(y_i - f_{\\tau_k}(x_i, \\theta_k))\n\\]\nThe corresponding optimization problem of minimizing \\(L(\\theta)\\) can be augmented by adding a non-crossing constraint\n\\[\nf_{\\tau_i}(x, \\theta_i) &lt; f_{\\tau_j}(x, \\theta_j), \\quad \\forall x, \\; i &lt; j\n\\]\nThe non-crossing constraint has been considered by several authors, including (Chernozhukov, Fernández-Val, and Galichon 2010; Cannon 2018).\n\n23.8.1 Cosine Embedding for \\(\\tau\\)\nTo learn an inverse CDF (quantile function) \\(F^{-1}(\\tau, y) = f_\\theta(\\tau, y)\\) we will use a kernel embedding trick and augment the predictor space. We then represent the quantile function as a function of superposition for two other functions:\n\\[\nF^{-1}(\\tau, y) = f_\\theta(\\tau, y) = g(\\psi(y) \\circ \\phi(\\tau))\n\\]\nwhere \\(\\circ\\) is the element-wise multiplication operator. Both functions \\(g\\) and \\(\\psi\\) are feed-forward neural networks. \\(\\phi\\) is a cosine embedding. To avoid over-fitting, we use a sufficiently large training dataset, see (Dabney et al. 2018) in a reinforcement learning context.\nLet \\(g\\) and \\(\\psi\\) be feed-forward neural networks and \\(\\phi\\) a cosine embedding given by\n\\[\n\\phi_j(\\tau) = \\mathrm{ReLU}\\left(\\sum_{i=0}^{n-1} \\cos(\\pi i \\tau) w_{ij} + b_j\\right)\n\\]\nWe now illustrate our approach with a simple synthetic dataset.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "24-qnn.html#synthetic-data",
    "href": "24-qnn.html#synthetic-data",
    "title": "23  Quantile Neural Networks",
    "section": "23.9 Synthetic Data",
    "text": "23.9 Synthetic Data\nConsider a synthetic data generated from the model\n\\[\nx \\sim U(-1, 1) \\\\\ny \\sim N(\\sin(\\pi x)/(\\pi x), \\exp(1-x)/10)\n\\]\nThe true quantile function is given by\n\\[\nf_{\\tau}(x) = \\sin(\\pi x)/(\\pi x) + \\Phi^{-1}(\\tau) \\sqrt{\\exp(1-x)/10}\n\\]\n\n\n\nWe trained both implicit and explicit networks on the synthetic data set. The explicit network was trained for three fixed quantiles (0.05, 0.5, 0.95). We see no empirical difference between the two.\n\n\nWe train two quantile networks, one implicit and one explicit. The explicit network is trained for three fixed quantiles \\((0.05, 0.5, 0.95)\\). The figure above shows fits by both of the networks; we see no empirical difference between the two.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "24-qnn.html#quantiles-as-deep-learners",
    "href": "24-qnn.html#quantiles-as-deep-learners",
    "title": "23  Quantile Neural Networks",
    "section": "23.10 Quantiles as Deep Learners",
    "text": "23.10 Quantiles as Deep Learners\nOne can show that quantile models are direct alternatives to other Bayes computations. Specifically, given \\(F(y)\\), a non-decreasing and continuous from right function, we define\n\\[\nQ_{\\theta|y}(u) \\defeq F^{-1}_{\\theta|y}(u) = \\inf\\{y : F_{\\theta|y}(y) \\geq u\\}\n\\]\nwhich is non-decreasing, continuous from left. Now let \\(g(y)\\) be a non-decreasing and continuous from left with\n\\[\ng^{-1}(z) = \\sup\\{y : g(y) \\leq z\\}\n\\]\nThen, the transformed quantile has a compositional nature, namely\n\\[\nQ_{g(Y)}(u) = g(Q(u))\n\\]\nHence, quantiles act as superposition (a.k.a. deep learner).\nThis is best illustrated in the Bayes learning model. We have the following result updating prior to posterior quantiles known as the conditional quantile representation:\n\\[\nQ_{\\theta | Y=y}(u) = Q_Y(s)\n\\] where \\(s = Q_{F(\\theta) | Y=y}(u)\\)\nTo compute \\(s\\) we use\n\\[\nu = F_{F(\\theta) | Y=y}(s) = P(F(\\theta) \\leq s | Y=y) = P(\\theta \\leq Q_\\theta(s) | Y=y) = F_{\\theta | Y=y}(Q_\\theta(s))\n\\]\n(Parzen 2004) also shows the following probabilistic property of quantiles:\n\\[\n\\theta = Q_\\theta(F_\\theta(\\theta))\n\\]\nHence, we can increase the efficiency by ordering the samples of \\(\\theta\\) and the baseline distribution as the mapping being the inverse CDF is monotonic.\n\n23.10.1 Quantile Reinforcement Learning\n(Dabney et al. 2017) use quantile neural networks for decision-making and apply quantile neural networks to the problem of reinforcement learning. Specifically, they rely on the fact that expectations are quantile integrals. The key identity in this context is the Lorenz curve:\n\\[\nE(Y) = \\int_{-\\infty}^{\\infty} y dF(y) = \\int_0^1 F^{-1}(u) du\n\\]\nThen, distributional reinforcement learning algorithm finds\n\\[\n\\pi(x) = \\arg\\max_a E_{Z \\sim z(x, a)}(Z)\n\\]\nThen a Q-Learning algorithm can be applied, since the quantile projection keeps contraction property of Bellman operator. Similar approaches that rely on the dual Expected Utility were proposed by (Yaari 1987).\n\n\n\n\nBrillinger, David R. 2012. “A Generalized Linear Model With ‘Gaussian’ Regressor Variables.” In Selected Works of David Brillinger, edited by Peter Guttorp and David Brillinger, 589–606. Selected Works in Probability and Statistics. New York, NY: Springer.\n\n\nCannon, Alex J. 2018. “Non-Crossing Nonlinear Regression Quantiles by Monotone Composite Quantile Regression Neural Network, with Application to Rainfall Extremes.” Stochastic Environmental Research and Risk Assessment 32 (11): 3207–25.\n\n\nChernozhukov, Victor, Iván Fernández-Val, and Alfred Galichon. 2010. “Quantile and Probability Curves Without Crossing.” Econometrica 78 (3): 1093–1125. https://www.jstor.org/stable/40664520.\n\n\nDabney, Will, Georg Ostrovski, David Silver, and Rémi Munos. 2018. “Implicit Quantile Networks for Distributional Reinforcement Learning.” arXiv. https://arxiv.org/abs/1806.06923.\n\n\nDabney, Will, Mark Rowland, Marc G. Bellemare, and Rémi Munos. 2017. “Distributional Reinforcement Learning with Quantile Regression.” arXiv. https://arxiv.org/abs/1710.10044.\n\n\nKallenberg, Olav. 1997. Foundations of Modern Probability. 2nd ed. edition. Springer.\n\n\nLevina, Elizaveta, and Peter Bickel. 2001. “The Earth Mover’s Distance Is the Mallows Distance: Some Insights from Statistics.” In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, 2:251–56. IEEE.\n\n\nParzen, Emanuel. 2004. “Quantile Probability and Statistical Data Modeling.” Statistical Science 19 (4): 652–62. https://www.jstor.org/stable/4144436.\n\n\nPolson, Nick, Fabrizio Ruggeri, and Vadim Sokolov. 2024. “Generative Bayesian Computation for Maximum Expected Utility.” Entropy 26 (12): 1076.\n\n\nYaari, Menahem E. 1987. “The Dual Theory of Choice Under Risk.” Econometrica 55 (1): 95–115. https://www.jstor.org/stable/1911158.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "25-cnn.html",
    "href": "25-cnn.html",
    "title": "24  Convolutional Neural Networks",
    "section": "",
    "text": "24.1 Convolutions\nThe problem of pattern recognition in images is a very broad one. It includes the following tasks:\nIn this chapter, we will focus on the first two tasks, which are the most common ones. We start by demonstrating how to classify images with a fully-connected neural network. Then, we will show how to use convolutional neural networks (CNNs) to improve classification accuracy. Finally, we will show how to detect objects in images by using a pre-trained model. The problem if image classification is the problem of mapping an image to a class label. In this chapter, we will use the MNIST dataset, which contains images of handwritten digits. The goal is to classify each image into one of the 10 possible classes (0-9).\nWe will use jax library to implement the neural networks. jax is a library for numerical computing that is similar to numpy, but it is designed to work with accelerators like GPUs and TPUs. It also supports automatic differentiation, which is very useful for training neural networks. The two main computational tools required to train a neural network model are\njax provides both of these tools. It also provides a jit function that can be used to compile a function to run on accelerators. This can significantly speed up the computation. The matrix operation in jax is implemented in the dot function.\nImage data is represented as a matrix of pixel values. For example, a grayscale image of size 28x28 pixels can be represented as a matrix of shape (28, 28). A color image of size 28x28 pixels can be represented as a three-dimensional array of shape (28, 28, 3), where the last dimension represents the red, green, and blue channels. In this chapter, we will use grayscale images, so we will represent each image as a matrix.\nIn this chapter we will consider the MNIST dataset, which contains images of handwritten digits. Each image is a grayscale image of size 28x28 pixels. The original data set contains 60k training images and 10k test images. For simplicity we will use only test data set. Each image is labeled with the digit it represents. The goal is to classify each image into one of the 10 possible classes (0-9).\nThe dataset contains images and corresponding labels with the digit it represents. The goal is to classify each image into one of the 10 possible classes (0-9). The straight forward approach to build a classification model by using a fully connected neural network. The input to the model is a 28x28 matrix, which is reshaped into a vector of size 784. The output of the model is a vector of size 10, which represents the probability of each class. First we split the set of 10k images and use 80% for training and 20% for validation. We shuffle the data before splitting to avoid any bias in the train/test datasets. Let’s load and prepare the data\nThe model is trained by minimizing the cross-entropy loss and we use accuracy to evaluate the model.\nFinally, we implement the feed forward network with \\(\\tanh\\) activation. The last layer is linear layer with \\(z_i - \\ln \\left(\\sum z_i\\right)\\) function applied component-wise. Thus, our predict function simply returns the logarithm of the probability of each class. Typically we would use softmax function to compute the probability of each class. Given a vector of logits \\(z\\), the softmax function is defined as follows: \\[\n\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K}{e^{z_j}}}\n\\] However, the softmax function is numerically unstable when the input is large. If we take the logarithm of the softmax function, we get the following expression: \\[\n\\ln{\\sigma(z)_i} = z_i - \\ln\\sum_{j=1}^{K}{e^{z_j}}\n\\] This is exactly what we need to compute the cross-entropy loss.\nNow, we are ready to implement the SGD loop to train the model.\nUsing a fully connected neural network to classify images has some limitations. The main limitation is that it does not take into account the spatial structure of the image and treats each pixel independently. As a result, it does not perform well on real-life images and have a large number of parameters. Images have a spatial structure, and the spatial structure contains important information that can be used to improve classification accuracy. For example, it is not unusual to find that the same element can appear in different parts of the image. For example background with a solid color or a specific texture, such as a grass or a sky.\nApplying a filter to an image is a way to extract features from the image. A filter is a small matrix that is used to extract features from the image. The filter is applied to the image by sliding it over the image and computing the dot product at each position. The result is a new image that contains the features extracted by the filter. The filter is also called a kernel. The size of the filter is called the kernel size. The kernel size is usually an odd number, such as 3x3 or 5x5. The kernel size determines the receptive field of the filter. The receptive field is the region of the input image that is used to compute the output. The stride is the number of pixels by which the filter is shifted at each step. The stride determines the size of the output image. The process of convolving a filter with an image is called a convolution operation and it is similar to the approach used by kernel smoothing in statistics.\nLet’s look at a one-dimensional example. Suppose we have a one-dimensional input signal \\(x\\) and a one-dimensional filter \\(w\\). The convolution of \\(x\\) and \\(w\\) is defined as follows: \\[\n(x*w)(t) = \\sum_{i=0}^{h}x(t+i)w(i),\n\\] where \\(h\\) is the size of the filter. The convolution operation is used to filter the input signal.\nimport matplotlib.pyplot as plt\n\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\n\nkey = random.PRNGKey(1701)\n\nx = jnp.linspace(0, 10, 500)\ny = jnp.sin(x) + 0.2 * random.normal(key, shape=(500,))\n\nwindow = jnp.ones(10) / 10\ny_smooth = jnp.convolve(y, window, mode='same')\n\nplt.plot(x, y, 'lightgray')\n\n[&lt;matplotlib.lines.Line2D object at 0x30121cbf0&gt;]\n\nplt.plot(x, y_smooth, 'black');\nWe will demonstrate the concept using a simple example, where we have a 5x5 image and a 3x3 filter. We need to classify each image into one of the tree classes: cross, diagonal, right-diagonal\nThe filter is applied to the image by sliding it over the image and computing the dot product at each position. The result is a new image that contains the features extracted by the filter. The filter is also called a kernel. The size of the filter is called the kernel size. The kernel size is usually an odd number, such as 3x3 or 5x5. The kernel size determines the receptive field of the filter. The receptive field is the region of the input image that is used to compute the output. The stride is the number of pixels by which the filter is shifted at each step. The stride determines the size of the output image. The process of convolving a filter with an image is called a convolution operation.\nWe will use two filters.\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n[[0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]]\nThe first filter designed to detect a diagonal line from the top left to the bottom right. The second filter is designed to detect a diagonal line from the top right to the bottom left. We will apply the filters to the cross image and the diagonal image.\n[[3. 0. 1.]\n [0. 3. 0.]\n [1. 0. 3.]]\n[[1. 0. 3.]\n [0. 3. 0.]\n [3. 0. 1.]]\nWe see that on both outputs, we have pixels with high values. However, when we apply the same two filters to the diagonal image, we get different results.\nThe result of applying the second filter to the diagonal image is a close to zero matrix. This is because the filter is designed to detect a diagonal line from the top right to the bottom left, and the diagonal image does not contain such a line. We use this observation design the next layer of our networks, which is a pooing layer. The simplest one is the max-pool that simply calculates returns the pixel with the highest value in the receptive field. Finally, we concatinate the outputs of the pooling layer and apply a fully connected layer to classify the image. Let’s compare the predictions for three images.\nzrd, outrd1, outrd2 = cnn(imgrd,f1,f2)\nprint(f'Cross: {zx}')\n\nCross: [0.26163495 0.26163495 0.47673005]\n\nprint(f'Diagonal: {zd}')\n\nDiagonal: [0.5937724  0.08035836 0.32586923]\n\nprint(f'Right-Diagonal: {zrd}')\n\nRight-Diagonal: [0.08035836 0.5937724  0.32586923]\nThe model correclty predicted all three classes.\nNow, we return to the MNIST example and use highel level functions toi implement the convolutional neural network.\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nx = parse_images(\"../data/t10k-images-idx3-ubyte.gz\")\ny = parse_labels(\"../data/t10k-labels-idx1-ubyte.gz\")\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\ndef data_stream():\n    perm = rng.permutation(num_train)\n    while True:\n        for i in range(num_batches):\n            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n            yield torch.from_numpy(x[batch_idx,:,:]/ np.float32(255.))[:, None, :, :], torch.from_numpy(y[batch_idx])\n\ndef train(model, optimizer):\n    model.train()\n    batches = data_stream()\n    for epoch in range(num_epochs):\n        for _ in range(num_batches):\n            optimizer.zero_grad()\n            data, target = next(batches)\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            loss.backward()\n            optimizer.step()\n        print(f\"Epoch {epoch}: Loss {loss.item()}\")\n\nmodel = Net()\noptimizer = torch.optim.Adadelta(model.parameters(), lr=0.1)\ntrain(model, optimizer)\n\nEpoch 0: Loss 0.7333387136459351\nEpoch 1: Loss 0.4385078549385071\nEpoch 2: Loss 0.3360370099544525\nEpoch 3: Loss 0.2583213746547699\nEpoch 4: Loss 0.19924606382846832\nEpoch 5: Loss 0.15826614201068878\nEpoch 6: Loss 0.12477153539657593\nEpoch 7: Loss 0.10036399215459824\nEpoch 8: Loss 0.08255010843276978\nEpoch 9: Loss 0.06689854711294174",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "25-cnn.html#convolutions",
    "href": "25-cnn.html#convolutions",
    "title": "24  Convolutional Neural Networks",
    "section": "",
    "text": "img = np.genfromtxt(\"../data/img-x.tsv\", delimiter=\"\\t\")\nimgx = img[0:5,:]\nimgd = img[5:10,:]\nimgrd = img[10:15,:]\nplt.imshow(imgx, cmap='binary'); plt.title(f'Cross'); plt.show();\nplt.imshow(imgd, cmap='binary');  plt.title(f'Diagonal'); plt.show();\nplt.imshow(imgrd, cmap='binary'); plt.title(f'Right-Diagonal'); plt.show();\n\n\nf = np.genfromtxt(\"../data/img-filter.tsv\", delimiter=\"\\t\")\nf1 = f[0:3,:]\nf2 = f[3:6,:]\nplt.imshow(f1, cmap='binary'); plt.title(f'Filter 1'); plt.show();\nplt.imshow(f2, cmap='binary'); plt.title(f'Filter 2'); plt.show();\nprint(f1)\nprint(f2)\n\n\ndef conv(img, f):\n    out = np.zeros((3,3))\n    for i in range(3):\n        for j in range(3):\n            out[i,j] = np.sum(img[i:i+3,j:j+3]*f)\n    return out\ndef maxpool(img):\n    return np.max(img)\ndef fc(x, w, b):\n    return jnp.dot(w,x) + b\ndef softmax(x):\n    return jnp.exp(x) / jnp.sum(jnp.exp(x))\n\ndef cnn(img, f1, f2):\n    out1 = conv(img,f1)\n    out2 = conv(img,f2)\n    x = np.array([maxpool(out1), maxpool(out2)])\n    w = np.array([[1,0],[0,1],[0.6,0.6]])\n    b = 0\n    z = fc(x,w,b)\n    return softmax(z),out1,out2\n\nzx, outx1, outx2 = cnn(imgx,f1,f2)\nplt.imshow(outx1, cmap='binary'); plt.title(f'Output 1'); plt.show();\nplt.imshow(outx2, cmap='binary'); plt.title(f'Output 2'); plt.show();\nprint(outx1); print(outx2)\n\n\nzd, outd1, outd2 = cnn(imgd,f1,f2)\nplt.imshow(outd1, vmin=0,vmax=3,cmap='binary'); plt.title(f'Output 1');  plt.show();\nplt.imshow(outd2, vmin=0,vmax=3, cmap='binary'); plt.title(f'Output 2'); plt.show();",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "26-llm.html",
    "href": "26-llm.html",
    "title": "25  Large Language Models",
    "section": "",
    "text": "Example 25.1 (David Bowie’s Verbasizer) David Bowie was one of the first artists to embrace AI and machine learning in his creative process. In the mid-1990s, Bowie collaborated with Ty Roberts to create the “Verbasizer” - an algorithmic text generator that would help Bowie overcome writer’s block and generate new lyrical ideas. The Verbasizer worked by taking existing text and randomly recombining words and phrases to create new combinations that Bowie could use as starting points for his songwriting. Bowie describes how this process results in a “kaleidoscope of meanings,” with words and ideas colliding in surprising ways. In the 1995 documentary, Bowie would input existing lyric into the Verbasizer, and whows how he used the system while writing his 1995 album “Outside,” explaining that the Verbasizer would take phrases like “I am a blackstar” and “I am a deadstar” and randomly combine them to create new variations that sparked his creative process.\nThe system was essentially an early form of natural language processing and text generation, though much simpler than modern AI systems like ChatGPT. Bowie describes in the documentary how the Verbasizer helped him break out of creative ruts and discover unexpected word combinations that he might not have thought of otherwise. He explains that the randomness of the algorithm would often produce surprising results that would lead him in new creative directions.\nBowie’s use of the Verbasizer demonstrates how AI can serve as a creative collaborator rather than just a tool for automation. The system didn’t write complete songs for Bowie, but rather provided him with creative prompts and starting points that he could then develop into finished works. This collaborative approach to AI-assisted creativity has become increasingly common in modern music production, with artists using AI tools for everything from melody generation to lyric writing assistance.\nThe Verbasizer represents an early example of how machine learning and AI can enhance human creativity rather than replace it. Bowie’s willingness to experiment with algorithmic tools in his creative process was ahead of its time and helped pave the way for the current generation of AI-powered creative tools used by musicians, writers, and other artists today.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "ex.html",
    "href": "ex.html",
    "title": "27  Exercises",
    "section": "",
    "text": "27.1 Probability",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "ex.html#probability",
    "href": "ex.html#probability",
    "title": "27  Exercises",
    "section": "",
    "text": "Exercise 27.1 (Joint Distributions) A credit card company collects data on \\(10,000\\) users. The data contained two variables: an indicator of he customer status: whether they are in default (def =1) or if they are current with their payments (def =0). Moreover, they have a measure of their loan balance relative to income with three categories: a low balance (bal=1), medium (bal=2) and high (bal=3). The data are given in the following table:\n\n\n\n\ndef\n\n\n\nbal\n0\n1\n\n\n1\n8,940\n64\n\n\n2\n651\n136\n\n\n3\n76\n133\n\n\n\n\nCompute the marginal distribution of customer status\nWhat is the conditional distribution of bal given def =1\nMake a prediction for the status of a customer with a high balance\n\nSolution:\n\nThe marginal distribution of customer status is given by the sum of the rows of the table.\n\n\nd = data.frame(def0=c(8940,651,76),def1=c(64,136,133),row.names = c(\"bal1\",\"bal2\",\"bal3\"))\napply(d,2,sum)/10000\n\n def0  def1 \n0.967 0.033 \n\n\n\nThe conditional distribution of bal given def =1 is given by the ratio of the joint (elements of the table) to the marginal (sum of the def =1 column)\n\n\nd[,\"def1\"]/sum(d[,\"def1\"])\n\n 0.19 0.41 0.40\n\n\n\nThe prediction for the status of a customer with a high balance is the conditional distribution over def =1, given balance. \\[\nP(def\\mid bal) = \\frac{P(def \\text{ and } bal)}{P(bal)}\n\\]\n\n\nbalmarginal = apply(d,1,sum)\nd1b3 = d[\"bal3\",\"def1\"]/balmarginal[\"bal3\"]\nd1b1 = d[\"bal1\",\"def1\"]/balmarginal[\"bal1\"]\nprint(d1b3)\n\nbal3 \n0.64 \n\nprint(d1b1)\n\n  bal1 \n0.0071 \n\n# The ratio is \nd1b3/d1b1\n\nbal3 \n  90 \n\n\nPerson with high balance has 63% chance of being in default, while a person with low balance has 0.7% chance of being in default, 90-times less likely!\n\n\nExercise 27.2 (Marginal) The table below is taken from the Hoff text and shows the joint distribution of occupations taken from a 1983 study of social mobility by Logan (1983). Each cell is P(father’s occupation, son’s occupation).\n\nd = data.frame(farm = c(0.018,0.002,0.001,0.001,0.001),operatives=c(0.035,0.112,0.066,0.018,0.029),craftsman=c(0.031,0.064,0.094,0.019,0.032),sales=c(0.008,0.032,0.032,0.010,0.043),professional=c(0.018,0.069,0.084,0.051,0.130), row.names = c(\"farm\",\"operative\",\"craftsman\",\"sales\",\"professional\"))\nd %&gt;% knitr::kable()\n\n\n\n\n\nfarm\noperatives\ncraftsman\nsales\nprofessional\n\n\n\n\nfarm\n0.02\n0.04\n0.03\n0.01\n0.02\n\n\noperative\n0.00\n0.11\n0.06\n0.03\n0.07\n\n\ncraftsman\n0.00\n0.07\n0.09\n0.03\n0.08\n\n\nsales\n0.00\n0.02\n0.02\n0.01\n0.05\n\n\nprofessional\n0.00\n0.03\n0.03\n0.04\n0.13\n\n\n\n\n\n\nFind the marginal distribution of fathers’ occupations.\nFind the marginal distribution of sons’ occupations.\nFind the conditional distribution of the son’s occupation given that the father is a farmer.\nFind the conditional distribution of the father’s occupation given that the son is a farmer.\nComment on these results. What do they say about changes in farming in the population from which these data are drawn?\n\nSolution:\n\n\n\n\napply(d,1,sum)\n\n        farm    operative    craftsman        sales professional \n       0.110        0.279        0.277        0.099        0.235 \n\n\n\n\n\n\napply(d,2,sum)\n\n        farm   operatives    craftsman        sales professional \n       0.023        0.260        0.240        0.125        0.352 \n\n\n\n\n\n\nd[\"farm\",]/sum(d[\"farm\",])\n\n\n\n\n\n\nfarm\noperatives\ncraftsman\nsales\nprofessional\n\n\n\n\nfarm\n0.16\n0.32\n0.28\n0.07\n0.16\n\n\n\n\n\n\n\n\n\n\nd[,\"farm\"]/sum(d[,\"farm\"])\n\n 0.783 0.087 0.043 0.043 0.043\n\n\n\n\n\n\ndd = data.frame(son=as.double(d[\"farm\",]/sum(d[\"farm\",])),father=as.double(d[,\"farm\"]/sum(d[,\"farm\"])))\nbarplot(height = t(as.matrix(dd)),beside=TRUE,legend=TRUE)\n\n\n\n\n\n\n\n\nThe bar chart allows us to visualize the marginal distributions of occupations of fathers and sons. The striking feature of this chart is that in the sons, the proportion of farmers has greatly decreased and the proportion of professionals has increased. From Part d, we see that of the sons who are farmers, 78% had fathers who are farmers. On the other hand, Part c shows that only 16% of the fathers who farmed produced sons who farmed. This is higher than the 2.3% of sons in the general population who became farmers, showing that sons of farmers went into farming at a rate higher than the general population of the study. However, most of the sons of farmers went into another profession. There was a great deal of movement out of farming and very little movement into farming from the fathers’ to the sons’ generation.\nTo determine the validity of inference outside the sample, we would need to know more about how the study was designed. We do not know how the sample was selected or what steps were taken to ensure it was representative of the population being studied. We also are not given the sample size, so we don’t know the accuracy of our probability estimates. The paper from which the table was taken, cited in the problem, provides more detail on the study.\n\n\nExercise 27.3 (Conditional) Netflix surveyed the general population as to the number of hours per week that you used their service. The following table provides the proportions of each category according to whether you are a teenager or adults.\n\n\n\nHours\nTeenager\nAdult\n\n\n\n\n\\(&lt;4\\)\n0.18\n0.20\n\n\n\\(4\\) to \\(6\\)\n0.12\n0.32\n\n\n\\(&gt;6\\)\n0.04\n0.14\n\n\n\nCalculate the following probabilities:\n\nGiven that you spend \\(4\\) to \\(6\\) hours a week watching movies, what’s the probability that you are a teenager?\nWhat is the marginal distribution of hours spent watching movies.\nAre hours spent watching Netflix movies independent of age?\n\nSolution:\n\n\\(\\frac{0.12}{0.12+0.32}=0.2727\\)\nSee Table:\n\n\n\nHours\nProbability\n\n\n\n\n\\(&lt;4\\)\n0.18+0.20=0.38\n\n\n\\(4\\) to \\(6\\)\n00.12+0.32=0.44\n\n\n\\(&gt;6\\)\n0.04+0.14=0.18\n\n\n\nNo, the marginal distribution of hours given you are a teenager or adult are not the same:\n\n\n\n\n\n\n\n\n\nHours\nTeenager\nAdult\n\n\n\n\n\\(&lt;4\\)\n\\(\\frac{0.18}{0.18+0.12+0.04}=0.5294\\)\n\\(\\frac{0.20}{0.20+0.32+0.14}=0.3030\\)\n\n\n\\(4\\) to \\(6\\)\n\\(\\frac{0.12}{0.18+0.12+0.04}=0.3529\\)\n\\(\\frac{0.32}{0.20+0.32+0.14}=0.4848\\)\n\n\n\\(&gt;6\\)\n\\(\\frac{0.04}{0.18+0.12+0.04}=0.1176\\)\n\\(\\frac{0.14}{0.20+0.32+0.14}=0.2121\\)\n\n\n\n\n\nExercise 27.4 (Joint and Conditional) The following probability table relates \\(Y\\) the number of TV shows watched by the typical student in an evening to the number of drinks \\(X\\) consumed.\n\n\n\n\nY\n\n\n\n\n\n\n\nX\n0\n1\n2\n3\n\n\n0\n0.07\n0.09\n0.06\n0.01\n\n\n1\n0.07\n0.06\n0.07\n0.01\n\n\n2\n0.06\n0.07\n0.14\n0.03\n\n\n3\n0.02\n0.04\n0.16\n0.04\n\n\n\n\nWhat is the probability that a student has more than two drinks in an evening?\nWhat is the probability that a student drink more than the number of TV shows they watch?\nWhat’s the conditional distribution of the number of TV shows watched given they consume \\(3\\) drinks?\nWhat’s the expected number of drinks given they do not watch TV\nAre drinking and watching TV independent?\n\nSolution:\n\n\\(P(X &gt; 2 ) = 0.26\\).\n\\(P(X&gt;Y) = 0.42\\)\n\\(P( Y | X=4 )\\) are given by \\(( 1/13 , 2/13 , 8/13 , 2/13 )\\).\n\\(P( X | Y=0 )\\) are given by \\(( 7/22 , 7/22 , 1/11 , 3/11 )\\) and so \\(E(X| Y=0) = 25/22 = 1.13\\)\nNo. \\(P( X = 2 \\text{ and } Y=2 )= 0.14 \\neq 0.43 \\times 0.30 = P(X=2 ) P(Y=2)\\)\n\n\n\nExercise 27.5 (Conditional probability) Shipments from an online retailer take between 1 and 7 days to arrive, depending on where they ship from, when they were ordered, the size of the item, etc. Suppose the distribution of delivery times has the following distribution function:\n\n\n\nx\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n\\(\\mbox{P}(X = x)\\)\n\n\n\n\n\n\n\n\n\n\\(\\mbox{P}(X \\leq x)\\)\n0.10\n0.20\n0.70\n0.75\n0.80\n0.90\n1\n\n\n\n\nFill in the above probability table.\nWhat is the conditional probability of a delivery arriving on day four given that it did not arrive in the first three days? (Hint: find \\(P(X = 4 \\mid X &gt;= 4)\\))\n\nSolution:\n\\[P(X = 1) = P(X \\leq 1) = 0.1\\] \\[P(X = 2) = P(X \\leq 2) - P(X \\leq 1) = 0.2 - 0.1 = 0.1\\] \\[P(X = 3) = P(X \\leq 3) - P(X \\leq 2) = 0.7 - 0.2 = 0.5\\]\n\n\n\nx\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n\\(\\mbox{P}(X = x)\\)\n0.10\n0.10\n0.50\n0.05\n0.05\n0.10\n0.10\n\n\n\\(\\mbox{P}(X \\leq x)\\)\n0.10\n0.20\n0.70\n0.75\n0.80\n0.90\n1\n\n\n\n\\[\n\\begin{aligned}\nP(X &gt;= 4) &= 0.05 + 0.05 + 0.10 + 0.10 = 0.30\\\\\nP(X = 4 \\text{ and } X &gt;=4) &= P(X = 4) = 0.05\\\\\nP(X = 4 \\mid X &gt;= 4) &= \\frac{P(X = 4 \\text{ and } X &gt;=4)}{P(X &gt;= 4)} = \\frac{0.05}{0.30} = \\frac{1}{6}\\end{aligned}\n\\]\n\n\nExercise 27.6 (Joint and Conditional) A cable television company has \\(10000\\) subscribers in a suburban community. The company offers two premium channels, HBO and Showtime. Suppose \\(2750\\) subscribers receive HBO and \\(2050\\) receive Showtime and \\(6200\\) do not receive any premium channel.\n\nWhat is the probability that a randomly selected subscriber receives both HBO and Showtime.\nWhat is the probability that a randomly selected subscriber receives HBO but not Showtime.\n\nYou now obtain a new dataset, categorized by gender, on the proportions of people who watch HBO and Showtime given below\n\n\n\nCable\nFemale\nMale\n\n\n\n\nHBO\n0.14\n0.48\n\n\nShowtime\n0.17\n0.21\n\n\n\n\nConditional on being female, what’s the probability you receive HBO?\nConditional on being female, what’s the probability you receive Showtime?\n\nSolution:\n\nWe can calculate the number of people who receive both HBO and Showtime by calculating how many were “double counted” in the HBO and Showtime counts.\n\n\n10000 - 6200 - 2750 - 2050\n\n -1000\n\n\nThus, the answer is \\(1000/10000 = 0.1\\).\n\n\\(\\frac{2750-1000}{10000}=\\frac{1750}{10000}=0.175\\)\n\\(P(HBO|F)=\\frac{0.14}{0.14+0.17}=0.4516\\)\n\\(P(Show|F)=\\frac{0.17}{0.14+0.17}=0.5484\\)\n\n\n\nExercise 27.7 (Conditionals and Expectations) The following probability table describes the daily sales volume, \\(X\\), in thousands of dollars for a salesperson for the number of years \\(Y\\) of sales experience for a particular company.\n\n\n\n\nY\n\n\n\n\n\n\n\nX\n1\n2\n3\n4\n\n\n10\n0.14\n0.03\n0.03\n0\n\n\n20\n0.05\n0.10\n0.12\n0.07\n\n\n30\n0.10\n0.06\n0.25\n0.05\n\n\n\n\nVerify that this is a legal probability table.\nWhat is the probability of at least two years experience?\nCalculate the mean daily sales volume\nGiven a salesperson has three years experience, calculate the mean daily sales volume.\nA salesperson is paid $1000 per week plus 2% of total sales. What is the expected compensation for a salesperson?\n\nSolution:\n\nAll probabilities are between zero and one, and the entire table adds to one\n\\(P(Y\\geq 2)=0.19+0.40+0.12=0.71\\)\n\\(E(X) = 10 \\times 0.20 + 20 \\times 0.34 + 30 \\times 0.46 = 22.6\\), that is $22,600\nFirst, the conditional probability distribution \\(p( X|Y)\\) is given by, \\(0.03 /0.4 , 0.12/0.4 , 0.25/0.4\\). Hence \\(E(X|Y=3)=10\\times 0.075+20\\times 0.3+30\\times 0.625=25.5,\\) that is $25,500\nAssuming a 5-day workweek, \\(E(Comp)=1000+0.02E(X_{1}+X_{2}+X_{3}+X_{4}+X_{5})=1000+0.02\\cdot 5\\cdot E(X)=1000+0.02\\cdot 5\\cdot 22600=\\$3,260\\). With a 7-day week, \\(E(Comp)=1000+0.02\\cdot 7\\cdot 22600=\\$4,164\\)\n\n\n\nExercise 27.8 (Expectation) \\(E(X+Y) = E(X) + E(Y)\\) only if the random variables \\(X\\) and \\(Y\\) are independent\nSolution:\nFalse. By the plug-in rule, this relation holds irrespective of whether \\(X\\) and \\(Y\\) are independent.\n\n\nExercise 27.9 (Conditional Probability) A super market carried out a survey and found the following probabilities for people who buy generic products depending on whether they visit the store frequently or not\n\n\n\n\nPurchase Generic\n\n\n\n\n\n\nVisit\nOften\nSometime\nNever\n\n\nFrequent\n0.10\n0.50\n0.17\n\n\nInfrequent\n0.03\n0.05\n0.15\n\n\n\n\nWhat is the probability that a customer who never buys generics visits the store?\nWhat is the probability that a customer often purchases generic?\nAre buying generics and visiting the store independent decisions?\nWhat is the conditional distribution of purchasing generics given that you frequently visit the store?\n\nSolution:\n\n\\(P ( F \\text{ and } N ) + P ( \\bar{F} \\text{ and } N ) = 0.17 + 0.15 = 0.32\\)\n\\(P( F \\text{ and } O ) + P( \\bar{F} \\text{ and } O ) = 0.10 + 0.03 = 0.13\\)\n\\(P( F | S ) = 0.9091 , P( \\bar{F} | S ) = 0.0909\\) and \\(P( F | O ) = 0.7692 , P( \\bar{F} | O ) = 0.2308\\). The probabilities are not the same so the events are not independent.\n\\(P( O | F ) = 0.1299 , P( S| F) = 0.6494 , P( N | F ) = 0.2208\\).\n\n\n\nExercise 27.10 (Conditional Probability) Cooper Realty is a small real estate company located in Albany, New York, specializing primarily in residential listings. They have recently become interested in determining the likelihood of one of their listings being sold within a certain number of days. An analysis of recent company sales of 800 homes in produced the following table:\n\n\n\nDays Listed until Sold\nUnder 20\n31-90\nOver 90\nTotal\n\n\n\n\nUnder $50K\n50\n40\n10\n100\n\n\n$50-$100K\n20\n150\n80\n250\n\n\n$100-$150K\n20\n280\n100\n400\n\n\nOver $ 150K\n10\n30\n10\n50\n\n\n\n\nEstimate the probability that a home listed for over 90 days before being sold\nEstimate the probability that the initial asking price is under $50K.\nWhat the the probability of both of the above happening? Are these two events independent?\nAssuming that a contract has just been signed to list a home that has an initial asking price of less than $100K, what is the probability that the home will take Cooper Realty more than 90 days to sell?\n\nSolution: Let \\(A\\) be the event that it takes more than 90 days to sell. Let \\(B\\) denote the event that the initial asking price is under $50K.\n\n\\(P(A) = ( 10 + 80 + 100 + 10)/800 = 200/800 = 0.25\\)\n\\(P(B) = 100/800 = 0.125\\)\n\\(P( A \\text{ and } B ) = 10/800 = 0.0125\\)\nFirst, \\(P( &lt; \\$100K ) = 350/800 = 0.4375\\). Secondly, \\[\nP( A | &lt; \\$100K ) = P( &lt; \\$100K | A ) P(A)/ P( &lt; \\$100K) = (90/200)\\times(200/800)/0.4375 = 0.2571\n\\]\n\n\n\nExercise 27.11 (Probability and Combinations.) In 2006, the St. Louis Cardinals and the Detroit Tigers played for the World Series. The two teams play seven games, and the first team to win four games wins the world series.\nThe Cardinals were leading the series 3 – 1. Given that each game is independent of another and that the probability of the Cardinals winning any single game is 0.55, what’s the probability that they would go on to win the World Series?\nIn 2012, the St. Louis Cardinals found themselves in a similar situation against the San Francisco Giants in the National League Championships. Now suppose that the probability of the Cardinals winning any single game is 0.45.\nHow does the probability that they get to the World Series differ from before?\n\n\nExercise 27.12 (Probability and Lotteries) The Powerball lottery is open to participants across several states. When entering the powerball lottery, a participant selects five numbers from 1-59 and then selects a powerball number from the digits 1-35. In addition, there’s a $1 million payoff for anybody selecting the first five numbers correctly.\n\nShow that the odds of winning the Powerball Jackpot are 1 in 175,223,510.\nShow that the odds of winning the $1 million are 1 in 5,153,632.\n\nOn February 18, 2006 the Jackpot reached $365 million. Assuming that you will either win the Jackpot or the $1 million prize, what’s your expected value of winning?\nMega Millions is a similar lottery where you pick 5 balls out of 56 and a powerball from 46. Show that the odds of winning mega millions are higher than the Powerball lottery On March 30, 2012 the Jackpot reached $656 million. Is your expected value higher or lower than that calculated for the Powerball lottery?\n\n\nExercise 27.13 (Joint Probability) A market research survey finds that in a particular week \\(28\\%\\) of all adults watch a financial news television program; \\(17\\%\\) read a financial publication and \\(13\\%\\) do both.\n\nFill in the blanks in the following joint probability table\n\n\n\n\n\nWatches TV\nDoesn’t Watch\nTotal\n\n\n\n\nReads\n.13\n\n.17\n\n\nDoesn’t Read\n\n\n\n\n\n\n.28\n\n1.00\n\n\n\n\nWhat is the probability that someone who watches a financial TV program read a publication oriented towards finance?\nWhat is the probability that someone who reads a finance publication watches a financial TV program.\nWhy aren’t the answers to the above questions equal?\n\nSolution:\n\nAll probabilities come from the given probabilities and the fact that the sum of the column/row probabilities must add to 1.\n\n\n\n\n\nWatches TV\nDoesn’t Watch\nTotal\n\n\n\n\nReads\n.13\n0.4\n.17\n\n\nDoesn’t Read\n.15\n.68\n.83\n\n\n\n.28\n.72\n1.00\n\n\n\n\n\\[P(reads\\mid TV)=P(reads \\text{ and } TV)/P(TV)=.13/.28=.4643\\]\n\\[P(TV\\mid reads) = P(reads \\text{ and } TV)/P(reads)=.13/.17=.7647\\]\nThe reason for the difference is the denominators of the equations are different. This is an example of a base rate issue, it is more likely that someone who reads watches TV because fewer people read. This is not a condition of independence.\n\n\n\nExercise 27.14 (Conditional Probability.) A local bank is reviewing its credit card policy. In the past 5% of card holders have defaulted. The bank further found that the chance of missing one or more monthly payments is 0.20 for customers who do not default. Of course, the probability of missing one or more payments for those who default is 1.\n\nGiven that a customer has missed a monthly payment, compute the probability that the customer will default.\nThe bank would like to recall its card if the probability that a customer will default is greater than 0.20. Should the bank recall its card if the customer misses a monthly payment? Why or why not?\n\n\n\nExercise 27.15 (Correlation) The following table shows the descriptive statistics from \\(1000\\) days of returns on IBM and Exxon’s stock prices.\n            N   Mean    StDev    SE Mean\nIBM      1000   0.0009   0.0157  0.00049 \nExxon    1000   0.0018   0.0224  0.00071\nHere is the covariance table\n          IBM     Exxon\nIBM     0.000247\nExxon   0.000068  0.00050\n\nWhat is the variance of IBM returns?\nWhat is the correlation between IBM and Exxon’s returns?\nConsider a portfolio that invests \\(50\\)% in IBM and \\(50\\)% in Exxon. What are the mean and variance of the portfolio? Do you prefer this portfolio to just investing in IBM on its own?\n\nSolution:\n\n\\[\\sigma^2_{IBM}=0.000247\\]\n\\[\\rho=\\frac{Cov(IBM,Exxon)}{\\sigma_{IBM}\\sigma_{Exxon}}=\\frac{0.000068}{\\sqrt{0.000247}\\sqrt{0.00050}}=0.1935\\]\n\\[\\mu_p=w_{I}\\mu_{I}+w_{E}\\mu_{E}=(0.5)(0.0009)+(0.5)(0.0018)=0.00135\\] \\[\\sigma^2_p=w_{I}^2\\sigma_{I}^2+w_E^2\\sigma_{E}^2+2w_Iw_E Cov(I,E)\\] \\[=(0.5)^2(0.000247)+(0.5)^2(0.00050)+2(0.5)(0.5)(0.000068=0.000221\\] Yes, because the portfolio has a higher mean return but with a lower variance.\n\n\n\nExercise 27.16 (Normal Distribution) After Facebook’s earnings announcement we have the following distribution of returns. First, the stock beats earnings expectations \\(75\\)% of the time, and the other \\(25\\)% of the time earnings are in line or disappoint. Second, when the stock beats earnings, the probability distribution of percent changes is normal with a mean of \\(10\\)% with a standard deviation of \\(5\\)% and, when the stock misses earnings, a normal with a mean of \\(-5\\)% and a standard deviation of \\(8\\)%, respectively.\n\nAhead of the earnings announcement, what is the probability that Facebook stock will have a return greater than \\(5\\)%?\nDo you get the same answer for the probability that it drops at least \\(5\\)%?\nUse simulation to provide empirical answers with sample of size N = 10, 000, check and see how close you get to the theoretical answers you’ve found to the questions posed above. Provide histograms of the distributions you simulate.\n\nSolution:\nWe have the following information: \\[\nP(\\textrm{Beat Earnings}) = 0.75 ~ \\mathrm{ and} ~ P(\\textrm{Not Beat Earnings}) = 0.25\n\\] together with the following probabilities distributions \\[\nR_{Beat}\\sim N(0.10,0.05^2) ~ \\mathrm{ and} ~ R_{Not}\\sim N(-0.05,0.08^2)\n\\] We want to find the probability that Facebook stock will have a return greater than 5%: \\[\n\\begin{aligned}\nP(\\textrm{Beat}) & \\times P(R_{Beat}&gt;0.05) + P(\\textrm{Not}) \\times P(R_{Not}&gt;0.05)\\\\\n&=0.75 \\times (1-F_{Beat}(0.05)) + 0.25 \\times (1-F_{Not}(0.05))\n\\end{aligned}\n\\]\n\n0.75*(1-pnorm(0.05,0.1,0.05)) + 0.25*(1-pnorm(0.05,-0.05,0.08)) #0.657\n\n 0.66\n\n\nTherefore, the probability that Facebook stock beats a 5% return is 65.7%.\nSimilarly, for dropping at least 5%, we want: \\[\n\\begin{aligned}\nP(\\textrm{Beat})\\times P(R_{Beat}&lt;-0.05) + P(\\textrm{Not}) \\times P(R_{Not}&lt;-0.05)\\\\\n=0.75 \\times (F_{Beat}(-0.05)) + 0.25 \\times (F_{Not}(-0.05))\n\\end{aligned}\n\\] We can compute this is in R by the following commands:\n\n0.75*pnorm(-0.05,0.1,0.05) + 0.25*pnorm(-0.05,-0.05,0.08) #0.1260124\n\n 0.13\n\n\nTherefore, the probability that Facebook stock drops at least 5% is 12.6%.\n\n\n\nExercise 27.17 (Probability) Answer the following statements TRUE or FALSE, providing a succinct explanation of your reasoning.\n\nIf the odds in favor of \\(A\\) are 3:5 then \\(\\mbox{P}(A) = 0.4\\).\nYou roll two fair three-sided dice. The probability the two dice show the same number is 1/4.\nIf events \\(A\\) and \\(B\\) are independent and \\(\\mbox{P}(A) &gt; 0\\) and \\(\\mbox{P}(B)&gt;0\\), then \\(\\mbox{P}(A \\mbox{ and } B) &gt; 0\\).\nIf \\(\\mbox{P}(A \\; \\text{ and} \\;  B) \\geq 0.5\\) then \\(P(A) \\leq 0.5\\).\nIf two random variables have non-zero correlation, then they must be dependent.\nIf two random variables have zero correlation, then they must be independent.\nIf two random variables are independent, then the correlation between them must be zero.\nIf \\(P(A \\text{ and } B) \\leq 0.2\\), then \\(P(A) \\leq 0.2\\).\n\nSolution:\n\nIf the odds in favor of \\(A\\) are 3:5 then \\(\\mbox{P}(A) = 0.4\\). FALSE \\(P(A) = \\frac{3}{3+5} = 0.375\\)\nYou roll two fair three-sided dice. The probability the two dice show the same number is 1/4. FALSE \\(P = \\frac{1}{3}\\times\\frac{1}{3}\\times 3 = \\frac{1}{3}\\).\nIf events \\(A\\) and \\(B\\) are independent and \\(\\mbox{P}(A) &gt; 0\\) and \\(\\mbox{P}(B)&gt;0\\), then \\(\\mbox{P}(A \\mbox{ and } B) &gt; 0\\). TRUE \\(\\mbox{P}(A \\mbox{ and } B) = \\mbox{P}(A) \\times \\mbox{P}(B)&gt;0\\).\nIf \\(\\mbox{P}(A \\; \\text{ and} \\;  B) \\geq 0.5\\) then \\(P(A) \\leq 0.5\\). FALSE Suppose A B are independent and \\(P(A) = 1\\), \\(P(B) = 0.5\\). \\(\\mbox{P}(A \\; \\text{ and} \\;  B) \\geq 0.5\\)\nIf two random variables have non-zero correlation, then they must be dependent. TRUE If two random variables are independent, they always have 0 correlation. Therefore if correlation is not 0, they cannot be independent.\nIf two random variables have zero correlation, then they must be independent. FALSE Suppose \\(X\\) takes value in \\(\\{-1, 0, 1\\}\\) with probability \\(\\frac{1}{3}\\) for each. \\(Y = X^2\\), we have\n\n\\[\nX = \\begin{cases}\n-1 & \\text{with prob } 1/3\\\\\n0  & \\text{with prob } 1/3\\\\\n1 & \\text{with prob } 1/3\n\\end{cases},\nY = \\begin{cases}\n1 & \\text{with prob } 2/3\\\\\n0  & \\text{with prob } 1/3\\\n\\end{cases}\n\\]\nIt’s easy to find that \\(\\text{Cov}(X, Y) = 0\\). But \\(Y\\) is a function of \\(X\\), so they are not independent.\n\nIf two random variables are independent, then the correlation between them must be zero. TRUE Independence implies uncorrelation, thus correlation is 0.\nFalse, We only know \\(P(A \\text{ and } B) \\leq P(A)\\)\n\n\n\nExercise 27.18 (Binomial Distribution) The Downhill Manufacturing company produces snowboards. The average life of their product is \\(10\\) years. A snowboard is considered defective if its life is less than \\(5\\) years. The distribution is approximately normal with a standard deviation for the life of a board of \\(3\\) years.\n\nWhat’s the probability of a snowboard being defective?\nIn a shipment of \\(120\\) snowboards, what is the probability that the number of defective boards is greater than \\(10\\)?\nUse simulation to provide empirical answers with sample of size N = 10, 000, check and see how close you get to the theoretical answers you’ve found to the questions posed above. Provide histograms of the distributions you simulate.\n\nYou can use R and simulation with rbinom, rnorm as an alternative\nSolution:\nHere we are given the following information: \\(L\\sim N(10,3^2)\\). We want to find the probability of a snowboard being defective: \\[\nP(L&lt;5)=F_L(5).\n\\] This is simply the cumulative distribution function. We can find the solution by using the following R commands:\n\npnorm(5,mean=10,sd=3) #0.04779035\n\n 0.048\n\n\nThus, the probability a snowboard is considered defective is 4.78%. Out of a shipment of 120 snowboards, the distribution of the number of defective boards is a Binomial Distribution parameterized as follows: \\[\nN_{def}\\sim Bin(120,0.0478).\n\\] We want to find the probability: \\[\nP(N_{def}&gt;10)\n\\]\n\npbinom(10,120,0.0478,lower.tail = FALSE) #0.02914742\n\n 0.029\n\n\n\n\nExercise 27.19 (Chinese Stock Market) On August 24th, 2015, Chinese equities ended down \\(- 8.5\\)% (Black Monday). In the last \\(25\\) years, average is \\(0.09\\)% with a volatility of \\(2.6\\)%, and \\(56\\)% time close within one standard deviation. SP500, average is \\(0.03\\)% with a volatility of \\(1.1\\)%. \\(74\\)% time close within one standard deviation\n\n\n\nEconomist article, August 2015.\n\n\n\n\nExercise 27.20 (Body Weight) Suppose that your model for weight \\(X\\): Normal distribution with mean \\(190\\) lbs and variance \\(100\\) lbs. The problem is to identify the proportion of people have weights over 200 lbs?\nSolution:\nWe are given that \\(\\mu = 190\\) and \\(\\sigma^2 = 100\\) so \\(\\sigma = 10\\). Our probability model is \\(X \\sim N ( 190 , 100 )\\).\n\\(X\\), to get \\[Z = \\frac{ X - \\mu}{ \\sigma } = \\frac{ X - 190 }{ 10}\\]\nNow we compute the desired probability \\[\n\\begin{aligned}\np(X&gt;200) & = P \\left ( \\frac{ X - 190 }{ 10 } &gt; \\frac{ 200 - 190 }{10 } \\right ) \\\\\n& = P \\left ( Z &gt; 1 \\right ).\n\\end{aligned}\n\\] Now use the cdf function (\\(F_Z\\)) to get \\[P ( Z &gt; 1 ) = 1 - F_Z ( 1 ) = 1 - 0.8413 = 0.1587\\]\n\n\nExercise 27.21 (Google Returns) We estimated sample mean and sample variance for daily returns of Google stock \\(\\bar x = 0.025\\), and \\(s^2 = 1.1\\). If I want to calculate the probability that I lose \\(3\\)% in a day, I need to assume a probabilistic model of the return and then calculate the \\(p(r &gt;3)\\). Say, we assume that returns are normally distributed \\(r \\sim N( \\mu , \\sigma^2 )\\). Estimate parameters of the distribution from the observed data and calculate \\(p(r&lt;-3) = 0.003\\)\n\n\nExercise 27.22 (Portfolio Means, Standard Deviations and Correlation) Suppose you have a portfolio that is invested with a weight of 75% in the U.S. and 25% in HK. You take a sample of 10 years, or 120 months of historical means, standard deviations and correlations for U.S. and Hong Kong stock market returns. Given this information compute the mean and standard deviation of the returns on your portfolio.\n\n\n\n\nN\nMEAN S\nTDEV\n\n\n\n\n\n\nHong Kong\n120\n0.0170\n0.0751\n\n\n\n\nUS\n120\n0.0115\n0.0330\n\n\n\n\n\nCorrelation = 0.3\nHint: you will find the following formulas useful. Let \\(R_p\\) denote the return on your portfolio which is a weighted combination \\(R_p = pX + (1 - p)Y\\) . Then \\[\nE(R_p) = p\\mu_X + (1 - p)\\mu_Y\n\\] \\[\nVar(R_p) = p^2\\sigma_X^2 + (1 - p)^2\\sigma_Y^2 + 2p(1-p)\\rho \\sigma_X \\sigma_Y\n\\] where \\(\\mu_X\\), \\(\\mu_Y\\) and \\(\\sigma_X\\), \\(\\sigma_Y\\) are the underlying means and standard deviations for \\(X\\) and \\(Y\\).\n\n\nExercise 27.23 (Binomial) In the game Chuck-a-Luck you pick a number from 1 to 6. You roll three dice. If your number doesn’t appear on any dice, you lose $1. If your number appears exactly once, you win $1. If your number appears on exactly two dice, you win $2. If your number appears on all three dice, you win $3.\nHence every outcome has how much you win or lose on the game, namely \\(-1, 1, 2\\) or \\(3\\).\n\nFill in the blanks in the pdf and cdf values\n\n\n\n\nX\n-1\n1\n2\n3\n\n\nP(X)\n\n\n\n\n\n\nF(X)\n\n\n\n\n\n\n\nExplain your reasoning carefully.\n\nCompute the expected value of the game, \\(E(X)\\).\n\nSolution:\n\nPDF: This is a binomial experiment with n=3 and p=1/6. Plugging in for \\(x=0\\): \\[P(x=0)=(5/6)^3=.5787\\] \\[P(x=1)=3 (1/6)^1 \\times (5/6)^2=.3472\\] \\[P(x=2)=3 (1/6)^2 \\times (5/6)^1=.0694\\] \\[P(x=3)=(1/6)^3=.0046\\]\nCDF: The values for this are the probabilities that X is less than or equal to a given value: \\[F(x=0)=P(x \\leq 0)=.5787\\] \\[F(x=1)=P(x \\leq 1)=P(x=0)+P(x=1)=.9259\\] \\[F(x=2)=.9954\\] \\[F(x=3)=1\\]\n\nThe expected value is \\[\n\\sum_x x P_X ( x) = -1 \\times 0.5787+1 \\times 0.3472+2 \\times 0.0694+3 \\times 0.0046=-\\$.08.\n\\] You expect to lose 8 cents per game.\n\n\nExercise 27.24 (Binomial Distribution) A real estate firm in Florida offers a free trip to Florida for potential customers. Experience has shown that of the people who accept the free trip, 5% decide to buy a property. If the firm brings \\(1000\\) people, what is the probability that at least \\(125\\) will decide to buy a property?\nSolution:\nIn order to find the probability that at least \\(125\\) decide to buy, the binomial distribution would require calculating the probabilities for 125-1000. Instead, we use the normal approximation for the binomial. \\[\n\\mu = np =50.\n\\] \\[\n\\sigma = \\sqrt{np(1-p)}=\\sqrt{1000\\times .05\\times .95}=\\sqrt{47.5}=6.89\n\\]\nCalculating the Z-score for 125: \\(Z=\\frac{125-50}{6.89}=10.9.\\) and \\(P(Z \\geq 10.9)=0\\).\n\n\nExercise 27.25 (Expectation and Strategy) An oil company wants to drill in a new location. A preliminary geological study suggests that there is a \\(20\\)% chance of finding a small amount of oil, a \\(50\\)% chance of a moderate amount and a \\(30\\)% chance of a large amount of oil. The company has a choice of either a standard drill that simply burrows deep into the earth or a more sophisticated drill that is capable of horizontal drilling and can therefore extract more but is far more expensive. The following table provides the payoff table in millions of dollars under different states of the world and drilling conditions\n\n\n\nOil\nsmall\nmoderate\nlarge\n\n\n\n\nStandard Drilling\n20\n30\n40\n\n\nHorizontal Drilling\n-20\n40\n80\n\n\n\nFind the following\n\nThe mean and variance of the payoffs for the two different strategies\nThe strategy that maximizes their expected payoff\nBriefly discuss how the variance of the payoffs would affect your decision if you were risk averse\nHow much are you willing to pay for a geological evaluation that would tell you with certainty the quantity of oil at the site prior to drilling?\n\nSolution:\n\nUsing the plug-in rule, we get \\[\n\\begin{aligned}\nE(Standard) & = & 31\\\\\nE(Horizontal) & = & 40\\\\\nVar(Standard) & = & E(X^{2})-\\left[E(X)\\right]^{2}\\\\\n& = & 1010-961\\\\\n& = & 49\\\\\nVar(Horizontal) & = & E(X^{2})-\\left[E(X)\\right]^{2}\\\\\n& = & 2800-1600\\\\\n& = & 1200\n\\end{aligned}\n\\]\n\n\nS = c(20,30,40)\nP = c(0.2,0.5,0.3)\nE_standard = sum(S*P)\nH = c(-20,40,80)\nE_horizontal = sum(H*P)\nE_standard\n\n 31\n\nE_horizontal\n\n 40\n\nsum(S^2*P) - E_standard^2\n\n 49\n\nsum(H^2*P) - E_horizontal^2\n\n 1200\n\n\n\nGiven our analysis, horizontal drilling maximizes the expected payoff.\nThe strategy with higher expected payoff has a substantially higher variance. A risk averse person will settle for a strategy with lower expected payoff and lower variance, i.e. standard drilling, while a risk seeking person will choose horizontal drilling.\nIf the geological evaluation tells us with certainty the type of oil, then we will be able to chose the strategy which maximizes our payoff under different types of oil. Thus, the amount one should be willing to pay is the expected payoff under this revised schedule of payoff minus the maximum payoff under the current schedule. Given this \\[\nWTP=0.2\\times20+0.5\\times40+0.3\\times80-40=48-40=8\n\\]\n\nTherefore, once should be willing to pay \\(\\$8\\) million for this information.\n\n\nExercise 27.26 (Google Survey) Visitors to your website are asked to answer a single survey Google website question before they get access to the content on the page. Among all of the users, there are two categories\n\nRandom Clicker (RC)\nTruthful Clicker (TC)\n\nThere are two possible answers to the survey: yes and no.\nRandom clickers would click either one with equal probability. You are also giving the information that the expected fraction of random clickers is \\(0.3\\).\nAfter a trial period, you get the following survey results. \\(65\\)% said Yes and \\(35\\)% said No.\nHow many people people who are truthful clickers answered yes?\nSolution:\nGiven the information that the expected fraction of random clickers is 0.3, \\[P(RC) = 0.3 \\mbox{ and } P(TC) = 0.7\\] Conditioning on a random clickers, he would click either one with equal probability. \\[\nP(Yes \\mid RC) = P(No \\mid RC) = 0.5\n\\] By the survey results, we know the proportion response of “Yes\" and”No\". \\[\nP(Yes) = 0.65 \\mbox{ and } P(No) = 0.35\n\\] Therefore, the probability that a truthful clicker answer “Yes\" is, \\[\n\\begin{aligned}         \nP(Yes \\mid TC) &=& \\frac{P(Yes)-P(Yes \\mid RC)*P(RC)}{P(TC)} \\\\         \n&=& \\frac{0.65 - 0.5*0.3}{0.7} = 0.71     \n\\end{aligned}\n\\] Notice that, we use law of total probability in the first equation.\n\n\n27.1.1 Computing\n\nExercise 27.27 (Portfolio Means, Standard Deviations and Correlation) You want to build a portfolio of exchange traded funds (ETFs) for your retirement strategy. You’re thinking of whether to invest in growth or value stocks, or maybe a combination of both. Vanguard has two ETFs, one for growth (VUG) and one for value (VTV).\n\nPlot the historical price series for VUG vs VTV.\nCalculate the means and standard deviations of both ETFs.\nCalculate their covariance.\nSuppose you decide on a portfolio that is a 50 / 50 split. Calculate the new mean and variance of your portfolio.\nWhich portfolio best suits you?\nWhat’s the probability that growth (VUG) will beat value (VTV) in the future?\n\nYou will find the following formulas useful. Let \\(P\\) denote the return on your portfolio which is a weighted combination \\(P = aX + bY\\). Then \\[\nE(P) = aE(X) + bE(Y )\n\\] \\[\nVar(P ) = a^2Var(X) + b^2Var(Y ) + 2abCov(X, Y ),\n\\] where \\(Cov(X, Y )\\) is the covariance for \\(X\\) and \\(Y\\).\nHint: You can use the following code to get the data\n\nlibrary(quantmod)\ngetSymbols(c(\"VUG\",\"VTV\"), from = \"2015-01-01\", to = \"2024-01-01\")\nVUG = VUG$VUG.Adjusted\nVTV = VTV$VTV.Adjusted\n\nSolution:\n\nlibrary(quantmod)\ngetSymbols(c(\"VUG\",\"VTV\"), from = \"2015-01-01\", to = \"2024-01-01\");\n\n \"VUG\" \"VTV\"\n\nVUG = VUG$VUG.Adjusted\nVTV = VTV$VTV.Adjusted\nplot(VUG, type = \"l\", col = \"red\", main = \"VUG\")\n\n\n\n\n\n\n\nplot(VTV, type = \"l\", col = \"red\", main = \"VTV\")\n\n\n\n\n\n\n\nVUG = as.numeric(VUG)\nVTV = as.numeric(VTV)\nn=length(VUG)\nVUG = VUG[2:n]/VUG[1:(n-1)]-1\nVTV = VTV[2:n]/VTV[1:(n-1)]-1\nmean(VUG)\n\n 0.00061\n\nmean(VTV)\n\n 0.00042\n\nsd(VUG)\n\n 0.013\n\nsd(VTV)\n\n 0.011\n\ncov(VUG, VTV)\n\n 0.00012\n\n# portfolio mean and variance\nportfolio_mean_1 = 0.5 * mean(VUG) + 0.5 * mean(VTV)\nportfolio_var_1 = 0.5^2 * var(VUG) + 0.5^2 * var(VTV) + 2 * 0.5 * 0.5 * cov(VUG, VTV)\nportfolio_sd_1 = sqrt(portfolio_var_1)\n\nYou may say VUG or VTV best suits you, as long as you give an explanation such as you are risk aversion or not. Now we consider mean and variance of VUG - VTV,\n\nmean_1 = mean(VUG) - mean(VTV)\nvar_1 = var(VUG) + var(VTV) + 2 * 1 * (-1) * cov(VUG, VTV)\nsd_1 = sqrt(var_1)\n\nTherefore the probability that VUG - VTV \\(&lt;0\\) is\n\npnorm(0, mean_1, sd = sd_1)\n\n 0.49\n\n\nHowever, we care about the probability that VUG beats VTV, that is $P(VUG &gt; VTV) = P(VUG - VTV &gt; 0)\n\npnorm(0, mean_1, sd = sd_1, lower.tail = FALSE)\n\n 0.51\n\n\n\n\nExercise 27.28 (Descriptive Statistics in R) Use the superbowl1.txt and derby2016.csv datasets. The Superbowl contains data on the outcome of all previous Superbowls. The outcome is defined as the difference in scores of the favorite minus the underdog. The spread is the bookmakers’ prediction of the outcome before the game begins. The Derby data consists of all of the results on the Kentucky Derby which is run on the first Saturday in May every year at Churchill Downs racetrack. Answer the following questions\nFor the Superbowl data.\n\nPlot the spread and outcome variables. Calculate means, standard deviations, covariances, correlations.\nWhat is the mean and the standard deviation of the winning margin (outcome)?\nUse a boxplot to compare the favorites’ score versus the underdog.\nDoes this data look normally distributed?\n\nFor the Derby data.\n\nPlot a histogram of the winning speeds and times of the horses. Why is there a long right-hand tail to the distribution of times?\nCan you identify the outlying horse with the best winning time?\n\nSolution:\n\n# import superbowl data\nsuperbowl&lt;- read.csv(\"../data/superbowl1.txt\",header=T)\n\n# look at data\nhead(superbowl)\n\n\n\n\n\nFavorite\nUnderdog\nSpread\nOutcome\nUpset\n\n\n\n\nGreenBay\nKansasCity\n14\n25\n0\n\n\nGreenBay\nOakland\n14\n19\n0\n\n\nBaltimore\nNYJets\n18\n-9\n1\n\n\nMinnesota\nKansasCity\n12\n-16\n1\n\n\nDallas\nBaltimore\n2\n-3\n1\n\n\nDallas\nMiami\n6\n21\n0\n\n\n\n\n\nsummary(superbowl)\n\n   Favorite           Underdog             Spread         Outcome      \n Length:48          Length:48          Min.   : 1.00   Min.   :-35.00  \n Class :character   Class :character   1st Qu.: 3.00   1st Qu.: -3.00  \n Mode  :character   Mode  :character   Median : 6.25   Median :  7.00  \n                                       Mean   : 7.15   Mean   :  6.24  \n                                       3rd Qu.:10.12   3rd Qu.: 17.00  \n                                       Max.   :19.00   Max.   : 45.00  \n     Upset      \n Min.   :0.000  \n 1st Qu.:0.000  \n Median :0.000  \n Mean   :0.333  \n 3rd Qu.:1.000  \n Max.   :1.000  \n\n# attach so R recognizes  each variable\nattach(superbowl)\n\n######################\n# part 1\n######################\n# plot Spread vs Outcome\nplot(Spread,Outcome,main=\"Spread v.s. Outcome\")\n# add a 45 degree line to compare\nabline(1,1)\n\n\n\n\n\n\n\n# Covariance, Correlation, alpha, beta\nx &lt;- Spread\ny &lt;- Outcome\n\nmean(x)\n\n 7.1\n\nmean(y)\n\n 6.2\n\nsd(x)\n\n 4.6\n\nsd(y)\n\n 17\n\ncov(x,y)\n\n 24\n\ncor(x,y)\n\n 0.3\n\nbeta &lt;- cor(x,y)*sd(y)/sd(x)\nalpha &lt;- mean(y)-beta*mean(x)\nbeta\n\n 1.1\n\nalpha\n\n -1.8\n\n# Regression check\nmodel &lt;- lm(y~x)\ncoef(model)\n\n(Intercept)           x \n       -1.8         1.1 \n\n####################\n# part 2\n####################\n# Compare boxplot \nboxplot(Spread,Outcome,horizontal=T,names=c(\"spread\",\"outcome\"),\n  col=c(\"red\",\"yellow\"),main=\"Superbowl\")\n\n\n\n\n\n\n\n#####################\n# part 3\n#####################\n# see the distribution of outcome and spread through histograms\nhist(Outcome,freq=FALSE)\n# add a normal distribution line to compare\nlines(seq(-50,50,0.01),dnorm(seq(-50,50,0.01),mean(Outcome),sd(Outcome)))\n\n\n\n\n\n\n\nhist(Spread,freq=FALSE)\nlines(seq(-10,30,0.01),dnorm(seq(-10,30,0.01),mean(Spread),sd(Spread)))\n\n\n\n\n\n\n\n######################\n## Kentucky Derby\n######################\n\nmydata2 &lt;- read.csv(\"../data/Kentucky_Derby_2014.csv\",header=T)\n\n# attach the dataset \nattach(mydata2)\n\nhead(mydata2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nyear_num\ndate\nwinner\nmins\nsecs\ntimeinsec\ndistance\nspeedmph\n\n\n\n\n1875\n1\nMay 17, 1875\nAristides\n2\n38\n158\n1.5\n34\n\n\n1876\n2\nMay 15, 1876\nVagrant\n2\n38\n158\n1.5\n34\n\n\n1877\n3\nMay 22, 1877\nBaden-Baden\n2\n38\n158\n1.5\n34\n\n\n1878\n4\nMay 21, 1878\nDay Star\n2\n37\n157\n1.5\n34\n\n\n1879\n5\nMay 20, 1879\nLord Murphy\n2\n37\n157\n1.5\n34\n\n\n1880\n6\nMay 18, 1880\nFonso\n2\n38\n158\n1.5\n34\n\n\n\n\n\nsummary(mydata2)\n\n      year         year_num         date              winner         \n Min.   :1875   Min.   :  1.0   Length:140         Length:140        \n 1st Qu.:1910   1st Qu.: 35.8   Class :character   Class :character  \n Median :1944   Median : 70.5   Mode  :character   Mode  :character  \n Mean   :1944   Mean   : 70.5                                        \n 3rd Qu.:1979   3rd Qu.:105.2                                        \n Max.   :2014   Max.   :140.0                                        \n      mins           secs         timeinsec      distance       speedmph   \n Min.   :1.00   Min.   : 0.00   Min.   :119   Min.   :1.25   Min.   :31.3  \n 1st Qu.:2.00   1st Qu.: 2.20   1st Qu.:122   1st Qu.:1.25   1st Qu.:35.0  \n Median :2.00   Median : 4.13   Median :124   Median :1.25   Median :36.3  \n Mean   :1.99   Mean   :10.42   Mean   :130   Mean   :1.29   Mean   :35.9  \n 3rd Qu.:2.00   3rd Qu.: 9.00   3rd Qu.:129   3rd Qu.:1.25   3rd Qu.:36.8  \n Max.   :2.00   Max.   :59.97   Max.   :172   Max.   :1.50   Max.   :37.7  \n\n##################\n# part 1\n##################\n# plot a histogram of speedmph\nhist(speedmph,col=\"blue\")\n\n\n\n\n\n\n\n# finer bins \nhist(speedmph,breaks=10,col=\"red\")\n\n\n\n\n\n\n\nhist(timeinsec,breaks=10,col=\"purple\")\n\n\n\n\n\n\n\n####################\n# part 2\n###################\n# to find the left tail observation\nk1 &lt;- which(speedmph == min(speedmph))\nmydata2[k1,]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nyear_num\ndate\nwinner\nmins\nsecs\ntimeinsec\ndistance\nspeedmph\n\n\n\n\n17\n1891\n17\nMay 13, 1891\nKingman\n2\n52\n172\n1.5\n31\n\n\n\n\n\n# to find the best horse\nk2 &lt;- which(speedmph == max(speedmph))\nmydata2[k2,] \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nyear_num\ndate\nwinner\nmins\nsecs\ntimeinsec\ndistance\nspeedmph\n\n\n\n\n99\n1973\n99\n5-May-73\nSecretariat\n1\n59\n119\n1.2\n38\n\n\n\n\n\n\n\n\nExercise 27.29 (Berkshire Hathaway: Yahoo Finance Data) Download daily return data in Warren Buffett’s firm Berkshire Hathaway (ticker symbol: BRK-A) from 1990 to the present. Analyze this data in the following way:\n\nPlot the Historical Price Performance of the stock.\nCalculate the Daily returns. Plot a histogram of the returns. Comment on the distribution that you obtain.\nUse the summary command to provide statistical data summaries.\nInterpret your findings.\n\nSolution:\nlibrary(quantmod)\ngetSymbols(\"BRK-A\", from = \"1990-01-01\", to = \"2024-01-01\")\nBRKA = get('BRK-A')\nBRKA = BRKA[,4]\nhead(BRKA)\nplot(BRKA,type=\"l\",col=20,main=\"BRKA Share Price\",ylab=\"Price\",xlab=\"Time\",bty='n')\n# calculate the simple return\nN &lt;- length(BRKA) \ny = as.vector(BRKA)\nret &lt;- y[-1]/y[-N]-1\n\n# create summaries of ret for BRK-A\nsummary(ret)\nsd(ret)\n# histogram of returns\nhist(ret,breaks=50,main=\"BRK-A daily returns\")\n# plots to show serial correlation in 1st and 2nd moments\n# to save the plot,click \"Export\" ans \"save as image\"\n\nacf(ret,lag.max=10,main=\"serial correlation in 1st moment\")\nacf(ret^2,lag.max=10,main=\"serial correlation in 2nd moment\")\n\n\n\n \"BRK-A\"\n\n\n           BRK-A.Close\n1990-01-02        8625\n1990-01-03        8625\n1990-01-04        8675\n1990-01-05        8625\n1990-01-08        8625\n1990-01-09        8555\n\n\n\n\n\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.120879 -0.005857  0.000000  0.000583  0.006473  0.161290 \n\n\n\n\n 0.014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 27.30 (Confidence Intervals) A sample of weights of 40 rainbow trout revealed that the sample mean is 402.7 grams and the sample standard deviation is 8.8 grams.\n\nWhat is the estimated mean weight of the population?\nWhat is the 99% confidence interval for the mean?\n\n\n\nExercise 27.31 (Confidence Intervals) A research firm conducted a survey to determine the mean amount steady smokers spend on cigarettes during a week.\nA sample of 49 steady smokers revealed that \\(\\bar{X} = 20\\) and \\(s = 5\\) dollars.\n\nWhat is the point estimate? Explain what it indicates.\nUsing a 95% confidence interval, determine the confidence interval for \\(\\mu\\). Explain what it indicates.\n\n\n\nExercise 27.32 (Back cast US Presidential Elections) Use data from presidential polls to predict the winner of the elections. We will be using data from http://www.electoral-vote.com/. The goal is to use simulations to predict the winning percentage for each of the candidates. Use election.Rmd script as the starter.\nReport prediction as a 50% confidence interval for each of the candidates.\n\n\nExercise 27.33 (Russian Parliament Election Fraud (5 pts)) On September 28, 2016 United Russia party won a super majority of seats, which will allow them to change the Constitution without any votes of other parties. Throughout the day there were reports of voting fraud including video purporting to show officials stuffing ballot boxes. Additionally, results in many regions demonstrate that United Russia on many poll stations got anomalously closed results, for example, 62.2% in more than hundred poll stations in Saratov Region.\nUsing assumption that United Russia’s range in Saratov was [57.5%, 67.5%] and results for each poll station are rounded to one decimal point (when measure in percent), calculate probability that in 100 poll stations out of 1800 in Saratov Region the majority party got exactly 62.2%.\nDo you think it can happen by a chance?\n\n\nExercise 27.34 (A/B Testing) Use dataset from ab_browser_test.csv. Here is the definition of the columns:\n\nuserID: unique user ID\nbrowser: browser which was used by userID\nslot: status of the user (exp = saw modified page, control = saw unmodified page)\nn_clicks: number of total clicks user did during as a result of n_queries\nn_queries: number of queries made by userID, who used browser browser\nn_nonclk_queries: number of queries that did not result in any clicks\n\nNote, that not everyone uses a single browser, so there might be multiple rows with the same userID. In this data set combination of userID and browser is the unique row identifier.\n\nCount how many users in each group. How much larger (in percent) exp group when compared to control group\nUsing bootstrap, construct 95% confidence interval for mean and median of number of clicks in group exp and group control. Are the mean and median significantly different?\nUsing bootstrap, check if mean of each group has a normal distribution. Generate \\(B = 1000\\) bootstrap samples, calculate mean of each and plot qqplot.\nUse z-ratio for the means, to perform sis testing, with \\(H_0\\): there is no difference in average number of clicks between 2 groups\nMann-Whitney (http://www.statmethods.net/stats/nonparametric.html) is another test for comparing means, that does not require normality assumption. Use this test to check hypothesis that means are equal.\nFor each browser type and each of the 2 groups (control and exp) count the percent of queries that did not result in any clicks. You can do it be dividing sum of n_nonclk_queries by sum of n_queries. Comment your on your results.\n\n\n\nExercise 27.35 (Chicago Crime Data Analysis) On January 24, 2017 Donald Tramp tweeted about \"horrible\" murder rate in Chicago.\n\n\n\nTrump Tweets\n\n\nOur goal is to analyze the data and check how statistically significant such a statement. I downloaded Chicago’s crime data from the data portal: data.cityofchicago.org. This data contains reported incidents of crime (with the exception of murders where data exists for each victim) that occurred in the City of Chicago from 2001 to present, minus the most recent seven days. Data is extracted from the Chicago Police Department’s CLEAR (Citizen Law Enforcement Analysis and Reporting) system. In order to protect the privacy of crime victims, addresses are shown at the block level only and specific locations are not identified. This data set has 6.3 million records. Each crime incident is categorized using one of the 35 primary crime types: NARCOTICS, THEFT, CRIMINAL TRESPASS, etc.. I frittered incidents of type HOMICIDE into a separate data set stored in chi_homicide.rds. Use chi_crime.R as a staring script for this problem.\n\nCreate a heat map for the homicide incidents. In which areas of the city you think houses are very affordable and in which they are not?\nCreate a map by plotting a dot for each of the homicide incidents. You will see similar picture as you saw with the heat plot. Look at the Hyde Park area in the south side Chicago. There is an \"island\" with no homicide incidents! Can you explain why? Hint: You might want open Google maps in your browser and zoom-in into this area.\n\nThough president’s tweet is consistent with the data (goo.gl/VTPzFw), observing 52 homicides in January is not that unusual. Calculate the total number of homicides for each January. Use bootstrap to estimate 95% confidence interval for the mean \\(\\mu\\) over January homicides. Is \\(52\\) within the interval? Calculate confidence interval using \\(t\\)-ratio. Do you think results from \\(t\\)-ratio based calculations are reliable?\n\nThe history of 2001-present data is rather short. Chicago tribune provided total number of homicides for Chicago for each month of the 1957-2014 period. Use this data set and calculate the confidence interval for \\(\\mu\\) using bootstrap and \\(t\\)-ratio. Further answer the following questions: (i) Assuming monthly homicide rate follows Normal distribution, what is the probability that we observe 52 homicides or more? (ii) Do you think Normality assumption is valid? (iii) Assuming monthly homicide rate follows Poisson distribution, what is the probability that we observe 52 homicides or more?\n\nThere is a hypothesis that crime rates are related to temperatures (goo.gl/nPpHwv). Check this hypothesis using simple regression. Use linear model to regress homicide rate to the average maximum temperature. Does this relation appear significant? Perform residual diagnostics and find outliers and leverage points.\n\nThere is another hypothesis that rise in murder is related to the pullback in proactive policing that started in November of 2015 as a result of Laquan McDonald video release (https://goo.gl/7cm1CC, https://goo.gl/WcH2uB). I calculated total number of homicides for each day and split data into two parts: before and after video release. Using \\(t\\)-ratio, check the hypothesis \\(H_0\\): the homicide rate did not change after video release.\n\n\n\nExercise 27.36 (Gibbs Sampler) Suppose we model data using the following model \\[\n\\begin{aligned}\ny_i \\sim &N(\\mu,\\tau^{-1})\\\\\n\\mu \\sim & N(0,1)\\\\\n\\tau \\sim & Gamma(2,1).\n\\end{aligned}\n\\]\nThe goal is to implement a Gibbs sample for the posterior \\(\\mu,\\tau | y\\), where \\(y = (y_1,\\ldots,y_n)\\) is the observed data. Gibbs sampler algorithms iterates between two steps\n\nSample \\(\\mu_i\\) from \\(\\mu \\mid \\tau_{i-1}, y\\)\nSample \\(\\tau_i\\) from \\(\\tau_i \\mid \\mu_i, y\\)\n\nShow that those full conditional distributions are given by \\[\n\\begin{aligned}\n\\mu \\mid \\tau, y \\sim & N\\left(\\dfrac{\\tau n\\bar y}{1+n\\tau},\\dfrac{1}{1+n\\tau}\\right)\\\\\n\\tau \\mid \\mu,y \\sim & \\mathrm{Gamma}\\left(2+\\dfrac{n}{2}, 1+\\dfrac{1}{2}\\sum_{i=1}^{n}(y_i-\\mu)^2\\right)\n\\end{aligned}\n\\]\nUse formulas for full conditional distributions and implement the Gibbs sampler. The data \\(y\\) is in the file MCMCexampleData.txt.\nPlot samples from the joint distribution over \\((\\mu,\\tau)\\) on a scatter plot. Plot histograms for marginal \\(\\mu\\) and \\(\\tau\\) (marginal distributions).\nSolution: First we write the joint distribution of \\((y,\\tau,\\mu)\\) as \\[\\begin{align*}\np(y,\\tau,\\mu) &= p(y|\\tau,\\mu) p(\\mu) p(\\tau)\\\\\n&=\\left(\\prod_{i=1}^{n} \\frac{\\sqrt{\\tau}}{\\sqrt{2\\pi}} e^{-\\frac{\\tau}{2}(y_i-\\mu)^2} \\right) \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\mu^2} \\frac{1}{\\Gamma(2)} \\tau e^{-\\tau}\n\\end{align*}\\] Therefore, \\[\\begin{align*}\np(\\mu|\\tau,y) &= \\frac{p(y,\\tau,\\mu)}{p(\\tau,y)}\\\\\n&\\propto \\left(\\prod_{i=1}^{n} e^{-\\frac{\\tau}{2}(y_i-\\mu)^2} \\right) e^{-\\frac{1}{2}\\mu^2}\\\\\n&\\propto  e^{-\\frac{\\tau n+1}{2}\\mu^2 + \\tau n\\bar{y}\\mu}\\\\\n&\\sim N\\left(\\frac{\\tau n \\bar{y}}{1+\\tau n}, \\frac{1}{1+\\tau n}\\right)\\\\\np(\\tau|\\mu, y) &= \\frac{p(y,\\tau,\\mu)}{p(\\mu,y)}\\\\\n&\\propto \\left(\\prod_{i=1}^{n} \\sqrt{\\tau}e^{-\\frac{\\tau}{2}(y_i-\\mu)^2} \\right) \\tau e^{-\\tau}\\\\\n&= \\tau^{1+n/2} e^{-(1+\\sum_{i=1}^{n}(y_i-\\mu)^2/2)\\tau}\\\\\n&\\sim \\mathrm{Gamma}\\left(2+\\frac{n}{2}, 1+\\frac{1}{2}\\sum_{i=1}^{n}(y_i-\\mu)^2\\right)\n\\end{align*}\\] \n\n\nExercise 27.37 (AAPL vs GOOG) Download AAPL and GOOG return data from 2018 to 2024. Plot box-plot and histogram. Calculate summary statistics using summary function. Describe clearly what you learn from the summary and the plots.\nSolution:\n\nlibrary(quantmod)\ngetSymbols(c(\"AAPL\",\"GOOG\"), from = \"2018-01-01\", to = \"2024-01-01\")\nAAPL = dailyReturn(AAPL$AAPL.Adjusted)\nGOOG = dailyReturn(GOOG$GOOG.Adjusted)\n\nboxplot(AAPL, col=\"red\"); boxplot(GOOG, col=\"green\")\nhist(AAPL, col=\"red\", breaks=50); hist(GOOG, col=\"green\", breaks=50)\nsummary(AAPL); summary(GOOG)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n     Index            daily.returns     \n Min.   :2018-01-02   Min.   :-0.12865  \n 1st Qu.:2019-07-03   1st Qu.:-0.00824  \n Median :2020-12-30   Median : 0.00118  \n Mean   :2020-12-30   Mean   : 0.00123  \n 3rd Qu.:2022-06-30   3rd Qu.: 0.01188  \n Max.   :2023-12-29   Max.   : 0.11981  \n\n\n     Index            daily.returns      \n Min.   :2018-01-02   Min.   :-0.111008  \n 1st Qu.:2019-07-03   1st Qu.:-0.008736  \n Median :2020-12-30   Median : 0.001068  \n Mean   :2020-12-30   Mean   : 0.000839  \n 3rd Qu.:2022-06-30   3rd Qu.: 0.010999  \n Max.   :2023-12-29   Max.   : 0.104486  \n\n\n\nFrom the box plots, we can see many extreme observations out of 1.5 IQR (inter-quarter-range) of the lower and upper quartiles, which are confirmed by the high kurtosis values. We can also see their heavy tails in the histograms. GOOG is positive skew (mean much higher than median) and AAPL is not “significantly” skew (mean close to median).\n\n\nExercise 27.38 (Berkshire Realty) Berkshire Realty is interested in determining how long a property stays on the housing market. For a sample of \\(800\\) homes they find the following probability table for length of stay on the market before being sold as a function of the asking price\n\n\n\nDays until Sold\nUnder 20\n20-40\nover 40\n\n\nUnder $250K\n50\n40\n10\n\n\n$250-500K\n20\n150\n80\n\n\n$500-1M\n20\n280\n100\n\n\nOver $1 M\n10\n30\n10\n\n\n\n\nWhat is the probability of a randomly selected house that is listed over \\(40\\) days before being sold?\nWhat is the probability that a randomly selected initial asking price is under \\(250\\)K?\nWhat is the joint probability of both of the above event happening?\nAssuming that a contract has just been signed to list a home for under $500K, what is the probability that Berkshire realty will sell the home in under \\(40\\) days?\n\nSolution:\n\n\\(P(\\mbox{over 40}) = \\frac{10+80+100+10}{800} = 25\\%\\)\n\\(P(\\mbox{under 250K}) = \\frac{50+40+10}{800} = 12.5\\%\\)\n\\(P(\\mbox{over 40 and under 250K}) = \\frac{10}{800} = 1.25\\%\\)\n\\(P(\\mbox{under 40 and under 500K}) = \\frac{50+40+20+150}{50+40+20+150+10+80} = 74.29\\%\\)\n\n\n\nExercise 27.39 (TrueF/False)  \n\nIf \\(\\mathbb{P} \\left ( A \\; \\mathrm{ and} \\; B \\right ) \\leq 0.2\\) then \\(\\mathbb{P} (A) \\leq 0.2\\).\nIf \\(P( A | B ) = 0.5\\) and \\(P(B ) = 0.5\\), then the events \\(A\\) and \\(B\\) are necessarily independent.\nA box has three drawers; one contains two gold coins, one contains two silver coins, and one contains one gold and one silver coin. Assume that one drawer is selected randomly and that a randomly selected coin from that drawer turns out to be gold. Then the probability that the chosen drawer contains two gold coins is \\(50\\)%.\nSuppose that \\(P(A) = 0.4 , P(B)=0.5\\) and \\(P( A  \\text{ or }B ) = 0.7\\) then\\(P( A  \\text{ and }B ) = 0.3\\).\nIf \\(P( A  \\text{ or }B ) = 0.5\\) and \\(P(A  \\text{ and }B ) = 0.5\\), then \\(P(A) = P( B)\\).\nThe following data on age and martial status of \\(140\\) customers of a Bondi beach night club were taken\n\n\n\nAge\nSingle\nNot Single\n\n\n\n\nUnder 30\n77\n14\n\n\nOver 30\n28\n21\n\n\n\n\n\n\n\n\nGiven this data, age and martial status are independent.\nIf \\(P( A  \\text{ and }B ) = 0.5\\) and \\(P(A) = 0.1\\), then \\(P(B|A) = 0.1\\).\nIn a group of students, \\(45\\)% play golf, \\(55\\)% play tennis and \\(70\\)% play at least one of these sports. Then the probability that a student plays golf but not tennis is \\(15\\)%.\nThe following probability table related age with martial status\n\n\n\nAge\nSingle\nNot Single\n\n\n\n\nUnder 30\n0.55\n0.10\n\n\nOver 30\n0.20\n0.15\n\n\n\n\n\n\n\n\nGiven these probabilities, age and martial status are independent.\nThirty six different kinds of ice cream can be found at Ben and Jerry’s.There are \\(58,905\\) different combinations of four choices of ice cream.\nSuppose that for a certain Caribbean island the probability of a hurricane is \\(0.25\\), the probability of a tornado is \\(0.44\\) and the probability of both occurring is \\(0.22\\). Then the probability of a hurricane or a tornado occurring is \\(0.05\\).\nIf \\(P ( A  \\text{ and }B ) \\geq 0.10\\) then \\(P(A) \\geq 0.10\\).\nIf \\(A\\) and \\(B\\) are mutually exclusive events, then \\(P(A|B) = 0\\).\nTrue. By definition, if \\(A\\) and \\(B\\) are mutually exclusive events then \\(P( A  \\text{ and }B)=0\\) and so \\(P(A|B) = P(A  \\text{ and }B)/P(B) = 0\\)\n\nSolution:\n\nFalse. We only know \\(\\mathbb{P} \\left ( A \\; \\mathrm{ and} \\; B \\right ) \\leq \\mathbb{P}(A)\\).\nFalse. This is not necessarily true. We need more information about each event to definitively say so.\nFalse. Knowing that we have a gold coin, there is 2/3 chance of being in the 2 gold coin drawer and a 1/3 chance of being in the 1 gold coin drawer. Therefore, the probability that the chosen drawer contains twofold coins is 2.\nFalse. \\(P(A \\text{ and }B)=P(A)+P(B)-P(A \\text{ or }B)=0.2\\)\nTrue. We know that \\(P( A  \\text{ or }B ) = P(A) +P(B) - P(A  \\text{ and }B )\\) and so\\(P(A) + P(B) =1\\). Also \\(P( A) \\geq P( A  \\text{ and }B ) = 0.5\\) and so\\(P(A)=P(B) = 0.5\\)\nFalse. We can compute conditional probabilities as\\(P ( S | U_{30} ) = 0.8462 , P ( M | U_{30} ) = 0.1638\\) and\\(P ( S | O_{30} ) = 0.5714 , P ( M | O_{30} ) = 0.4286\\). The events aren’t independent as the conditional probabilities are not the .\nFalse. \\(P(B|A) = P( A  \\text{ and }B ) / P( A)\\) would be greater than one\nTrue. \\(P(G)=0.45, P(T)=0.55, P(G  \\text{ or }T) = 0.70\\) implies \\(P(G  \\text{ and }\\bar{T}) = 0.15\\)\nFalse. \\(P ( X &gt; 30  \\text{ and }Y = \\mathrm{ married} ) = 0.15 \\neq -.35 \\times 0.25\n=  P ( X &gt; 30 )P( Y = \\mathrm{ married} )\\)\nTrue. Combinations are given by \\(36!/(36-6)!6! =58,905\\)\nIf two events \\(A\\) and \\(B\\) are independent then both \\(P(A|B) =P(A)\\) and\\(P(B|A)=P(A)\\).\nFalse. \\(P(B|A)=P(A)\\). Note that the statement is true if (and only if)\\(P(A)=P(B)\\)\nFalse. \\(P(H \\text{ or }T)=P(H)+P(T)-P(H \\text{ and }T)=0.25+0.44-0.22=0.47\\)\nTrue. \\(A  \\text{ and }B\\) is a subset of \\(A\\) and so\\(P(A) \\geq P(A  \\text{ and }B) = 0.10\\)\n\n\n\nExercise 27.40 (Marginal and Joint) Let \\(X\\) and \\(Y\\) be independent with a joint distribution given by \\[f_{X,Y}(x,y) = \\frac{1}{2 \\pi} \\sqrt{ \\frac{1}{xy} }        \\exp \\left( - \\frac{x}{2} - \\frac{y}{2} \\right)     \\text{ where }  x, y &gt; 0 .\\] Identify the following distributions\n\nThe marginal distribution of \\(X\\)\nCompute the joint distribution of \\(U = X\\) and \\(V = X+Y\\)\nCompute the marginal distribution of \\(V\\).\n\nSolution:\n\n\\[\\begin{aligned}         \\frac{1}{2\\pi} \\int_0^\\infty \\frac{1}{\\sqrt{x y}} e^{\\frac{1}{2}-(x+y)} dx &=         \\frac{1}{\\sqrt{2 \\pi}} \\frac{1}{\\sqrt{y}} e^{\\frac{-y}{2}}         \\end{aligned}\\]\nWe perform the substitutions \\(y \\to v-x\\) and \\(x \\to u\\) and obtain the parameter vector \\(\\begin{pmatrix}u \\\\ v-u\\end{pmatrix}\\), which has the following Jacobian: \\[\\begin{aligned}         \\left|\\begin{pmatrix}         1 & 0 \\\\         -1 & 1          \\end{pmatrix}\\right| = 1         \\end{aligned}\\] Thus, the transformed distribution is \\[\\begin{aligned}         f_{U,V}(u,v) &= \\frac{1}{2 \\pi} \\frac{1}{\\sqrt{u(v-u)}} e^{-v/2}         \\end{aligned}\\]\n\\[\\begin{aligned}         \\frac{1}{2 \\pi} \\int_0^v \\frac{1}{\\sqrt{u(v-u)}} e^{-v/2} du &=          \\frac{1}{2} e^{-v/2}         \\end{aligned}\\]\n\n\n\nExercise 27.41 (Conditional)  \n\nLet \\(X\\) and \\(Y\\) be independent standard \\(N(0,1)\\) random variables. Then \\(X^2 + Y^2\\) is an exponential distribution.\nLet \\(X\\) and \\(Y\\) be independent Poisson random variables with rates \\(\\lambda\\) and \\(\\mu\\), respectively. Show that the conditional distribution of \\(X | (X+Y)\\) is Binomial with \\(n = X+Y\\) and \\(p = \\lambda / ( \\lambda + \\mu)\\).\n\nSolution:\n\nTrue. The random variable \\((X^2+Y^2)\\) has a \\(\\chi^2_2\\) distribution, and this is an exponential distribution with \\(\\lambda=1/2\\).\n\\[P(X=l | X+Y =n) = \\frac{P(X=l , X+Y=n)}{P(X+Y=n)} = \\frac{P(X=l, Y= n-l)}{P(X+Y=n)}\\] \\[=\\frac{P(X=l)P(Y=n-l)}{P(X+Y=n)}=\\frac{(e^{-\\lambda}\\lambda^l/l!)(e^{-\\mu}\\mu^{n-l}/(n-l)!)}{e^{-\\lambda-\\mu}(\\lambda+\\mu)^n/n!}=\\frac{n!}{l!(n-l)!}\\frac{\\lambda^l\\mu^{n-l}}{(\\lambda+\\mu)^n}\\] \\[=p^l (1-p)^{n-l}\\frac{n!}{l!(n-l)!}\\]\n\n\n\nExercise 27.42 (Joint and marginal) Let \\(X\\) and \\(Y\\) be independent exponential random variables with means \\(\\lambda\\) and \\(\\mu\\), respectively.\n\nFind the joint distribution of \\(U = X+Y\\) and \\(V = X / ( X+Y)\\).\nFind the marginal distributions for \\(U\\) and \\(V\\).\n\nSolution:\n\nThe joint distribution of \\(X\\) and \\(Y\\) is given by \\[f_{X,Y} ( x , y ) = \\frac{1}{\\lambda \\mu} \\exp \\left ( - \\frac{x}{\\lambda}  - \\frac{y}{\\mu} \\right )\\] The transformation \\(u =x+y\\) and \\(v= x / ( x+y)\\) has inverse given by \\(x = uv\\), \\(y = u ( 1 - v)\\). Hence, the Jacobian is \\(u\\). The joint distribution of \\((U,V)\\) is then \\[f_{U,V} ( u,v) =  \\frac{u}{\\lambda \\mu} \\exp \\left ( - \\frac{uv}{\\lambda} - \\frac{u(1-v)}{\\mu} \\right )\\]\nThe marginal distribution of \\(U\\) is given by\n\\[\n\\begin{aligned}\nf_U(u)& = \\int_0^1 f_{U,V} ( u,v) dv\\\\\n&= \\frac{1}{\\lambda - \\mu} \\left ( \\exp \\left ( - \\frac{ u}{ \\lambda } \\right )      -  \\exp \\left ( - \\frac{ u}{ \\mu } \\right ) \\right )\n\\end{aligned}\n\\]\n\nThe marginal distribution of \\(V\\) is given by \\[\nf_V ( v) = \\int_{0}^{\\infty} f_{U,V} ( u,v) du\n\\] Hence, \\[\nf_V(v) = \\frac{ \\lambda \\mu }{ ( \\mu v + \\lambda ( 1-v) )^2 }\n\\]\n\n\nExercise 27.43 (Toll road) You are designing a toll road that carries trucks and cars. Each week you see an average of 19,000 vehicles pass by. The current toll for cars is 50 cents and you wish to set the toll for trucks so that the revenue reaches $11,500 per week. You observe the following data: three of every four trucks on the road are followed by a car, while only one of every five cars is followed by a truck.\n\nWhat is the equilibrium distribution of trucks and cars on the road?\nWhat should you charge trucks so as to reach your goal of $11,500 in revenues per week?\n\nSolution:\nThe transition matrix for the number of trucks and cars is given by \\[P = \\left (  \\begin{array}{cc}  \\frac{1}{4} & \\frac{3}{4} \\\\  \\frac{1}{5} & \\frac{4}{5}   \\end{array}    \\right )\\] The equilibrium distribution \\(\\pi = ( \\pi_1 , \\pi_2 )\\) is given by \\(\\pi P = \\pi\\). This has solution given by \\[\\frac{1}{4} \\pi_1 + \\frac{1}{5} \\pi_2 = \\pi_1 \\; \\; \\mathrm{ and} \\; \\; \\frac{3}{4} \\pi_1 + \\frac{4}{5} \\pi_2 = \\pi_2\\] with \\(\\pi_1 = \\frac{4}{19}\\) and \\(\\pi_2 = \\frac{15}{19}\\).\nWith cars at a toll of 50 cents we see that we need to charge trucks $1.00 to generate revenues of $11,500 per week.\n\n\nExercise 27.44 Let \\(X, Y\\) have a bivariate normal density with density given by \\[f_{X,Y} ( x, y) = \\frac{1}{ 2 \\pi \\sqrt{ ( 1 - \\rho^2 )} } \\exp \\left ( - \\frac{1}{2}  \\left ( x^2 - 2 \\rho x y + y^2 \\right ) \\right )\\] Consider the transformation \\(W=X\\) and \\(Z = \\frac{ Y - \\rho X}{ \\sqrt{ 1 - \\rho^2 } }\\). Show that \\(W , Z\\) are independent and identify their distributions.\nSolution:\nThe Jacobian of this linear transformation is given by \\(J = \\sqrt{ 1 - \\rho^2 }\\). Knowing that \\(X = g(W,Z) = W\\), and \\(Y = h(W,Z) = \\sqrt{1-\\rho^2} Z + \\rho W\\), and Jacobian \\(J\\), we can apply the transformation formula: \\[\n\\begin{aligned}     \nf_{W,Z} (w,z) &=& f_{X,Y} (g(w,z),h(w,z)) |J|\\\\\n&=& f_{X,Y} (w, \\sqrt{1-\\rho^2} Z + \\rho W)\\sqrt{1-\\rho^2}\\\\\n&=& \\frac{1}{2\\pi} \\exp \\left ( -\\frac{1}{2} \\left ( w^2-2\\rho w \\left ( \\sqrt{1-\\rho^2} z + \\rho w \\right ) + \\left ( \\sqrt{1-\\rho^2} z + \\rho w \\right )^2 \\right ) \\right )\\\\\n&=& \\frac{1}{2\\pi} \\exp\\left ( -\\frac{1-\\rho^2}{2}(w^2+z^2) \\right )\\\\\n&\\propto& \\exp \\left ( -\\frac{(1-\\rho^2)w^2}{2} \\right ) \\exp\\left ( -\\frac{(1-\\rho^2)z^2}{2} \\right )\\\\\n&\\propto& f_W(w) f_Z(z)\n\\end{aligned}\n\\] Because we can write the joint distribution of \\((W,Z)\\) as the product of two marginal distributions, \\(W\\) and \\(Z\\) are independent. \\(W\\), \\(Z\\) both follow normal distribution with mean \\(0\\), and variance \\((1-\\rho^2)^{-1}\\).",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "ex.html#bayes-rule",
    "href": "ex.html#bayes-rule",
    "title": "27  Exercises",
    "section": "27.2 Bayes Rule",
    "text": "27.2 Bayes Rule\n\nExercise 27.45 (Manchester) While watching a game of Champions League football in a cafe, you observe someone who is clearly supporting Manchester United in the game. What is the probability that they were actually born within 20 miles of Manchester? Assume that you have the following base rate probabilities\n\nThe probability that a randomly selected person in a typical local bar environment is born within 20 miles of Manchester is 1/20\nThe chance that a person born within 20 miles of Manchester actually supports United is 7/10\nThe probability that a person not born within 20 miles of Manchester supports United with probability 1/10\n\nSolution:\nDefine\n\n\nB - event that the person is born within 20 miles of Manchester\n\nU - event that the person supports United.\n\n\nWe want \\(P(B|U)\\). By Bayes’ Theorem, \\[\nP(B|U) = \\frac{P(U|B)P(B)}{P(U|B)P(B) + P(U|NB)P(NB)} = 0.269\n\\]\n\n\nExercise 27.46 (Lung Cancer) According to the Center for Disease Control (CDC), we know that “compared to nonsmokers, men who smoke are about \\(23\\) times more likely to develop lung cancer and women who smoke are about \\(13\\) times more likely” You are also given the information that \\(17.9\\)% of women in 2016 smoke.\nIf you learn that a woman has been diagnosed with lung cancer, and you know nothing else, what’s the probability she is a smoker?\nSolution:\nIf we draw the decision tree we get\n\n\n\nLung Cancer Tree\n\n\nIf \\(y\\) is the fraction of women who smoke, and \\(x\\) is the fraction of nonsmokers who get lung cancer, the number of smokers who get cancer is proportional to 13xy, and the number of nonsmokers who get lung cancer is proportional to x(1-y).\nOf all women who get lung cancer, the fraction who smoke is \\(13xy / (13xy + x(1-y))\\).\nThe x’s cancel, so it turns out that we don’t actually need to know the absolute risk of lung cancer, just the relative risk. But we do need to know y, the fraction of women who smoke. According to the CDC, y was 17.9% in 2009. So we just have to compute \\[\n13y / (13y + 1-y) \\approx 74\\%\n\\] This is higher than most people guess.\n\n\nExercise 27.47 (Tesla Chip) Tesla purchases a particular chip called the HW 5.0 auto chip from three suppliers Matsushita Electric, Philips Electronics, and Hitachi. From historical experience we know that 30% of the chips are purchased from Matsushita; 20% from Philips and the remaining 50% from Hitachi. The manufacturer has extensive histories on the reliability of the chips. We know that 3% of the Matsushita chips are defective; 5% of the Philips and 4% of the Hitachi chips are defective.\nA chip is later found to be defective; what is the probability it was manufactured by each of the manufacturers?\nSolution:\nLet \\(A_1\\) denotes the event that the chip is purchased from Matsushita. Similarly \\(A_2\\) and \\(A_3\\). Let \\(D\\) be the event that the chip is defective and \\(\\bar{D}\\) is not. We know \\[\nP( D | A_1 ) = 0.03 \\; \\; P( D | A_2 ) = 0.05 \\; \\; P( D | A_3 ) = 0.04 \\; \\;\n\\] and \\[\nP( A_1 ) = 0.30 \\; \\; P( A_2 ) = 0.20 \\; \\; P( A_3 ) = 0.50\n\\]\nThe probability that it was manufactured by Philips given that its defective is given by Bayes rule which gives \\[\nP( A_2 | D ) = \\frac{ P( D | A_2  ) P( A_2 ) }{P( D | A_1  )P( A_1 ) +P( D | A_2  ) P( A_2 ) + P( D | A_3  ) P( A_3 ) }\n\\] That is \\[\nP( A_2 | D ) = \\frac{ 0.20 \\times 0.05 }{  0.30 \\times 0.03 +0.20 \\times 0.05 + 0.50 \\times 0.04 } = \\frac{10}{39}\n\\] Similarly, \\(P( A_1 | D ) = 9 / 39\\) and \\(P( A_3 | D  ) = 20/39\\)\n\n\nExercise 27.48 (Light Aircraft) Seventy percent of the light aircraft that disappear while in flight in a certain country are subsequently discovered. Of the aircraft that are discovered, 60% have an emergency locator, whereas 90% of the aircraft not discovered do not have such a locator. Suppose that a light aircraft has disappeared. If it has an emergency locator, what is the probability that it will be discovered?\nSolution:\nLet D = {The disappearing aircraft is discovered.} and E = {The aircraft has an emergency locator}. Given: \\[\nP(D) = 0.70 \\quad P(D^c) = 1- P(D) = 0.30 \\quad P(E|D) = 0.60\n\\] \\[\nP(E^c|D^c) = 0.90 \\quad P(E|D^c) = 1 - P(E^c|D^c) = 0.10\n\\] Then, \\[\n\\begin{aligned}\nP(D|E) &=& \\frac{P(D  \\text{ and }E)}{P(E)}\\\\\n&=& \\frac{P(D) \\times P(E|D)}{P(D) \\times P(E|D) + P(D^c) \\times P(E|D^c)}\\\\\n&=& \\frac{0.70 \\times 0.60}{(0.70\\times0.60) + (0.30 \\times 0.10)} = 0.93\n\\end{aligned}\n\\]\n\n\nExercise 27.49 (Floyd Landis) Floyd Landis was disqualified after winning the 2006 Tour de France. This was due to a urine sample that the French national anti-doping laboratory flagged after Landis had won stage \\(17\\) because it showed a high ratio of testosterone to epitestosterone. Because he was among the leaders he provided \\(8\\) pairs of urine samples – so there were \\(8\\) opportunities for a true positive and \\(8\\) opportunities for a false positive.\n\nAssume that the test has a specificity of \\(95\\)%. What’s the probability of all \\(8\\) samples being labeled “negative”?\nNow assume the specificity is \\(99\\)%. What’s the false positive rate for the test\nBased on this data, explain how you would assess the probability of guilt of Landis in a court hearing.\n\nSolution:\n\nWe’re given that \\(0.95= \\frac{ \\text{number of true negatives}}{\\text{number of true negatives + false positives}}.\\)    \\(P( \\text{8 negatives} | \\text{not guilty})= P(\\text{8 true negatives})=0.95^8\\).\nFalse Positive rate = \\(\\frac {\\text{False positives}}{\\text{False positives + True negatives}}=\\frac{1}{100}=0.01.\\)\n\\(P(8 negatives| \\text{not guilty})=P(\\text{8 true negatives})=0.95^8\\) (or \\(0.99^8\\) if we follow the part 2).  \\(P(\\text{at least 1 positive}|\\text{not guilty})=1-P(\\text{8 negatives}|\\text{not guilty})=1-0.95^8\\approx 0.34\\). Hence, if the specificity is 0.95, the chance of a false positive is 0.34, which is pretty large.\n\n\n\nExercise 27.50 (BRCA1) Approximately \\(1\\)% of woman aged \\(40-50\\) have breast cancer. A woman with breast cancer has a \\(90\\)% chance of a positive mammogram test and a \\(10\\)% chance of a false positive result. Given that someone has a positive test, what’s the posterior probability that they have breast cancer?\nIf you have the BRCA1 gene mutation, you have a \\(90\\)% chance of developing breast cancer. The prevalence of mutations at BRCA1 has been estimated to be 0.04%-0.20% in the general population. The genetic test for a mutation of this gene has a \\(99.9\\)% chance of finding the mutation. The false positive rate is unknown, but you are willing to assume its still \\(10\\)%.\nGiven that someone tests positive for the BRCA1 mutation, what’s the posterior probability that they have breast cancer?\nSolution: We know \\(P(C) = 0.01\\) and \\(P(T\\mid C) = 0.9\\), \\(P(T\\mid \\bar C) = 0.1\\), \\[\nP(C\\mid T) = \\frac{P(T\\mid C)P(C)}{P(T\\mid C)P(C) + P(T\\mid \\bar C)P(\\bar C)} = \\frac{0.9 \\times 0.01}{0.9 \\times 0.01 + 0.1 \\times 0.99} = 0.083\n\\]\n\n0.9*0.01/(0.9*0.01 + 0.1*0.99)\n\n 0.083\n\n\nFor part 2, we know \\(P(C\\mid B) = 0.9\\), \\(P(B) \\in [0.04,0.2]\\), \\(P(BT\\mid B) = 0.999\\), \\(P(BT\\mid \\bar B) = 0.1\\), then\n\\[\nP(C\\mid BT) = \\frac{P(BT, C)}{P(BT)}\n\\] We will do the calculation for the low-end of the BRCA1 mutation \\(P(B) = 0.0004\\) The total probability of a positive test is given by \\[\nP(BT) = P(BT\\mid B)P(B) + P(BT\\mid \\bar B)P(\\bar B) =  0.999*0.0004 + 0.1*0.9996 = 0.1003596\n\\]\nWe can calculate joint over \\(BT\\) and \\(C\\) by marginalizing \\(B\\) from the joint distribution of \\(B\\) and \\(C\\) and \\(BT\\). \\[\nP(BT,C,B) = P(BT\\mid B,C)P(B,C) = P(BT\\mid B)P(C\\mid B)P(B)\n\\] Here we used the fact that \\(BT\\) is conditionally independent of \\(C\\) given \\(B\\). Now we can marginalize over \\(B\\) to get \\[\nP(BT,C) = P(BT,C,\\bar B) + P(BT,C,B) = P(BT\\mid \\bar B)P(\\bar B)P(C\\mid \\bar B) + P(BT\\mid B)P(B)P(C\\mid B) = 0.1*0.9996*P(C\\mid \\bar B) + 0.999*0.0004*0.9\n\\] We can find \\(P(C\\mid \\bar B)\\) from the total probability \\[\nP(C) = P(C\\mid B)P(B) + P(C\\mid \\bar B)P(\\bar B) = 0.9*0.0004 + P(C\\mid \\bar B)*0.9996\n\\] \\[\nP(C\\mid \\bar B) = (0.01 - 0.9*0.0004)/0.9996 = 0.009643858\n\\] \\[\nP(BT,C)  = 0.1*0.9996*0.009643858 + 0.999*0.0004*0.9 = 0.00132364\n\\]\n\\[\nP(C\\mid BT) = \\frac{P(BT, C)}{P(BT)} = 0.00132364/0.1003596 = 0.01318897\n\\] A small probability of 1.32%.\nFor the high-end of the BRCA1 mutation \\(P(B) = 0.002\\), we get \\[\nP(C\\mid BT) = 0.02637794\n\\] Slightly large probability of 2.64%.\n\nPB = seq(0.0004,0.002, by = 0.0001)\nPCBT = 0.1*(1-PB)*(0.01 - 0.9*PB)/(1-PB) + 0.999*PB*0.9\nplot(PB,PCBT, type = \"l\", xlab = \"P(B)\", ylab = \"P(C|BT)\")\n\n\n\n\n\n\n\n\n\n\nExercise 27.51 (Another Cancer Test) Bayes is particularly useful when predicting outcomes that depend strongly on prior knowledge.\nSuppose that a woman is her forties takes a mammogram test and receives the bad news of a positive outcome. Since not every positive outcome is real, you assess the following probabilities. The base rate for a woman is her forties to have breast cancer is \\(1.4\\)%. The probability of a positive test given breast cancer is \\(75\\)% and the probability of a false positive is \\(10\\)%.\nGiven the positive test, what’s the probability that she has breast cancer?\n\n\nExercise 27.52 (Bayes Rule: Hit and Run Taxi) A certain town has two taxi companies: Blue Birds, whose cabs are blue, and Uber, whose cabs are black. Blue Birds has 15 taxis in its fleet, and Uber has 75. Late one night, there is a hit-and-run accident involving a taxi.\nThe town’s taxis were all on the streets at the time of the accident. A witness saw the accident and claims that a blue taxi was involved. The witness undergoes a vision test under conditions similar to those on the night in question. Presented repeatedly with a blue taxi and a black taxi, in random order, they successfully identify the colour of the taxi 4 times out of 5.\nWhich company is more likely to have been involved in the accident?\nSolution:\nWe need to know \\(P(Blue \\mid \\mbox{identified Blue})\\) and \\(P(Black \\mid \\mbox{identified Blue})\\).\nFirst of all, write down some probability statements given in the problem. \\(P(Blue) = 16.7\\% \\mbox{ and } P(Black) = 83.3\\%\\) \\[\nP(\\mbox{identified Blue} \\mid Blue) = 80\\% \\mbox{ and } P(\\mbox{identified Black} \\mid Blue) = 20\\%\n\\] \\[\nP(\\mbox{identified Black} \\mid Black) = 80\\% \\mbox{ and } P(\\mbox{identified Blue} \\mid Black) = 20\\%\n\\] Therefore, by Bayes Rule, \\[\n\\begin{aligned}\nP(Blue \\mid \\mbox{identified Blue}) =& \\frac{P(\\mbox{identified Blue} \\mid Blue)*P(Blue) }{P(\\mbox{identified Black})} \\\\\n=& \\frac{P(\\mbox{identified Blue} \\mid Blue)*P(Blue) }{P(\\mbox{identified Blue} \\mid Blue)*P(Blue) + P(\\mbox{identified Black} \\mid Black)*P(Black) }  \\\\\n=& 44.5\\%\n\\end{aligned}\n\\] \\[\nP(Black \\mid \\mbox{identified Blue}) = 1- P(Blue \\mid \\mbox{identified Blue}) = 55.5\\%\n\\]\nTherefore, even though the witness said it was a Blue car, the probability that it was a Black car is higher.\n\n\nExercise 27.53 (Gold and Silver Coins) A chest has two drawers. It is known that one drawer has \\(3\\) gold coins and no silver coins. The other drawer is known to contain \\(1\\) gold coin and \\(2\\) silver coins.\nYou don’t know which drawer is which. You randomly select a drawer and without looking inside you pull out a coin. It is gold. Show that the probability that the remaining two coins in the drawer are gold is \\(75\\)%.\nSolution:\nSuppose drawer A contains 3G and drawer B contains 1G2S. We know the probability \\(P(G \\mid A) = 1\\) and \\(P(G \\mid B) = 1/3\\). Also, it must be either of two drawers, so P(A) = P(B) = 1/2. What we are are looking for is \\(P(A \\mid G)\\): the probability that it is drawer A. By Bayes’ rule, \\[\n\\begin{aligned}\nP(A \\mid G) &=& \\frac{P(G \\mid A)\\times P(A)}{P(G)} \\\\\n&=& \\frac{P(G \\mid A)\\times P(A)}{P(G \\mid A)\\times P(A) + P(G \\mid B)\\times P(B)} \\mbox{ (Law of total probability)} \\\\\n&=& \\frac{1\\times \\frac{1}{2}}{1*\\frac{1}{2}+\\frac{1}{3}\\times\\frac{1}{2}} = \\frac{3}{4}\n\\end{aligned}\n\\] The probability that it is drawer A is 75%.\n\n\nExercise 27.54 (The Monty Hall Problem.) This problem is named after the host of the long-running TV show, Let’s Make a Deal. A contestant is given a choice of 3 doors. There is a prize (a car, say) behind one of the doors and something worthless behind the other two doors (say two goats).\nAfter the contestant chooses a door Monty opens one of the other two doors, revealing a goat. The contestant has the choice of switching doors. Is it advantageous to switch doors or not?\nSolution:\nSuppose you plan to switch:\nYou can either pick a winner, a loser, or the other loser when you make your first choice. Each of these options has a probability of \\(1/3\\) and are marked by a “1” below. In each case, Monty will reveal a loser (X). In the first case, he has a choice, but whichever he reveals, you will switch to the other and lose. But in the other two cases, there is only one loser for him to reveal. He must reveal this one, leaving only the winner. So, if you initially pick a loser, you will win by switching. That is, there is a \\(2/3\\) chance of winning if you use the switch strategy.\n\n\n\n\nW\nL\nL\n\n\n\n\n1/3\n1\nX\n\n\n\n1/3\n\n1\nX\n\n\n1/3\n\nX\n1\n\n\n\nNot convinced? Try thinking about it this way: Imagine the question with 1000 doors, and Monty will reveal 998 wrong doors after you pick one, so you are left with your choice, and one of the remaining 999 doors. Now do you want to stay or switch?\nAgain, suppose you are going to switch. Define the events\n\nW = the one you switch to is a winner\nFW = your first choice is a winner\nFL = your first choice is a loser\n\nSince you must pick either a winner or loser with your first choice, and cannot pick both, FW and FL are mutually exclusive and collectively exhaustive. By the rule of total probability: \\[\nP(W) = P(W \\text{ and }FW) + P(W \\text{ and }FL) = P(W|FW)P(FW) + P(W|FL)P(FL)\n\\] \\[\nP(W) = 0 \\times \\frac{1}{3} + 1 \\times \\frac{2}{3} = \\frac{2}{3}\n\\] Why is \\(P(W\\mid FW)=0\\)? Because if we choose correctly, and we do switch, we must be on a loser.\nWhy is \\(P(W|FL)=1\\)? If we first picked a loser, and then switched, we will now have a winner. These both come from above.\nThere’s a longer explanation: Suppose you choose door A and Monty opens B.\nConsider the events\n\nA=prize is behind A\nB=prize is behind B\nC=prize is behind C\nMA=Monty opens A\nMB=Monty opens B\nMC=Monty Opens C\n\nBefore we choose, each door was equally likely: \\(P(A)=P(B)=P(C)=1/3\\).\nIn this case, we know Monty opened B. To decide whether to switch, we want to know if \\(P(A |MB)\\) and \\(P(C|MB)\\) are the same. If they are, then there is no gain in switching: \\[\nP(A|MB) = \\frac{P(A \\text{ and }MB)}{P(MB)} = \\frac{P(MB|A)P(A)}{P(MB)}\n\\] We need these components: \\(P(A)=1/3,~P(MB|A)=1/2\\). This is because you picked A, and Monty can open either B or C. He cannot open A. He can open B or C because the condition in this conditional probability is that the prize is actually behind A. \\[\n\\begin{aligned}\nP(MB) & = P(MB \\text{ and }A) + P(MB \\text{ and }B) + P(MB \\text{ and }C)\\\\\n&= P(MB|A)P(A) + P(MB|B)P(B) + P(MB|C)P(C)\\\\\n& = \\frac{1}{2}\\times\\frac{1}{3} + 0\\times \\frac{1}{3} + 1\\times \\frac{1}{3} = \\frac{1}{6} + \\frac{1}{3} = \\frac{1}{2}\n\\end{aligned}\n\\] I already showed that \\(Pr(MB\\)|A)=1/2$. We have that \\(Pr(MB|B)=0\\) because Monty cannot reveal B if this is where the prize is. If the prize is behind C, he must open B, since you have picked A.\nPutting it all together: \\(P(A|MB) = \\frac{P(MB|A)P(A)}{P(MB)} = \\frac{\\frac{1}{2}\\times \\frac{1}{3}}{\\frac{1}{2}} = \\frac{1}{3}\\)\nSimilarly for C: \\(P(C|MB) = \\frac{P(MB|C)P(C)}{P(MB)} = \\frac{1\\times \\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\\)\nSo we are always better off switching$!\n\n\nExercise 27.55 (Medical Testing for HIV) A controversial issue in recent years has been the the possible implementation of random drug and/or disease testing (e.g. testing medical workers for HIV virus, which causes AIDS). In the case of HIV testing, the standard test is the Wellcome Elisa test.\nThe test’s effectiveness is summarized by the following two attributes:\n\nThe sensitivity is about 0.993. That is, if someone has HIV, there is a probability of 0.993 that they will test positive.\nThe specificity is about 0.9999. This means that if someone doesn’t have HIV, there is probability of 0.9999 that they will test negative.\n\nIn the general population, incidence of HIV is reasonably rare. It is estimated that the chance that a randomly chosen person has HIV is \\(0.000025\\).\nTo investigate the possibility of implementing a random HIV-testing policy with the Elisa test, calculate the following:\n\nThe probability that someone will test positive and have HIV.\nThe probability that someone will test positive and not have HIV.\nThe probability that someone will test positive.\nSuppose someone tests positive. What is the probability that they have HIV?\n\nIn light of the last calculation, do you envision any problems in implementing a random testing policy?\nSolution:\nFirst, introduce some notation: let \\(H\\) = “Has HIV” and \\(T\\) = “Tests Positive”. We are given the following sensitivities and specificities \\[\nP(T|H) = 0.993 \\; \\; \\text{ and} \\; \\; P(\\bar{T}|\\bar{H}) = 0.9999\n\\] The base rate is \\(P(H) = 0.000025\\).\n\n\\(P(T \\text{ and }H) = P(T|H)P(H) = 0.993*0.000025 = 0.000024825\\)\n\\[\\begin{aligned}\nP(T|\\bar{H})P(\\bar{H}) & = (1-P(\\bar{T}|\\bar{H}))(1-P(H))\\\\\n& = 0.0001 \\times 0.999975 = 0.0000999975\n\\end{aligned}\\]\n\\(P(T) = P(T \\text{ and }H) + P(T \\text{ and }\\bar{H}) = 0.000024825 + 0.00009975 = 0.0001248225\\)\n\\(P(H|T) = \\frac{P(H \\text{ and }T)}{P(T)} = \\frac{0.000024825}{0.0001248225} = 0.1988\\)\n\nThis says that if you test positive, you have a 20% chance of having the disease. In other words, 80% of people who test positive will not have the disease. The large number of false positives means that implementing such a policy will be pretty unpopular among the people who have to be tested. Testing high risk people only might be a better idea, as this will increase the \\(P(H)\\) in the population.\n\n\nExercise 27.56 (The Three Prisoners) An unknown two will be shot, the other freed. Prisoner A asks the warder for the name of one other than himself who will be shot, explaining that as there must be at least one, the warder won’t really be giving anything away. The warder agrees, and says that B will be shot. This cheers A up a little: his judgmental probability for being shot is now 1/2 instead of 2/3.\nShow (via Bayes theorem) that\n\nA is mistaken - assuming that he thinks the warder is as likely to say \"C\" as \"B\" when he can honestly say either; but that\nA would be right, on the hypothesis that the warder will say \"B\" whenever he honestly can.\n\n\n\nExercise 27.57 (The Two Children) You meet Max walking with a boy whom he proudly introduces as his son.\n\nWhat is your probability that his other child is also a boy, if you regard him as equally likely to have taken either child for a walk?\nWhat would the answer be if you regarded him as sure to walk with the boy rather than the girl, if he has one of each?\nWhat would the answer be if you regarded him as sure to walk with the girl rather than the boy, if he has one of each?\n\n\n\nExercise 27.58 (Medical Exam) As a result of medical examination, one of the tests revealed a serious illness in a person. This test has a high precision of 99% (the probability of a positive response in the presence of the disease is 99%, the probability of a negative response in the absence of the disease is also 99%). However, the detected disease is quite rare and occurs only in one person per 10,000. Calculate the probability that the person being examined does have an identified disease.\n\n\nExercise 27.59 (The Jury) Assume that the probability is 0.95 that a jury selected to try a criminal case will arrive at the correct verdict whether innocent or guilty. Further, suppose that the 80% of people brought to trial are in fact guilty.\n\nGiven that the jury finds a defendant innocent what’s the probability that they are in fact innocent?\nGiven that the jury finds a defendant guilty what’s the probability that they are in fact guilty?\nDo these probabilities sum to one?\n\nSolution:\nLet \\(I\\) denote the event of innocence and let \\(VI\\) denote the event that the jury proclaims an innocent verdict.\n\nWe want \\(P(I|VI)\\) which is given by Bayes’ rule \\[\nP(I|VI)=\\frac{P(VI|I)P(I)}{P(VI)}\n\\] where, by the law of total probability \\(P(VI)=P(VI|I)P(I)+P(VI|\\bar{I})P(\\bar{I})\\) Hence \\(P(VI) =0.95\\cdot 0.2+0.05\\cdot 0.8=0.23\\) Hence \\(P(I|VI)=\\frac{0.95\\cdot 0.2}{0.23}=0.83\\)\nSimilarly \\[\nP(G|VG)=\\frac{P(VG|G)P(G)}{P(VG)}=\\frac{P(VG|G)P(G)}{1-P(VI)}=\\frac{%\n0.95\\cdot 0.8}{1-0.23}=0.99\n\\]\nNo. \\(P(I|VI)+P(G|{VG})\\neq 1\\) because they are conditioned on different events.\n\n\n\nExercise 27.60 (Oil company) An oil company has purchased an option on land in Alaska. Preliminary geologic studies have assigned the following probabilities of finding oil \\[\nP ( \\text{ high \\; quality \\; oil} ) = 0.50 \\; \\;\nP ( \\text{ medium \\; quality \\; oil} ) = 0.20 \\; \\;\nP ( \\text{ no \\; oil} ) = 0.30 \\; \\;\n\\] After 200 feet of drilling on the first well, a soil test is taken. The probabilities of finding the particular type of soil identified by the test are as follows: \\[\nP ( \\text{ soil} \\; | \\; \\text{ high \\; quality \\; oil} ) = 0.20 \\; \\;\nP ( \\text{ soil} \\; | \\; \\text{ medium \\; quality \\; oil} ) = 0.80 \\; \\;\nP ( \\text{ soil} \\; | \\; \\text{ no \\; oil} ) = 0.20 \\; \\;\n\\]\n\nWhat are the revised probabilities of finding the three different types of oil?\nHow should the firm interpret the soil test?\n\nSolution:\n\nUsing Bayes’ Theorem, we get \\[P ( \\text{ high \\; quality \\; oil} \\; | \\;  \\text{ soil}) =\n   \\frac{ P ( \\text{ soil} \\; | \\; \\text{ high \\; quality \\; oil} ) P ( \\text{ high \\; quality \\; oil} )}\n{ P ( \\text{ soil} ) }\\] Using the law of total probability \\[P ( \\text{ soil}) = 0.5 \\times 0.2 + 0.2 \\times 0.8 + 0.3 \\times 0.2 = 0.32\\] Hence \\[P ( \\text{ high \\; quality \\; oil} \\; | \\;  \\text{ soil}) =\n   \\frac{ 0.5 \\times 0.2}{ 0.32 } = 0.3125\\] Similarly \\[P ( \\text{ medium \\; quality \\; oil} \\; | \\;  \\text{ soil}) =\n   \\frac{ 0.8 \\times 0.2}{ 0.32 } = 0.50\\] \\[P ( \\text{ no \\; oil} \\; | \\;  \\text{ soil}) =\n   \\frac{ 0.3 \\times 0.2}{ 0.32 } = 0.1875\\]\nThe firm should interpret the soil test as increasing the probability of medium quality oil, but making both no oil and high quality oil less likely than their initial probabilities.\n\n\n\nExercise 27.61 A screening test for high blood pressure, corresponding to a diastolic blood pressure of \\(90\\)mm Hg or higher, produced the following probability table\n\n\n\n\nHypertension\n\n\n\n\n\nTest\nPresent\nAbsent\n\n\n+ve\n0.09\n0.03\n\n\n-ve\n0.03\n0.85\n\n\n\n\nWhat’s the probability that a random person has hypertension?\nWhat’s the probability that someone tests positive on the test?\nGiven a person who tests positive, what is the probability that they have hypertension?\nWhat would happen to your probability of having hypertension given you tested positive if you initially thought you had a \\(50\\)% chance of having hypertension.\n\nSolution:\n\n\\(P(H) = 0.09 + 0.03 = 0.12\\)\n\\(P(+)=0.09+0.03 =0.12\\)\n\\(P(H|+)=P(+|H)P(H)/P(+)= 0.09/0.12 = 0.75\\)\nNow you have to re-calculate \\(P(+) = P( +|H)P(H) + P( + | \\bar{H} )P( \\bar{H} )\n= 0.38\\). Then, by Bayes rule \\[P(H|+)= \\frac{P(+|H)P(H)}{P(+)} = \\frac{0.09 \\times 0.5}{0.38} = 0.99\\]\n\n\n\nExercise 27.62 (Steroids) Suppose that a hypothetical baseball player (call him “Rafael”) tests positive for steroids. The test has the following sensitivity and specificity\n\nIf a player is on Steroids, there’s a \\(95\\)% chance of a positive result.\nIf a player is clean, there’s a \\(10\\)% chance of a positive result.\n\nA respected baseball authority (call him “Bud”) claims that \\(1\\)% of all baseball players use Steroids. Another player (call him “Jose”) thinks that there’s a \\(30\\)% chance of all baseball players using Steroids.\n\nWhat’s Bud’s probability that Rafael uses Steroids?\nWhat’s Jose’s probability that Rafael uses Steroids?\n\nExplain any probability rules that you use.\nSolution:\nLet \\(T\\) and \\(\\bar{T}\\) be positive and negative test results. Let \\(S\\) and \\(\\bar{S}\\) be using and not using Steroids, respectively. We have the following conditional probabilities \\[\nP( T | S ) = 0.95 \\; \\; \\text{ and} \\; \\; P( T | \\bar{S} ) = 0.10\n\\] For our prior distributions we have \\(P_{ Bud } ( S  ) = 0.01\\) and \\(P_{ Jose } ( S  ) = 0.30\\).\nFrom Bayes rule and we have \\[\nP( S | T )  = \\frac{ P( T | S ) P( S )}{ P(T )}\n\\] and by the law of total probability \\[\nP(T)  = P( T| S) P(S) + P( T| \\bar{S} ) P( \\bar{S} )\n\\] Applying these two probability rules gives \\[\nP_{ Bud} ( S | T ) = \\frac{ 0.95 \\times 0.01 }{  0.95 \\times 0.01 + 0.10 \\times 0.99 } = 0.0876\n\\] and \\[\nP_{ Jose } ( S | T ) = \\frac{ 0.95 \\times 0.3 }{  0.95 \\times 0.3 + 0.10 \\times 0.7 } = 0.8028\n\\]\n\n\nExercise 27.63 A Breathalyzer test is calibrated so that if it is used on a driver whose blood alcohol concentration exceeds the legal limit, it will read positive \\(99\\)% of the time, while if the driver is below the limit it will read negative \\(90\\)% of the time. Suppose that based on prior experience, you have a prior probability that the driver is above the legal limit of \\(10\\)%.\n\nIf a driver tests positive, what is the posterior probability that they are above the legal limit?\nAt Christmas \\(20\\)% of the drivers on the road are above the legal limit. If all drivers were tested, what proportion of those testing positive would actually be above the limit\nHow does your answer to part \\(1\\) change. Explain\n\nSolution:\nLet the events be defined as follows:\n\nE - Alcohol concentration exceeds legal limit\nNE - Alcohol concentration does not exceed legal limit\nP - Breathalyser reads positive\nN - Breathalyser reads negative\n\n\\[\n\\begin{aligned}\nP(P|E) & = & 0.99\\\\\nP(N|NE) & = & 0.90\\\\\nP(E) & = & 0.10\\end{aligned}\n\\]\nBased on above, we have \\(P(P|NE)=1-P(N|NE)=0.10\\) and \\(P(NE)=1-P(E)=0.90\\). Then,\n\\[\n\\begin{aligned}\nP(E|P) & = & \\frac{P(P|E)P(E)}{P(P)}\\\\\n& = & \\frac{P(P|E)P(E)}{P(P|E)P(E)+P(P|NE)P(NE)}\\\\\n& = & \\frac{0.99\\times0.10}{0.99\\times0.10+0.10\\times0.90}\\\\\n& = & 52.38\\%\\end{aligned}\n\\]\nNow, we have \\(P(E)=0.20\\). Thus, \\(P(NE)=1-P(E)=0.80\\). We now calculate\n\\[\n\\begin{aligned}\nP(E|P) & = & \\frac{P(P|E)P(E)}{P(P)}\\\\\n& = & \\frac{P(P|E)P(E)}{P(P|E)P(E)+P(P|NE)P(NE)}\\\\\n& = & \\frac{0.99\\times0.20}{0.99\\times0.20+0.10\\times0.80}\\\\\n& = & 71.22\\%\\end{aligned}\n\\]\nThus, 71.22% of all the drivers who test positive would be above the legal limit.\nCompared to Part 1, the posterior probability increases due to the fact that the probability of a driver testing positive and exceeding the legal limit increases as does the probability of testing positive. However, the increase in the probability of testing positive and exceeding the legal limit is greater than the increase in the probability of testing positive which results in an increase in the posterior probability.\n\n\nExercise 27.64 (Chicago bearcats) The Chicago bearcats baseball team plays \\(60\\)% of its games at night and \\(40\\)% in the daytime. They win \\(55\\)% of their night games and only \\(35\\)% of their day games. You found out the next day that they won their last game\n\nWhat is the probability that the game was played at night\nWhat is the marginal probability that they will win their next game?\n\nExplain clearly any rules of probability that you use.\nSolution:\nWe have the following info:\n\n\\(P(Night) = 0.6, \\quad P(Day)=0.4\\)\n\\(P(Win|Night) = 0.55, \\quad P(Lose|Night)=0.45\\)\n\\(P(Win|Day) = 0.35, \\quad P(Lose|Day)=0.65\\)\n\n\n\\[P(Night|Win) = \\frac{P(Win|Night)P(Night)}{P(Win)}\\] \\[=\\frac{P(Win|Night)P(Night)}{P(Win|Day)P(Day)+P(Win|Night)P(Night)}\\] \\[\\frac{0.55*0.6}{0.35*0.4+0.55*0.6}=\\frac{0.33}{0.47}=0.7021\\]\n\\(P(Win) = P(Win|Day)P(Day)+P(Win|Night)P(Night)=0.35*0.4+0.55*0.6=0.47\\)\n\n\n\nExercise 27.65 (Spam Filter) Several spam filters use Bayes rule. Suppose that you empirically find the following probability table for classifying emails with the phrase “buy now” in their title as either “spam” or “not spam”.\n\n\n\n\nSpam\nNot Spam\n\n\n\n\n“buy now”\n0.02\n0.08\n\n\nnot “buy now”\n0.18\n0.72\n\n\n\n\nWhat is the probability that you will receive an email with spam?\nSuppose that you are given a new email with the phrase “buy now” in its title. What is the probability that this new email is spam?\nExplain clearly any rules of probability that you use.\n\nSolution:\n\nThe probability that you will receive an email with spam is: \\[\n\\begin{aligned}\nPr(Spam) = .02 + .18 = .2\n\\end{aligned}\n\\]\nThe posterior probability is \\[\n\\begin{aligned}\nPr(Spam|buy \\; now) = \\frac{Pr(Spam\\;and\\;buy\\;now)}{Pr(buy\\;now)} = \\frac{.02}{.02+.08} = .2\n\\end{aligned}\n\\]\nThe marginal probability is given by the law of total probability: \\(P( B) = P(B|A)P(A) + P(B|\\bar{A})P(\\bar{A})\\). We also use the definition of conditional probability (not Bayes’ rule) \\(Pr(A|B) = \\frac{Pr(A\\;and\\;B)}{Pr(B)}\\)\n\n\n\nExercise 27.66 (Chicago Cubs) The Chicago Cubs are having a great season. So far they’ve won \\(72\\) out of the \\(100\\) games played so far. You also have the expert opinion of Bob the sports analysis. He tells you that he thinks the Cubs will win. Historically his predictions have a \\(60\\)% chance of coming true.\n\nCalculate the probability that the Cubs will win given Bob’s prediction\nSuppose you now learn that it’s a home game and that the Cubs win \\(60\\)% of their games at Wrigley field. What’s you updated probability that the Cubs will win their game?\n\nSolution:\n\nFirst we have \\[\nP ( \\text{ Win} | \\text{ Bob} ) = \\frac{ 0.72 \\times 0.6}{ 0.72 \\times 0.6 + 0.28 \\times 0.4} = 0.79\n\\]\n\n\nLearning the new information, we use the previous posterior as a prior for the next Bayes update \\[\nP ( \\text{ Win} | \\text{ home} , \\text{ Bob}  ) = \\frac{ 0.79 \\times 0.60}{ 0.79 \\times 0.60 + 0.21 \\times 0.40} = 0.85\n\\] it’s highly likely that the Cubs will win!\n\n\n\nExercise 27.67 (Student-Grade Causality) Consider the following probabilistic model. The student does poorly poorly in a class (\\(c = 1\\)) or well (\\(c = 0\\)) depending on the presence/absence of depression (\\(d = 1\\) or \\(d = 0\\)) and weather he/she partied last night (\\(v = 1\\) or \\(v = 0\\)) . Participation in the party can also lead to the fact that the student has a headache (\\(h = 1\\)). As a result of poor student’s performance, the teacher gets upset (\\(t = 1\\)). The probabilities are given by:\n\n\n\n\n\n\n\n\n\n\\(p(c=1|d,v)\\)\nv\nd\n\n\n\n\n0.999\n1\n1\n\n\n0.9\n1\n0\n\n\n0.9\n0\n1\n\n\n0.01\n0\n0\n\n\n\n\n\n\n\n\n\\(p(h=1|v)\\)\nv\n\n\n\n\n0.9\n1\n\n\n0.1\n0\n\n\n\n\n\n\n\n\n\\(p(t=1|c)\\)\nc\n\n\n\n\n0.95\n1\n\n\n0.05\n0\n\n\n\n\n\n\n\\(p(v=1)=0.2\\), and \\(p(d=1) = 0.4\\).\nDraw the causal relationships in the model. Calculate \\(p(v=1|h=1)\\), \\(p(v=1|t=1)\\), \\(p(v=1|t=1,h=1)\\).\n\n\nExercise 27.68 (Prisoner) An unknown two will be shot, the other freed. Prisoner A asks the warder for the name of one other than himself who will be shot, explaining that as there must be at least one, the warder won’t really be giving anything away. The warder agrees, and says that B will be shot. This cheers A up a little: his judgmental probability for being shot is now 1/2 instead of 2/3. Show (via Bayes theorem) that\n\nA is mistaken - assuming that he thinks the warder is as likely to say \"C\" as \"B\" when he can honestly say either; but that\nA would be right, on the hypothesis that the warder will say \"B\" whenever he honestly can.\n\nSolution:\n\n\nExercise 27.69 (True/False)  \n\nIn a sample of \\(100,000\\) emails you found that \\(550\\) are spam. Your next email contains the word “bigger”. From historical experience, you know that half of all spam email contains the word “bigger” and only \\(2\\)% of non-spam emails contain it. The probability that this new email is spam is approximately \\(12\\)%.\nSuppose that there’s a \\(5\\)% chance that it snows tomorrow and a \\(80\\)%chance that the Chicago bears play their football game tomorrow given that it snows. The probability that they play tomorrow is then \\(80\\)%.\nBayes’ rule states that \\(p(A|B) =p(B|A)\\).\nIf \\(P( A  \\text{ and }B ) = 0.4\\) and \\(P( B) = 0.8\\), then \\(P( A|B ) = 0.5\\).\n\nSolution:\n\nTrue. \\[P(bigger \\mid spam) = 50\\% \\mbox{ and } P(bigger \\mid non-spam) = 2\\%\\]\\[P(spam) = 0.55\\% \\mbox{ and } P(non-spam) = 99.45\\%\\]\\[\\begin{aligned}P(spam \\mid bigger) &=& \\frac{P(bigger \\mid spam)*P(spam)}{P(bigger \\mid spam)*P(spam) + P(bigger \\mid non-spam)*P(non-spam) } \\\\&=& 12.1\\%\\end{aligned}\\]\nFalse, 99% \\[P(Play) = P(Play|Snow)P(Snow) + P(Play|NoSnow)P(NoSnow)\\]\\[= 0.8*0.05+1*0.95=0.99\\]\nFalse. Recall Bayes’ rule:* \\[p(A|B)=\\frac{p(B|A)p(A)}{p(B)}\\]\nTrue. \\(P( A|B ) =  P( A  \\text{ and }B )/P(B)\\) by Bayes rule.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "ex.html#utility-and-decisions",
    "href": "ex.html#utility-and-decisions",
    "title": "27  Exercises",
    "section": "27.3 Utility and Decisions",
    "text": "27.3 Utility and Decisions\n\nExercise 27.70 (Two Gambles) In an experiment, subjects were given the choice between two gambles:\n\n\n\nExperiment 1\n\n\n\n\n\n\n\nGamble \\({\\cal G}_A\\)\n\nGamble \\({\\cal G}_B\\)\n\n\n\nWin\nChance\nWin\nChance\n\n\n$2500\n0.33\n$2400\n1\n\n\n$2400\n0.66\n\n\n\n\n$0\n0.01\n\n\n\n\n\nSuppose that a person is an expected utility maximizer. Set the utility scale so that u($0) = 0 and u($2500) = 1. person is an expected utility maximizer. Set the utility scale so that u($0) = 0 and u($2500) = 1. Whether a utility maximizing person would choose Option A or Option B depends on the person’s utility for $2400. For what values of u($2400) would a rational person choose Option A? For what values would a rational person choose Option B?\n\n\n\nExperiment 2\n\n\n\n\n\n\n\nGamble \\({\\cal G}_C\\)\n\nGamble \\({\\cal G}_D\\)\n\n\n\nWin\nChance\nWin\nChance\n\n\n$2500\n0.33\n$2400\n0.34\n\n\n$0\n0.67\n$0\n0.66\n\n\n\nFor what values of u($2400) would a person choose Option C? For what values would a person choose Option D? Explain why no expected utility maximizer would prefer B and C.\nThis problem is a version of the famous Allais paradox, named after the prominent critic of subjective expected utility theory who first presented it. Kahneman and Tversky found that 82% of subjects preferred B over A, and 83% preferred C over D. Explain why no expected utility maximizer would prefer both B in Gamble 1 and C in Gamble 2. (A utility maximizer might prefer B in Gamble 1. A different utility maximizer might prefer C in Gamble 2. But the same utility maximizer would not prefer both B in Gamble 1 and C in Gamble\nDiscuss these results. Why do you think many people prefer B in Gamble 1 and C in Gamble 2? Do you think this is reasonable even if it does not conform to expected utility theory?\nSolution:\nDefine x = u($2400), the utility of $2400.\nFor A versus B:\n\nExpected utility of A is 0.33* (1) + 0.66(x) + 0.1(0)\nExpected utility of B is x\n\nSetting them equal and solving for x tells us the value of x for which an expected utility maximizer would be indifferent between the two options \\[0.33* (1) + 0.66*(x) + 0.1*(0) = x\\] \\[x = 0.33/0.34.\\]\nFor C versus D:\n\nExpected utility of C is 0.33* (1) + 0.67*(0)\nExpected utility of D is 0.34*(x)\n\nSetting them equal and solving for x tells us the value of x for which an expected utility maximizer would be indifferent between the two options \\[0.33* (1) = 0.34*(x)\\] \\[x = 0.33/0.34\\]\nIf x &lt; 33/34, then an expected utility maximizer would choose option C. If x&gt;33/34, option D an expected utility maximizer would choose option D\nWhy no utility maximizer would prefer B and C?\nA utility maximizer would pick B if x&gt;33/34, and would pick C if x&lt;33/34. These regions do not overlap. By definition, an expected utility maximizer has a consistent utility value for a given payout regardless of the probability structure. Therefore, no utility maximizer would prefer B and C. A utility maximizer would be indifferent among all four of these gambles if x = 33/34. But no utility maximizer would strictly prefer B over A, and C over D.\nMany people’s choices violate subjective expected utility theory in this problem. In fact, Allais carefully crafted this problem to exploit what Kahneman and Tversky called the “certainty effect.” In the choice of A vs B, many prefer a sure gain to a small chance of no gain. On the other hand, in the choice of C vs D, there is no certainty, and so people are willing to reduce their chances of winning by what feels like a small amount to give themselves a chance to win a larger amount. When given an explanation for why these choices are inconsistent with “economic rationality,” some people say they made an error and revise their choices, but others stand firmly by their choices and reject expected utility theory.\nIt is also interesting to look at how people respond to a third choice:\nExperiment 3:\nThis is a two-stage problem. In the first stage there is a 66% chance you will win nothing and quit. There is a 34% chance you will go on to the second stage. In the second stage you may choose between the following two options.\n\n\n\nGamble\nPayout\nProbability\n\n\n\n\nE\n$2500\n33/34\n\n\nE\n$0\n1/34\n\n\nF\n$2400\n1\n\n\nF\n$0\n1/34\n\n\n\nExperiment 3 is mathematically equivalent to Experiment 2. That is, the probability distributions of the outcomes for E and C are the same, and the probability distributions of the outcomes for F and D are the same. In experiments, the most common pattern of choices on these problems is to prefer B, C, and F. There is an enormous literature on the Allais paradox and related ways in which people systematically violate the principles of expected utility theory. For more information see Wikipedia on Allais paradox.\n\n\nExercise 27.71 (Decisions) You are sponsoring a fund raising dinner for your favorite political candidate. There is uncertainty about the number of people who will attend (the random variable \\(X\\)), but based on past dinners, you think that the probability function looks like this:\n\n\n\n\\(x\\)\n100\n200\n300\n400\n500\n\n\n\n\n\\(P_X(x)\\)\n0.1\n0.2\n0.3\n0.2\n0.2\n\n\n\n\nCalculate \\(E(X)\\), the expected number of people who will attend.\nThe owner of the venue is going to charge you $1500 for rental and other miscellaneous costs. You know that you will make a profit (after per person costs) of $40 for each person attending. Calculate the expected profit after the rental cost.\nThe owner of the venue proposes an alternative pricing scheme. Instead of charging $1500, she will charge you either $5 per person or $2100, whichever is smaller. So if 100 people come, you only pay $500. If 500 come, you pay $2100. Calculate the expected profit under this scheme (still assuming $40 per plate profit before you pay the owner).\nLet \\(Y_1\\) be your profit under the first scheme and \\(Y_2\\) be your profit under the second. If you do the calculations, it turns out that the standard deviations of these profits are: \\[\\sigma_{Y_1} = 4996 \\qquad \\sigma_{Y_2} = 4488\\] Using the expected values calculated above explain which of the two scenarios you prefer.\n\nSolution:\n\n\\(E(X) = 100(.1) + 200(.2) + 300(.3) + 400(.2) +500(.2) = 320\\)\n\\(Y=40X-1500\\) \\(E(Y) = E(40X-1500) = 40 E(X) -1500  = 40(320) -1500 = 11300\\)\nProfit = \\(40X - \\min(2100, 5X)\\). Note that the profit formula is not linear (just like in the newspaper example). Consequently, you need to re-calculate profit for each of the five cases to get the expected value.\n\n\n\n\\(x\\)\n100\n200\n300\n400\n500\n\n\n\n\n\\(p_X(x)\\)\n0.1\n0.2\n0.3\n0.2\n0.2\n\n\ncosts\n500\n1000\n1500\n2000\n2100\n\n\ngross\n4000\n8000\n12000\n16000\n20000\n\n\nprofit = gross – costs\n3500\n7000\n10500\n14000\n17900\n\n\n\n\\(E(profit) = 3500(.1) + 7000(.2) + 10500(.3) + 14000(.2) + 17900(.2)\n= 11280\\)\nThe profit and standard deviation are larger in (b) than in (c). If you are risk averse, go for (c) (less uncertainty). If you are a gambler, go for (b) (higher average return). Since the difference in \\(\\sigma\\) is larger than in the mean.\n\n\n\nExercise 27.72 (Marjorie Visit) Marjorie is worried about whether it is safe to visit a vulnerable relative during a pandemic. She is considering whether to take an at-home test for the virus before visiting her relative. Assume the test has sensitivity 85% and specificity 92%. That is, the probability that the test will be positive is about 85% if an individual is infected with the virus, and the probability that test will be negative is about 92% if an individual is not infected.\nFurther, assume the following losses for Marjorie\n\n\n\nEvent\nLoss\n\n\n\n\nVisit relative, not infected\n0\n\n\nVisit relative, infected\n100\n\n\nDo not visit relative, not infected\n1\n\n\nDo not visit relative, infected\n5\n\n\n\n\nAssume that about 2 in every 1,000 persons in the population is currently infected. What is the posterior probability that an individual with a positive test has the disease?\nSuppose case counts have decreased substantially to about 15 in 100,000. What is the posterior probability that an individual with a positive test has the disease?\nSuppose Marjorie is deciding whether to visit her relative and if so whether to test for the disease before visiting. If the prior probability that Marjorie has the disease is 200 in 100,000, find the policy that minimizes expected loss. That is, given each of the possible test results, should Marjorie visit her relative? Find the EVSI. Repeat for a prior probability of 15 in 100,000. Discuss.\nFor the decision of whether Marjorie should visit her relative, find the range of prior probabilities for which taking the at-home test results in lower expected loss than ignoring or not taking the test (assuming the test is free). Discuss your results.\n\nSolution:\n\nFor a person in the given population, we have the following probabilities for the test outcome:\n\n\n\n\nCondition\nPositive\nNegative\n\n\n\n\nDisease present\n0.85\n0.15\n\n\nDisease absent\n0.08\n0.92\n\n\n\nPrior Probability: P(Disease present) = 0.002 and P(Disease absent)=0.998\nWe can calculate the posterior probability that the individual has the disease as follows:\nFirst, we calculate the probability of a positive test (this will be the denominator of Bayes Rule):\nP(Positive) = P(Positive | Present) * P(Present)+P(Positive | Absent) * P(Absent) = \\(0.85\\times 0.002 + 0.08\\times 0.998 = 0.08154\\)\nThen, we calculate the posterior probability that the individual is has the disease by applying Bayes rule:\nP(Present | Positive) = P(Positive | Present) * P(Present)/P(Positive) = 0.85 * 0.002/0.08154 = 0.0208\nThe posterior probability that an individual who tests positive has the disease is 0.0208.\n\nThe sensitivity and specificity of the test are the same as above:\n\n\n\n\nCondition\nPositive\nNegative\n\n\n\n\nDisease present\n0.85\n0.15\n\n\nDisease absent\n0.08\n0.92\n\n\n\nBut the prior probability is different:\nP(Disease present) = 0.00015 and P(Disease absent)=0.99985\nAgain, we calculate the probability of a positive test (this will be the denominator of Bayes Rule):\nP(Positive) = P(Positive | Present) * P(Present)+P(Positive | Absent) * P(Absent) = 0.85 * 0.00015 + 0.08 * 0.99985 = 0.0801155\nThen, we calculate the posterior probability that the individual is has the disease by applying Bayes rule:\nP(Present | Positive) = P(Positive | Present) * P(Present)/P(Positive) = 0.85 * 0.00015/0.0801155 = 0.00159\nThe posterior probability that an individual who tests positive has the disease is 0.00159.\nIn both cases, the posterior probability that the individual has the disease is small, even after taking the at-home test. But it is important to note that the probability has increased by more than a factor of 10, from 2 in 1000 to more than 2% in the first case, and from 15 in 100,000 to more than 15 in 10,000 in the second case. When the prior probability is lower, so is the posterior probability.\nSome students who are inexperienced with Bayesian reasoning are surprised at how small the posterior probability is after a positive test. Untrained people often expect the probability to be closer to 85%, the sensitivity of the test. I have found that a good way to explain these results to people who have trouble with it is to ask them to imagine a population of 100,000 people. For part a, we would expect 200 have the disease. Of these, we expect about 85%, or 170 people, to test positive, and 15%, or only 30 people, to test negative. So the test identified most of the ill people. But now let’s consider the 99,800 people who do not have the disease. We expect only 8% of these to test positive, but this is 7984 people testing positive. We expect 92%, or 91,816 people, to test negative. Of those who test positive, 170 have the disease and 7,984 do not. So even though most ill people test positive and most well people test negative, because there are so many more people who do not have the disease, most of the ones testing positive actually do not have the disease.\nA helpful way to visualize the role of the prior probability is to plot the posterior probability as a function of the prior probability (this was not required as part of the assignment). For prior probabilities near zero, a positive test increases the probability of the disease, but it remains very low because the positives are dominated by false positives. But once the prior probability reaches 10%, there is a better than even chance that someone testing positive has the disease; at a 20% prior probability, the posterior probability is over 70%.\n\np = seq(0.0001, 0.2, length.out = 100)\nposterior = 0.85*p/(0.85*p + 0.08*(1-p))\nplot(p, posterior, type = \"l\", xlab = \"Prior Probability\", ylab = \"Posterior Probability\")\n\n\n\n\n\n\n\n\n\nWe will consider two different prior probabilities:\n\nHigh prevalence: P(Present) = 0.002 and P( Absent ) = 0.998\nLow prevalence: P(Present) = 0.00015 and P( Absent ) = 0.99985\nTo calculate EVSI, we do the following:\nCalculate the expected loss if we do not do the test. To do this, we use the prior probability to calculate the expected loss if we visit and if we don’t. The minimum of these is the expected loss if we do not do the test.\nEL[visit] = 100p + 0(1-p)\nEL[no visit] = 5p + 1(1-p)\nHere, p is either 0.002 (part a) or 0.00015 (part b)\nCalculate the expected loss if we do the test as follows:\n\nAssume the test is positive. Using the posterior probability of disease, calculate the expected loss of visiting and not visiting. The minimum of these is the expected loss if the test is positive.\nAssume the test is negative. Using the posterior probability of disease, calculate the expected loss of visiting and not visiting. The minimum of these is the expected loss if the test is negative.\nUse the Law of Total Probability to find the probability of a positive test. (We already found this in Problems 1 and 2 - it’s the denominator of Bayes Rule.) Combine the results from a and b using the probability of disease as follows:\nE(L | test) = P(positive)E(L | positive)+P(negative)E(L | negative)\n\nNow, the EVSI is how much doing the test reduces our expected loss:\nEVSI = E(L | no test) - E(L | test)\nWe will now apply these steps for the two base rates. The table below shows the results.\n\n\n\n\n\n\n\n\n\nHigh prevalence\nLow prevalence\n\n\n\n\nProbability of positive test\n0.08154\n0.08012\n\n\nPosterior probability of disease given positive test\n0.0208\n0.00159\n\n\nPosterior probability of disease given negative test\n0.00033\n0.0000245\n\n\nExpected loss - no test\nVisit: 0.2 No visit: 1.008\nVisit: 0.015 No visit: 1.0006\n\n\nExpected loss if positive\nVisit: 2.0849 No visit: 1.0834\nVisit: 0.15915 No visit: 1.00637\n\n\nExpected loss if negative\nVisit: 0.03266 No visit: 1.00131\nVisit: 0.00245 No visit: 1.0001\n\n\nExpected loss with test\n0.11834\n0.015\n\n\nExpected loss - no test\nVisit: 0.2 No visit: 1.008\nVisit: 0.015 No visit: 1.0006\n\n\nEVSI\n0.2-0.11834 = 0.08166\n0.015-0.015 = 0\n\n\n\nIn the high prevalence case, it is better to test and follow the test result. The EVSI is 0.08166; there is 0.08166 lower loss from testing than from not testing. In the low prevalence case, however, the optimal decision is to visit no matter what the result of the test, because the posterior probability is so low even if the test is positive. Therefore, the expected loss is the same for doing or not doing the test because we ignore the test result and visit anyway. Therefore, the EVSI is zero in the low prevalence case.\n\nPrior Probability: P(Disease present) = p and P( Disease absent ) = 1- p\n\nIf we do not do the test, we cannot use the test result, so we have two choices: visit or do not visit. The expected loss of visiting is: E(L|V) = 100p + 0(1-p) = 100p\nThe expected loss of not visiting (lose 5 if ill, 1 if not ill): E(L|N) = 5p + (1-p)\nNow consider the strategy of doing the test and visiting if the test is negative. We get the following losses\n\n\n\nPatient status\nTest result\nAction\nLoss (L)\nProbability\n\n\n\n\nNo disease\nNegative\nvisit\n0\n(1-p)*0.92\n\n\nNo disease\nPositive\nno visit\n1\n(1-p)*0.08\n\n\nDisease\nNegative\nvisit\n100\np*0.15\n\n\nDisease\nPositive\nno visit\n5\np*0.85\n\n\n\nThe total expected loss of this strategy is:\nE(L|T) = 0 + 10.08(1-p) + 1000.15p + 5*0.85p = 0.08 + 19.17p\nFind strategy regions. With a little algebra, we can find where the lines intersect and then find the regions of optimality. These are:\n\n\n\n\n\n\n\nRange\nPolicy\n\n\n\n\n\\(0 \\le p &lt; 0.00099\\)\nOptimal policy is visit regardless of test\n\n\n\\(0.00099 &lt; p &lt; 0.0606\\)\nOptimal policy is to do the test and visit if negative\n\n\n\\(0.0606 &lt; p \\le 1\\)\nOptimal policy is to stay home regardless of test\n\n\n\n\n\n\n\nThe graph below shows the plots of expected loss for the three policies. I show results only for probabilities up to 0.1 in order to have a better view of the results for small probabilities.\n\np = seq(0.0001, 0.1, length.out = 100)\nELV = 100*p\nELN = 5*p + (1-p)\nELT = 0.08 + 19.17*p\nplot(p, ELV, type = \"l\", xlab = \"Prior Probability\", ylab = \"Expected Loss\", col = \"red\", lwd = 2)\nlines(p, ELN, col = \"blue\", lwd = 2)\nlines(p, ELT, col = \"green\", lwd = 2)\nlegend(\"topright\", legend = c(\"Visit\", \"No Visit\", \"Test and Visit if Negative\"), col = c(\"red\", \"blue\", \"green\"), lwd = 2)\n\n\n\n\n\n\n\n\nTherefore, in the range of values 0.00099 &lt; p &lt; 0.0606, doing the test has lower expected loss than either just visiting or just staying home. For the high prevalence situation, the prior probability of 0.002 is larger than 0.00099 and lower than 0.0606, so it is optimal to take the test and follow the result. For the low prevalence situation, the prior probability of 0.00015 is below 0.00099, and it is optimal just to visit without considering the test. That is, in very low prevalence situations, there is no point in doing the test, because even if it is positive, the posterior probability would still be low enough that it is optimal to visit. On the other hand, if Marjorie is experiencing symptoms or knows she has been exposed, that would increase the probability of having the disease. Even in a low prevalence situation, if symptoms or exposure increase the prior probability to above 0.00099 but below 0.0606, then she should take the test and follow the result. If symptoms or exposure increase the prior probability to above 0.0606, then she should not bother with the test and just stay home.\n\n\nExercise 27.73 (True/False Variance)  \n\nIf the sample covariance between two variables is one, then there must be a strong linear relationship between the variables\nIf the sample covariance between two variables is zero, then the variables are independent.\nIf \\(X\\) and \\(Y\\) are independent random variables, then \\(Var(2X-Y)= 2 Var(X)-Var(Y)\\).\nThe sample variance is unaffected by outlying observations.\nSuppose that a random variable \\(X\\) can take the values \\(\\{0,1,2\\}\\) all with equal probability. Then the expected and variance of \\(X\\) are both\\(1\\).\nThe maximum correlation is \\(1\\) and the minimum is \\(0\\).\nFor independent random variables \\(X\\) and \\(Y\\), we have \\(var(X-Y)=var(X)-var(Y)\\).\nIf the correlation between \\(X\\) and \\(Y\\) is zero then the standard deviation of \\(X+Y\\) is the square root of the sum of the standard deviations of \\(X\\) and \\(Y\\).\nIt is always true that the standard deviation is less than the variance\nIf the correlation between \\(X\\) and \\(Y\\) is \\(r = - 0.81\\) and if the standard deviations are \\(s_X = 20\\) and \\(s_Y = 25\\), respectively, then the covariance is \\(Cov (X, Y) = - 401\\).\nIf we drop the largest observation from a sample, then the sample mean and variance will both be reduced.\nSuppose \\(X\\) and \\(Y\\) are independent random variables and \\(Var(X) = 6\\) and \\(Var(Y) = 6\\). Then \\(Var(X+Y) = Var(2X)\\).\nLet investment \\(X\\) have mean return 5% and a standard deviation of 5%and investment \\(Y\\) have a mean return of 10% with a standard deviation of 6%. Suppose that the correlation between returns is zero. Then I can find a portfolio with higher mean and lower variance then \\(X\\).\n\nSolution:\n\nFalse. \\(Cov(X,Y) = Corr(X,Y) \\sqrt{Var(X)*Var(Y)}\\), so knowing \\(Cov(X,Y) = 1\\) doesn’t inform you about the correlation.\nFalse. This is only a special case for normal distribution.\nFalse. \\(Var(2X-Y) = 4Var(X) + Var(Y)\\)\nFalse. We have the following formula for the sample variance: \\[\\begin{aligned}\\hat{\\sigma}^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar{x})^2\\end{aligned}\\]If \\(x_i\\) is large relative to \\(\\bar{x}\\) then it has an undue influence.\nFalse \\[E[X] = \\frac{1}{3}*0+\\frac{1}{3}*1+\\frac{1}{3}*2=1\\]\\[Var[X] = \\frac{1}{3}(0-1)^2+\\frac{1}{3}(1-1)^2+\\frac{1}{3}(2-1)^2\\]\\[=\\frac{1}{3}+0+\\frac{1}{3}=\\frac{2}{3}\\]\nFalse, the maximum is 1 and the minimum in -1.\nFalse. \\(Var(X-Y) = Var(X) + Var(Y)\\)\nFalse. Using the plug in rule, the standard deviation of \\(X + Y\\) is the square root of the sum of the variances of \\(X\\) and \\(Y\\)\nFalse. \\(s_X \\geq Var(X)\\) if the \\(Var (X ) \\leq 1\\)\nFalse. Using the formula relating covariance and correlation, namely\\(Cov( X, Y ) = r s_X s_Y\\) gives\\(Cov ( X,Y) = - 0.81 \\times 20 \\times 25 = - 405.\\)\nFalse. For example, \\(100, 101, 102\\) has mean \\(101\\) and variance \\(0.333\\),but \\(100, 101\\) has mean \\(100.5\\) and variance \\(0.5\\)\nFalse. First, \\(Var(X+Y)=Var(X)+Var(Y)\\) (as \\(Cov(X,Y)=0)\\) and so \\(Var(X+Y)=2 Var(X)\\). Secondly \\(Var(2X) = 4 Var(X)\\)\nTrue. For example, consider the portfolio \\(P=  0.5 X + 0.5 Y\\). This has expected return \\(\\mu_P = 0.5\\times 5 + 0.5 \\times 0.10 = 7.5 \\%\\). As the correlation is zero, its variance is given by \\(\\sigma_P^2 =0.5^2 \\times 0.0025 + 0.5^2  0.0036 = 0.001525 &lt; Var(X)\\)\n\n\n\nExercise 27.74 (True/False Expectation)  \n\nLeBron James makes \\(85\\)% of his free throw attempts and \\(50\\)% of his regular shots from the field (field goals). Suppose that each shot is independent of the others. He takes \\(20\\) field goals and \\(10\\) free throws in a typical game. He gets one point for each free throw and two points for each field goal assuming no 3-point shots. The number of points he expects to score in a game is 28.5.\nSuppose that you have a one in a hundred chance of hitting the jackpot on a slot machine. If you play the machine \\(100\\) times then you are certain to win.\nThe expected value of the sample mean is the population mean, that is \\(E \\left ( \\bar{X} \\right ) = \\mu\\).\nThe expectation of \\(X\\) minus \\(2Y\\) is just the expectation of \\(X\\) minus twice the expectation of \\(Y\\), that is \\(E (X-2Y)= E(X) - 2E (Y)\\).\nA firm believes it has a 50-50 chance of winning a $80,000 contract if it spends $5,000 on a proposal. If the firm spends twice this amount,it feels its chances of winning improve to 60%. If the firm wants to maximize its expected value then it should spend $10,000 to try and gain the contract.\n\\(E(X+Y)=E(X)+E(Y)\\) only if the random variables \\(X\\) and \\(Y\\) are independent.\n\nSolution:\n\nTrue.\\[E(points) = 2E(\\# FG made) + 1E(\\#FT made) = 2 \\times 20 \\times 0.5+1 \\times 10 \\times 0.85 = 28.5\\]This holds even if FG and FT are dependent\nFalse. The expected waiting time until you hit the jackpot is 100 times,but since each outcome of the game is independent of all previous outcomes, there is no guarantee about the number of plays until a jackpot it hit\nTrue. The expected value of the sample does equal the true value\nTrue. The plug-in rule states that \\(E(aX+bY)=aE(X)+bE(Y)\\). Plug in \\(A=1\\)and \\(B =-2\\). Hence \\(E(X-2Y)=E(X)+E(-2Y)=E(X)-2E(Y)\\)\nTrue. \\(E(spend \\$5k) = 0.5\\cdot\\$80000 - 5000 = 35000\\). \\(E(spend \\$10k) = 0.6\\cdot 80000 - 10000 = 38000\\)\nFalse. By the plug-in rule, this relation holds irrespective of whether X and Y are.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "ex.html#bayesian-parameter-learning",
    "href": "ex.html#bayesian-parameter-learning",
    "title": "27  Exercises",
    "section": "27.4 Bayesian Parameter Learning",
    "text": "27.4 Bayesian Parameter Learning\n\nExercise 27.75 (Beta-Binomial for Allais gambles) We’ve collected data on people’s preferences in the two Allais gambles from. For this problem, we will assume that responses are independent and identically distributed, and the probability is \\(\\theta\\) that a person chooses both B in the first gamble and C in the second gamble.\n\nAssume that the prior distribution for \\(\\theta\\) is Beta(1, 3). Find the prior mean and standard deviation for \\(\\theta\\). Find a 95% symmetric tail area credible interval for the prior probability that a person would choose B and C. Do you think this is a reasonable prior distribution to use for this problem? Why or why not?\nIn 2009, 19 out of 47 respondents chose B and C. Find the posterior distribution for the probability \\(\\theta\\) that a person in this population would choose B and C.\nFind the posterior mean and standard deviation. Find a 95% symmetric tail area credible interval for \\(\\theta\\).\nMake a triplot of the prior distribution, normalized likelihood, and posterior distribution.\nComment on your results.\n\nSolution:\nPart a. It is convenient to use a prior distribution in the Beta family because it is conjugate to the Binomial likelihood function, which simplifies Bayesian updating. A plot of the prior density function is shown below.\n\ncurve(dbeta(x,1,3),0,1)\n\n\n\n\n\n\n\n\nThe prior distribution has mean a/(a+b) = 0.25 and standard deviation \\[\n\\sqrt{\\dfrac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}} = 0.194.\n\\] An expected value of 0.25 would be reasonable if we believed that people were choosing randomly among the four options A&C, B&C, A&D, and B&D. In fact, a Beta(1,3) distribution is the marginal distribution we would obtain if we placed a uniform distribution on all probability vectors \\((p_1, p_2, p_3, p_4)\\) on the four categories (i.e. a uniform distribution on four non-negative numbers that sum to 1). In other words, this is the prior distribution we might assign if all we knew was that there were four possible responses, and knew nothing about the relative likelihoods of the four responses. However, we actually do know something about the relative probabilities of the categories. According to the research literature, people choose B&C more often than 25% of the time. The center of the prior distribution does not reflect this knowledge. But the prior distribution does show a very high degree of uncertainty about the probability. A 95% symmetric tail area credible interval for the prior distribution of \\(\\Theta\\) is [0.008, 0.708]. The virtual count is small in relation to the sample size, so the data will have a large impact on the posterior distribution relative to the prior. Therefore, we won’t go too far wrong using this prior distribution, even though we do have knowledge that suggests the prior mean is likely to be higher than 0.25.\nPart b. We will use the Binomial/Beta conjugate pair. The prior distribution is a member of the conjugate Beta(a, b) family, with a=1 and b=3. Therefore, the posterior distribution is a Beta(a, b) distribution, with a=a+x= 1+19 = 20, and b=b+n-x = 3+28 = 31. The posterior distribution is shown below.\n\ncurve(dbeta(x,20,31),0,1)\n\n\n\n\n\n\n\n\nPart c. The posterior mean is a/(a+b*) = 20/51 = 0.392. The posterior standard deviation is \\[\n\\sqrt{\\dfrac{a^*b^*}{(a^*+b^*)^2(a^*+b^*+1)}} = 0.077.\n\\] A 95% symmetric tail area credible interval for the posterior distribution of \\(\\Theta\\) is [0.248, 0.536].\n\nqbeta(c(0.025,0.975),20,31)\n\n 0.26 0.53\n\n\nPart d. The triplot is shown below.\n\ncurve(dbeta(x,1,3),0,1,ylab=\"Density\",lty=2,xlim=c(0,1),ylim=c(0,6))\ncurve(dbeta(x,20,31),0,1,add=TRUE)\ncurve(dbeta(x,19,47-19),0,1,add=TRUE)       # Normalized Likelihood is Beta(x,n-x)\n\n\n\n\n\n\n\n\nThe triplot shows the prior, normalized likelihood, and posterior distributions. The normalized likelihood is a Beta(19,47-19) density function. The posterior distribution is focused on slightly smaller values and is slightly narrower than the normalized likelihood. This is because the prior mean is smaller than the sample mean, and the posterior distribution has slightly more information than the prior distribution.\n\n\nExercise 27.76 (Poisson for Car Counts) Times were recorded at which vehicles passed a fixed point on the M1 motorway in Bedfordshire, England on March 23, 1985.2 The total time was broken into 21 intervals of length 15 seconds. The number of cars passing in each interval was counted. The result was:\n\ncnt = c(2, 2, 1, 1, 0, 4, 3, 0, 2, 1, 1, 1, 4, 0, 2, 2, 3, 2, 4, 3, 2)\n\nThis can be summarized in the following table, that shows 3 intervals with zero cars, 5 intervals with 1 car, 7 intervals with 2 cars, 3 intervals with 3 cars and 3 intervals with 4 cars.\n\ntable(cnt)\n\ncnt\n0 1 2 3 4 \n3 5 7 3 3 \n\n\n\nDo you think a Poisson distribution provides a good model for the count data? Justify your answer.\nAssume that \\(\\Lambda\\), the rate parameter of the Poisson distribution for counts (and the inverse of the mean of the exponential distribution for inter arrival times), has a discrete uniform prior distribution on 20 equally spaced values between \\((0.2, 0.4,\\ldots 3.8, 4.0)\\) cars per 15-second interval. Find the posterior distribution of \\(\\Lambda\\).\nFind the posterior mean and standard deviation of \\(\\Lambda\\).\nDiscuss what your results mean in terms of traffic on this motorway.\n\nSolution:\n\nA Poisson distribution is a good model to consider for the arrival counts because:\n\nThe data consist of counts of discrete events.\nIt seems reasonable that the events would be independent.\nIt seems reasonable that the rate of occurrence would remain constant over the 4+ minutes that the sample was taken.\n\n\nThese are reasons to consider a Poisson distribution, but we still need to investigate how well a Poisson distribution fits the observations. We will do this by comparing the sample frequencies with Poisson probability mass function. To do this, we need to estimate the mean of the distribution. We calculate the sample mean by summing these observations (40 total cars passing) and dividing by the number of intervals (21), to obtain an estimated arrival rate of about 1.9 cars per 15 second time interval.\nNext, we use the Poisson probability mass function \\(f(x|\\Lambda=1.9) = e^{-1.9} (1.9)^x / x!\\) to estimate the probability of each number of cars in a single interval, and multiply by 21 to find the expected number of times each value occurs (Note: for the 5 or more entry, we use \\(1-F(4|\\Lambda=1.9)\\), or 1 minus the Poisson cdf at 4. The probability of more than 5 cars in 15 seconds is very small.) The expected counts are:\n\nd = data.frame(cars=0:5,sample=c(3,5,7,3,3,0),expected=c(21*dpois(0:4,1.9),ppois(4,1.9)))\nprint(d)\n\n  cars sample expected\n1    0      3     3.14\n2    1      5     5.97\n3    2      7     5.67\n4    3      3     3.59\n5    4      3     1.71\n6    5      0     0.96\n\n\nThe values in this table seem to show fairly good agreement. We can do a visual inspection by drawing a bar chart, shown below. The bar chart shows good agreement between the empirical and the Poisson probabilities.\n\nbarplot(height = t(as.matrix(d[,2:3])),beside=TRUE,legend=TRUE)\n\n\n\n\n\n\n\n\nWe can also perform a Pearson chi-square test (this is not required). The test statistic is calculated as: \\[\n\\chi^2 =\\sum_i (Y_i -E_i)^2/E_i\n\\] where \\(Y_i\\) is the observed count for i cars passing, and \\(E_i\\) is the expected number of instances of i cars passing in a sample of size 21. The chi-square test should not be applied when counts are very small. A common (and conservative) rule of thumb is to avoid using the chi-square test if any cell has an expected count less than 5. Other less conservative rules have been proposed.4 We will combine the last two categories to increase the expected count to 2.37. To find the expected counts in this last category, we assign it probability 1 minus the cdf at x=3, and multiply by 21. The observed and expected counts are:\n\nsum(((d$sample-d$expected)^2/d$expected)[1:4]) + (3-2.65)^2/2.65\n\n 0.62\n\n\nNext, we find the degrees of freedom, n - p -1, where n is the number of categories and p is the number of parameters estimated. We have 5 categories, and we estimated the mean of the Poisson distribution, so the degrees of freedom are 5 - 1 - 1, or 3. Therefore, we compare our test statistic 0.616 against the critical value of the chi-square distribution with 3 degrees of freedom. The 95th percentile for the chi- square distribution with 3 degrees of freedom is 7.8.\n\nqchisq(0.95,3)\n\n 7.8\n\n\nSome students have tried a Q-Q plot of the Poisson quantiles against the data quantiles. Because there are only 4 distinct values in the sample, the plot shows most of the dots on top of each other. Comparing empirical and theoretical frequencies to is much more informative for discrete distributions with very few values.\nWe can also compare the sample mean and sample variance of the observations, recalling that the mean and variance of the Poisson distribution are the same. The sample mean of the observations is 1.90 and the sample variance is 1.59. The standard deviation of the sample mean is approximately equal to the sample standard deviation divided by the square root of the sample size, or \\(\\sqrt{1.59/21} = 0.275\\). Therefore, the CI is\n\n1.90 + c(-1,1)*0.275*qt(0.975,20)\n\n 1.3 2.5\n\n\nThe observations seem consistent with the hypothesis that the mean and standard deviation are equal.\nYou did not need to do everything described in this solution. If you gave a thoughtful argument for whether the Poisson distribution was a good model, and if you used the data in a reasonable way to evaluate the fit of the distribution, you would receive credit for this problem.\n\nTo find the posterior distribution, we calculate the Poisson likelihood at each of the 20 lambda values, and multiply by the prior probability of 1/20. Then we divide each of these values by their sum. The formula is: \\[\np(\\lambda|X) = \\frac{f(X|\\lambda)g(\\lambda)}{\\sum_{i=1}^{20} f(X|\\lambda_i)g(\\lambda_i)}\n\\] We use the constant prior of \\(g(\\lambda) = 1/20\\) factors out of both the numerator and denominator, so it is unnecessary to include it. In fact, when the prior distribution is uniform, the posterior distribution is the same as the normalized likelihood.\n\nThe posterior pmf is shown in the plot below.\n\nlambda = seq(0,4,0.2)\nlikelihood=function(lambda) prod(dpois(cnt,lambda))\nl = sapply(lambda,likelihood)\nposterior = l/sum(l)\nbarplot(posterior,names.arg=lambda,xlab=\"lambda\",ylab=\"posterior probability\")\n\n\n\n\n\n\n\n\n\n\n\n\nm = sum(posterior*lambda)\nprint(m)\n\n 2\n\nsqrt(sum(posterior*(lambda-m)^2))\n\n 0.3\n\n\n\nFrom this analysis, we see that a Poisson distribution provides a good model for counts of cars passing by this point on the roadway. The rate is approximately 2 cars per 15 seconds. There is uncertainty about this rate. There is a 95% chance the rate is less than or equal to 2.4, and there is about a 98% probability that the rate lies between 1.4 and 2.6 cars per second. If we gathered more data, we would gain more information about the rate, which would narrow the range of uncertainty.\n\n\n\nExercise 27.77 (Car Count Part 2) This problem continues analysis of the automobile traffic data. As before, assume that counts of cars per 15-second interval are independent and identically distributed Poisson random variables with unknown mean \\(\\Lambda\\).\n\nAssume that \\(\\Lambda\\), the rate parameter of the Poisson distribution for counts, has a continuous gamma prior distribution for \\(\\Lambda\\) with shape 1 and scale 10e6. (The gamma distribution with shape 1 tends to a uniform distribution as the scale tends to \\(\\infty\\), so this prior distribution is “almost” uniform.) Find the posterior distribution of \\(\\Lambda\\). State the distribution type and hyperparameters.\nFind the posterior mean and standard deviation of \\(\\Lambda\\). Compare your results to Part I. Discuss.\nFind a 95% symmetric tail area posterior credible interval for \\(\\Lambda\\). Find a 95% symmetric tail area posterior credible interval for \\(\\theta\\), the mean time between vehicle arrivals.\nFind the predictive distribution for the number of cars passing in the next minute. Name the family of distributions and the parameters of the predictive distribution. Find the mean and standard deviation of the predictive distribution. Find the probability that more than 10 cars will pass in the next minute. (Hint: one minute is four 15-second time intervals.)\n\nSolution:\n\nWe observed 40 cars passing in \\(n\\)=21 15-second intervals.\n\n\nsum(cnt)\n\n 40\n\n\nThe posterior distribution is a gamma distribution with shape 1 + 40 = 41\n\n1+sum(cnt)\n\n 41\n\n\nand scale\n\nn = length(cnt)\n1/(1e-6 + n)\n\n 0.048\n\n\nThe posterior distribution is \\[\n\\Lambda|X \\sim \\text{Gamma}(41,0.04761905)\n\\]\n\ncurve(dgamma(x,shape=41,scale=0.04761905),from=0,to=5)\n\n\n\n\n\n\n\n\n\nThe posterior mean is 41/21=1.952, and the posterior standard deviation is \\(\\sqrt{41}/21 = 0.304\\). Let’s compare\n\n\n\n\n\nmean\nsd\nmode\nmedian\n95% CI\n\n\n\n\nDiscretized\n1.95\n0.305\n2\n2\n[1.4, 2.6]\n\n\nContinuous\n1.95\n0.305\n1.9\n1.94\n[1.40, 2.59]\n\n\n\nThese values are almost identical. The slight differences in the median and mode are clearly artifacts of discretization. The discretized analysis gives a good approximation to the continuous analysis.\n\nThe 95% symmetric tail area posterior credible interval for \\(\\Lambda\\) is\n\n\nqgamma(c(0.025,0.975),shape=41,scale=0.04761905)\n\n 1.4 2.6\n\n\nand for \\(\\Theta\\) is\n\n1/qgamma(c(0.975,0.025),shape=41,scale=0.04761905)\n\n 0.39 0.71\n\n\n\nThe Poisson / Gamma predictive distribution is a negative binomial distribution. For this problem, we predict the number of cars in the next four time periods (1 minute is four 15-second blocks). The predictive distribution is negative binomial with the parameters:\n\n\nsize a= 41\nprob p = 1/(1+4b) = 1/(1+4/21) = 0.84\n\nThe mean is \\(nab = 4\\cdot 41\\cdot 0.84 = 7.81\\) and standard deviation of the predictive distribution is \\[\n\\sqrt{na \\dfrac{1-p}{p^2}} = \\sqrt{4\\cdot 41 \\cdot \\dfrac{1-0.84}{0.84^2}} = 2.83\n\\] The probabilities, as computed by the R dnbinom function, are shown in the table. Results from the Poisson probability mass function with mean \\(4ab = 7.81\\) are shown for comparison.\n\nk = 0:20\np = dnbinom(k,size=41,prob=1/(1+4/21))\np = p/sum(p)\np = round(p,3)\np = c(p,1-sum(p))\npoisson = round(dpois(k,7.81),3)\npoisson = c(poisson,1-sum(poisson))\nd = cbind(k,p,poisson)\nprint(d)\n\n       k      p poisson\n [1,]  0  0.001   0.000\n [2,]  1  0.005   0.003\n [3,]  2  0.017   0.012\n [4,]  3  0.040   0.032\n [5,]  4  0.070   0.063\n [6,]  5  0.101   0.098\n [7,]  6  0.124   0.128\n [8,]  7  0.133   0.143\n [9,]  8  0.127   0.139\n[10,]  9  0.111   0.121\n[11,] 10  0.089   0.094\n[12,] 11  0.066   0.067\n[13,] 12  0.046   0.044\n[14,] 13  0.030   0.026\n[15,] 14  0.018   0.015\n[16,] 15  0.011   0.008\n[17,] 16  0.006   0.004\n[18,] 17  0.003   0.002\n[19,] 18  0.002   0.001\n[20,] 19  0.001   0.000\n[21,] 20  0.000   0.000\n[22,]  0 -0.001   0.000\n\nbarplot(t(d[,2:3]),beside=TRUE,names.arg=d[,1],xlab=\"k\",ylab=\"probability\")\nlegend(\"topright\",c(\"negative binomial\",\"poisson\"),fill=c(\"black\",\"white\"))\n\n\n\n\n\n\n\n\nThe difference between the Bayesian predictive distribution and the Poisson distribution using a point estimate of the rate is even more pronounced if we predict for the next 21 time periods (5 1/4 minutes). This was not required, but is interesting to examine. Again, the distribution is negative binomial with: size \\(a = 41\\), and probability \\(p = 1/(1 + 21b) = 0.5\\)\n\nk = 0:65\np = dnbinom(k,size=41,prob=0.5)\np = p/sum(p)\np = c(p,1-sum(p))\npoisson = dpois(k,21*41/21)\npoisson = c(poisson,1-sum(poisson))\nd = cbind(k,p,poisson)\nbarplot(t(d[,2:3]),beside=TRUE,names.arg=d[,1],xlab=\"k\",ylab=\"probability\")\nlegend(\"topright\",c(\"negative binomial\",\"poisson\"),fill=c(\"black\",\"white\"))\n\n\n\n\n\n\n\n\nTo summarize, the predictive distribution is negative binomial with size \\(a = 41\\), and probability \\(p = 1/(1 + nb)\\) where \\(n\\) is the number of periods into the future we are predicting. The uncertainty in \\(\\Lambda\\) makes the predictive distribution more spread out than the Poisson distribution with the same mean. When n=1, the Poisson and negative binomial probabilities are very similar. The differences become more pronounced as we try to predict further into the future.\n\n\nExercise 27.78 (Lung disease) Chronic obstructive pulmonary disease (COPD) is a common lung disease characterized by difficulty in breathing. A substantial proportion of COPD patients admitted to emergency medical facilities are released as outpatients. A randomized, double-blind, placebo-controlled study examined the incidence of relapse in COPD patients released as outpatients as a function of whether the patients received treatment with corticosteroids. A total of 147 patients were enrolled in the study and were randomly assigned to treatment or placebo group on discharge from an emergency facility. Seven patients were lost from the study prior to follow-up. For the remaining 140 patients, the table below summarizes the primary outcome of the study, relapse within 30 days of discharge.\n\n\n\n\nRelapse\nNo Relapse\nTotal\n\n\n\n\nTreatment\n19\n51\n70\n\n\nPlacebo\n30\n40\n70\n\n\nTotal\n49\n91\n140\n\n\n\n\n\n\n\n\n\n\nLet \\(Y_1\\) and \\(Y_2\\) be the number of patients who relapse in the treatment and placebo groups, respectively. Assume \\(Y_1\\) and \\(Y_2\\) are independent Binomial(70,\\(\\theta_i\\) ) distributions, for \\(i=1,2\\). Assume \\(\\theta_1\\) and \\(\\theta_2\\) have independent Beta prior distributions with shape parameters 1/2 and 1/2 (this is the Jeffreys prior distribution). Find the joint posterior distribution for \\(\\theta_1\\) and \\(\\theta_2\\). Name the distribution type and its hyperparameters.\nGenerate 5000 random pairs \\((\\theta_1, \\theta_2)\\), \\(k=1,\\ldots,5000\\) from the joint posterior distribution. Use this random sample to estimate the posterior probability that the rate of relapse is lower for treatment than for placebo. Discuss your results.\n\nSolution:\n\nThe prior distribution for each condition is \\(Beta(1/2,1/2)\\). Therefore, the posterior distributions for \\(\\theta_1\\) and \\(\\theta_2\\) are Beta distributions with parameters and quantiles as shown below.\n\n\n# Study results\nk.trt=19    # Number of relapses in treatment group\nn.trt=70    # Number of patients in treatment group\nk.ctl=30    # Number of relapses in control group\nn.ctl=70    # Number of patients in control group\n\n#Prior hyperparameters\nalpha.0 =0.5\nbeta.0=0.5\n\n# The joint posterior distribution is the product of two independent beta\n# distributions with posterior hyperparameters as shown below\nalpha.trt=alpha.0 + k.trt\nbeta.trt=beta.0 + n.trt-k.trt\nalpha.ctl=alpha.0 + k.ctl\nbeta.ctl=beta.0 + n.ctl-k.ctl\n\n# Posterior credible intervals\ntrt.intrv=qbeta(c(0.025,0.975),alpha.trt,beta.trt)\nctl.intrv=qbeta(c(0.025,0.975),alpha.ctl,beta.ctl)\n\nd = data.frame(treatment=c(alpha.trt,beta.trt,trt.intrv), control=c(alpha.ctl,beta.ctl,ctl.intrv), row.names=c(\"alpha\",\"beta\",\"2.5%\",\"97.5%\"))\nknitr::kable(t(d))\n\n\n\n\n\nalpha\nbeta\n2.5%\n97.5%\n\n\n\n\ntreatment\n20\n52\n0.18\n0.38\n\n\ncontrol\n30\n40\n0.32\n0.55\n\n\n\n\n\nIt is important to remember that the joint density or mass function is a function of all the random variables. Because \\(\\theta_1\\) and \\(\\theta_2\\) are independent and the \\(Y_i\\) are independent conditional on the \\(\\theta_i\\), the two posterior distributions are independent. We can see this by writing out the joint distribution of the parameters and observations: \\[\nBin(x_1|\\theta_1,70)Bin(x_2|\\theta_2,70)Be(\\theta_1|1,1)Be(\\theta_2|1,1) = {70 \\choose 19}\\theta_1^{19}(1-\\theta_1)^{51}{70 \\choose 30}\\theta_2^{30}(1-\\theta_2)^{40}\n\\] This is proportional to the product of two Beta density functions, the first with parameters (19.5,51.5), and the second with parameters (30.5,40.5).\nConditional on the observations, \\(\\theta_1\\) and \\(\\theta_2\\) are independent Beta random variables. The marginal distribution of \\(\\theta_1\\) is Beta(19.5, 51.5) and the marginal distribution of \\(\\theta_1\\) is Beta(30.5, 40.5).\nThe joint density function is the product of the individual density functions (remember that if \\(n\\) is an integer, \\(n! = \\Gamma(n+1))\\) and \\(B(\\alpha, \\beta) = \\Gamma(\\alpha)\\Gamma(\\beta)/\\Gamma(\\alpha+\\beta)\\) \\[\nBe(\\theta_1|20,52)Be(\\theta_2\\mid 31,41) = \\dfrac{\\Gamma(20+52)}{\\Gamma(20)\\Gamma(52)}\\theta_1^{19}(1-\\theta_1)^{51}\\dfrac{\\Gamma(31+41)}{\\Gamma(31)\\Gamma(41)}\\theta_2^{30}(1-\\theta_2)^{40}\n\\] A plot of the two posterior density functions is shown below. The 95% credible intervals are shown on the plot as vertical lines\n\ncurve(dbeta(x,19.5,51.5),0,0.6,ylab=\"Density\",lty=2,ylim=c(0,8))\ncurve(dbeta(x,30.5,40.5),0,0.6,add=TRUE)\nabline(v=trt.intrv,lty=2, lwd=0.5)\nabline(v=ctl.intrv,lty=2, lwd=0.5)\n\n\n\n\n\n\n\n\nTo visualize the joint density function of \\(\\theta_1\\) and \\(\\theta_2\\), we can do a surface or contour plot (you were not required to do this). The joint density function is a function of two variables, \\(\\theta_1\\) and \\(\\theta_2\\). To visualize the joint density function, we need either a surface plot or a contour plot (you were not required to do this). A surface plot is shown below.\n\ngs&lt;-200        # grid size\ntheta.t&lt;-seq(0.25,0.6,length=gs)   # treatment mean\ntheta.c&lt;-seq(0.1,0.45 ,length=gs)  # control mean\n\ndens&lt;-matrix(0,gs,gs)             # density function\nfor(i in 1:gs) {\n  dens[i,]=dbeta(theta.t[i],alpha.trt,beta.trt) * dbeta(theta.c,alpha.ctl,beta.ctl)\n}\n\n# Perspective plot\n\npersp(theta.t,theta.c,dens,theta=30,phi=5,xlab=\"Treatment Relapse Prob\",\n      ylab=\"Control Relapse Prob\",zlab=\"Probability Density\",\n      xlim=theta.t[c(1,gs)] ,ylim=theta.c[c(1,gs)],\n      main=\"3D Plot of Joint Density for Treatment and Control Relapse Probs\")\n\n\n\n\n\n\n\n\nThe posterior mean relapse probability for the treatment group is 19.5/71x100% = 27.5%. A 95% symmetric tail area credible interval is [17.8%, 38.3%]. The posterior mean relapse probability for the control group is 30.5/71x100% = 43.0%. A 95% symmetric tail area credible interval is [31.7%, 54.5%].\nAbout 1/4 of the treatment patients relapsed, whereas almost half the control patients relapsed. Most of the posterior density function for the treatment group lies below most of the posterior density function for the control group, but the 95% credible intervals for the two parameters overlap somewhat (you were not required to compute credible intervals). While this analysis suggests that treatment lowers the relapse probability, it does not provide a statistical basis for concluding that it does.\n\nThe joint posterior distribution is Beta(19.5,51.5)Beta(30.5, 40.5). That is, \\(\\theta_1\\) and \\(\\theta_2\\) are independent Beta random variables with shape parameters as given in the table above. To sample from this joint distribution, we can draw 5000 random samples from each of these Beta distributions\n\n\nset.seed(92) # Kuzy\nnsample=5000\ncontr = rbeta(nsample,19.5,51.5)\ntreat = rbeta(nsample,30.5,40.5)\n\ndiff=treat-contr     # Estimate of difference in relapse probs\nefficacious = diff&gt;0  # If treat &gt; control\nprob.efficacious=sum(efficacious)/nsample\nsd.probefficacious = sd(efficacious)/sqrt(nsample)\n\n#Posterior quantiles, mean and standard deviation of the difference in \n#relapse rates between treatment and control\nquantile(diff,c(0.05,0.5,0.95))\n\n   5%   50%   95% \n0.021 0.156 0.287 \n\nmean(diff)\n\n 0.15\n\nsd(diff)\n\n 0.08\n\n#Density Function for Difference in Treatment and Control Relapse Probabilities\nplot(density(diff),main=\"Kernel Density Estimator\",xlab=\"Difference in Treatment and Control Probabilities\",ylab=\"Posterior Probability Density\")\n\n\n\n\n\n\n\n\nThe analysis above provides evidence that there is a lower relapse probability in the treatment than in the control group. We estimate a 97.2% probability that the treatment has a lower relapse probability. The mean of the posterior distribution of the difference in relapse probabilities is approximately 0.155, and an approximate 90% credible interval for the difference in relapse rates is [0.021, 0.287].\n\n\nExercise 27.79 (Normal-Normal) Concentrations of the pollutants aldrin and hexachlorobenzene (HCB) in nanograms per liter were measured in ten surface water samples, ten mid-depth water samples, and ten bottom samples from the Wolf River in Tennessee. The samples were taken downstream from an abandoned dump site previously used by the pesticide industry. The full data set can be found at http://www.biostat.umn.edu/~lynn/iid/wolf.river.dat. For this problem, we consider only HCB measurements taken at the bottom and the surface. The question of interest is whether the distribution of HCB concentration depends on the depth at which the measurement was taken. The data for this problem are given below.\n\n\n\nSurface\nBottom\n\n\n\n\n3.74\n5.44\n\n\n4.61\n6.88\n\n\n4.00\n5.37\n\n\n4.67\n5.44\n\n\n4.87\n5.03\n\n\n5.12\n6.48\n\n\n4.52\n3.89\n\n\n5.29\n5.85\n\n\n5.74\n6.85\n\n\n5.48\n7.16\n\n\n\nAssume the observations are independent normal random variables with unknown depth-specific means \\(\\theta_s\\) and \\(\\theta_b\\) and precisions \\(\\rho_s  = 1/\\sigma^2_s\\) and \\(\\rho_b = 1/\\sigma_s^2\\). Assume independent improper reference priors for the surface and bottom parameters: \\[\ng(\\theta_s,\\theta_b ,\\rho_s,\\rho_b ) = g(\\theta_s,\\rho_s)g(\\theta_b ,\\rho_b) \\propto \\rho_s^{-1}\\rho_b^{-1}.\n\\]\n\nThis prior can be treated as the product of two normal-gamma priors with \\(\\mu_s = \\mu_b = 0\\), \\(\\sigma_s \\rightarrow 0\\) and \\(\\sigma_b \\rightarrow 0\\), \\(a_s = a_b = -1/2\\), and \\(b_s = b_b \\rightarrow 0\\). (These are not valid normal-gamma distributions, but you can use the usual Bayesian conjugate updating rule to find the posterior distribution.) Find the joint posterior distribution for the parameters \\((\\theta_s,\\theta_b,\\rho_s,\\rho_b)\\). Find 90% posterior credible intervals for \\((\\theta_s,\\theta_b,\\rho_s,\\rho_b)\\). Comment on your results.\nUse direct Monte Carlo to sample 10,000 observations from the joint posterior distribution of \\((\\theta_s,\\theta_b,\\rho_s,\\rho_b)\\). Use your Monte Carlo samples to estimate 90% posterior credible intervals for all four parameters. Compare with the result of part a.\nUse your direct Monte Carlo sample to estimate the probability that the mean bottom concentration \\(\\theta_b\\) is higher than the mean surface concentration \\(\\theta_s\\) and to estimate the probability that the standard deviation \\(\\sigma\\) of the bottom concentrations is higher than the standard deviation \\(\\sigma_b\\) of the surface concentrations.\nComment on your analysis. What are your conclusions about the distributions of surface and bottom concentrations? Is the assumption of normality reasonable? Are the means different for surface and bottom? The standard deviations?\nFind the predictive distribution for the sample mean of a future sample of size 10 from the surface and a future sample of size 10 from the bottom. Find 95% credible intervals on the sample mean of each future sample. Repeat for future samples of size 40. Compare your results and discuss.\nUse direct Monte Carlo to estimate the predictive distribution for the difference in the two sample means for 10 future surface and bottom samples. Plot a kernel density estimator for the density function for the difference in means. Find a 95% credible interval for the difference in the two sample means. Repeat for future samples of 40 surface and 40 bottom observations. Comment on your results.\nRepeat part e, but use a model in which the standard deviation is known and equal to the sample standard deviation, and the depth-specific means \\(\\theta_s\\) and \\(\\theta_b\\) have a uniform prior distribution. Compare the 95% credible intervals for the future sample means for the known and unknown standard deviation models. Discuss.\nAssume that experts have provided the following prior information based on previous studies.\n\n\nThe unknown means \\(\\theta_s\\) and \\(\\theta_b\\) are independent and normally distributed with mean \\(\\mu\\) and standard deviation \\(\\tau\\). The unknown precisions \\(\\rho_s\\) and \\(\\rho_b\\) are independent of \\(\\theta_s\\) and \\(\\theta_b\\) and have gamma distributions with shape \\(a\\) and scale \\(b\\).\nExperts specified a 95% prior credible interval of [3, 9] for \\(\\theta_s\\) and \\(\\theta_b\\). A good fit to this credible interval is obtained by setting the prior mean to \\(\\mu =6\\) and the prior standard deviation to \\(\\tau=1.5\\).\nA 95% prior credible interval of [0.75, 2.0] is given for the unknown standard deviations \\(\\Sigma_s\\) and \\(\\Sigma_b\\). This translates to a credible interval of [0.25, 1.8] for \\(\\rho_s = \\Sigma_s^{-1}\\) and \\(\\rho_b = \\Sigma_b^{-2}\\). A good fit to this credible interval is obtained by setting the prior shape to $a = 4.5. and the prior scale to \\(b\\) = 0.19. Find the following conditional distributions: \\(p(\\theta_s \\mid D,\\theta_b,\\rho_s,\\rho_b)\\), \\(p(\\theta_b \\mid D,\\theta_s,\\rho_s,\\rho_b)\\), \\(p(\\rho_s \\mid D,\\theta_s,\\theta_b,rho_b)\\), \\(p(\\rho_b \\mid D, \\theta_s,\\theta_b,\\rho_s)\\)\n\n\nUsing the distributions you found, draw 10,000 Gibbs samples of \\((\\theta_s,\\theta_b,\\rho_s,\\rho_b)\\). Estimate 90% credible intervals for \\((\\theta_s,\\theta_b,\\rho_s^{-1/2},\\rho_b^{-1/2})\\) and \\(\\theta_b-\\theta_s\\).\n\n\nDo a traceplot of \\(\\theta_b-\\theta_s\\). Find the autocorrelation function of \\(\\theta_b-\\theta_s\\) and the effective sample size for your Monte Carlo sample for \\(\\theta_b-\\theta_s\\).\nComment on your results. Compare with parts a,b, and c.\n\nSolution:\n\nThe joint posterior distribution for the parameters \\((\\theta_s,\\theta_b,\\rho_s,\\rho_b)\\) is the product of an improper prior density for the surface parameters times an improper prior density for the bottom parameters. The joint posterior density is proportional to the joint prior density times the surface and bottom likelihood functions: \\[\ng(\\theta_s,\\theta_b ,\\rho_s,\\rho_b \\mid x_s,x_b) \\propto g(\\theta_s,\\rho_s)g(\\theta_b ,\\rho_b) g(x_s \\mid \\theta_s,\\rho_s)g(x_b \\mid \\theta_b,\\rho_b)\n\\]\n\nThe first two factors are proportional to the posterior density of the surface parameters; the second two factors are proportional to the posterior density of the bottom parameters. Therefore, the surface and bottom parameters are independent given the data with joint posterior density: \\[\ng(\\theta_s,\\theta_b ,\\rho_s,\\rho_b \\mid x_s,x_b) \\propto g(\\theta_s,\\rho_s \\mid x_s)g(\\theta_b ,\\rho_b \\mid x_b)\n\\] We will calculate the posterior distributions for the surface and bottom parameters in turn.\nThe prior density for the surface parameters is \\(g(\\theta_s,\\rho_s) \\propto \\rho_s^{-1}\\)\nThis is the limit of the density function for a normal-gamma conjugate prior distribution. We use the Normal-Normal-Gamma update rules. The prior is given by \\[\ng(\\theta,\\rho \\mid \\nu,\\mu,a,b) = \\sqrt{\\frac{\\nu\\rho}{2\\pi}} \\exp\\left(-\\frac{\\nu\\rho}{2}(\\theta-\\mu)^2\\right) \\frac{b^a}{\\Gamma(a)}\\rho^{a-1}e^{-b\\rho}\n\\]\nFor prior parameters \\(\\mu_0 =0 ,\\, \\nu=0,\\, \\alpha=-0.5 ,\\, \\beta=0\\), the posterior values are \\[\n\\frac{\\nu\\mu_0+n\\bar{x}}{\\nu+n} ,\\, \\nu+n,\\, \\alpha+\\frac{n}{2},\n\\beta + \\tfrac{1}{2} \\sum_{i=1}^n (x_i - \\bar{x})^2 + \\frac{n\\nu}{\\nu+n}\\frac{(\\bar{x}-\\mu_0)^2}{2}\n\\] Pluggin-in our values \\(n=10\\) and \\(\\bar{x}=4.8\\), we get the posterior values for the Surface \\[\n\\mu^* = \\frac{0+10*4.8}{0+10} = 4.8 ,\\, \\nu^* = 0+10=10,\\, \\alpha^* = -0.5+\\frac{10}{2} = 4.5, \\, \\beta^* = 0 + \\tfrac{1}{2} \\sum_{i=1}^{10} (x_i - 4.8)^2 = 1.794\n\\]\n\ns = c(3.74,4.61,4.00,4.67,4.87,5.12,4.52,5.29,5.74,5.48)\nb = c(5.44,6.88,5.37,5.44,5.03,6.48,3.89,5.85,6.85,7.16)\nprint(mean(s))\n\n 4.8\n\nprint(mean(b))\n\n 5.8\n\nprint(0.5*sum((s - mean(s))^2))\n\n 1.8\n\nprint(0.5*sum((b - mean(b))^2))\n\n 4.6\n\n\nSimilar calculations for the bottom parameters give us the posterior values \\[\n\\mu^*=\\frac{0+10*5.84}{0+10} = 5.84,~ \\nu^* = 0+10=10,\\, \\alpha^* = -0.5+\\frac{10}{2} = 4.5, \\, \\beta^* = 0 + \\tfrac{1}{2} \\sum_{i=1}^{10} (x_i - 5.84)^2 = 4.627\n\\]\nTo summarise, we have:\n\n\n\n\n\\(\\mu^*\\)\n\\(\\nu^*\\)\n\\(\\alpha^*\\)\n\\(\\beta^*\\)\n\n\n\n\nsurface\n4.8\n10\n4.5\n1.794\n\n\nbottom\n5.84\n10\n4.5\n4.627\n\n\n\nThe marginal distribution for \\(\\theta_s\\) is nonstandard \\(t\\) with center 4.80, spread \\(1/\\sqrt{\\nu^*\\alpha^*/\\beta^*} = 0.20\\), and degrees of freedom 9. The marginal distribution for \\(\\theta_s\\) is nonstandard \\(t\\) with center 5.84, spread \\(1/\\sqrt{\\nu^*\\alpha^*/\\beta^*} = 0.32\\), and degrees of freedom 9. Using the quantiles of the t distribution\n\nqgamma(c(0.05,0.95),4.5,1.794) # marginal posterior for \\rho_s\n\n 0.93 4.72\n\nqgamma(c(0.05,0.95),4.5,4.627) # marginal posterior for \\rho_b\n\n 0.36 1.83\n\n4.8 + c(-1,1)*1.833*(1/sqrt(10*4.5/1.794)) # marginal posterior for \\theta_s\n\n 4.4 5.2\n\n5.84 + c(-1,1)*1.833*(1/sqrt(10*4.5/4.627)) # marginal posterior for \\theta_b\n\n 5.3 6.4\n\n\n\n\n\n\nset.seed(92) # Kuzy\n# Direct Monte Carlo\nnumsim=5000\nrhos=rgamma(numsim,4.5,1.794)\nrhob=rgamma(numsim,4.5,4.627)\nthetas=rnorm(numsim,4.8,1/sqrt(10*rhos))\nthetab=rnorm(numsim,5.84,1/sqrt(10*rhob))\n\n\n# Monte Carlo quantiles\nquantile(rhos,c(0.05,0.95))\n\n  5%  95% \n0.92 4.70 \n\nquantile(rhob,c(0.05,0.95))\n\n  5%  95% \n0.36 1.84 \n\nquantile(thetas,c(0.05,0.95))\n\n 5% 95% \n4.4 5.2 \n\nquantile(thetab,c(0.05,0.95))\n\n 5% 95% \n5.3 6.4 \n\n\n\n\n\n\n# Estimate probability that bottom concentration is larger\nsum(thetab&gt;thetas)/numsim        # Estimated prob bottom mean is larger\n\n 0.99\n\nsd(thetab&gt;thetas)/sqrt(numsim)   # Standard error of estimate\n\n 0.0014\n\nquantile(thetab-thetas, c(0.05,0.95))  # 90% credible interval\n\n  5%  95% \n0.36 1.73 \n\n# Estimate probability that bottom  precision is smaller\nsum(rhob&gt;rhos)/numsim        # Estimated prob bottom precision is larger\n\n 0.088\n\nsd(rhob&gt;rhos)/sqrt(numsim)   # Standard error of estimate\n\n 0.004\n\nquantile(rhob-rhos, c(0.05,0.95))  # 90% credible interval\n\n   5%   95% \n-3.84  0.28 \n\n# Estimate probability that bottom standard deviation is larger\nsigmab=1/sqrt(rhob)         # bottom standard deviation\nsigmas=1/sqrt(rhos)         # surfact standard deviation\nsum(sigmab&gt;sigmas)/numsim    # Estimated prob bottom std dev is larger\n\n 0.91\n\nsd(sigmab&gt;sigmas)/sqrt(numsim)   # Standard error of estimate\n\n 0.004\n\nquantile(sigmab-sigmas, c(0.05,0.95))  # 90% credible interval\n\n  5%  95% \n-0.1  1.0 \n\n\n\nThe results provide strong evidence that the mean HCB concentration in the bottom samples is larger than the mean HCB concentration in the surface samples. There is also evidence that the surface concentrations may have less variation around the mean than the bottom observations, but because of the small sample size, more evidence is needed to make a definitive assessment of whether the standard deviations are different. The p-value is 0.93 for the surface observations and 0.53 for the bottom observations, which does not reject the hypothesis of normality. Therefore, it seems reasonable to proceed with the assumption that the observations are normally distributed.\n\nqqnorm(s,main=\"Normal Q-Q Plot for Surface Data\"); qqline(s)\nqqnorm(b,main=\"Normal Q-Q Plot for Bottom Data\"); qqline(b)\n\n\n\n\n\n\n\n\n\n\n\nThe posterior-predictive for the sample mean for Normal-Gamma of size \\(m\\) is \\(t\\) distribution with mean \\(\\mu^*\\) and spread \\[\n\\dfrac{1}{\\sqrt{\\dfrac{\\nu^*m}{\\nu^*+m}\\alpha^*/\\beta^*}}\n\\] and degrees of freedom \\(m-1\\). For \\(m=10\\) and the posterior parameters of the surface data, we have spread \\[\n\\dfrac{1}{\\sqrt{\\dfrac{10*10}{10+10}*4.5/1.794}} = 0.28\n\\]\n\n\nss10 = 1/sqrt(100/20*4.5/1.794)\nsprintf(\"Spread for surface data with m=10: %.2f\",ss10)\n\n \"Spread for surface data with m=10: 0.28\"\n\n\nfor \\(m=40\\), we have spread\n\nss40 = 1/sqrt(400/50*4.5/1.794)\nsprintf(\"Spread for surface data with m=40: %.2f\",ss40)\n\n \"Spread for surface data with m=40: 0.22\"\n\n\nFor the bottom data, we have spread\n\nsb10 = 1/sqrt(100/20*4.5/4.627)\nsprintf(\"Spread for bottom data with m=10: %.2f\",sb10)\n\n \"Spread for bottom data with m=10: 0.45\"\n\nsb40 = 1/sqrt(400/50*4.5/4.627)\nsprintf(\"Spread for bottom data with m=40: %.2f\",sb40)\n\n \"Spread for bottom data with m=40: 0.36\"\n\n\nNow we can calculate 95% credible intervals for the sample mean of each future sample\n\n4.8+ss10*qt(c(0.025,0.975),9)\n\n 4.2 5.4\n\n4.8+ss40*qt(c(0.025,0.975),9)\n\n 4.3 5.3\n\n5.839+sb40*qt(c(0.025,0.975),9)\n\n 5.0 6.7\n\n5.839+sb10*qt(c(0.025,0.975),9)\n\n 4.8 6.9\n\n\nAs we would expect, the intervals for m=40 are entirely contained within the intervals for m=10, and the intervals for the posterior mean (computed above) lie entirely within the intervals for the corresponding sample means. This is because the posterior mean intervals include uncertainty only about the surface and bottom mean, whereas the intervals in this assignment include uncertainty about both the true means and the observations. The sample mean for 40 observations has less uncertainty than the sample mean for 10 observations. The intervals for surface and bottom sample means overlap more for m=10 than for m=40, and both have more overlap than the posterior mean intervals.\n\nWe do this two different ways and verify that they give the same results to within sampling error. First draw samples of Theta and Rho, draw ssize observations, compute the sample mean, and average.\n\n\nset.seed(92) # Kuzy\nnumsim=10000\nssize=10\nrhob=rgamma(numsim,4.5,4.627) # sample b precision\nthetab=rnorm(numsim,5.84,1/sqrt(10*rhob))     # sample b precision\nrhos=rgamma(numsim,4.5,1.794) # sample s mean\nthetas=rnorm(numsim,4.8,1/sqrt(10*rhos))     # sample surface mean\ndiff = 0     # Difference in sample means - initialize to 0\ndd=0\nfor (i in 1:ssize) {\n  diff = diff + \n    rnorm(numsim,thetab,1/sqrt(rhob)) -    # sample bottom observation\n    rnorm(numsim,thetas,1/sqrt(rhos))      # sample surface observation and subtrace\n  }\ndiff = diff / ssize    # Calculate average - divide sum by number of observations\n\nquantile(diff, c(0.025,0.975))   # quantiles\n\n 2.5% 97.5% \n-0.16  2.23 \n\nplot(density(diff),              # kernel density plot\n     main=paste(\"Density for Difference in Sample Means for m =\",ssize))\n\n\n\n\n\n\n\n\nWe can also sample from the t distribution (need two independent samples one for surface and one for bottom)\n\ndiff1 = 5.84 + rt(numsim,2*4.5)*ss10 -\n  4.8 + rt(numsim,2*4.5)*sb10 \nquantile(diff1,c(0.025,0.975))  # Compare with quantiles from full sampling method\n\n 2.5% 97.5% \n-0.17  2.23 \n\n\n\n\n\n\n# Known standard deviation model\n\nmu0=0           # prior mean for theta is 0\ntau0=Inf        # prior SD of theta is infinite\n\nsigmas = sd(s) # observation SD is assumed known and equal to sample SD\nsigmab = sd(b) # observation SD is assumed known and equal to sample SD\nn=10\nmus.star =      # Posterior mean for surface\n  (mu0/tau0^2 + sum(s)/sigmas^2)/(1/tau0^2+n/sigmas^2)\ntaus.star =     # Posterior SD of surface mean\n  (1/tau0^2+n/sigmas^2)^(-1/2)\nmub.star =      # Posterior mean for bottom\n  (mu0/tau0^2 + sum(b)/sigmab^2)/(1/tau0^2+n/sigmab^2)  \ntaub.star =     # Posterior SD of surface mean\n  (1/tau0^2+n/sigmab^2)^(-1/2)\n\nsprintf(\"Posterior mean for surface: %.2f\",mus.star)\n\n \"Posterior mean for surface: 4.80\"\n\nsprintf(\"Posterior mean for bottom: %.2f\",mub.star)\n\n \"Posterior mean for bottom: 5.84\"\n\n# Predictive intervals\n\nqnorm(c(0.025,0.975),    # Surface, sample size 10\n      mus.star,sqrt(taus.star^2 + sigmas^2/10))\n\n 4.3 5.4\n\nqnorm(c(0.025,0.975),    # Surface, sample size 40\n      mus.star,sqrt(taus.star^2 + sigmas^2/40))\n\n 4.4 5.2\n\nqnorm(c(0.025,0.975),    # Surface, sample size 10\n      mub.star,sqrt(taub.star^2 + sigmab^2/10))\n\n 5.0 6.7\n\nqnorm(c(0.025,0.975),    # Surface, sample size 40\n      mub.star,sqrt(taub.star^2 + sigmab^2/40))\n\n 5.1 6.5\n\n# Kernel density plot\nplot(density(diff),main=paste(\"Density for Difference in Sample Means\"))\n\n\n\n\n\n\n\n\n\n\nExercise 27.80 (Gibbs: Bird feeders) A biologist counts the number of sparrows visiting six bird feeders placed on a given day.\n\n\n\nFeeder\nNumber of Birds\n\n\n\n\n1\n11\n\n\n2\n22\n\n\n3\n13\n\n\n4\n24\n\n\n5\n19\n\n\n6\n16\n\n\n\n\n\n\n\n\nAssume that the bird counts are independent Poisson random variables with feeder- dependent means \\(\\lambda_i\\), for \\(i=1,\\ldots,6\\).\nAssume that the means \\(\\lambda_i\\) are independent and identically distributed gamma random variables with shape \\(a\\) and scale \\(b\\) (or equivalently, shape $a and mean \\(m = ab\\) )\nThe mean \\(m = ab\\) of the Gamma distribution is uniformly distributed on a grid of 200 equally spaced values starting at 5 and ending at 40.\nThe shape \\(a\\) is independent of the mean \\(m\\) and has a distribution that takes values on a grid of 200 equally spaced points starting at 1 and ending at 50, with prior probabilities proportional to a gamma density with shape \\(1\\) and scale \\(5\\).\n\n\nUse Gibbs sampling to draw 10000 samples from the joint posterior distribution of the mean \\(m\\), the shape parameter \\(a\\), and the six mean parameters \\(\\lambda_i\\), \\(i=1,\\ldots,6\\), conditional on the observed bird counts. Using your sample, calculate 95% credible intervals for the mean m, the shape a, and the six mean parameters \\(\\lambda_i\\), \\(i=1,\\ldots,6\\).\nFind the effective sample size for the Monte Carlo samples of the mean m, the shape parameter a, and the six mean parameters \\(\\lambda_i\\), \\(i=1,\\ldots,6\\).\nDo traceplots for the mean m, the shape parameter a, and the six rate parameters \\(\\lambda_i\\), \\(i=1,\\ldots,6\\).\nThe fourth feeder had the highest bird count and the first feeder had the lowest bird count. Use your Monte Carlo sample to estimate the posterior probability that the first feeder has a smaller mean bird count than the fourth feeder. Explain how you obtained your estimate.\nDiscuss your results.\n\nSolution:\nA plate diagram for this problem is shown at the right. Gibbs sampling on this model can be done as follows:\n\nInitialize \\(m^{(0)}\\) and \\(\\alpha^{(0)}\\). Any initial values will do, but I used the empirical Bayes estimates \\(m^{(0)}=17.3\\) (average of the observations) and \\(\\alpha^{(0)}=10.48\\) (square of mean of observations divided by variance of observations). I did not initialize the \\(\\lambda_{i}\\) because they are sampled before they are needed.\nFor \\(k=1, \\ldots, 10,000\\) :\n\n\nFor \\(i=1, \\ldots 6\\) :\n\n\nCalculate the posterior shape \\(\\alpha_{i}^{*}=\\left(\\alpha^{(k-1)}+\\right.\\) \\(Y_{i}\\) ) for the \\(i^{\\text {th }}\\) feeder.\nCalculate the posterior scale \\(\\beta_{i}{ }^{*}=\\left(m^{(k-1)} / \\alpha_{i}^{(k-}\\right.\\)  \\(\\left.{ }^{1)}+1\\right)^{-1}\\) for the \\(i^{\\text {th }}\\) feeder.\nSimulate \\(\\lambda_{i}{ }^{(k)}\\) from a gamma distribution with shape \\(\\alpha_{i}^{*}\\) and scale \\(\\beta_{i}^{*}\\).\n\n\nFind the conditional distribution for \\(m\\) given \\(\\alpha^{(k-1)}\\) and \\(\\lambda_{i}{ }^{(k)}, i=1, \\ldots, 6\\). To do this, find the prior times likelihood for each possible value of \\(m\\). Because the prior is uniform, this is the product of the six Gamma likelihoods. Then divide by sum of prior times likelihood. So for each possible value of \\(m\\) :\n\n\nCalculate the six gamma likelihoods \\(g\\left(\\lambda_{i}^{(k)} \\mid \\alpha^{(k-1)}, \\alpha^{(k-1)} / m\\right)\\) evaluated at the current estimate \\(\\lambda_{i}^{(k)}\\) with shape \\(\\alpha^{(k-1)}\\) and scale \\(\\alpha^{(k-1)} / m\\).\nMultiply these six gamma likelihoods: \\(l(m)=\\Pi_{i} g\\left(\\lambda_{i}^{(k)} \\mid \\alpha^{(k-1)}, \\alpha^{(k-1)} / m\\right)\\). Then normalize these values to obtain a probability distribution \\(p(m)=l(m) /\\left(\\Sigma_{m^{\\prime}} l\\left(m^{\\prime}\\right)\\right)\\) and sample a new value \\(m^{(k)}\\) with probability \\(p(m)\\).\n\n\nFind the conditional distribution for \\(\\alpha\\) given \\(m^{(k)}\\) and \\(\\lambda_{i}^{(k)}, i=1, \\ldots, 6\\). To do this, find the prior times likelihood for each possible value of \\(\\alpha\\) and normalize to sum to 1 . For each possible value of \\(\\alpha\\) :\n\n\nCalculate the six Gamma likelihoods \\(g\\left(\\lambda_{i}^{(k)} \\mid \\alpha, \\alpha / m^{(k)}\\right)\\) evaluated at the current estimate \\(\\lambda_{i}^{(k)}\\) with shape \\(\\alpha\\) and scale \\(\\alpha / m^{(k)}\\).\nMultiply these six Gamma likelihoods: \\(l(\\alpha)=g\\left(\\lambda_{i}^{(k)} \\mid \\alpha, \\alpha / m^{(k)}\\right)\\)\nMultiply this by the prior to obtain prior times likelihood \\(l(\\alpha) g(\\alpha)\\) where \\(g(\\alpha)\\) is the gamma prior density function. Then normalize these values to obtain a probability distribution \\(p(\\alpha)=\\frac{l(\\alpha) / g(\\alpha)}{\\sum_{\\alpha \\prime} l(\\alpha \\prime) / g\\left(\\alpha^{\\prime}\\right)}\\), and sample a new value \\(\\alpha^{(k)}\\) with probability \\(p(\\alpha)\\).\n\n\nThe result of this process is the requested sample.\n\nR code for doing the sampling is available on Blackboard. The table below shows credible intervals calculated from the raw Gibbs samples and credible intervals from a thinned sample with interval 3 (that is, only every third sample is kept). The intervals are almost the same. Due to sampling variation, the intervals will be different each time the sampler is run but your results should be similar.\n\n\n\n\n\n\n\n\nParameter\nGibbs Interval  (raw)\nGibbs Interval  (thinned x3)\n\n\n\n\n\\(m\\)\n\\([12.74,27.34]\\)\n\\([12.73,26.98]\\)\n\n\n\\(\\alpha\\)\n\\([2.23,24.64]\\)\n\\([2.23,24.32]\\)\n\n\n\\(\\lambda_{1}\\)\n\\([7.61,19.92]\\)\n\\([7.71,19.5]\\)\n\n\n\\(\\lambda_{2}\\)\n\\([13.71,29.45]\\)\n\\([13.92,29.45]\\)\n\n\n\\(\\lambda_{3}\\)\n\\([8.81,21.55]\\)\n\\([8.76,21.40]\\)\n\n\n\\(\\lambda_{4}\\)\n\\([14.89,31.18]\\)\n\\([14.96,31.13]\\)\n\n\n\\(\\lambda_{5}\\)\n\\([12.12,26.78]\\)\n\\([12.23,26.60]\\)\n\n\n\\(\\lambda_{6}\\)\n\\([10.46,24.21]\\)\n\\([10.48,24.08]\\)\n\n\n\nIn general, thinning increases the variance of the estimator (because we are throwing away information), but has other advantages: (1) it reduces data storage; (2) it reduces computation for any post-processing of the Monte Carlo samples; (3) it allows use of data processing methods intended for independent and identically distributed data, without having to adjust for the correlation.\nIt is also common to discard a burn-in sample when doing MCMC. I did not require either thinning or discarding a burnin sample.\n\n\nExercise 27.81 (Poisson Distribution for EPL) We will analyze EPL data. Use epl.R as a starting script. This script uses football-data.org API to download the results for two EPL teams: Manchester United and Chelsea. Model the GoalsHome and GoalsAway for each of the teams using Poisson distribution. Given these distributions calculate the following\n\nWhat’s the probability of a nil-nil (0 - 0) draw?\nWhat’s the probability that MU wins the match?\nDiscuss how could you improve your model based on four Poisson distributions.\nUse R and random number simulation to provide empirical answers. Using a random sample of size N = 10, 000, check and see how close you get to the theoretical answers you’ve found to the questions posed above. Provide histograms of the distributions you simulate.\n\nHint: The difference of two Poisson random variables follows a Skellam distribution, defined in skellam package. You can use dskellam to calculate the probability of a draw. You can use rpois to simulate draws from Poisson distribution.\n\n\nExercise 27.82 (Homicide Rate (Poisson Distribution)) Suppose that there are \\(1.5\\) homicides a week. The is the rate, so \\(\\lambda=1.5\\). The tells us that there is a still a \\(1.4\\)% chance of seeing \\(5\\) homicides in a week \\[\np( X= 5 ) = \\frac{e^{ - 1.5 } ( 1.5 )^5 }{5!} = 0.014\n\\] On average this will happen once every \\(71\\) weeks, nearly once a year.\nWhat’s the chance of having zero homicides in a week?\nSolution:\nFrom the probability distribution we calculate \\[\np( X= 0 ) = \\frac{e^{ - 1.5 } ( 1.5 )^0 }{0!} = 0.22\n\\] In R\n\ndpois(0, 1.5)\n\n 0.22\n\n\nSo the probability is 22% You shouldn’t be surprised to see this happening.\n\n\nExercise 27.83 (True/False (Binomial Distribution))  \n\nThe binomial distribution is a discrete probability distribution.\nAssuming the Joe DiMaggio’s batting average is \\(0.325\\) per at-bat and his hits are independent, then he has a probability of about \\(12\\)% of getting more than \\(2\\) hits in \\(4\\) at-bats.\nSuppose that you toss a fair coin with probability \\(0.5\\) a head. The probability of getting five heads is a row is less than three percent.\nSuppose that you toss a biased coin with probability \\(0.25\\) of getting ahead. The probability of getting five heads out of ten tosses is less than thirty percent.\nSuppose that you toss a coin \\(5\\) times. Then there are \\(10\\) ways of getting \\(3\\) heads.\nThe probability of observing three heads out of five tosses of a fair coin is \\(0.6\\).\nA mortgage bank knows from experience that \\(2\\)% of residential loan swill go into default. Suppose it makes \\(10\\) such loans, then the probability that at least one goes into default is \\(95\\)%.\nJessica Simpson is not a professional bowler and \\(40\\)% of her bowling swings are gutter balls. She is planning to take \\(90\\) blowing swings.The mean and standard deviation of the number of gutter balls is\\(\\mu = 36\\) and \\(\\sigma = 3.65\\).\nThe probability of at least one head when tossing a fair coin \\(4\\) times is \\(0.9375\\).\nThe Red Sox are to play the Yankees in a seven game series. Assume that the Red Sox have a 50% chance of winning each game, with the results being independent of each other. Then the probability of the series ending 4-3 in favor of the Red Sox is \\(0.5^{7}=0.0078\\).\nSuppose that \\(X\\) is Binomially distributed with \\(E(X)=5\\) and \\(Var(X)=2\\),then \\(n=10\\) and \\(p=0.5\\).\nIf \\(X\\) is a Bernoulli random variable with probability of success, \\(p\\),then its variance is \\(V(X)=p(1-p)\\).\nHistorically 15% of chips manufactured by a computer company are defective. The probability of a random sample of 10 chips containing exactly one defect is 0.15.\n\nSolution:\n\nTrue.\nFalse. We need to calculate \\[\\begin{aligned}P(hits &gt; 2) &=& P(hits = 3) + P(hits = 4) \\\\&=& \\binom{4}{3}\\hat{p}^3(1-\\hat{p}) + \\binom{4}{4}\\hat{p}^4 \\\\&=& 10.4\\%\\end{aligned}\\]\nFalse. Given a fair coin with 50% probability for a head,\\(0.5^5 = 0.0312.&gt; 0.03\\)\nTrue. This follows from a binomial distribution with \\(n=10\\). \\(Prob(5H) = \\left ( \\begin{array}{c}10\\\\5 \\end{array} \\right ) ( 0.25)^5 (0.75)^5 =  0.0583\\)\nTrue. We see this by computing: \\[\\binom{5}{3} = \\frac{5!}{3!(5-3)!}=10\\]\nFalse. \\(P(3 \\; heads)=0.5^{5}=0.03125\\). \\(10\\) such combinations are possible so the probability is \\(0.3125\\)\nFalse. Using \\(P( \\mathrm{ at \\; least \\; one} ) = 1 - P( \\mathrm{ none} )\\) gives\\(1 - ( 0.98 )^{10} = 0.1829\\)\nFalse. From a Binomial distribution \\(E(X) = np = 90 \\times 0.4 = 36\\) and the standard deviation\\(s_X  = \\sqrt{ n p ( 1 - p ) } = \\sqrt{ 21.6 } = 4.65\\)\nTrue. \\(P( \\mathrm{ at \\; least \\; one \\; head} ) = 1 - ( 0.5 )^4 = 0.9375\\)\nFalse. The total wins of the Red Sox is a binomial random variable with\\(n=7\\) and \\(p=0.5\\). The probability of \\(4\\) wins is then\\(\\binom{7}{4} 0.5^{4} 0.5^{7-4}=35\\cdot (0.5)^{7}=0.27\\). Alternatively, if the series is stopped as soon as one time wins 4matches, the probability of a 4-3 outcome in favor of the Red Sox is\\(\\binom{6}{3} 0.5^{3} 0.5^{6-3}\\cdot 0.5=0.16\\), since the teams should first reach a 3-3 tie\nFalse. The variance of a \\(Bin(10, 0.5)\\) random variable is\\(np(1-p) = 2.5\\) and not \\(2\\)\nTrue. The Bernoulli random variable is the building block of the binomial (\\(n=1\\)) and hence has variance \\(p(1-p)\\)\nFalse. The probability of one defective is given by the binomial probability \\(10 \\times 0.15 \\times (0.85)^9 = 0.3474\\)\n\n\n\nExercise 27.84 (True/False (Poisson Distribution))  \n\nIf \\(X \\sim Poi (2)\\) and \\(Y \\sim Poi (3)\\), then \\(X+Y \\sim Poi (6)\\).\nArsenal are playing Burnley at home in an English Premier League (EPL)game this weekend. They are favorites to win. They have a Poisson distribution for the number of goals they will score with a mean rate of \\(2.5\\) per game. Given this, the odds of Arsenal scoring at least two goals is greater than \\(50\\)%.\nArsenal are playing Swansea tomorrow in an English Premier League (EPL). They are favorites to win. The number of goals they expect to score is Poisson with a mean rate of \\(2.2\\). Given this, the odds of Arsenal scoring at least one goal is greater than \\(60\\%\\).\nArsenal are playing Liverpool at home in an EPL game this weekend. You think that the number of goals to be scored by both teams follow a Poisson distribution with rates \\(2.2\\) and \\(1.6\\) respectively. Given this, the odds of a scoreless \\(0-0\\) draw are \\(45-1\\).\nSuppose your website gets on average \\(2\\) hits per hour. Then the probability of at least one hit in the next hour is \\(0.135\\).\nThe soccer team Manchester United scores on average two goals per game.Given that the distribution of goals is Poisson, the chance that they score two or less goals is \\(87\\)%\n\nSolution:\n\nFalse. By the properties of Poisson distribution,\\[E(X+Y) = E(X) + E(Y) = 2+3 = 5.\\] So, X+Y does not follow Poi(6),which has a mean 6\nTrue. Here \\(S\\sim Pois(2.5)\\). We need\\(P(S \\geq 2) = 1 - P(S \\leq 1) = 1 - 0.2873 = 0.7127.\\) In R we have ppois(1,2.5) = 0.28729\nTrue. P(#Goal &gt; 0) = 1-P(#Goal = 0) = 1-ppois(0,2.2)=0.89.\nFalse. Assuming each scoring’s ability is independent, we have that:\\[\\begin{aligned}Pr(Arsenal=0) &=& \\frac{(2.2)^0 e^{-2.2}}{0!} = e^{-2.2} \\\\Pr(Liverpool=0) &=& \\frac{(1.6)^0 e^{-1.6}}{0!} = e^{-1.6} \\\\\\longrightarrow\\quad Pr(Arsenal=0\\;and\\; Liverpool=0) &=& e^{-2.2}\\times e^{-1.6} = e^{-3.8}\\end{aligned}\\]The odds are: \\(O = (1-e^{-3.8})/e^{-3.8}\\) or \\(43.7\\) to \\(1\\).\nFalse, dpois(0,2) = 0.135 \\(= e^{-2}\\) so that \\(P(hit&gt;1)= 1 - P(hit=0)=0.865\\)\nFalse. We have that :\\[E[g]=2\\quad\\Rightarrow\\quad g\\sim Pois(\\lambda=2)\\] So, we find the chance that they score two or fewer goals is:\\[P(g\\le2)=0.6767.\\] In R: ppois(2,2)=0.67667\n\n\n\nExercise 27.85 (True/False (Normal Distribution))  \n\nThe returns for Google stock on the day of earnings are normally distributed with a mean of \\(5\\)% and a standard deviation of \\(5\\)%. The probability that you will make money on the day of earnings is approximately \\(60\\)%.\nFor any normal random variable, \\(X\\), we have \\(\\mathbb{P} \\left ( \\mu - \\sigma &lt; X &lt; \\mu + \\sigma \\right ) = 0.64\\). Hint: You may use \\(pnorm(1) = 0.841\\)\nSuppose that the annual returns for Facebook stock are normally distributed with a mean of \\(15\\)% and a standard deviation of \\(20\\)%. The probability that Facebook has returns greater than \\(10\\)% for next year is \\(60\\)%\nConsider the standard normal random variable \\(Z \\sim N ( 0 , 1 )\\). Then the random variable \\(-Z\\) is also standard normal.\nA local bank experiences a \\(2\\)% default rate on residential loans made in a certain city. Suppose that the bank makes \\(2000\\) loans. Then the probability of more than \\(50\\) defaults is \\(25\\) percent.\nThe Binomial distribution can be approximated by a normal distribution when the number of trials is large.\nLet \\(X \\sim N(5, 10)\\). Then \\(P \\left (X&gt;5 \\right ) = \\frac{1}{2}\\).\nShaquille O’Neal has a \\(55\\)% chance of making a free throw in Basketball. Suppose he has \\(900\\) free throws this year. Then the chance he makes more than \\(500\\) free throws is \\(45\\)%\nSuppose that the random variable \\(X \\sim N ( -2 , 4 )\\) then \\(- 2 X \\sim N ( 4 , 16 )\\).\nMortimer’s steak house advertises that it is the home of the \\(16\\) ounce steak. They claim that the weight of their steaks is normally distributed with mean \\(16\\) and standard deviation \\(2\\). If this is so,then the probability that a steak weights less that \\(14\\) ounces is\\(16\\)%.\nAdvertising costs for a \\(30\\)-second commercial are assumed to be normally distributed with a mean of \\(10,000\\) and a standard deviation of\\(1000\\). Then the probability that a given commercial costs between\\(9000\\) and \\(10,000\\) is \\(50\\)%.\nIn a sample of \\(120\\) Zagat’s ratings of Chicago restaurants, the average restaurant had a rating of \\(19.6\\) with a standard deviation of \\(2.5\\). If you randomly pick a restaurant, the chance that you pick one with with a rating over \\(25\\) is less than \\(1\\)%.\nA hospital finds that \\(20\\)% of its bills are at least one month in arrears. A random sample of \\(50\\) bills were taken. Then the probability that less than \\(10\\) bills in the sample were at least one month in arrears is \\(50\\)%\nA Chicago radio station believes \\(30\\)% of its listeners are younger than\\(30\\). Out of a sample of \\(500\\) they find that \\(250\\) are younger than\\(30\\). This data supports their claim at the \\(1\\)% level.\nThe probability that a standard normal distribution is more than \\(1.96\\)standard deviations from the mean is \\(0.05\\).\nSuppose that the amount of money spent at Disney World is normally distributed with a mean of $60 and a standard deviation of $15. Then approximately 45% of people spend more than $70 per visit.\nA Normal distribution with mean 4 and standard deviation 3.6 will provide a good approximation to a Binomial random variable with parameters \\(n=40\\) and \\(p= 0.10\\).\nIf \\(X\\) is normally distributed with mean \\(3\\) and variance \\(9\\) then the probability that \\(X\\) is greater than \\(1\\) is \\(0.254\\).\n\nSolution:\n\nFalse.\\[P(ret &gt; 0) = 1 - P(ret \\leq 0) = 1 - pnorm(0, 0.05, 0.05) = 84.2\\%\\]\nFalse. \\(pnorm(1) = 0.841\\) and \\(2 \\times pnorm(1)-1=0.6828&gt;0.64\\)\nTrue. Let R denote returns. Convert to standard normal and compute\\[\\mathbb{P} \\left ( \\frac{R- \\mu}{\\sigma} &gt; \\frac{0.1-\\mu}{\\sigma} \\right ) = \\mathbb{P} \\left ( Z &gt; - 0.25 \\right ) = 0.5987\\]where \\(\\mu = 0.15\\) and \\(\\sigma = 0.2\\).\nTrue. If \\(X \\sim N(0,1)\\), then\\(-X \\sim N\\Big(-1(0),(-1)^2\\times 1\\Big) = N(0,1)\\). Moreover, we have that the pdf of \\(Z\\) and \\(-Z\\) is the same: \\[\\begin{aligned}f(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp{-\\frac{1}{2}z^2} = \\frac{1}{\\sqrt{2\\pi}}\\exp{-\\frac{1}{2}(-z)^2} = f(-z)\\end{aligned}\\]Lastly, we can observe that all the even moments are the same due to symmetry and all the odd moments are zero. Thus, they are the same distribution.\nFalse. The probability of having more than 50 defaults can be calculated using a Normal approximation to the \\(Bin ( 2000, 0.02) \\approx N ( 2000 \\times 0.02 , 2000 \\times 0.02 \\times 0.98 )\\).So, the mean is 40 and the standard deviation is 6.261. Therefore, \\[\n\\begin{aligned}Pr(default &gt; 50) &=& Pr\\Big(\\frac{default-40}{6.261} - \\frac{50-40}{6.261}\\Big) = Pr(z&gt;1.5972) \\approx .055\n\\end{aligned}\n\\]\nTrue, if \\(n\\) is large enough, a reasonable approximation to \\(B(n,p)\\) is\\(N(np,np(1-p))\\)\nTrue, since the mean is 5, then half the density will be to the right of 5\nFalse. \\(\\mu=np=495,\\,\\sigma=\\sqrt{np(1-p)}=14.92\\). Then\\(P(X&gt;500)=P(Z&gt;0.335)=1-0.631=36.9\\%\\)\nTrue. We have \\(- 2 X \\sim N ( 4 , 16 )\\).\nTrue. We have\\(P( X &lt; 14 ) = P \\left ( \\frac{ X - 16 }{2} &lt;  \\frac{ 14 - 16 }{2} \\right ) = P( Z &lt; - 1 ) = 0.1587\\)\nFalse. Required probability is \\(0.5 - P( Z &lt; -1 ) = 0.3413\\)\nFalse. \\(P(X&gt;25)= 1 - P( Z &lt; 2.16) = 0.0154\\)\nTrue. Using normal approximation, \\(n=50, p=0.2\\), have\\(X \\sim N ( 10 , 8 )\\)\nFalse. \\(n=500\\), \\(\\hat{p} \\sim N \\left ( p , \\frac{p(1-p)}{n} \\right )\\)and \\(H_0 : p = 0.3\\) versus \\(H_1 : p \\neq 0.3\\). Yields a \\(99\\)% confidence interval of \\((0.1835 , 0.4165 )\\). Sample proportion doesn’t lie in confidence interval so the data does not support their claim. Or could use a \\(Z\\)-test\nTrue. In a normal distribution, 2.5% of the population is more than 1.96standard deviations below the mean, and 2.5% is more than 1.96 standard deviations above the mean. Hence 5% of the distribution is more than1.96 standard deviations from the mean\nFalse. \\(z_{70}=\\frac{70-60}{15}=0.67\\) and 1-F(\\(z_{70}\\)) = 0.25. Hence,about 25% of people spend more than $70 per visit.\nFalse. For \\(n \\geq 40\\), we can approximate a Binomial distribution \\(Bin(n,p)\\) with a normal \\(N ( np , np (1-p))\\). In this case, we get\\(np = 4\\) and variance \\(40 \\times 0.1 \\times 0.9 = 3.6\\) or a standard deviation of \\(1.90\\)\nFalse. The standardized \\(Z\\)-score is given by \\(Z= (1-3)/3 = -0.667\\). \\(P(X&gt; 1) = P(Z&gt; -0.667) = P(Z &lt; 0.667) = 0.7486\\)\n\n\npnorm(0.667) # 0.7476139\n\n 0.75\n\n\n\n\nExercise 27.86 (Tarone Study) Tarone (1982) reports data from 71 studies on tumor incidence in rats\n\nIn one of the studies, 2 out of 13 rats had tumors. Assume there are 20 possible tumor probabilities: \\(0.025, 0.075,\\ldots, 0.975\\). Assume that the tumor probability is uniformly distributed. Find the posterior distribution for the tumor probability given the data for this study.\nRepeat Part a for a second study in which 1 in 18 rats had a tumor.\nParts a and b assumed that each study had a different tumor probability, and that these tumor probabilities were uniformly distributed a priori. Now, assume the tumor probabilities are the same for the two studies, and that this probability has a uniform prior distribution. Find the posterior distribution for the common tumor probability given the combined results from the two studies.\nCompare the three distributions for Parts a, b, and c. Comment on your results.\n\nSolution:\nWe assume that tumors occur independently, with a probability of \\(\\Theta\\) of a tumor in each rat in the study. Our data \\(X\\) consist of observations on \\(N\\) rats, in which we record which rats have tumors. We can use either of two methods to represent the likelihood of what we have observed:\nMethod 1: We list, for each observation, whether or not the rat has a tumor. We multiply together the probability of getting the observation we obtained: \\(\\Theta\\) if the rat has a tumor, and 1-\\(\\Theta\\) if the rat does not have a tumor. If x out of n rats have tumors, the probability of observing this sequence of observations is \\(\\Theta^k(1-\\Theta)^{n-k}\\). Using Method 1, the posterior distribution for \\(\\Theta\\) is given by: \\[\ng (\\theta | x ) = \\dfrac{f ( x | \\theta ) g (\\theta )}{\\sum_{i=1}^{20} f ( x | \\theta_i ) g (\\theta_i )} = \\dfrac{\\theta^x (1 - \\theta )^{n- x}}{\\sum_{i=1}^{20} \\theta_i^x (1 - \\theta_i )^{n - x}}\n\\]\nMethod 2: We count how many of the rats have tumors. We remember from our previous statistics class that if the tumors occur independently with probability \\(\\Theta\\), the number of rats with tumors out of a total of \\(n\\) rats will have a Binomial distribution with parameters \\(n\\) and \\(\\Theta\\). The probability of getting \\(k\\) tumors is \\[\nC_n^k \\theta^k (1 - \\theta )^{n - k},\\quad k = 0, 1, 2, ..., n,\\quad \\mathrm{where}\\quad C_n^k = \\dfrac{n!}{k!(n-k)!}\n\\] Then, the posterior distribution for \\(\\Theta\\) is given by: \\[\ng (\\theta | x ) = \\dfrac{f ( x | \\theta ) g (\\theta )}{\\sum_{i=1}^{20} f ( x | \\theta_i ) g (\\theta_i )} = \\dfrac{C_n^k \\theta^x (1 - \\theta )^{n - x}}{\\sum_{i=1}^{20} C_n^k \\theta_i^x (1 - \\theta_i )^{n - x}}\n\\]\nIn Method 1, we are representing the likelihood of getting this exact sequence of observations. In Method 2, we are representing the likelihood of getting this number of tumors.\nIt is clear that the two methods give the same posterior distribution, because the constant \\(C_n^k\\) cancels out of the numerator and denominator of Bayes rule. That is, the number of tumors is a sufficient statistic for \\(\\Theta\\), and the order in which the tumors occur is irrelevant.\nPart a. The posterior distribution for \\(\\Theta_1\\), the tumor probability in the first study, is: \\[\ng (\\theta | x =2) = \\dfrac{\\theta^2 (1 - \\theta )^{11}}{\\sum_{i=1}^{20} \\theta_i^2 (1 - \\theta_i )^{11}}\n\\]\ntheta &lt;- seq(0.025, 0.975, 0.05)\nprior &lt;- rep(1/20, 20)\ntarone = function(n,k) {\n    likelihood &lt;- theta^k * (1 - theta)^(n-k)\n    posterior &lt;- likelihood * prior / sum(likelihood * prior)\n    print(knitr::kable(data.frame(theta, likelihood, posterior)))\n    barplot(posterior, names.arg = theta, xlab = expression(theta), ylab = \"Pr(theta | X)\")\n    return(posterior)\n}\nposta = tarone(13,2)\n\n\n\n\n\n| theta| likelihood| posterior|\n|-----:|----------:|---------:|\n|  0.03|          0|      0.03|\n|  0.08|          0|      0.13|\n|  0.12|          0|      0.20|\n|  0.18|          0|      0.20|\n|  0.22|          0|      0.17|\n|  0.28|          0|      0.12|\n|  0.33|          0|      0.08|\n|  0.38|          0|      0.04|\n|  0.43|          0|      0.02|\n|  0.48|          0|      0.01|\n|  0.52|          0|      0.00|\n|  0.58|          0|      0.00|\n|  0.63|          0|      0.00|\n|  0.68|          0|      0.00|\n|  0.73|          0|      0.00|\n|  0.78|          0|      0.00|\n|  0.83|          0|      0.00|\n|  0.88|          0|      0.00|\n|  0.92|          0|      0.00|\n|  0.98|          0|      0.00|\n\n\n\n\n\n\nPart b. The posterior distribution for \\(\\Theta_2\\), the tumor probability in the second study, is: \\[\ng (\\theta | x =2) = \\dfrac{\\theta (1 - \\theta )^{17}}{\\sum_{i=1}^{20} \\theta_i(1 - \\theta_i )^{17}}\n\\]\npostb  = tarone(18,1)\n\n\n\n\n\n| theta| likelihood| posterior|\n|-----:|----------:|---------:|\n|  0.03|       0.02|      0.27|\n|  0.08|       0.02|      0.33|\n|  0.12|       0.01|      0.21|\n|  0.18|       0.01|      0.11|\n|  0.22|       0.00|      0.05|\n|  0.28|       0.00|      0.02|\n|  0.33|       0.00|      0.01|\n|  0.38|       0.00|      0.00|\n|  0.43|       0.00|      0.00|\n|  0.48|       0.00|      0.00|\n|  0.52|       0.00|      0.00|\n|  0.58|       0.00|      0.00|\n|  0.63|       0.00|      0.00|\n|  0.68|       0.00|      0.00|\n|  0.73|       0.00|      0.00|\n|  0.78|       0.00|      0.00|\n|  0.83|       0.00|      0.00|\n|  0.88|       0.00|      0.00|\n|  0.92|       0.00|      0.00|\n|  0.98|       0.00|      0.00|\n\n\n\n\n\n\nPart c. If the tumor probabilities are the same in the two studies, it is appropriate to treat them as a single study in which 3 rats out of 31 have a tumor. The posterior distribution for \\(\\Theta\\), the common tumor probability, is: \\[\ng (\\theta | x =2) = \\dfrac{\\theta^3 (1 - \\theta )^{28}}{\\sum_{i=1}^{20} \\theta_i^3(1 - \\theta_i )^{28}}\n\\]\npostc = tarone(31,3)\n\n\n\n\n\n| theta| likelihood| posterior|\n|-----:|----------:|---------:|\n|  0.03|          0|      0.06|\n|  0.08|          0|      0.34|\n|  0.12|          0|      0.34|\n|  0.18|          0|      0.18|\n|  0.22|          0|      0.07|\n|  0.28|          0|      0.02|\n|  0.33|          0|      0.00|\n|  0.38|          0|      0.00|\n|  0.43|          0|      0.00|\n|  0.48|          0|      0.00|\n|  0.52|          0|      0.00|\n|  0.58|          0|      0.00|\n|  0.63|          0|      0.00|\n|  0.68|          0|      0.00|\n|  0.73|          0|      0.00|\n|  0.78|          0|      0.00|\n|  0.83|          0|      0.00|\n|  0.88|          0|      0.00|\n|  0.92|          0|      0.00|\n|  0.98|          0|      0.00|\n\n\n\n\n\n\nIt is useful to verify that we get the same posterior distribution by using Method 2 to calculate the likelihood of the data. It is also useful to verify that we would get the same results in Part c if we used the posterior distribution from Part a as the prior distribution for Part c, and the likelihood from Part b as the likelihood for part c. That is, we would get the same result from a two-step process, in which we started with a uniform prior distribution, incorporated the results from the first study to obtain a posterior distribution, and then incorporated the results from the second study to get a new posterior distribution.\nPart d. We can compare the distributions more easily if we plot the prior distribution, the posterior distributions for the parameters of the two individual studies, and the combined posterior distribution together (see below). We see that the posterior distribution from the first study puts more probability on higher values of \\(\\Theta\\) than the posterior distribution for the second study. This is not surprising, because a higher proportion of rats in the first study had tumors. There is a great deal of overlap in the two distributions, which suggests that the two probabilities may be the same. Thus, it is reasonable to consider combining the two samples. If there had been very little overlap in the posterior distributions from Parts a and b, it would not have been reasonable to combine the studies. Later in the semester, we will look more carefully at the problem of when to combine probabilities and when to estimate them separately. When the studies are combined, the posterior distribution is narrower than the posterior distribution for either of the two individual studies, with less probability on very low and very high values of \\(\\Theta\\). Note that the combined distribution is consistent with both of the individual distributions, in the sense that it places high probability on values that also have high probability in the other two distributions. The combined distribution is more concentrated around values close to the combined sample proportion of about 1 in 10.\n\nallDist=rbind(prior,posta,postb,postc) \nbarplot(allDist,main=\"Prior and Posterior Distributions for TumorProbability\",xlab=\"Theta\",ylab=\"Probability\", col=c(\"mediumpurple\",\"lightblue\",\"pink\",\"lightgreen\"), border=c(\"darkviolet\",\"darkblue\", \"red\",\"darkgreen\"),names.arg=theta, legend=c(\"Prior\",\"Study 1\", \"Study 2\", \"Both Studies\"), beside=TRUE, ylim=c(0,0.4))\n\n\n\n\n\n\n\n\n\n\nExercise 27.87 Let \\(X\\) and \\(Y\\) be independent and identically distributed as a \\(Exp(1)\\) distribution. Their joint distribution is given by\n\\[\nf_{X,Y}(x,y) = \\exp(-x)\\exp(-y) \\mathrm{ , where} \\; \\; 0 &lt; x,y &lt; \\infty\n\\]\n\nUse the convolution formula to find the distribution of \\(X + \\frac{1}{2} Y\\). Check your answer by also using a moment generating function approach.\nGuess want happens if you consider \\(X + \\frac{1}{2} Y + \\frac{1}{3} Z\\) where \\(Z\\) is also \\(Exp(1)\\)?\n\nSolution:\nFirst, we need to show the family of exponential distributions is closed under scaling. Let \\(Z = aY\\) so that \\[P(Z \\leq z) = P(Y \\leq z/a) = 1 - \\exp(-z/a)\\] Therefore, \\(Z \\sim Exp(1/a)\\) and \\(f_{aY} = \\frac{1}{a}\\exp(-Y/a)\\).\nLet \\(W = X + \\frac{1}{2}Y\\). The density function of W is \\[\\begin{aligned}     f_W(W = w) &=& P(X+\\frac{1}{2}Y = w) \\\\     &=& \\int_0^w f_{X,\\frac{1}{2}Y}(x, w-x)dx  \\\\     &=& \\int_0^w f_X(x)f_{\\frac{1}{2}Y}(w-x)dx \\\\     &=& \\exp(-w)*\\left(\\frac{1}{2}\\exp(-2w)\\right)\\end{aligned}\\]\n\\[\\begin{aligned}     E(W) &=& \\int w*f_W(w)dw \\\\     &=& \\frac{3}{2}\\left(\\int w*\\frac{1}{3}\\exp(-3w)dw\\right) \\\\     &=& \\frac{3}{2} = E(X) + \\frac{1}{2}E(Y)\\end{aligned}\\]\nBy induction, the density function for \\(W = X + \\frac{1}{2} Y + \\frac{1}{3} Z\\) should be \\(\\exp(-w)*\\left(\\frac{1}{2}\\exp(-2w)\\right)*\\left(\\frac{1}{3}\\exp(-3w)\\right)\\)\n\n\nExercise 27.88 (Poisson MLE vs Baye) You are developing tools for monitoring number of advertisemnt clicks on a website. You have observed the following data:\n\ny = c(4,1,3,4,3,2,7,3,4,6,5,5,3,2,4,5,4,7,5,2)\n\nwhich represents the number of clicks every minute over the last 10 minutes. You assume that the number of clicks per minute follows a Poisson distribution with parameter \\(\\lambda\\).\n\nPlot likelihood function for \\(\\lambda\\).\nEstimate \\(\\lambda\\) using Maximum Likelihood Estimation (MLE). MLE is the value of \\(\\lambda\\) that maximizes the likelihood function or log-likelihood function. Maximizing likelihood is equivalent to maximizing the log-likelihood function (log is a monotonically increasing function).\nUsing barplot, plot the predicted vs observed probabilities of for number of clicks from 1 to 7. Is the model a good fit?\nAssume that you know, that historically, the average number of clicks per minute is 4 and variance is also 4. Those numbers were valculated over a long period of time. You can use this information as a prior. Assume that the prior distribution is \\(Gamma(\\alpha,\\beta)\\). What would be appropriate values for \\(\\alpha\\) and \\(\\beta\\) that would represent this prior information?\nFind the posterior distribution for \\(\\lambda\\) and calculate the Bayesian estimate for \\(\\lambda\\) as the expectation over the posterior.\nAfter collecting data for a few days, you realized that about 20% of the observations are zero. How this information would change your prior distribution? This is an open-ended question.\n\nHint: For part c, you can use the following code to calculate the predicted probabilities for the number of clicks from 0 to 5.\n\ntb = table(y)\nobserved = tb/length(y)\npredicted = dpois(1:7,lambda_mle)\n\nSolution: a)\n\nlambda = seq(0, 10, 0.1)\nlikelihood = sapply(lambda, function(l) prod(dpois(y, l)))\nplot(lambda, likelihood, type = \"l\", xlab = expression(lambda), ylab = \"Likelihood\")\n\n\n\n\n\n\n\n\n\n\n\n\nlambda_mle = optim(par = 1, fn = function(l) -sum(dpois(y, l, log = TRUE)))$par\nlambda_mle\n\n 4\n\n\nAlternatively, the likelihood function is \\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!}\n\\] and the log-likelihood function is \\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} y_i \\log(\\lambda) - n\\lambda\n\\] The MLE is the value of \\(\\lambda\\) that maximizes the log-likelihood function. We can find the MLE by taking the derivative of the log-likelihood function with respect to \\(\\lambda\\) and setting it equal to zero. This gives \\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\frac{1}{\\lambda} \\sum_{i=1}^{n} y_i - n = 0\n\\] Solving for \\(\\lambda\\) gives \\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} y_i.\n\\] A simple average of the data gives the MLE for \\(\\lambda\\).\n\nmean(y)\n\n 4\n\n\n\n\n\n\npred = table(y)/length(y)\npoisson = dpois(1:7,lambda_mle)\nd = cbind(pred, poisson)\nbarplot(t(d),names.arg=1:7, xlab = \"Clicks\", ylab = \"Frequency\", beside = TRUE)\n\n\n\n\n\n\n\n\n\nWe know that \\(E(X) = \\alpha/\\beta = 4\\) and \\(Var(X) = \\alpha/\\beta^2 = 4\\). For example, \\(\\alpha = 4\\) and \\(\\beta = 1\\) would match the historical data.\n\n\n\nalpha = 4\nbeta = 1\nalphas = alpha + sum(y)\nbetas = beta + length(y)\nlambda_bayes = alphas/betas\nknitr::kable(data.frame(alpha = alphas, beta = betas, lambda_bayes = lambda_bayes))\n\n\n\n\nalpha\nbeta\nlambda_bayes\n\n\n\n\n83\n21\n4\n\n\n\n\n\n\nWe can use a mixture prior with one component concentrated around zero and another component concentrated around the mean that is calculated by removing zeroes from the data. The component weights are 0.2 and 0.8 respectively.\n\n\n\nExercise 27.89 (Exponential Distribution) Let \\(x_1, x_2,\\ldots, x_N\\) be an independent sample from the exponential distribution with density \\(p (x | \\lambda) = \\lambda\\exp (-\\lambda x)\\), \\(x \\ge 0\\), \\(\\lambda&gt; 0\\). Find the maximum likelihood estimate \\(\\lambda_{\\text{ML}}\\). Choose the conjugate prior distribution \\(p (\\lambda)\\), and find the posterior distribution \\(p (\\lambda | x_1,\\ldots, x_N)\\) and calculate the Bayesian estimate for \\(\\lambda\\) as the expectation over the posterior.\n\n\nExercise 27.90 (Bernoulli Baeys) We have \\(N\\) Bernoulli trials with success probability in each trial being equal to \\(q\\), we observed \\(k\\) successes. Find the conjugate distribution \\(p (q)\\). Find the posterior distribution \\(p (q | k, N)\\) and its expectation.\n\n\nExercise 27.91 (Exponential Family (Gamma)) Write density function of Gamma distribution in a standard exponential family form. Find \\(Ex\\) and \\(E\\log x\\) by differentiating the normalizing constant.\n\n\nExercise 27.92 (Exponential Family (Binomial)) Write density function of Binomial distribution in a standard exponential family form. Find \\(Ex\\) and \\(Var x\\) by differentiating the normalizing constant.\n\n\nExercise 27.93 (Normal) Let \\(X_1\\) and \\(X_2\\) are independent \\(N(0,1)\\) random variables. Let \\[Y_1 = X_1^2 + X_2^2, \\quad Y_2 = \\frac{X_1}{\\sqrt{Y_1}}\\]\n\nFind joint distribution of \\(Y_1\\) and \\(Y_2\\).\nAre \\(Y_1\\) and \\(Y_2\\) independent or not?\nCan you interpret the result geometrically?\n\nDensity of \\(N(\\mu, \\sigma^2)\\) is \\[f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "ex.html#ab-testing-1",
    "href": "ex.html#ab-testing-1",
    "title": "27  Exercises",
    "section": "27.5 AB Testing",
    "text": "27.5 AB Testing\n\nExercise 27.94 (A/B Testing) During a recent breakout of the flu, 850 out 6,224 people diagnosed with the virus presented severe symptoms. During the same flu season, a experimental anti-virus drug was being tested. The drug was given to 238 people with the flu and only 6 of them developed severe symptoms. Based on this information, can you conclude, for sure, that the drug is a success?\nSolution:\nNull hypothesis is \\(H_0: p_1 - p_2 = 0\\), \\(H_a: p_1 - p_2 \\neq 0\\). \\[\n\\hat{p}_1 = \\frac{850}{6224} = 0.136, \\quad \\hat{p}_2 = \\frac{6}{238} = 0.025, \\quad \\hat{p}_1 - \\hat{p}_2 = 0.136-0.025 = 0.111\n\\] The standard error is \\[\ns = \\sqrt{\\frac{0.136 \\times (1 - 0.136)}{6224} + \\frac{0.025 \\times ( 1- 0.025)}{238} }= 0.011\n\\] t-stat is \\[\nt = \\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{s} = \\frac{0.136 - 0.025}{0.011} = 10.09 &gt; 1.96\n\\]\nYes, given this information we can conclude that the drug is working.\nIf you do confidence interval, the \\(95\\%\\) confidence interval of \\(p_1 - p_2\\) is \\[\n[0.111 - 1.96 \\times 0.011, 0.111 + 1.96\\times 0.011] = [0.089, 0.133]\n\\] which doesn’t contain 0. We reject the null hypothesis.\n\n\nExercise 27.95 (Tesla Supplier) Tesla purchases Lithium as a raw material for their batteries from either of two suppliers and is concerned about the amounts of impurity the material contains. The percentage impurity levels in consignments of the Lithium follows closely a normal distribution with the means and standard deviations given in the table below. The company is particularly anxious that the impurity level not exceed \\(5\\)% and wants to purchase from the supplier who is ore likely to meet that specification.\n\n\n\n\nMean\nStandard Deviation\n\n\n\n\nSupplier A\n4.4\n0.4\n\n\nSupplier B\n4.2\n0.6\n\n\n\n\nWhich supplier should be chosen?\nWhat if Supplier B implements some quality control which has no effect on the standard deviation but raises their mean to \\(4.6\\)?\n\n\n\nExercise 27.96 (Red Sox) On September 24, 2003, Pete Thamel in the New York Times reported that the Boston Red Sox had been accused of cheating by another American League Team. The claim was that the Red Sox had a much better winning record at home games than at games played in other cities.\nThe following table provides the wins and losses for home and away games for the Red Sox in the 2003 season\n\n\n\n\nRecord\n\n\n\n\n\n\n\nTeam\nHome Wins\nHome Losses\nAway Wins\nAway Losses\n\n\nBoston Red Sox\n53\n28\n42\n39\n\n\n\nIs there any evidence that the proportion of Home wins is significantly different from home and away games?\nDiscuss any other issues that are relevant.\nHint: a 95% confidence interval for a difference in proportions \\(p_1 - p_2\\) is given by \\[\n( \\hat{p}_1 - \\hat{p}_2 ) \\pm 1.96\n\\sqrt{ \\frac{ \\hat{p}_1 ( 1 - \\hat{p}_1 ) }{ n_1 }  +\n\\frac{ \\hat{p}_2 ( 1 - \\hat{p}_2 ) }{ n_2 } }\n\\]\nSolution:\nThe home and away winning proportions are given by \\[\np_1 = \\frac{53}{81} = 0.654  \\; \\; \\text{ and} \\; \\;  \\text{ Var} = \\frac{ p_1 ( 1- p_1 ) }{ n_1 } = \\frac{53 \\times 28 }{81^3} = 0.0028\n\\] and \\[\np_2 = \\frac{42}{81} = 0.518 \\; \\; \\text{ and} \\; \\;  \\text{ Var} = \\frac{ p_2 ( 1- p_2 ) }{ n_2 } = \\frac{42 \\times 39}{81^3} = 0.0031\n\\] Can either do the problem as a confidence interval or a hypothesis test.\nFor the CI, we get \\[\n\\left (  \\frac{53}{81} - \\frac{42}{81} \\right ) \\pm 1.96 \\sqrt{  \\frac{53 \\times 28 }{81^3} + \\frac{42 \\times 39}{81^3} } = ( 0.286 , -0.014 )\n\\] As the confidence interval contains zero there is no significant difference at the \\(5\\)% level.\nFor the hypothesis test we have the null hypothesis that \\(H_0 : p_1 = p_2\\) versus \\(H_1 : p_1 \\neq p_2\\). Then the test statistic is approximately normally distributed (large \\(n = n_1 + n_2\\)) is and given by \\[Z = \\frac{ 0.654 - 0.518 }{\\sqrt{ 0.0059} } = 1.875\\] At the \\(5\\)% level the critical value is \\(1.96\\) and so there’s not statistical significance.\nThe major issue here is whether you should test whether the proportions from home and away are different. There might be a significant home team bias in general and we should test to see if the Red Sox advantage is significantly different from that. All things else equal this will reduce the significance of the Red Sox home advantage bias.\n\n\nExercise 27.97 (Myocardial Infarction) In a five year study of the effects of aspirin on Myocardial Infarction (MI), or heart attack, you have the following dataset on the reduction of the probability of getting MI from taking aspirin versus a Placebo, or control.\n\n\n\nTreatment\nwith MI\nwithout MI\n\n\n\n\nPlacebo\n198\n10845\n\n\nAspirin\n104\n10933\n\n\n\n\nFind a \\(95\\)% confidence interval for the difference in proportions \\(p_1-p_2\\).\nPerform a hypothesis test of the null \\(H_0 : p_1 = p_2\\) at a \\(1\\)% significance level.\n\nSolution:\nConverting to proportions:\n\n\n\nTreatment\nwith MI\nwithout MI\nn\n\n\n\n\nPlacebo\n0.01793\n0.98207\n11043\n\n\nAspirin\n0.00942\n0.99058\n11037\n\n\n\n\nThe CI for a difference in proportion is: \\[(\\hat{p}_1-\\hat{p}_2)\\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1}+\\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\\] \\[=(0.01793-0.00942)\\pm 1.96\\sqrt{\\frac{0.01793(1-0.01793)}{11043}+\\frac{0.00942(1-0.00942)}{11037}}\\] \\[=0.00851\\pm 1.96\\sqrt{0.0000015945+0.0000008454}=0.00851\\pm 1.96\\sqrt{0.0000024399}\\] \\[=0.00851\\pm 1.96*0.00156205=0.00851\\pm 0.00306\\] Therefore, the CI for the difference in these proportions is: \\[(0.00545,0.01157)\\]\nAt the 1% level, we would have a implied CI for the difference in proportions of: \\[=0.00851\\pm 2.58*0.00156205=0.00851\\pm 0.00403\\] Therefore, the CI for the difference in these proportions is: \\[(0.00448,0.01254)\\] We can see that this CI does not contain 0, so therefore we can reject the null that \\(p_1=p_2\\) at the 1% significance level.\n\n\n\nExercise 27.98 (San Francisco Giants) In October 1992, the ownership of the San Francisco Giants considered a sale of the franchise that would have resulted in a move to Florida. A survey from the San Francisco Chronicle found that in a random sample of \\(625\\) people, 50.7% would be disappointed by the move. Find a 95% confidence interval of the population proportion\nSolution:\nThe 95% CI is found by \\[\n\\hat{p} \\pm z_{\\alpha /2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}=.507\\pm 1.96\\sqrt{\\frac{.507\\times .493}{625}}=.507\\pm 2\\times .020=\\left(.467,.547\\right)\n\\]\n\n\nExercise 27.99 (Pfizer) Pfizer introduced Viagra in early 1998 and during \\(1998\\) of the \\(6\\) million Viagra users \\(77\\) died from coronary problems such as heart attacks. Pfizer claimed that this rate is no more than that in the general population.\nYou find from a clinical study of \\(1,500,000\\) men who were not on Viagra that \\(11\\) of then died of coronary problems in the same length of time during the \\(77\\) Viagra users who dies in \\(1998\\).\nDo you agree with Pfizer’s claim that the proportion of Viagra users dying from coronary problems is no more than that of other comparable men?\nSolution:\nA 95% confidence interval for a difference in proportions \\(p_1 - p_2\\) is given by \\[( \\hat{p}_1 - \\hat{p}_2 ) \\pm 1.96\n\\sqrt{ \\frac{ \\hat{p}_1 ( 1 - \\hat{p}_1 ) }{ n_1 }  +\n\\frac{ \\hat{p}_2 ( 1 - \\hat{p}_2 ) }{ n_2 } }\\]\nCan do a confidence interval or a \\(Z\\)-score test.\nWith Viagra, \\(\\hat{p}_1 = 77/6000000 = 0.00001283\\) and without Viagra \\(\\hat{p}_2 = 11/1500000 = 0.00000733\\). Need to test whether these are equal.\nWith a \\(95\\)% confidence interval for \\(( p_1 - p_2 )\\), you get an interval \\(( 0.00000549 , 0.0000055 )\\) which doesn’t contain zero. Hence evidence that the proportion is higher.\nMeasured very accurately due to the large sample sizes.\nWith testing might use a one-sided test and an \\(\\alpha\\) of \\(0.01\\).\n\n\nExercise 27.100 (Voter: CI) Given a random sample of \\(1000\\) voters, \\(400\\) say they will vote for Donald Trump if the Republican nomination for the 2016 US Presidential Election. Given that he gets the nomination, a \\(95\\%\\) confidence interval for the true proportion of voters that will vote for him includes \\(45\\%\\).\nSolution:\nFalse, \\(\\hat p = \\frac{400}{1000} = 40\\%\\) and \\[\ns_{\\hat p} = \\sqrt{\\frac{\\hat p(1-\\hat p)}{1000}} = 0.0072\n\\] The \\(95\\%\\) confidence interval is \\([0.4 \\pm] 0.0142\\). which does not include \\(45\\%\\).\n\n\nExercise 27.101 (Survey AB testing) In a pre-election survey of \\(435\\) voters, \\(10\\) indicated that they planned to vote for Ralph Nader. In a separate survey of \\(500\\) people, \\(250\\) said they planned to vote for George Bush.\n\nPerform a hypothesis test at the 5% level for the hypothesis that Nader will get 3% or less of the vote.\nFind a 95% confidence interval for the difference in the proportion of people that will vote for George Bush versus Ralph Nader.\n\nSolution:\n\n\\(H_{0}:p\\leq 0.03\\) \\(H_{1}:p&gt;0.03\\) \\(z=\\frac{\\hat{p}-p_{0}}{\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}}\\) where \\(\\hat{p}=\\frac{10}{435}=0.023\\) and \\(p_{0}=0.03\\). Hence, \\(z=\\frac{0.023-0.03}{\\sqrt{\\frac{0.03\\cdot 0.97}{435}}}=-0.86\\), \\(z_{\\alpha }=1.65\\) (one-sided test!) so we do not reject \\(H_{0}\\) as \\(z&lt;z_{\\alpha }\\).\nWe know that the confidence interval is given by \\((\\hat{p}_{1}-\\hat{p}_{2})\\pm 1.96\\sqrt{\\frac{\\hat{p}_{1}(1-\\hat{p}_{1})}{n_{1}}+\\frac{\\hat{p}_{2}(1-\\hat{p}_{2})}{n_{2}}}\\) and we have \\(\\hat{p}_{1}=\\frac{250}{500}=0.5\\) and \\(\\hat{p}_{2}=\\frac{10}{435}=0.023\\). Plugging everything into the formula we find: \\((0.5-0.023)\\pm 1.96\\sqrt{\\frac{0.5\\cdot 0.5}{500}+\\frac{0.023\\cdot 0.977}{435}}=[0.431,0.523]\\)\n\n\n\nExercise 27.102 (Significance Tests.) Some defendants in southern states have challenged verdicts made during the ’50s and ’60s, because of possible unfair jury selections. Juries are supposed to be selected at random from the population. In one specific case, only 12 jurors in an 80 person panel were African American. In the state in question, 50% of the population was African American. Could a jury with 12 African Americans out of 80 people be the result of pure chance?\nUsing \\(n = 80\\), \\(X = 12\\) find a 99% confidence interval for the proportion \\(p\\). Is that significantly different from \\(p = 0.5\\).\n\n\nExercise 27.103 A marketing firm is studying the effects of background music on people’s buying behavior. A random sample of \\(150\\) people had classical music playing while shopping and \\(200\\) had pop music playing. The group that listened to classical music spent on average $ \\(74\\) with a standard deviation of $ \\(18\\) while the pop music group spent $ \\(78.4\\) with a standard deviation of $ \\(12\\).\n\nTest whether there is any significant difference between the difference in purchasing habits. Describe clearly your null and alternative hypotheses and any test statistics that you use.\nIs there a difference between using a \\(5\\)% and \\(1\\)% significance level.\n\nSolution:\nWe are given the following: \\(\\bar{x}=74,s_{x}=18,n_{x}=150,\\bar{y}=78.4,s_{y}=12,n_{y}=200\\)\n\\[\\begin{aligned}\nH_{0}:\\mu_{x}-\\mu_{y} & = & 0\\\\\nH_{1}:\\mu_{x}-\\mu_{y} & \\neq & 0\\end{aligned}\n\\]\nIn order to test for this hypothesis, we calculate the z-score and compare it with the z score at 5% and 1% confidence level.\\[z=\\frac{\\bar{x}-\\bar{y}}{\\sqrt{\\frac{s_{x}^{2}}{n_{x}}+\\frac{s_{y}^{2}}{n_{y}}}}=\\frac{74-78.4}{\\sqrt{\\frac{324}{150}+\\frac{144}{200}}}=-2.592\\]\nSince \\(z&gt;z_{0.025}\\), we can reject the null hypothesis at 95% confidence level and say that there exist significant differences between purchasing habits.\nFor \\(\\alpha=0.05\\), \\(z_{\\alpha/2}=1.96\\) and for \\(\\alpha=0.01\\), \\(z_{\\alpha/2}=2.575\\). Thus, the answer does not change when we move from 95% to 99% confidence level.\nAlternately, the above question can be solved by creating a confidence interval based on the differences. Again, we will reach the same conclusion.\n\n\nExercise 27.104 (Sensitivity and Specificity) The quality of Nvidia’s graphic chips have the probability that a randomly chosen chip being defective is only \\(0.1\\)%. You have invented a new technology for testing whether a given chip is defective or not. This test will always identify a defective chip as defective and only “falsely” identify a good chip as defective with probability \\(1\\)%\n\nWhat are the sensitivity and specificity of your testing device?\nGiven that the test identifies a defective chip, what’s the posterior probability that it is actually defective?\nWhat percentage of the chips will the new technology identify as being defective?\nShould you advise Nvidia to go ahead and implement your testing device? Explain.\n\nSolution:\nWe have the following probabilities (\\(D\\) represents “defective chip” and \\(T\\) represents “test result indicates defective chip”): \\[P(D)=0.001\\] \\[P(T|D)=1\\] \\[P(T|\\bar{D})=0.01\\]\n\nSensitivity: \\(P(T|D)=1\\) and Specificity: \\(P(\\bar{T}|\\bar{D})=1-P(T|\\bar{D})=1-0.01=0.99\\)\n\\[P(D|T)=\\frac{P(T|D)P(D)}{P(T)}=\\frac{P(T|D)P(D)}{P(T|D)P(D)+P(T|\\bar{D})P(\\bar{D})}\\] \\[=\\frac{(1)(0.001)}{(1)(0.001)+(0.01)(1-0.001)}=0.090992\\]\n\\[P(T) = P(T|D)P(D)+P(T|\\bar{D})P(\\bar{D})\\] \\[=(1)(0.001)+(0.01)(1-0.001)=0.01099\\]\nNo, only 9% of those chips indicated to be defective by the test will actually be defective. Essentially, we would be throwing away 91% of the chips indicated to be defective even though they are perfectly fine!\n\n\n\nExercise 27.105 (CI fo Google) Google is test marketing a new website design to see if it increases the number of click-through on banner ads. In a small study of a million page views they find the following table of responses\n\n\n\n\nTotal Viewers\nClick-Throughs\n\n\n\n\nnew design\n700,000\n10,000\n\n\nold design\n300,000\n2,000\n\n\n\nFind a \\(99\\)% confidence interval for the increase in the proportion of people who click-through on banner ads using the new web design.\nHint: a 99% confidence interval for a difference in proportions \\(p_1 - p_2\\) is given by \\(( \\hat{p}_1 - \\hat{p}_2 ) \\pm 2.58\\sqrt{ \\frac{ \\hat{p}_1 ( 1 - \\hat{p}_1 ) }{ n_1 }  + \\frac{ \\hat{p}_2 ( 1 - \\hat{p}_2 ) }{ n_2 } }\\)\nSolution:\nWe are interested in the distribution of \\(p_1-p_2\\). We have that: \\[\n\\begin{aligned}\n\\hat{p}_1 = \\frac{10000}{700000} = \\frac{1}{70} \\\\\n\\hat{p}_1 = \\frac{2000}{300000} = \\frac{1}{150}\n\\end{aligned}\n\\]\nFollowing the hint, we have that the \\(99\\%\\) confidence interval is:\n\\[\n\\begin{aligned}\np_1 - p_2 &\\in & \\Big[\\hat{p}_1- \\hat{p}_2 - 2.58\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1 )}{n_1}+\\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}, \\hat{p}_1- \\hat{p}_2 + 2.58\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1 )}{n_1}+\\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\\Big] \\\\\n&=& [0.00762-0.00053,0.00762+0.00053]  \\\\\n&=& [0.00709,0.00815]\n\\end{aligned}\n\\]\n\n\nExercise 27.106 (Amazon) Amazon is test marketing a new package delivery system. It wants to see if same-day service is feasible for packages bought with Amazon prime. In a small study of a hundred thousand delivers they find the following times for delivery\n\n\n\n\nDeliveries\nMean-Time (Hours)\nStandard Deviation-Time\n\n\n\n\nnew system\n80,000\n4.5\n2.1\n\n\nold system\n20,000\n5.6\n2.5\n\n\n\n\nFind a \\(95\\)% confidence interval for the decrease in delivery time.\nIf they switch to the new system, what proportion of deliveries will be under \\(5\\) hours which is required to guarantee same day service.\n\nHint: a 95% confidence interval for a difference in means \\(\\mu_1 - \\mu_2\\) is given by \\(( \\bar{x}_1 - \\bar{x}_2 ) \\pm 1.96\n\\sqrt{ \\frac{s_1^2 }{ n_1 }  + \\frac{ s_2^2 }{ n_2 } }\\) ]\nSolution:\n\nFollow the hint, the answer is (-1.14, -1.06).\n\n\n(4.5-5.6)-1.96*sqrt(2.1^2/80000+2.5^2/20000) #-1.14\n\n -1.1\n\n(4.5-5.6)+1.96*sqrt(2.1^2/80000+2.5^2/20000) #-1.06\n\n -1.1\n\n\nHence, the 95% confidence interval is (-1.14, -1.06). a) As we have a large sample size, we can assume a normal distribution with given sample mean and standard deviation, \\(P(T \\leq 5) = 0.594\\)\n\npnorm(5,mean=4.5,sd=2.1) #0.594\n\n 0.59\n\n\nHence, 59.4% of deliveries will be under 5 hours.\n\n\nExercise 27.107 (Vitamin C) In the very famous study of the benefits of Vitamin C, \\(279\\) people were randomly assigned to a dose of vitamin C or a placebo (control of nothing). The objective was to study where vitamin C reduces the incidence of a common cold. The following table provides the responses from the experiment\n\n\n\nGroup\nColds\nTotal\n\n\n\n\nVitamin C\n17\n139\n\n\nPlacebo\n31\n140\n\n\n\n\nIs there a significant difference in the proportion of colds between the vitamin C and placebo groups?\nFind a \\(99\\)% confidence interval for the difference. Would you recommend the use of vitamin C to prevent a cold?\n\nHint: a 95% confidence interval for a difference in proportions \\(p_1 - p_2\\) is given by \\[\n( \\hat{p}_1 - \\hat{p}_2 ) \\pm 1.96  \\sqrt{ \\frac{ \\hat{p}_1 (1- \\hat{p}_1) }{ n_1 }  + \\frac{ \\hat{p}_2 (1- \\hat{p}_2)   }{ n_2 } }\n\\]\nSolution:\nTo test \\[H_0: p_1 - p_2 \\neq 0,\\] we could perform a two-sample t-test. \\[\n\\hat{p}_1 = \\frac{17}{139} = 12.23\\% \\mbox{ and } \\hat{p}_2 = \\frac{31}{140} = 22.14\\%\n\\] \\[\n\\hat{\\sigma} = \\sqrt{ \\frac{ \\hat{p}_1 (1- \\hat{p}_1) }{ n_1 }  + \\frac{ \\hat{p}_2 (1- \\hat{p}_2)   }{ n_2 } }  = 4.48\\%\n\\] \\[\nt-stat = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\hat{\\sigma} } = -2.21 &lt; -1.96 = qnorm(0.025)\n\\] Yes, we can reject the null hypothesis and there is a significant difference at 5% level.\nTo form the 99% confidence interval of the difference, we use\n\nqnorm(0.995) #2.58.\n\n 2.6\n\n\n\\[\n0 \\in [-9.91\\% - 2.58*4.48\\%, -9.91\\% + 2.58*4.48\\%]\n\\] We cannot recommend the use of vitamin to prevent a cold at the 1% significance level.\n\n\nExercise 27.108 (Facebook vs Reading) In a recent article it was claimed that “\\(96\\)% of Americans under the age of \\(50\\)” spent more than three hours a day on Facebook.\nTo test this hypothesis, a survey of \\(418\\) people under the age of \\(50\\) were taken and it was found that \\(401\\) used Facebook for more than three hours a day.\nTest the hypothesis at the \\(5\\)% level that the claim of \\(96\\)% is correct.\nSolution:\nWe want to test the proportion of people under the age of 50 on Facebook. \\[H_0: p = 96\\%\\]\nThe estimated proportion from the survey is \\[\\hat{p} = 401/418 = 0.959\\]\nThe standard deviation of the sample proportion is \\[\\hat{\\sigma} = \\sqrt{\\frac{\\hat{p}*(1-\\hat{p})}{n}} = 0.0096\\]\nTo perform a t-test, the corresponding statistic is \\[\\mbox{t-stat} = \\frac{0.959 - 0.96}{0.0096} = -0.1\\]\nIt seems that we cannot reject the null hypothesis at the 5% level.\n\n\nExercise 27.109 (Paired T-Test) The following table shows the outcome of eight years of a ten year bet that Warren Buffett placed with Protege Partners, a New York hedge fund. Buffett claimed that a simple index fund would beat a portfolio strategy (fund-of-funds) picked by Protege over a ten year time frame. At Buffett’s shareholder meeting, he provided an update of the current state of the bet. The bundle of hedge funds picked by Protege had returned \\(21.9\\)% in the eight years through \\(2015\\) and the S&P500 index fund had soared \\(65.7\\)%.\n\n\n\n\nSP Index\nHedge Funds\n\n\n\n\n2008\n-37.0%\n-23.9%\n\n\n2009\n26.6%\n15.9%\n\n\n2010\n15.1%\n8.5%\n\n\n2011\n2.1%\n-1.9%\n\n\n2012\n16.0%\n6.5%\n\n\n2013\n32.3%\n11.8%\n\n\n2014\n13.6%\n5.6%\n\n\n2015\n1.4%\n1.7%\n\n\ncumulative\n65.7%\n21.9%\n\n\n\n\nUse a paired \\(t\\)-test to assess the statistical significance between the two return strategies\nHow likely is Buffett to win his bet in two years?\n\nSolution:\nTo conduct a paired t-test, we need define a new variable \\[\nD = SP.Index - Hedge.Funds.\n\\] Assume that \\(D_i \\overset{i.i.d.}{\\sim} N(\\mu, \\sigma^2)\\). the null hypothesis we want to test is \\[\nH_0: \\mu=0\n\\] The mean and standard deviation of the difference \\(\\{D_i\\}_{i=1}^n\\) are \\[\n\\bar{D} = 0.0574,\\quad s_D = 0.0969\n\\]\n\nSP.Index = c(-37,26.6,15.1,2.1,16,32.3,13.6,1.4)\nHedge.Funds = c(-23.9,15.9,8.5,-1.9,6.5,11.8,5.6,1.7)\nSP.Index = SP.Index/100\nHedge.Funds = Hedge.Funds/100\nmean(SP.Index-Hedge.Funds)\n\n 0.057\n\nsd(SP.Index-Hedge.Funds)\n\n 0.097\n\n\nTherefore the t-statistic is \\[t=\\frac{\\bar{D}}{s_D\\sqrt{1/n}} = \\frac{0.0574}{0.0969\\times\\sqrt{1/8}}=1.6755.\\] Notice that the degree of freedom is \\(8 - 1 = 7\\). The corresponding p-value is \\[P(|t_7|&gt;1.6755) = 0.1378\\]\n\n2*pt(1.6755,df=7,lower.tail = FALSE) #0.1377437\n\n 0.14\n\n\nIn R, you can easily perform t-test using the following command, which gives the same test result as above.\n\nt.test(SP.Index,Hedge.Funds,paired = TRUE)\n\n\n    Paired t-test\n\ndata:  SP.Index and Hedge.Funds\nt = 2, df = 7, p-value = 0.1\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.024  0.138\nsample estimates:\nmean difference \n          0.057 \n\n\nThe p-value is larger than 0.05. Thus we don’t reject the null hypothesis at 5% significance level. In other words, these two return strategies are not significantly different.\nFor the second question, the exact probability of Buffett winning his bet is calculated by \\[P\\left(\\frac{(1+S_9)(1+S_{10})(1+65.7\\%)}{(1+H_9)(1+H_{10})(1+21.9\\%)}&gt;1\\right)\\] where \\(S_9, S_{10}, H_9, H_{10}\\) are returns of two strategies in 9th and 10th year. To derive the distribution of the left hand side expression is difficult. Here we instead give a simulation solution, where the returns are assumed to be i.i.d. Normal variables.\n\nm1 = mean(SP.Index)\ns1 = sd(SP.Index)\nm2 = mean(Hedge.Funds)\ns2 = sd(Hedge.Funds)\nN = 1000 \ncum1 = NULL # cumulative return of SP.Index\ncum2 = NULL # cumulative return of Hedge.Funds\nset.seed(410)\nfor (i in 1:N)\n{ \n  # simulate returns in 2016 and 2017\n  return1 = c(0.657, rnorm(n=2, mean = m1, sd = s1)) \n  # compute the cumulative return\n  cum1 = c(cum1, cumprod(1+return1)) \n  \n  return2 = c(0.219, rnorm(n=2, mean = m2, sd = s2))\n  cum2 = c(cum2, cumprod(1+return2))\n}\n\n# compute the prob. of Buffett winning\nmean(cum1&gt;cum2) #0.9296667\n\n 0.93\n\n\nThe estimated probability is 92.97%, indicating that Buffett is very likely to win his bet. Since the paired t - test suggests no significant difference in average returns, you can also estimate the mean by pooling the data and redo the simulation, which gives a smaller estimate (89.27%).\n\n\nExercise 27.110 (Shaquille O’Neal) Shaquille O’Neal (nicknamed Shaq) is an ex-American Professional basketball player. He was notoriously bad at free throws (an uncontested shot given to a player when they are fouled). The following table compares the first three years of Shaq’s career to his last three years.\n\n\n\nGroup\nFree Throws Made\nFree Throws Attempted\n\n\n\n\nEarly Years\n1352\n2425\n\n\nLater Years\n1121\n2132\n\n\n\nDid Shaq get worse at free throws over his career?\nSolution:\n\\[\n\\hat{p}_1 = \\frac{1352}{2425} = 0.5575, \\quad \\hat{p}_2 = \\frac{1121}{2132} = 0.5258,\n\\quad se( \\hat{p_1} - \\hat{p_2} ) = \\sqrt{ \\frac{ \\hat{p}_1 (1- \\hat{p}_1) }{ n_1 }  + \\frac{ \\hat{p}_2 (1- \\hat{p}_2)   }{ n_2 } } = 0.0148\n\\] The \\(Z\\)-score is \\[\nZ = \\frac{ \\hat{p}_1 - \\hat{p}_2 }{ se( \\hat{p_1} - \\hat{p_2} ) } = 2.14 \\; .\n\\] The p-value of Z-test is \\[P(|N(0,1)|&gt;2.14) = 0.0324\\] Thus we conclude that Shaq got worse at free throws.\n\n\nExercise 27.111 (Furniture Website) Furniture.com wishes to estimate the average annual expenditure on furniture among its subscribers. A sample of 1000 subscribers is taken. The sample standard deviation is found to be $670 and the sample mean is $3790.\n\nGive a 90% confidence interval for the average expenditure.\nTest the hypothesis that the average expenditure is less than $2,500 at the 1% level.\n\n\n\nExercise 27.112 (Grocery AB Testting) A grocery delivery service is studying the impact of weather on customer ordering behavior. To do so, they identified households which made orders both in October and in February on the same day of the week and approximately the same time of day. They found 112 such matched orders. The mean purchase amount in October was $121.45, with a standard deviation of $32.78, while the mean purchase amount in February was $135.99 with a standard deviation of $24.81. The standard deviation of the difference between the two orders was $38.28. To quantify the evidence in the data regarding a difference in the average purchase amount between the two months, compute the \\(p\\)-value of the data.\n\n\nExercise 27.113 (SimCity AB Testing) SimCity 5 is one of Electronic Arts (EA’s) most popular video games. As EA prepared to release the new version, they released a promotional offer to drive more pre-orders. The offer was displayed on their webpage as a banner across the top of the pre-order page. They decided to test some other options to see what design or layout would drive more revenue.\nThe control removed the promotional offer from the page altogether. The test lead to some very surprising results. With a sample size of \\(1000\\) visitors, of the \\(500\\) which got the promotional offer they found \\(143\\) people wanted to purchase the games and of the half that got the control they found that \\(199\\) wanted to buy the new version of SimCity.\nTest at the \\(1\\)% level whether EA should provide a promotional offer or not.\nSolution:\nTo see whether the promotion program works, we need to find out whether the proportion of people who want to buy the game is larger. Denote \\(p_1\\) is the proportion from the experimental group with promotion and \\(p_2\\) from the control group without promotion. Therefore, the null hypothesis we are testing is \\[H_0: p_1 &gt; p_2\\]\nIn the sample, we observe \\[n_1 = n_2 = 500\\] \\[\\hat{p_1} = 199/500 \\mbox{ and } \\hat{p_2} = 143/500\\]\nTo perform the two-sample t-test, we need to calculate the pooled sample standard deviation first. \\[\\bar{p} = \\hat{p_1}*\\frac{n_1}{n_1+n_2} + \\hat{p_2}*\\frac{n_2}{n_1+n_2} = 171/500\\] \\[\\bar{\\sigma} = \\sqrt{\\bar{p}*(1-\\bar{p})*(\\frac{1}{n_1} + \\frac{1}{n_2})} = 0.03\\]\nThen, \\[\\mbox{t-stat} = \\frac{\\hat{p_1} - \\hat{p_2}}{\\bar{\\sigma}} = 3.73 &gt; \\mbox{qnorm(0.995)} = 2.58\\]\nNow we can conclude that the promotional program works statistically at the 1% level.\n\n\nExercise 27.114 (True/False)  \n\nAt the Apple conference it was claimed that “\\(97\\)% of people love the iWatch”. From a market survey, you found empirically that \\(1175\\) out of a sample of \\(1400\\) people love the new iWatch. Statistically speaking,you can reject Apple’s claim at the \\(1\\)% significance level. Hint: You may use \\(pnorm(2.58) = 0.995\\)\nGiven a random sample of \\(2000\\) voters, \\(800\\) say they will vote for Hillary Clinton in the 2016 US Presidential Election. At the \\(95\\)%level, I can reject the null hypothesis that Hillary has an evens chance of winning the election.\nThe average movie is Netflix’s database has an average customer rating of \\(3.1\\) with a standard deviation of \\(1\\). The last episode of Breaking Bad had a rating of \\(4.7\\) with a standard deviation of \\(0.5\\). The\\(p-value\\) for testing whether Breaking Bad’s rating is statistical different from the average is a lot less than \\(1\\)%.\nA chip manufacturer needs to add the right amount of chemicals to make the chips resistant to heat. On average the population of chips needs to be able to withstand heat of 300 degrees. Suppose you have a random sample of \\(30\\) chips with a mean of \\(305\\) and a standard deviation of\\(8\\). Then you can reject the null hypothesis \\(H_0: \\mu= 300\\) versus\\(H_a: \\mu&gt; 300\\) at the \\(5\\)% level.\nZagats rates restaurants on food quality. In a random sample of \\(100\\)restaurants you observe a mean of \\(20\\) with a standard deviation of\\(2.5\\). Your favorite restaurant has a score of \\(25\\). This is statistically different from the population mean at the \\(5\\)% level.\nThe \\(t\\)-score is used to test whether a null hypothesis can be rejected.\nAn oil company introduces a new fuel that they claim has on average no more than \\(100\\) milligrams of toxic matter per liter. They take a sample of \\(100\\) liters and find that \\(\\bar{X} = 105\\) with a given\\(\\sigma_X = 20\\). Then there is evidence at the \\(5\\)% level that their claim is wrong.\nA wine producer claims that the proportion of customers who cannot distinguish his product from grape juice is at most 5%. For a sample of\\(100\\) people he finds that \\(10\\) fail the taste test. He should reject his null hypothesis \\(H_{0}:p=0.05\\) at the \\(5\\)% level.\nAs the \\(t\\)-ratio increases the \\(p\\)-value of a hypothesis test decreases.\n\nSolution:\n\nTrue. \\[\\hat{p} = \\frac{1175}{1400} = 83.9\\%\\]\\[\\hat{\\sigma} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{1400}} = 0.98\\%\\] For\\(H_0: p = 97\\%\\),\\[t-stat = \\frac{\\hat{p} - 97\\%}{0.98\\%} = -13.37 &lt; -2.58 = qnorm(0.005)\\]Yes, we can reject the null hypothesis at 1% significance level.\nTrue. The t-statistic is\\(T = (\\hat{p}-0.5)/\\sqrt{\\frac{\\hat{p} \\times (1-\\hat{p})}{n}} = -9.13 &lt;\\)where \\(qnorm(0.025) = -1.96\\)\nFalse. The formula for a T-ratio is \\[\\begin{aligned}T = \\frac{\\bar{x}_1-\\bar{x}_2 - (\\mu_1 -\\mu_2)}{\\sqrt{\\frac{s_{x_1}^2}{n_1}+\\frac{s_{x_2}^2}{n_2}}}\\end{aligned}\\] However, since we do not know \\(n_1\\) and \\(n_2\\) we cannot get the T-ratio and thus p-value.\nTrue. Since the sample size is \\(n\\ge30\\) the t test is effectively the same as the z test. Therefore, we find the following z value:\\[z=\\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}=\\frac{305-300}{8/\\sqrt{30}}=3.4233\\]Here we use a one-sided test: \\[z_\\alpha=1.65\\] Since \\(z&gt;z_\\alpha\\), we can reject \\(H_0\\) in favor of \\(H_a\\) at the 5% level\nTrue. We can calculate the t statistic as \\((25-20)/(2.5/10)&gt;1.96\\). Hence we reject the null.\nTrue. The \\(t\\) score can be used to test whether a null hypothesis can be rejected or not. Alternately, a z score can be used\nTrue. This is a one-side test \\(H_0 : \\mu_x \\leq 100\\) versus\\(H_1 : \\mu_X &gt; 100\\). The \\(Z\\)-score is\\(Z = \\frac{105 -100}{ 20 / \\sqrt{100} } = 2.5\\). The critical value for a 1-sided test at the \\(5\\)% level is \\(1.64\\). Therefore you can reject \\(H_0\\)are the \\(5\\)% .\nTrue. \\(z=\\frac{\\hat{p}-p_{0}}{\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}}=\\frac{0.1-0.05}{\\sqrt{\\frac{0.05\\cdot 0.95}{100}}}=2.29\\) \\(z_{\\alpha /2}=1.96\\)Reject the null-hypothesis since \\(\\left\\vert z\\right\\vert &gt;z_{\\alpha/2}\\)\nTrue. If \\(t&gt;0\\) moving out into the tails decreases the \\(p\\)-value. If \\(t&lt;0\\) it will increase the \\(p\\)-value as the \\(t\\) is getting less negative.\n\n\n\nExercise 27.115 (True/False: CI)  \n\nThere is much discussion of the effects of second-hand smoke. In a survey of \\(500\\) children who live in families where someone smokes, it was found that \\(10\\) children were in poor health. A \\(95\\)% confidence interval for the probability of a child living in a smoking family being in poor health is then \\(2\\)% to \\(4\\)%.\nYou are finding a confidence interval for a population mean. Holding everything else constant, an interval based on an unknown standard deviation will be wider than one based on a known standard deviation no matter what the sample size is.\nThere is a \\(95\\)% probability that a normal random variable lies between \\(\\mu \\pm \\sigma\\).\nA recent CNN poll found that \\(49\\)% of \\(10000\\) voters said they would vote for Obama versus Romney if that was the election in November 2012. A \\(95\\)% confidence interval for the true proportion of voters that would vote for Obama is then \\(0.49 \\pm 0.03\\).\nA mileage test for a new electric car model called the “Pizzazz” is conducted. With a sample size of \\(n=30\\) the mean mileage for the sample is \\(36.8\\) miles with a sample standard deviation of \\(4.5\\). A \\(95\\)% confidence interval for the population mean is \\((32.3,41.3)\\) miles.\nIn a random sample of \\(100\\) NCAA basketball games, the team leading after one quarter won the game \\(72\\) times. Then a 95% confidence interval for the proportion of teams leading after the first quarter that go on to win is approximately \\((0.6,0.84)\\).\nFor the same sample, a 95% prediction interval for a particular team winning is also \\((0.6,0.84)\\).\nIn playing poker in Vegas, from \\(100\\) hours of play, you make an average of $50 per hour with a standard deviation of $10. A 95% confidence interval for your mean gain per hour is approximately $ \\(( 48 , 52 )\\)\nIf 27 out of 100 respondents to a survey state that they drink Pepsi then a 95% confidence interval for the proportion \\(p\\) of the population that drinks Pepsi is \\(( 0.26 , 0.28 )\\).\nThe \\(p\\)-value is the probability that the Null hypothesis is true.\n\nSolution:\n\nFalse The \\(95\\%\\) confidence interval should be: \\[\\begin{aligned} \\Bigg[.02 - 1.96\\sqrt{\\frac{.02(1-.02)}{500}},.02 + 1.96\\sqrt{\\frac{.02(1-.02)}{500}}\\Bigg]\\end{aligned}\\] So, the lower bound on this interval will definitely be less than 2 percent.\nTrue Because you need to use the same data to estimate the standard deviation, and thus contains more noise or error. In other words, we are comparing between \\(z\\) distribution and \\(t\\) distribution\nFalse. There is a 95% probability that it lies between \\(\\mu\\pm1.96\\sigma\\)\nFalse. The confidence interval for the proportion is given by: \\[\\hat{p}\\pm1.96\\sqrt{\\hat{p}(1-\\hat{p})/n}\\] This gives: \\[0.49\\pm1.96\\sqrt{(0.49)(0.51)/10000}=0.49\\pm0.009798\\]\nFalse. \\(CI=36.8\\pm1.96\\frac{4.5}{\\sqrt{30}}=(35.19,38.41)\\)\nFalse. The 95% CI is approximately equal to \\[.72 \\pm  1.96 \\sqrt{ \\frac{ \\hat{p} (1- \\hat{p})}{n} } = 0.72 \\pm 1.96 \\times 0.045=(.63,.81)\\]\nFalse. The prediction interval is equal to \\[0.72 \\pm 2 \\sqrt{ \\hat{p} (1- \\hat{p} ) } \\sqrt{1+1/n}\\] In this case, that contains the entire interval (0,1)\nTrue. The CI is given by \\(50 \\pm 2 \\times \\frac{10}{\\sqrt{100} } =(48,52)\\)\nFalse. The 95% confidence interval for the proportion is given by \\(\\hat{p} \\pm 1.96 \\sqrt{ \\hat{p} ( 1 - \\hat{p} ) / n }\\). Which here gives \\(0.27 \\pm 1.96 \\sqrt{ 0.27 \\times 0.73 / 100 }\\) or the interval \\(( 0.18 , 0.36 )\\)\nFalse. The p-value is the probability of the observing something more extreme than the observed sample assuming the null hypothesis is true\n\n\n\nExercise 27.116 (True/False: Sampling Distribution)  \n\nThe Central Limit Theorem states that the distribution of a sample mean is approximately Normal.\nThe Central Limit Theorem states that the distribution of the sample mean \\(\\bar{X}\\) is Normally distributed for large samples.\nThe Central Limit Theorem guarantees that the distribution of \\(\\bar{X}\\) is constant.\nThe sample mean, \\(\\bar{x}\\), approximates the population mean for large random samples.\nThe trimmed mean of a dataset is more sensitive to outliers than the mean.\nThe sample mean of a dataset must be larger than its standard deviation\nSelection bias is not a problem when you are estimating a population mean.\nThe kurtosis of a distribution is not sensitive to outliers.\n\nSolution:\n\nTrue. The standard deviation of the sample mean is \\(\\frac{\\sigma}{\\sqrt{n}}\\). The CLT has (approximately) \\(\\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})\\), where \\(\\mu\\) and \\(\\sigma^2\\) are the population true value\nTrue. If \\(X_i\\) is a random sample (or \\(iid\\)) with mean \\(\\mu\\), then \\(\\sqrt{n}\\big(\\bar{X} - \\mu \\big) \\longrightarrow^d N(0,\\sigma^2)\\)\nFalse. For some set of independent random variables, the mean of a sufficiently large number of them (each with finite mean and variance) will have a distribution with variance depending on the number of samples.\nTrue. The CLT states that the \\(\\bar{X} \\sim N \\left ( \\mu, \\frac{\\sigma^2}{n} \\right )\\), or the best guess for the population mean is \\(\\bar{x}\\)\nFalse. The trimmed mean is the average of the observations deleting the outer 5% in the tails. Hence, it is less sensitive to outliers than the sample .\nFalse. Many datasets have a negative mean and the standard deviation must be positive.\nFalse. When sampling to estimate a population mean, you need to be certain to select a random sample, or selection bias may affect your results.\nFalse. Kurtosis depends on each observation in the distribution and is extremely sensitive to the outliers",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "ex.html#field-vs-observational",
    "href": "ex.html#field-vs-observational",
    "title": "27  Exercises",
    "section": "27.6 Field vs Observational",
    "text": "27.6 Field vs Observational\n\nExercise 27.117 What is the difference between a randomized trial and an observational study?\n\n\n\nExercise 27.118 Consider a complex A/B experiment, with 6 alternatives, when you you have 5 variations to your page, plus the original.\n\nUse Bonferroni correction for multiple comparisons. Calculate the significance level for each of the 5 tests and find the number of samples needed to achieve a power of 0.95. Assume that the significance level for each test is .05.\nImplement TS for the same experiment. Assume an original arm with a 4% conversion rate, and an optimal arm with a 5% conversion rate. The other 4 arms include one suboptimal arm that beats the original with conversion rate of 4.5%, and three inferior arms with rates of 3%, 2%, and 3.5%. Plot the savings from a six-armed experiment, relative to a Bonferroni adjusted power calculation for a classical experiment. First plot should show the number of days required to end the experiment, with the vertical line showing the time required by the classical power calculation. The second plot should show the number of conversions that were saved by the bandit. What is the overall cost savings due to ending the experiment more quickly, and due to to the experiment being less wasteful while it is running?\nRun your simulator 500 times and shows the history of the serving weights for all the arms in the first of our 500 simulation runs. Comment on the results.\nPlot the daily cost of running the multi-armed bandit relative to an “oracle” strategy of always playing arm 2, the optimal arm\n\nSolution:\n\nThe standard power calculation with a significance level of 0.05 is .05 / (6 - 1), and find that we need 15,307 observations in each arm of the experiment. With 6 arms that’s a total of 91,842 observations\nThe TS algorithm will converge to the optimal arm in 3,000 observations, and will have a 95% chance of having converged to the optimal arm in 5,000 observations. The savings are 86,842 observations, or 95% of the observations required by the classical power calculation.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "ex.html#added-after-course-started",
    "href": "ex.html#added-after-course-started",
    "title": "27  Exercises",
    "section": "27.7 Added After Course Started",
    "text": "27.7 Added After Course Started\n\nExercise 27.119 (Emily, Car, Stock Market, Sweepstakes, Vacation and Bayes.)  \n\nEmily is taking Bayesian Analysis course. She believes she will get an A with probability 0.6, a B with probability 0.3, and a C or less with probability 0.1. At the end of semester she will get a car as a present form her (very) rich uncle depending on her class performance. For getting an A in the course Emily will get a car with probability 0.8, for B with probability 0.5, and for anything less than B, she will get a car with probability of 0.2. These are the probabilities if the market is bullish. If the market is bearish, the uncle is less likely to make expensive presents, and the above probabilities are 0.5, 0.3, and 0.1, respectively. The probabilities of bullish and bearish market are equal, 0.5 each. If Emily gets a car, she would travel to Redington Shores with probability 0.7, or stay on campus with probability 0.3. If she does not get a car, these two probabilities are 0.2 and 0.8, respectively. Independently, Emily may be a lucky winner of a sweepstake lottery for a free air ticket and vacation in hotel Sol at Redington Shores. The chance to win the sweepstake is 0.001, but if Emily wins, she will go to vacation with probability of 0.99, irrespective of what happened with the car.\nAfter the semester was over you learned that Emily is at Redington Shores.\n\nWhat is the probability that she got a car?\nWhat is the probability that she won the sweepstakes?\nWhat is the probability that she got a B in the course?\nWhat is the probability that the market was bearish?\n\nHint: You can solve this problem by any of the 3 ways: (ii) direct simulation using R, or Python, and (ii) exact calculation. Use just one of the two ways to solve it. The exact solution, although straightforward, may be quite messy.\nSolution: We introduce the following RV’s:\n\n\\(C \\in \\{0,1\\}\\): Emily gets a car\n\\(G\\in \\{A,B,\\le C\\}\\): Emily’s grade\n\\(M\\in \\{1, 0\\}\\): Market condition (Bullish, Bearish)\n\\(R\\in \\{0, 1\\}\\): Vacation (Redington Shores)\n\\(L\\): Emily is a lucky winner of a sweepstake\n\nThe conditional independence is given by the following graph \n\nwe can write the joint distribution as \\[\nP(C,G,M,R,L) = P(M)P(G)P(C|G,M)P(L)P(R|C,L)\n\\]\nHere is what we know\n\n\n\n\n\n\n\nMarket (\\(M\\))\n\n\n0\n1\n\n\n\n\n0.5\n0.5\n\n\n\n\n\n\nGrade (\\(G\\))\n\n\nA\nB\n\\(\\le C\\)\n\n\n\n\n0.6\n0.3\n0.1\n\n\n\n\n\n\n\n\nCar (\\(C\\mid M,G\\))\n\n\n0\n1\nM\nG\n\n\n\n\n0.5\n0.5\n0\nA\n\n\n0.7\n0.3\n0\nB\n\n\n0.9\n0.1\n0\nC\n\n\n0.2\n0.8\n1\nA\n\n\n0.5\n0.5\n1\nB\n\n\n0.8\n0.2\n1\nC\n\n\n\n\n\n\nVacation (\\(R\\mid L,C\\))\n\n\n0\n1\nL\nC\n\n\n\n\n0.8\n0.2\n0\n0\n\n\n0.3\n0.7\n0\n1\n\n\n0.01\n0.99\n1\n0\n\n\n0.01\n0.99\n1\n1\n\n\n\n\n\n\n\nWe can calculate the probability of getting a car as \\[\nP(C=1\\mid R=1) = P(C,R)/P(R) = \\sum_{G,M,L}P(C=1,G,M,R=1,L)/\\sum_{C,G,M,L}P(C,G,M,R=1,L)\n\\]\n\n\nm=c(0.5,0.5) # P(M)\ng=c(0.6,0.3,0.1) # P(G)\njmg = c(t(outer(m, g, FUN = \"*\")))  # P(G)P(M)\ncmg = c(0.5,0.3,0.1,0.8,0.5,0.2) # P(C=1 | M,G)\njcmg=jmg*cmg # P(M)P(G)P(C=1 | M,G)\nl=c(0.001,0.999) # L\nrlc = c(0.99,0.7) # P(R=1 | C=1,L)\ncr=outer(jcmg, l*rlc, FUN = \"*\") # P(C=1 | M,G)P(M)P(G)P(R=1 | C=1,L)\nsum(cr) # sum P(C=1 | M,G)P(M)P(G)P(R=1 | C=1,L)\n\n 0.37\n\nc0mg = c(0.5,0.7,0.9,0.2,0.5,0.8) # P(C=0 | M,G)\njc0mg=jmg*c0mg # P(C=0 | M,G)P(M)P(G)\nrlc0 = c(0.99,0.2) # P(R=1 | C=0,L)\nc0r=outer(jc0mg, l*rlc0, FUN = \"*\") # P(C=1 | M,G)P(M)P(G)P(R=1 | C=1,L)\nsum(c0r)\n\n 0.095\n\nPR = sum(cr) + sum(c0r)\n\nThus \\(P(C=1,R=1) = 0.3676523\\) and \\(P(R=1) = 0.4630275\\), take the ratio to get \\(P(C=1\\mid R=1)\\)\n\nsum(cr)/PR\n\n 0.79\n\n\n\\[\nP(C=1\\mid R=1) = 0.3676523/0.4630275 = 0.794\n\\] We can also use MC simulations\n\ngmc  = array(c(0.5,0.7,0.9,0.2,0.5,0.8, 0.5,0.3,0.1,0.8,0.5,0.2), dim=c(3,2,2)) # G,M,C\nclr = array(c(0.8 ,0.3 ,0.01,0.01,0.2 ,0.7 ,0.99,0.99), dim=c(2,2,2)) # C,L,R\n\nsimvac = function(){\n    m = sample(1:2,1,prob=c(0.5,0.5))\n    g = sample(1:3,1,prob=c(0.6,0.3,0.1))\n    c = sample(1:2,1,prob=gmc[g,m,])\n    l = sample(1:2,1,prob=c(0.999,0.001))\n    r = sample(1:2,1,prob=clr[c,l,])    \n    return(c(m,g,c,l,r))\n}\nN = 10000\nd = matrix(0,N,5)\nfor (i in 1:N){\n    d[i,] = simvac()\n}\nPR = sum(d[,5]==2)/N\nprint(PR)\n\n 0.46\n\nsum(d[,3]==2 & d[,5]==2)/sum(d[,5]==2)\n\n 0.79\n\n\n\n\n\n\nPLR = 0.001 * 0.99\nPLR/PR\n\n 0.0021\n\n\n\\[\nP(L=1\\mid R=1) = \\dfrac{0.00099}{0.4630} = 0.002138102\n\\]\n\n\n\n\nPRGB =\n0.5*0.3*0.999*0.5*0.7+\n0.5*0.3*0.999*0.5*0.2+\n0.5*0.3*0.999*0.3*0.7+\n0.5*0.3*0.999*0.7*0.2+\n0.3 * 0.001 * 0.99\n# P(G=B , R=1)\nprint(PRGB)\n\n 0.12\n\n# P(G=B | R=1)\nPRGB/PR\n\n 0.26\n\n\n\nPMR\n\n\nPMR = #P(M,R)\n0.5*0.6*0.999*0.8*0.7+\n0.5*0.6*0.999*0.2*0.2+\n0.5*0.3*0.999*0.5*0.7+\n0.5*0.3*0.999*0.5*0.2+\n0.5*0.1*0.999*0.2*0.7+\n0.5*0.1*0.999*0.8*0.2+\n0.5 * 0.001 * 0.99\n#P(M,R)\nprint(PMR)\n\n 0.26\n\n#P(M=0,R)\n1-PMR/PR\n\n 0.43\n\n\n\n\nExercise 27.120 (Poisson: Website visits) A web designer is analyzing traffic on a web site. Assume the number of visitors arriving at the site at a given time of day is modeled as a Poisson random variable with a rate of \\(\\lambda\\) visitors per minute. Based on prior experience with similar web sites, the following estimates are given:\n\nThere is a 90% probability that the rate is greater than 5 visitors per minute.\nThe rate is equally likely to be greater than or less than 14 visitors per minute.\nThere is a 90% probability that the rate is less than 27 visitors per minute.\n\nFind a Gamma prior distribution for the arrival rate that fits these judgments as well as possible. Comment on your results.\nHint: there is no “right” answer to this problem. You can use trial and error to find a distribution that fits as well as possible. You can also use an optimization method such as Excel Solver to minimize a measure of how far apart the given quantiles are from the ones in the target distribution.\nSolution:\nWe can try different values to see whether we can get a result that is close to the given quantiles. Given, there are only 2 parameters, I simply did a grid-search. I generated a grid of 200x200 points and calculated the quantiles for each point. Then I found the point that was closest to the given quantiles. I used sum of absolute value differences to measure the distance between the given and calculated quantiles. You can use squared differences instead.\n\ng = expand.grid(seq(from=0.5,to=7,length=200),seq(from=0.5,to=10,length=200))\nqf = function(th) {\n  qgamma(c(0.1, 0.5, 0.9), shape=th[1], scale=th[2])\n}\nr = apply(g,1,qf)\ntarget = c(5,14,27)\nind = which.min((colSums(abs(r-target))))\nprint(g[ind,])\n\n      Var1 Var2\n20272  2.8  5.3\n\nd = data.frame(quantile = c(0.1, 0.5, 0.9), expert = target, calculated = qf(as.numeric(g[ind,])))\nknitr::kable(d)\n\n\n\n\nquantile\nexpert\ncalculated\n\n\n\n\n0.1\n5\n5.3\n\n\n0.5\n14\n13.3\n\n\n0.9\n27\n27.0\n\n\n\n\n\nNotice instead of looping through the grid, I used expand.grid combined with apply to calculate the quantiles for each point. This is a more efficient way to do the same thing.\n\n\nExercise 27.121 (Normal-Likelihod) We observe samples of normal random variable \\(Y_i\\mid \\mu,\\sigma \\sim N(\\mu,\\sigma^2)\\) with known \\(\\sigma\\), specify and plot likelihood for \\(\\mu\\)\n\ny = (-4.3,0.7,-19.4), \\(\\sigma=10\\)\ny = (-12,12,-4.5,0.6), \\(\\sigma=6\\)\ny = (-4.3,0.7,-19.4), \\(\\sigma=2\\)\ny = (12.4,12.1), \\(\\sigma=5\\)\n\nHint: Remember that the nornal likelihood is the product of the normal density function evaluated at each observation \\[\nL(\\mu|y,\\sigma) = \\prod_{i=1}^n \\dfrac{1}{\\sqrt{2\\pi}\\sigma}e^{-(y_i-\\mu)^2/2\\sigma^2} = \\dfrac{1}{(2\\pi\\sigma^2)^{n/2}}e^{-\\sum_{i=1}^n(y_i-\\mu)^2/2\\sigma^2}\n\\] The expression under the exponenta can be simplified to \\[\n\\sum_{i=1}^n(y_i-\\mu)^2 = \\sum_{i=1}^n (\\mu^2 - 2\\mu y_i + y_i^2) = n\\mu^2 - 2\\mu\\sum_{i=1}^n y_i + \\sum_{i=1}^n y_i^2 = n\\mu^2 - 2\\mu n\\bar{y} + \\sum_{i=1}^n y_i^2 = n(\\mu^2 - 2\\mu\\bar{y} + \\sum_{i=1}^n y_i^2)      \n\\] Now \\[\n\\mu^2 - 2\\mu\\bar{y} + \\sum_{i=1}^n y_i^2 = \\mu^2 - 2\\mu\\bar{y} + \\bar y^2 - \\bar y^2 + \\sum_{i=1}^n y_i^2 (\\mu - \\bar{y})^2 + \\sum_{i=1}^n(y_i-\\bar{y})^2\n\\] The last summad does not depend on \\(\\mu\\) and can be ignored. Thus, the likelihood is proportional to \\[\ne^{\\dfrac{-(\\mu - \\bar{y})^2}{2\\sigma^2/n}}\n\\]\nSolution: Let’s implement the likelihood function. We will use two different functions to calculate the likelihood. The first function uses the \\(\\bar y\\) as summary statistic and evaluates normal density function to calculate the likelihood for the mean of the data. The second function uses the product of the normal density function to calculate the likelihood for the mean of the data. We will compare the results of the two functions to see if they are consistent.\n\nlhood1 = function(y, mu, sigma) {\n   dnorm(mean(y), mu, sigma/sqrt(length(y))) \n}\nlhood2 = function(y, mu, sigma) {\n   prod(dnorm(y, mu, sigma))\n}\nmu = seq(-15,15,0.1)\na1 = lhood1(c(-4.3,0.7,-19.4),mu,10)\na2 = sapply(mu,lhood2,y=c(-4.3,0.7,-19.4),sigma=10)\n(a1/a2)[1:5]\n\n 3253 3253 3253 3253 3253\n\nplot(mu,a1)\nlines(mu,3253.175*a2,col=\"red\")\n\n\n\n\n\n\n\n\nWe see that two approaches to calculating the likelihood agree (up to a constant).\n\nplot(mu,a1,type=\"l\",xlab=\"mu\",ylab=\"likelihood\")\n\n\n\n\n\n\n\nplot(mu,lhood1(c (-12,12,-4.5,0.6),mu,6),type=\"l\",xlab=\"mu\",ylab=\"likelihood\")\n\n\n\n\n\n\n\nplot(mu,lhood1(c(-4.3,0.7,-19.4),mu,2),type=\"l\",xlab=\"mu\",ylab=\"likelihood\")\n\n\n\n\n\n\n\nplot(mu,lhood1(c(12.4,12.1),mu,5),type=\"l\",xlab=\"mu\",ylab=\"likelihood\")\n\n\n\n\n\n\n\n\n\n\nExercise 27.122 (Normal-Normal for Lock5Data) Lock5Data package (Lock et al. 2016), includes results for a cross-sectional study of hippocampal volumes among 75 subjects (Singh et al. 2014): 25 collegiate football players with a history of concussions (FBConcuss), 25 collegiate football players that do not have a history of concussions (FBNoConcuss), and 25 control subjects. For our analysis, we’ll focus on the subjects with a history of concussions:\n\nlibrary(Lock5Data)\ndata(FootballBrain)\nd = FootballBrain[FootballBrain$Group==\"FBConcuss\",]\nplot(density(d$Hipp,bw=300))\n\n\n\n\n\n\n\n\n\nAssume that the hippocampal volumes of the subjects with a history of concussions are independent normal random variables with unknown mean \\(\\mu\\) and known standard deviation \\(\\sigma\\). Estimate \\(\\sigma\\) using the sample standard deviation of the hippocampal volumes (FBConcuss group). Calculate the likelihood for \\(\\mu\\).\nAssume that \\(\\mu\\) has a normal prior distribution with mean \\(\\mu_0\\) and standard deviation \\(\\sigma_0\\). Estimate those parameters using the sample mean and standard error of the hippocampal volumes across all subjects.\nFind the posterior distribution for \\(\\mu\\).\nFind a 95% posterior credible interval for \\(\\mu\\).\n\nSolution:\n\nThe sample standard deviation of the hippocampal volumes is\n\n\nsigma = sd(d$Hipp)\nprint(sigma)\n\n 593\n\n\nThus, we assume that \\[\np(\\mu\\mid y) \\propto e^{\\dfrac{-(\\bar y-\\mu)^2}{2\\cdot 593.3^2/25}}\n\\]\n\nmu = seq(5000,6500,length.out=200)\nlhood = dnorm(mean(d$Hipp),mu,sigma/sqrt(length(d$Hipp))) \nplot(mu,lhood,type=\"l\",xlab=\"mu\",ylab=\"likelihood\")\n\n\n\n\n\n\n\n\n\nThe sample mean and standard deviation of the hippocampal volumes across all subjects are\n\n\nmu0 = mean(FootballBrain$Hipp)\nsigma0 = sd(FootballBrain$Hipp)/sqrt(75) \nprint(mu0)\n\n 6599\n\nprint(sigma0)\n\n 131\n\n\nThus, we assume that the prior is \\[\n\\mu \\sim N(6598.8, 226.7^2)\n\\]\n\nThe posterior distribution for \\(\\mu\\) has mean of \\[\n\\mu^* = \\frac{\\sigma^2}{n\\sigma_0^2 + \\sigma^2}\\mu_0 + \\frac{n\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\\bar{y}\n\\] and variance of \\[\n\\sigma^{*2} = \\sigma_n^2 = \\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\n\\]\n\n\nk = 25*sigma0^2 + sigma^2\nmus = sigma^2/k*mu0 + 25*sigma0^2/k*mean(d$Hipp)\nsigmas2 = sigma^2*sigma0^2/k\nprint(mus)\n\n 6125\n\nprint(sigmas2)\n\n 7730\n\n\n\nA 95% posterior credible interval for \\(\\mu\\) is\n\n\nqnorm(c(0.025,0.975),mus,sqrt(sigmas2))\n\n 5952 6297\n\n\n\n\nExercise 27.123 (Normal-Normal chest measurements) We have chest measurements of 10 000 men. Now, based on memories of my experience as an assistant in a gentlemen’s outfitters in my university vacations, I would suggest a prior \\[\n\\mu \\sim N(38, 9).\n\\] Of course, it is open to question whether these men form a random sample from the whole population, but unless I am given information to the contrary I would stick to the prior I have just quoted, except that I might be inclined to increase the variance. My data shows that the mean turned out to be 39.8 with a standard deviation of 2.0 for the sample of 10 000.\n\nCalculate the posterior mean for the chest measurements of men in this population\nCalculate the predictive distribution for the next observation \\(x_{n+1}\\)\n\nSolution:\n\n\n\n\nmu0 = 38\nsigma0 = 9\nyhat = 39.8\nsigma = 2\nn = 10000\nk = n*sigma0^2 + sigma^2\nmus = sigma^2/k*mu0 + n*sigma0^2/k*yhat\nsigmas2 = sigma^2*sigma0^2/k\nprint(mus)\n\n 40\n\nprint(sigmas2)\n\n 4e-04\n\n\nNote that the posterior mean is equal to \\(\\bar y = 39.8\\) equal to the sample mean and variance is \\(0.0004\\) which is equal to 4/10000, the . Thus, our posterior is standardized likelihood \\[\nN(\\bar y, \\sigma^2/n)\n\\] Naturally, the closeness of the posterior to the standardized likelihood results from the large sample size, and whatever my prior had been, unless it were very very extreme, I would have got very much the same result.\nMore formally, the posterior will be close to the standardized likelihood if the ratio of posterior and prior variances \\(\\sigma_*^2/\\sigma_0^2\\) is small or in other words when when \\(\\sigma_0^2\\) is large compared to \\(\\sigma^2/n\\).\n\nThe predictive distribution for the next observation \\(x_{n+1}\\) can be derived by writing \\[\nx_{n+1}  = (x_{n+1} - \\mu) + \\mu\n\\] Now \\(x_{n+1}-\\mu \\sim N(0,\\sigma^2)\\) and \\(\\mu \\sim N(\\mu^*,\\sigma^{*2})\\). Thus, the predictive distribution is \\[\nx_{n+1} \\sim N(\\mu^*,\\sigma^{*2} + \\sigma^2)\n\\]\n\n\nmu = mus\nsigma = sqrt(sigmas2 + sigma^2)\nprint(mu)\n\n 40\n\nprint(sigma)\n\n 2\n\n\n\n\nExercise 27.124 (Normal-Normal Hypothesis Test) Assume Normal-Normal model with known variance, \\(X\\mid \\theta \\sim  N(\\theta,\\sigma^2)\\), \\(\\theta \\sim  N(\\mu ,\\tau^2)\\), and \\(\\theta\\mid X \\sim  N(\\mu^*,\\rho^2)\\). Given \\(L_0 = L(d_1\\mid H_0)\\) and \\(L_1 = L(d_0\\mid H_1)\\) losses, show that for testing \\(H_0 : \\theta \\le \\theta_0\\) the “rejection region” is \\(\\mu^* &gt; \\theta_0 + z\\rho\\), where \\(z = \\Phi^{-1}\\left(L_1/(L_0+L_1)\\right)\\). Remember, that in the classical \\(\\alpha\\)-level test, the rejection region is \\(X &gt; \\theta_0 + z_{1-\\alpha}\\sigma\\)\n\n\nExercise 27.125 (Normal-Normal Hypothesis Test) Assume \\(X\\mid \\theta \\sim  N(\\theta,\\sigma^2)\\) and \\(\\theta \\sim p(\\theta)=1\\). Consider testing \\(H_0 : \\theta  \\le \\theta_0\\) v.s. \\(H_1 : \\theta  &gt; \\theta_0\\). Show that \\(p_0 = P(\\theta  \\le \\theta_0 \\mid X)\\) is equal to classical p-value.\n\n\n\n\n\nLogan, John A. 1983. “A Multivariate Model for Mobility Tables.” American Journal of Sociology 89 (2): 324–49.\n\n\nTarone, Robert E. 1982. “The Use of Historical Control Information in Testing for a Trend in Proportions.” Biometrics. Journal of the International Biometric Society, 215–20.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "A. N. Kolmogorov. 1938. “On the Analytic Methods of Probability\nTheory.” Rossíiskaya Akademiya Nauk, no. 5:\n5–41.\n\n\nAcemoglu, Daron, and Pascual Restrepo. 2018. “Artificial\nIntelligence, Automation and Work.” National Bureau of Economic\nResearch.\n\n\nActor, Jonas. 2018. “Computation for the Kolmogorov\nSuperposition Theorem.” {{MS Thesis}}, Rice.\n\n\nAlbert, Jim. 1993. “A Statistical Analysis of\nHitting Streaks in Baseball:\nComment.” Journal of the American Statistical\nAssociation 88 (424): 1184–88. https://www.jstor.org/stable/2291255.\n\n\nAltić, Mirela Slukan. 2013. “Exploring Along the Rome Meridian:\nRoger Boscovich and the First Modern Map of the Papal\nStates.” In History of Cartography:\nInternational Symposium of the ICA, 2012,\n71–89. Springer.\n\n\nAmazon. 2021. “The History of Amazon’s Forecasting\nAlgorithm.” Amazon Science.\nhttps://www.amazon.science/latest-news/the-history-of-amazons-forecasting-algorithm.\n\n\nAmit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. “Multiple\nRandomized Classifiers: MRCL.”\n\n\nAndrews, D. F., and C. L. Mallows. 1974. “Scale\nMixtures of Normal Distributions.”\nJournal of the Royal Statistical Society. Series B\n(Methodological) 36 (1): 99–102. https://www.jstor.org/stable/2984774.\n\n\nArnol’d, Vladimir I. 2006. “Forgotten and Neglected Theories of\nPoincaré.” Russian Mathematical\nSurveys 61 (1): 1.\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural\nMachine Translation by Jointly Learning to\nAlign and Translate.” arXiv. https://arxiv.org/abs/1409.0473.\n\n\nBaum, Leonard E., Ted Petrie, George Soules, and Norman Weiss. 1970.\n“A Maximization Technique Occurring in the\nStatistical Analysis of Probabilistic\nFunctions of Markov Chains.” The Annals\nof Mathematical Statistics 41 (1): 164–71. https://www.jstor.org/stable/2239727.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo,\nZakaria Haque, Salem Haykal, et al. 2017. “Tfx: A\nTensorflow-Based Production-Scale Machine Learning Platform.” In\nProceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBehnia, Farnaz, Dominik Karbowski, and Vadim Sokolov. 2021. “Deep\nGenerative Models for Vehicle Speed Trajectories.” arXiv\nPreprint arXiv:2112.08361. https://arxiv.org/abs/2112.08361.\n\n\nBenoit, Dries F., and Dirk Van den Poel. 2012. “Binary Quantile\nRegression: A Bayesian Approach Based on the Asymmetric\nLaplace Distribution.” Journal of Applied\nEconometrics 27 (7): 1174–88.\n\n\nBerge, Travis, Nitish Sinha, and Michael Smolyansky. 2016. “Which\nMarket Indicators Best Forecast Recessions?”\nFEDS Notes, August.\n\n\nBertsimas, Dimitris, Angela King, and Rahul Mazumder. 2016. “Best\nSubset Selection via a Modern Optimization Lens.” The Annals\nof Statistics 44 (2): 813–52.\n\n\nBhadra, Anindya, Jyotishka Datta, Nick Polson, Vadim Sokolov, and\nJianeng Xu. 2021. “Merging Two Cultures: Deep and Statistical\nLearning.” arXiv Preprint arXiv:2110.11561. https://arxiv.org/abs/2110.11561.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard\nFirner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016.\n“End to End Learning for Self-Driving Cars.” arXiv\nPreprint arXiv:1604.07316. https://arxiv.org/abs/1604.07316.\n\n\nBonfiglio, Rita, Annarita Granaglia, Raffaella Giocondo, Manuel Scimeca,\nand Elena Bonanno. 2021. “Molecular Aspects and Prognostic\nSignificance of Microcalcifications in Human Pathology: A\nNarrative Review.” International Journal of Molecular\nSciences 22 (120).\n\n\nBottou, Léon, Frank E Curtis, and Jorge Nocedal. 2018.\n“Optimization Methods for Large-Scale Machine Learning.”\nSIAM Review 60 (2): 223–311.\n\n\nBox, George E. P., and George C. Tiao. 1992. Bayesian\nInference in Statistical Analysis. New\nYork: Wiley-Interscience.\n\n\nBrillinger, David R. 2012. “A Generalized Linear Model\nWith ‘Gaussian’ Regressor\nVariables.” In Selected Works of\nDavid Brillinger, edited by Peter Guttorp and David\nBrillinger, 589–606. Selected Works in\nProbability and Statistics. New York, NY:\nSpringer.\n\n\nBryson, Arthur E. 1961. “A Gradient Method for Optimizing\nMulti-Stage Allocation Processes.” In Proc. Harvard\nUniv. Symposium on Digital Computers and Their\nApplications. Vol. 72.\n\n\nCampagnoli, Patrizia, Sonia Petrone, and Giovanni Petris. 2009.\nDynamic Linear Models with R. New\nYork, NY: Springer.\n\n\nCannon, Alex J. 2018. “Non-Crossing Nonlinear Regression Quantiles\nby Monotone Composite Quantile Regression Neural Network, with\nApplication to Rainfall Extremes.” Stochastic Environmental\nResearch and Risk Assessment 32 (11): 3207–25.\n\n\nCarreira-Perpinán, Miguel A, and Weiran Wang. 2014. “Distributed\nOptimization of Deeply Nested Systems.” In\nAISTATS, 10–19.\n\n\nCarvalho, Carlos M, Hedibert F Lopes, Nicholas G Polson, and Matt A\nTaddy. 2010. “Particle Learning for General Mixtures.”\nBayesian Analysis 5 (4): 709–40.\n\n\nCarvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010.\n“The Horseshoe Estimator for Sparse Signals.”\nBiometrika, asq017.\n\n\nChernozhukov, Victor, Iván Fernández-Val, and Alfred Galichon. 2010.\n“Quantile and Probability Curves Without\nCrossing.” Econometrica 78 (3): 1093–1125. https://www.jstor.org/stable/40664520.\n\n\nChib, Siddhartha. 1998. “Estimation and Comparison of Multiple\nChange-Point Models.” Journal of Econometrics 86 (2):\n221–41.\n\n\nCook, R. Dennis. 2007. “Fisher Lecture: Dimension\nReduction in Regression.” Statistical Science, 1–26. https://www.jstor.org/stable/27645799.\n\n\nCootner, Paul H. 1967. The Random Character of Stock Market\nPrices. MIT press.\n\n\nCoppejans, Mark. 2004. “On Kolmogorov’s\nRepresentation of Functions of Several Variables by Functions of One\nVariable.” Journal of Econometrics 123 (1): 1–31.\n\n\nDabney, Will, Georg Ostrovski, David Silver, and Rémi Munos. 2018.\n“Implicit Quantile Networks for Distributional\nReinforcement Learning.” arXiv. https://arxiv.org/abs/1806.06923.\n\n\nDabney, Will, Mark Rowland, Marc G. Bellemare, and Rémi Munos. 2017.\n“Distributional Reinforcement Learning with\nQuantile Regression.” arXiv. https://arxiv.org/abs/1710.10044.\n\n\nDavison, Anthony Christopher. 2003. Statistical Models. Vol.\n11. Cambridge university press.\n\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark\nMao, Andrew Senior, et al. 2012. “Large Scale Distributed Deep\nNetworks.” In Advances in Neural Information Processing\nSystems, 1223–31.\n\n\nDemb, Robert, and David Sprecher. 2021. “A Note on Computing with\nKolmogorov Superpositions Without Iterations.”\nNeural Networks 144 (December): 438–42.\n\n\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation.\nSpringer Science & Business Media.\n\n\nDiaconis, Persi, and David Freedman. 1987. “A Dozen de Finetti-style Results in Search of a\nTheory.” In Annales de l’IHP\nProbabilités Et Statistiques, 23:397–423.\n\n\nDiaconis, Persi, and Mehrdad Shahshahani. 1981. “Generating a\nRandom Permutation with Random Transpositions.” Probability\nTheory and Related Fields 57 (2): 159–79.\n\n\n———. 1984. “On Nonlinear Functions of Linear Combinations.”\nSIAM Journal on Scientific and Statistical Computing 5 (1):\n175–91.\n\n\nDiaconis, P., and D. Ylvisaker. 1983. “Quantifying Prior\nOpinion.”\n\n\nDixon, Mark J., and Stuart G. Coles. 1997. “Modelling\nAssociation Football Scores and Inefficiencies\nin the Football Betting Market.” Journal of the\nRoyal Statistical Society Series C: Applied Statistics 46 (2):\n265–80.\n\n\nDixon, Matthew F, Nicholas G Polson, and Vadim O Sokolov. 2019.\n“Deep Learning for Spatio-Temporal Modeling: Dynamic Traffic Flows\nand High Frequency Trading.” Applied Stochastic Models in\nBusiness and Industry 35 (3): 788–807.\n\n\nDreyfus, Stuart. 1962. “The Numerical Solution of Variational\nProblems.” Journal of Mathematical Analysis and\nApplications 5 (1): 30–45.\n\n\n———. 1973. “The Computational Solution of Optimal Control Problems\nwith Time Lag.” IEEE Transactions on Automatic Control\n18 (4): 383–85.\n\n\nEfron, Bradley, and Carl Morris. 1977. “Stein’s Paradox in\nStatistics.” Scientific American 236 (5): 119–27.\n\n\nEnikolopov, Ruben, Vasily Korovkin, Maria Petrova, Konstantin Sonin, and\nAlexei Zakharov. 2013. “Field Experiment Estimate of Electoral\nFraud in Russian Parliamentary Elections.”\nProceedings of the National Academy of Sciences 110 (2):\n448–52.\n\n\nEric Tassone, and Farzan Rohani. 2017. “Our Quest for Robust Time\nSeries Forecasting at Scale.”\n\n\nFeller, William. 1971. An Introduction to Probability Theory and Its\nApplications. Wiley.\n\n\nFeynman, Richard. n.d. “Feynman :: Rules of\nChess.”\n\n\nFrank, Ildiko E., and Jerome H. Friedman. 1993. “A\nStatistical View of Some Chemometrics Regression\nTools.” Technometrics 35 (2): 109–35. https://www.jstor.org/stable/1269656.\n\n\nFredholm, Ivar. 1903. “Sur Une Classe d’équations\nFonctionnelles.” Acta Mathematica 27 (none): 365–90.\n\n\nFriedman, Jerome H., and Werner Stuetzle. 1981. “Projection\nPursuit Regression.” Journal of the American\nStatistical Association 76 (376): 817–23.\n\n\nFrühwirth-Schnatter, Sylvia, and Rudolf Frühwirth. 2007.\n“Auxiliary Mixture Sampling with Applications to Logistic\nModels.” Computational Statistics & Data Analysis 51\n(April): 3509–28.\n\n\n———. 2010. “Data Augmentation and MCMC\nfor Binary and Multinomial Logit\nModels.” In Statistical Modelling and\nRegression Structures: Festschrift in\nHonour of Ludwig Fahrmeir, 111–32.\n\n\nFrühwirth-Schnatter, Sylvia, Rudolf Frühwirth, Leonhard Held, and Håvard\nRue. 2008. “Improved Auxiliary Mixture Sampling for Hierarchical\nModels of Non-Gaussian Data.” Statistics and\nComputing 19 (4): 479.\n\n\nGan, Link, and Alan Fritzler. 2016. “How to Become an\nExecutive.”\n\n\nGarcía-Arenzana, Nicolás, Eva María Navarrete-Muñoz, Virginia Lope,\nPilar Moreo, Carmen Vidal, Soledad Laso-Pablos, Nieves Ascunce, et al.\n2014. “Calorie\nIntake, Olive Oil Consumption and Mammographic Density Among\nSpanish Women.” International Journal of\nCancer 134 (8): 1916–25.\n\n\nGeorge, Edward I., and Robert E. and McCulloch. 1993. “Variable\nSelection via Gibbs Sampling.”\nJournal of the American Statistical Association 88 (423):\n881–89.\n\n\nGramacy, Robert B., and Nicholas G. Polson. 2012.\n“Simulation-Based Regularized Logistic\nRegression.” arXiv. https://arxiv.org/abs/1005.3430.\n\n\nGriewank, Andreas, Kshitij Kulshreshtha, and Andrea Walther. 2012.\n“On the Numerical Stability of Algorithmic\nDifferentiation.” Computing. Archives for Scientific\nComputing 94 (2-4): 125–49.\n\n\nHahn, P. Richard, Jared S. Murray, and Carlos M. Carvalho. 2020.\n“Bayesian Regression Tree Models for Causal\nInference: Regularization, Confounding,\nand Heterogeneous Effects (with\nDiscussion).” Bayesian Analysis 15 (3):\n965–1056.\n\n\nHalevy, Alon, Peter Norvig, and Fernando Pereira. 2009. “The\nUnreasonable Effectiveness of Data.” IEEE Intelligent\nSystems 24 (2): 8–12.\n\n\nHardt, Moritz, Ben Recht, and Yoram Singer. 2016. “Train Faster,\nGeneralize Better: Stability of Stochastic Gradient\nDescent.” In International Conference on Machine\nLearning, 1225–34. PMLR.\n\n\nHeld, Leonhard, and Chris C. Holmes. 2006. “Bayesian Auxiliary\nVariable Models for Binary and Multinomial Regression.”\nBayesian Analysis 1 (1): 145–68.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo:\nUber’s Machine Learning Platform.”\n\n\nHuang, Jian, Joel L. Horowitz, and Shuangge Ma. 2008. “Asymptotic\nProperties of Bridge Estimators in Sparse High-Dimensional Regression\nModels.” The Annals of Statistics 36 (2): 587–613.\n\n\nHyndman, Rob J., and George Athanasopoulos. 2021. Forecasting:\nPrinciples and Practice. 3rd ed. edition.\nMelbourne, Australia: Otexts.\n\n\nIgelnik, B., and N. Parikh. 2003. “Kolmogorov’s Spline\nNetwork.” IEEE Transactions on Neural Networks 14 (4):\n725–33.\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in\nArtificial Intelligence.”\n\n\nIrwin, Neil. 2016. “How to Become a\nC.E.O.? The Quickest Path\nIs a Winding One.” The New York\nTimes, September.\n\n\nIwata, Shigeru. 2001. “Recentered and Rescaled Instrumental\nVariable Estimation of Tobit and Probit\nModels with Errors in\nVariables.” Econometric Reviews 20 (3):\n319–35.\n\n\nJanuschowski, Tim, Yuyang Wang, Kari Torkkola, Timo Erkkilä, Hilaf\nHasson, and Jan Gasthaus. 2022. “Forecasting with Trees.”\nInternational Journal of Forecasting, Special\nIssue: M5 competition, 38 (4): 1473–81.\n\n\nJeffreys, Harold. 1998. Theory of Probability.\nThird Edition, Third Edition. Oxford Classic Texts in the\nPhysical Sciences. Oxford, New York: Oxford University\nPress.\n\n\nkaggle. 2020. “M5 Forecasting -\nAccuracy.”\nhttps://kaggle.com/competitions/m5-forecasting-accuracy.\n\n\nKallenberg, Olav. 1997. Foundations of Modern\nProbability. 2nd ed. edition. Springer.\n\n\nKeskar, Nitish Shirish, Dheevatsa Mudigere, Jorge Nocedal, Mikhail\nSmelyanskiy, and Ping Tak Peter Tang. 2016. “On Large-Batch\nTraining for Deep Learning: Generalization Gap and Sharp\nMinima.” arXiv Preprint arXiv:1609.04836. https://arxiv.org/abs/1609.04836.\n\n\nKeynes, John Maynard. 1921. A Treatise on Probability.\nMacmillan.\n\n\nKingma, Diederik, and Jimmy Ba. 2014. “Adam: A Method\nfor Stochastic Optimization.” arXiv Preprint\narXiv:1412.6980. https://arxiv.org/abs/1412.6980.\n\n\nKlartag, Bo’az. 2007. “A Central Limit Theorem for Convex\nSets.” Inventiones Mathematicae 168 (1): 91–131.\n\n\nKolmogoroff, Andrei. 1931. “Über Die Analytischen\nMethoden in Der\nWahrscheinlichkeitsrechnung.” Mathematische\nAnnalen 104 (1): 415–58.\n\n\nKolmogorov, AN. 1942. “Definition of Center of Dispersion and\nMeasure of Accuracy from a Finite Number of Observations (in\nRussian).” Izv. Akad. Nauk SSSR Ser. Mat.\n6: 3–32.\n\n\n———. 1956. “On the Representation of Continuous Functions of\nSeveral Variables as Superpositions of Functions of Smaller Number of\nVariables.” In Soviet. Math.\nDokl, 108:179–82.\n\n\nKreps, David. 1988. Notes On The Theory Of Choice.\nBoulder: Westview Press.\n\n\nLevina, Elizaveta, and Peter Bickel. 2001. “The Earth Mover’s\nDistance Is the Mallows Distance: Some Insights from\nStatistics.” In Proceedings Eighth IEEE\nInternational Conference on Computer Vision. ICCV\n2001, 2:251–56. IEEE.\n\n\nLin, Zhouhan, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing\nXiang, Bowen Zhou, and Yoshua Bengio. 2017. “A Structured Self-attentive Sentence\nEmbedding.” arXiv. https://arxiv.org/abs/1703.03130.\n\n\nLindgren, Georg. 1978. “Markov Regime Models for\nMixed Distributions and Switching\nRegressions.” Scandinavian Journal of Statistics\n5 (2): 81–91. https://www.jstor.org/stable/4615692.\n\n\nLinnainmaa, Seppo. 1970. “The Representation of the Cumulative\nRounding Error of an Algorithm as a Taylor Expansion of the\nLocal Rounding Errors.” Master’s Thesis (in Finnish), Univ.\nHelsinki, 6–7.\n\n\nLogan, John A. 1983. “A Multivariate Model for Mobility\nTables.” American Journal of Sociology 89 (2): 324–49.\n\n\nLogunov, A. A. 2004. “Henri Poincare and Relativity\nTheory.” https://arxiv.org/abs/physics/0408077.\n\n\nLorentz, George G. 1976. “The 13th Problem of\nHilbert.” In Proceedings of\nSymposia in Pure Mathematics, 28:419–30.\nAmerican Mathematical Society.\n\n\nMaharaj, Shiva, Nick Polson, and Vadim Sokolov. 2023. “Kramnik Vs\nNakamura or Bayes Vs p-Value.” {{SSRN\nScholarly Paper}}. Rochester, NY.\n\n\nMalthouse, Edward, Richard Mah, and Ajit Tamhane. 1997. “Nonlinear\nPartial Least Squares.” Computers & Chemical\nEngineering 12 (April): 875–90.\n\n\nMazumder, Rahul, Friedman, and Trevor and Hastie. 2011. “SparseNet:\nCoordinate Descent With Nonconvex Penalties.”\nJournal of the American Statistical Association 106 (495):\n1125–38.\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg\nMori. 2017. “Learning Person Trajectory Representations for Team\nActivity Analysis.” arXiv Preprint arXiv:1706.00893. https://arxiv.org/abs/1706.00893.\n\n\nMilman, Vitali D, and Gideon Schechtman. 2009. Asymptotic Theory of\nFinite Dimensional Normed Spaces: Isoperimetric\nInequalities in Riemannian Manifolds. Vol. 1200. Springer.\n\n\nMitchell, T. J., and J. J. Beauchamp. 1988. “Bayesian\nVariable Selection in Linear\nRegression.” Journal of the American Statistical\nAssociation 83 (404): 1023–32. https://www.jstor.org/stable/2290129.\n\n\nNadaraya, E. A. 1964. “On Estimating\nRegression.” Theory of Probability & Its\nApplications 9 (1): 141–42.\n\n\nNaik, Prasad, and Chih-Ling Tsai. 2000. “Partial Least\nSquares Estimator for Single-Index Models.”\nJournal of the Royal Statistical Society. Series B (Statistical\nMethodology) 62 (4): 763–71. https://www.jstor.org/stable/2680619.\n\n\nNareklishvili, Maria, Nicholas Polson, and Vadim Sokolov. 2022.\n“Deep Partial Least Squares for Iv Regression.” arXiv\nPreprint arXiv:2207.02612. https://arxiv.org/abs/2207.02612.\n\n\n———. 2023a. “Generative Causal Inference,”\nJune. https://arxiv.org/abs/2306.16096.\n\n\n———. 2023b. “Feature Selection for Personalized\nPolicy Analysis,” July. https://arxiv.org/abs/2301.00251.\n\n\nNesterov, Yurii. 1983. “A Method of Solving a Convex Programming\nProblem with Convergence Rate O (1/K2).” In\nSoviet Mathematics Doklady, 27:372–76.\n\n\n———. 2013. Introductory Lectures on Convex Optimization:\nA Basic Course. Vol. 87. Springer Science &\nBusiness Media.\n\n\nNicosia, Luca, Giulia Gnocchi, Ilaria Gorini, Massimo Venturini,\nFederico Fontana, Filippo Pesapane, Ida Abiuso, et al. 2023.\n“History of Mammography: Analysis of Breast Imaging\nDiagnostic Achievements over the Last Century.”\nHealthcare 11 (1596).\n\n\nOstrovskii, GM, Yu M Volin, and WW Borisov. 1971. “Uber Die\nBerechnung von Ableitungen.” Wissenschaftliche Zeitschrift\nDer Technischen Hochschule f Ur Chemie, Leuna-Merseburg 13 (4):\n382–84.\n\n\nParzen, Emanuel. 2004. “Quantile Probability and\nStatistical Data Modeling.” Statistical\nScience 19 (4): 652–62. https://www.jstor.org/stable/4144436.\n\n\nPetris, Giovanni. 2010. “An R Package for\nDynamic Linear Models.” Journal of Statistical\nSoftware 36 (October): 1–16.\n\n\nPoincaré, Henri. 1898. “La Mesure Du Temps.” Revue de\nmétaphysique Et de Morale 6 (1): 1–13.\n\n\nPolson, Nicholas G., and James G. Scott. 2011. “Shrink\nGlobally, Act Locally: Sparse Bayesian\nRegularization and Prediction.” In\nBayesian Statistics 9, edited by José M. Bernardo,\nM. J. Bayarri, James O. Berger, A. P. Dawid, David Heckerman, Adrian F.\nM. Smith, and Mike West, 0. Oxford University Press.\n\n\nPolson, Nicholas G., James G. Scott, and Jesse Windle. 2013.\n“Bayesian Inference for Logistic\nModels Using Pólya–Gamma Latent\nVariables.” Journal of the American Statistical\nAssociation 108 (504): 1339–49.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How\nPeople and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPolson, Nicholas G., and Vadim Sokolov. 2023. “Generative\nAI for Bayesian Computation,” June. https://arxiv.org/abs/2305.14972.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep\nLearning: A Bayesian Perspective.”\nBayesian Analysis 12 (4): 1275–1304.\n\n\nPolson, Nicholas, and Steven Scott. 2011. “Data\nAugmentation for Support Vector\nMachines.” Bayesian Analysis 6 (March).\n\n\nPolson, Nicholas, and Vadim Sokolov. 2020. “Deep Learning:\nComputational Aspects.” Wiley Interdisciplinary\nReviews: Computational Statistics 12 (5): e1500.\n\n\nPolson, Nicholas, Vadim Sokolov, and Jianeng Xu. 2021. “Deep\nLearning Partial Least Squares.” arXiv Preprint\narXiv:2106.14085. https://arxiv.org/abs/2106.14085.\n\n\nPolson, Nick, Fabrizio Ruggeri, and Vadim Sokolov. 2024.\n“Generative Bayesian Computation for Maximum\nExpected Utility.” Entropy 26 (12): 1076.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V\nMcConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018.\n“Prediction of Cardiovascular Risk Factors from Retinal Fundus\nPhotographs via Deep Learning.” Nature Biomedical\nEngineering 2 (3): 158.\n\n\nRobbins, Herbert, and Sutton Monro. 1951. “A Stochastic\nApproximation Method.” The Annals of Mathematical\nStatistics 22 (3): 400–407.\n\n\nRubin, Hal S. Stern, John B. Carlin. 2015. Bayesian Data\nAnalysis. 3rd ed. New York: Chapman and\nHall/CRC.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986.\n“Learning Representations by Back-Propagating Errors.”\nNature 323 (6088): 533.\n\n\nSchmidhuber, Jürgen. 2015. “Deep Learning in Neural Networks:\nAn Overview.” Neural Networks 61: 85–117.\n\n\nSchmidt-Hieber, Johannes. 2021. “The\nKolmogorov–Arnold Representation Theorem\nRevisited.” Neural Networks 137 (May): 119–26.\n\n\nSchwertman, Neil C, AJ Gilks, and J Cameron. 1990. “A Simple\nNoncalculus Proof That the Median Minimizes the Sum of the Absolute\nDeviations.” The American Statistician 44 (1): 38–39.\n\n\nScott, Steven L. 2015. “Multi-Armed Bandit Experiments in the\nOnline Service Economy.” Applied Stochastic Models in\nBusiness and Industry 31 (1): 37–45.\n\n\nScott, Steven L. 2022. “BoomSpikeSlab:\nMCMC for Spike and Slab\nRegression.”\n\n\nScott, Steven L., and Hal R. Varian. 2015. “Bayesian\nVariable Selection for Nowcasting Economic Time\nSeries.” In Economic Analysis of the\nDigital Economy, 119–35. University of Chicago Press.\n\n\nScott, Steven, and Hal Varian. 2014. “Predicting the\nPresent with Bayesian Structural Time\nSeries.” Int. J. Of Mathematical Modelling and\nNumerical Optimisation 5 (January): 4–23.\n\n\nSean J. Taylor, and Ben Letham. 2017. “Prophet: Forecasting at\nScale - Meta Research.” Meta Research.\nhttps://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/.\n\n\nShen, Changyu, Enrico G Ferro, Huiping Xu, Daniel B Kramer, Rushad\nPatell, and Dhruv S Kazi. 2021. “Underperformance of Contemporary\nPhase III Oncology Trials and Strategies for\nImprovement.” Journal of the National Comprehensive Cancer\nNetwork 19 (9): 1072–78.\n\n\nShiryayev, A. N. 1992. “On Analytical Methods in Probability\nTheory.” In Selected Works of a. N.\nKolmogorov: Volume II Probability Theory and\nMathematical Statistics, edited by A. N. Shiryayev, 62–108.\nDordrecht: Springer Netherlands.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,\nMatthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering\nChess and Shogi by Self-Play with\na General Reinforcement Learning Algorithm.” arXiv.\nhttps://arxiv.org/abs/1712.01815.\n\n\nSimpson, Edward. 2010. “Edward Simpson:\nBayes at Bletchley Park.”\nSignificance 7 (2): 76–80.\n\n\nSmith, A. F. M. 1975. “A Bayesian Approach to\nInference about a Change-Point in a\nSequence of Random Variables.”\nBiometrika 62 (2): 407–16. https://www.jstor.org/stable/2335381.\n\n\nSokolov, Vadim. 2017. “Discussion of ‘Deep\nLearning for Finance: Deep Portfolios’.” Applied\nStochastic Models in Business and Industry 33 (1): 16–18.\n\n\nSpiegelhalter, David, and Yin-Lam Ng. 2009. “One Match to\nGo!” Significance 6 (4): 151–53.\n\n\nStein, Charles. 1964. “Inadmissibility of the Usual Estimator for\nthe Variance of a Normal Distribution with Unknown Mean.”\nAnnals of the Institute of Statistical Mathematics 16 (1):\n155–60.\n\n\nStern, H, Adam Sugano, J Albert, and R Koning. 2007. “Inference\nabout Batter-Pitcher Matchups in Baseball from Small Samples.”\nStatistical Thinking in Sports, 153–65.\n\n\nStigler, Stephen M. 1981. “Gauss and the Invention of Least\nSquares.” The Annals of Statistics, 465–74.\n\n\nSun, Duxin, Wei Gao, Hongxiang Hu, and Simon Zhou. 2022. “Why 90%\nof Clinical Drug Development Fails and How to Improve It?”\nActa Pharmaceutica Sinica B 12 (7): 3049–62.\n\n\nSutskever, Ilya, James Martens, George Dahl, and Geoffrey Hinton. 2013.\n“On the Importance of Initialization and Momentum in Deep\nLearning.” In International Conference on Machine\nLearning, 1139–47.\n\n\nTaleb, Nassim Nicholas. 2007. The Black Swan: The\nImpact of the Highly Improbable. Annotated\nedition. New York. N.Y: Random House.\n\n\nTarone, Robert E. 1982. “The Use of Historical Control Information\nin Testing for a Trend in Proportions.” Biometrics. Journal\nof the International Biometric Society, 215–20.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and\nTD-Gammon.” Communications of the ACM 38\n(3): 58–68.\n\n\nTiao, Louis. 2019. “Pólya-Gamma Bayesian\nLogistic Regression.” Blog post.\n\n\nTsai, Yao-Hung Hubert, Shaojie Bai, Makoto Yamada, Louis-Philippe\nMorency, and Ruslan Salakhutdinov. 2019. “Transformer\nDissection: A Unified Understanding of\nTransformer’s Attention via the\nLens of Kernel.” arXiv. https://arxiv.org/abs/1908.11775.\n\n\nVarian, Hal R. 2010. “Computer Mediated\nTransactions.” American Economic Review 100 (2):\n1–10.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023.\n“Attention Is All You Need.” arXiv. https://arxiv.org/abs/1706.03762.\n\n\nVecer, Jan, Frantisek Kopriva, and Tomoyuki Ichiba. 2009.\n“Estimating the Effect of the Red Card\nin Soccer: When to Commit an\nOffense in Exchange for\nPreventing a Goal Opportunity.”\nJournal of Quantitative Analysis in Sports 5 (1).\n\n\nViterbi, A. 1967. “Error Bounds for Convolutional Codes and an\nAsymptotically Optimum Decoding Algorithm.” IEEE Transactions\non Information Theory 13 (2): 260–69.\n\n\nWatson, Geoffrey S. 1964. “Smooth Regression\nAnalysis.” Sankhyā: The Indian Journal of\nStatistics, Series A (1961-2002) 26 (4): 359–72. https://www.jstor.org/stable/25049340.\n\n\nWerbos, Paul. 1974. “Beyond Regression:\" New Tools for Prediction\nand Analysis in the Behavioral Sciences.” Ph. D.\nDissertation, Harvard University.\n\n\nWerbos, Paul J. 1982. “Applications of Advances in Nonlinear\nSensitivity Analysis.” In System Modeling and\nOptimization, 762–70. Springer.\n\n\nWindle, Jesse. 2023. “BayesLogit:\nBayesian Logistic Regression.” R package version\n2.1.\n\n\nWindle, Jesse, Nicholas G. Polson, and James G. Scott. 2014.\n“Sampling Polya-Gamma Random Variates: Alternate and\nApproximate Techniques.” arXiv. https://arxiv.org/abs/1405.0506.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu,\nYeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of\nStructured Information from Street View Imagery.” arXiv\nPreprint arXiv:1704.03549. https://arxiv.org/abs/1704.03549.\n\n\nWold, Herman. 1975/ed. “Soft Modelling by\nLatent Variables: The Non-Linear Iterative Partial\nLeast Squares (NIPALS)\nApproach.” Journal of Applied Probability\n12 (S1): 117–42.\n\n\nYaari, Menahem E. 1987. “The Dual Theory of\nChoice Under Risk.”\nEconometrica 55 (1): 95–115. https://www.jstor.org/stable/1911158.\n\n\nZeiler, Matthew D. 2012. “ADADELTA: An Adaptive\nLearning Rate Method.” arXiv Preprint arXiv:1212.5701.\nhttps://arxiv.org/abs/1212.5701.\n\n\nZhang, Yichi, Anirban Datta, and Sudipto Banerjee. 2018. “Scalable\nGaussian Process Classification with Pólya-Gamma Data\nAugmentation.” arXiv Preprint arXiv:1802.06383. https://arxiv.org/abs/1802.06383.",
    "crumbs": [
      "References"
    ]
  }
]