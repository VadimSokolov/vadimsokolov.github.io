[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes, AI and Deep Learning",
    "section": "",
    "text": "Preface\nWelcome to the fascinating world of Bayesian learning, artificial intelligence, and deep learning! This book is your guide to understanding these powerful tools and their applications in various fields. This book is a culmination of our experiences teaching these exciting fields to two distinct audiences: business school students at the University of Chicago and engineers at George Mason University.\nThis unique blend of perspectives allows us to present these complex concepts in a way that is accessible to data scientists, business professionals and technical experts. Whether you’re a manager seeking to leverage AI in your organization or an engineer building the next generation of intelligent systems, this book has something for you.\nTechniques discussed in this book emerged as a transformative force in modern society, and its impact on automation is undeniable. From self-driving cars to virtual assistants, these technologies are already a part of our daily lives. In the coming years, they will become even more ubiquitous, impacting every industry and aspect of our lives. Understanding these technologies is essential for anyone who wants to stay ahead of the curve.\nThrough its ability to learn, adapt, and make decisions, AI is accelerating the pace of automation across various industries and sectors. This impact is multifaceted, encompassing both positive and negative aspects that warrant careful consideration. AI algorithms can analyze vast amounts of data to identify patterns and trends, providing valuable insights for informed decision-making. This leads to better resource allocation, optimized processes, and improved outcomes across various domains. Chatbots and virtual assistants powered by AI can handle customer inquiries and provide support 24/7, offering a personalized and efficient experience. It even works on Jewish holidays! This improves customer satisfaction and loyalty, ultimately benefiting businesses.\nAs a result, AI enables the creation of entirely new business models and industries that were previously not possible. This disrupts traditional markets and creates opportunities for innovation and growth. AI is driving significant progress in fields like self-driving cars, personalized medicine, and space exploration. This has the potential to revolutionize these industries and improve lives in numerous ways.\nThe term AI has morphed over time. It was first coined in 1956 by John McCarthy, who defined it as “the science and engineering of making intelligent machines.” Since then, the field has evolved significantly, and the definition of AI has changed accordingly. Today, AI is a broad field that encompasses various subfields, including machine learning, deep learning, and natural language processing. These subfields are often used interchangeably, but they are not the same thing. Machine learning is a subfield of AI that focuses on algorithms that can learn from data. Deep learning is a subfield of machine learning that uses artificial neural networks to learn complex patterns and relationships in data. Natural language processing is a subfield of AI that focuses on algorithms that can understand and generate human language.\nSince 1956, the field of artificial intelligence (AI) has undergone significant transformations traditional AI was mostly focused on rule-based systems and boolean logic programming, with limited learning capabilities. It lead to them being brittle in changing environments. On the other hand, emerging AI is focused on modeling uncertainties, pattern matching, and deep learning. All of those are data-driven approaches. These approaches are more adaptable and can handle complex and unstructured data. They are also more data-dependent and lack interpretability.\n\n\n\n\n\n\n\n\n\nOld AI\n\n\n\nIf rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nNew AI\n\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\nThis book is based on the lecture notes from our courses, which have been refined and expanded over years of teaching. We have incorporated valuable feedback from students, both at the University of Chicago and George Mason University, to create a comprehensive and engaging learning experience. This book is organized into three parts:\n\nPart 1: Bayesian Learning: This part covers the basics of probability and Bayesian inference.\nPart 2: Artificial Intelligence: This part explores the core concepts of AI such and mostly focuses on pattern matching techniques such as decision trees and generalized linear models.\nPart 3: Deep Learning: This part delves into the world of deep learning, focusing on the architecture and training of deep neural networks. It covers convolutional neural networks, recurrent neural networks, and generative adversarial networks.\n\nThis work is inspired by the contributions of many great thinkers in the field of AI and machine learning. We acknowledge the foundational work of pioneers such as: Shannon (Reinforcement Learning), von Neumann (Theory of Choice), Bellman (Optimality and Learning).\nThe evolution of AI can be summarised in three stages:\n\nSearch. Started with one question and webpages ranked by relevance. Larry Page and Sergey Brin developed PageRank algorithm that uses power iterations to rank webpages by relevance. The statistical tools such as Kendall’s tau and Spearman’s rank correlation were used to measure the similarity between the ranking and the actual relevance.\nSuggestions. The first popular suggestion algorithm was developed by Netflix. It used collaborative filtering to recommend movies to users based on their viewing history and the viewing history of other users. The idea was to minimize the burden on the user to search for movies to watch.\nSummaries. Current AI systems like ChatGPT and Perplexity have revolutionized how we interact with information by excelling at summarization and generalization. These large language models can take vast amounts of complex information and distill it into clear, coherent summaries that capture the essential points while maintaining accuracy. They can generalize across different domains, connecting concepts from various fields and providing insights that might not be immediately obvious. For example, ChatGPT can summarize lengthy research papers into key findings, while Perplexity can synthesize information from multiple sources to provide comprehensive answers to complex questions. This ability to summarize and generalize represents a significant leap forward from simple search and recommendation systems, as these AI agents can now act as intelligent intermediaries that understand context, identify patterns, and present information in ways that are most useful to users.\n\nFirst it was one question and one answer, next it was suggestions. You do not know what to search for, collaborative filtering, recommender systems, and search engines. Now, it is summarise, you are the manager of AI agents agents interpret your request, formulate a plan and necessary questions, and then execute the plan. This is the future of AI, where agents can work together to solve complex problems and provide valuable insights.\nBayesian learning is a powerful statistical framework based on the work of Thomas Bayes. It provides a probabilistic approach to reasoning and learning, allowing us to update our beliefs about the world as we gather new data. This makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information. Artificial intelligence (AI) is a vast field that seeks to create intelligent agents capable of performing tasks that typically require human intelligence. These tasks can include perception, reasoning, learning, problem-solving, decision-making, and language processing. AI has made significant progress in recent years, driven by advances in computing power, data availability, and algorithms. Deep learning is a subfield of AI that uses artificial neural networks to learn from data. These networks are inspired by the structure and function of the human brain and have the ability to learn complex patterns and relationships in data. Deep learning has achieved remarkable results in various tasks such as image recognition, natural language processing, and machine translation.\nThe world of business and engineering are increasingly intertwined, as AI becomes an essential tool in both domains. This book bridges the gap between these disciplines by demonstrating how Bayesian learning, AI, and deep learning can be applied to address real-world challenges in:\n\nBusiness: Market analysis, customer segmentation, risk management, and strategic decision-making.\nEngineering: Robotics, image recognition, natural language processing, and data-driven automation.\n\nKey Features of This Book:\n\nAccessible explanations: We break down complex concepts into manageable chunks, using real-world examples and analogies to illustrate key principles.\nCase studies: We showcase practical applications of Bayesian learning, AI, and deep learning across diverse industries.\nHands-on exercises: We provide practical exercises and code examples to help you apply the concepts covered in the book to your own projects.\n\nJoining the AI Revolution:\nThe field of AI is rapidly evolving, and this book equips you with the knowledge and skills necessary to stay ahead of the curve. Whether you’re looking to enhance your business acumen or advance your engineering career, understanding the power of Bayesian learning, AI, and deep learning is crucial.\nWe invite you to join us on this exciting journey and discover the transformative potential of these powerful tools!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Generative AI\n“If you tell me precisely what it is a machine cannot do, then I can always make a machine which will do just that. John von Neumann, 1956”\nWhen you open an Amazon page there are many personal suggestions of goods to purchase. By analyzing previous product pages visited and purchases made by you and other people who have bought similar products Amazon uses AI and machine learning to predict what would of interest to you next time you shop.\nWhen you apply for a loan online, you typically get an immediate answer after filling an application. The information you provide, combined with your credit history pulled from a credit history bureau is used by a predictive model which can tell with high level of confidence whether you are to default on the loan or not.\nYou might ask, what is common among one of the most successful Internet retail company, finance industry and a phenomenal baseball team? All of these decisions use AI and methods of predictive analytics to improve the operations. They used historical observations combined with rigorous statistical analysis and efficient computer algorithms to predict future outcomes and change the decisions. The ability to collect and analyze complex data sets has been a prerogative of a small number of people for many year. It vital to have experience in data engineering, statistics, machine learning and probability. A data scientists has all of those skills. Current tools developed by industry and academic institutions makes data science profession accessible to a wider audience without requiring a training in a specific technical filed.\nOver the past decade, there has been an explosion of work, mostly applied, on deep learning. Applications of deep learning are everywhere. The main reason for this is that large Internet companies such as Google, Facebook, Amazon and Netflix increasingly displace traditional statistical and machine learning methods with deep learning techniques. Though, such companies are at the frontier of applying deep learning, virtually any industry can be impacted by applying deep learning (DL).\nData Science is a relatively new field that refers to sets of mathematical and statistical models, algorithms, and software that allow extracting patterns from data sets. The algorithms are the adoptions of applied mathematics techniques to specific computer architectures and the software implements those algorithms.\nPredictive analytics applies AI models to design predictive rules which then can be used by engineers and business for forecasting or what-if analysis. For example, a company that is interested in predicting sales as a result of advertisement campaign would use predictive model to identify the best way to allocate its marketing budget or a logistics company would use a predictive model to forecast demand for shipments to estimate the number of drivers it would need in the next few months.\nArtificial Intelligence has been around for decades. In fact the term AI was coined by a famous computer scientist John McCarthy in 1955. While being tightly connected to the field of robotics for many years, the AI concepts are widely applicable in other fields, including predictive analytics. Currently, the AI is understood as a set of mathematical tools that are used to develop algorithms that can perform tasks, typically done by humans, for example, drive a car or schedule a doctor’s appointment. This set of mathematical tools include probabilistic models, machine learning algorithms and deep learning. The previous successful applications included the victory of IBM’s DeepBlue over then world champion Garry Kasparov in 1997.\nTree search algorithms were developed by DeepBlue engineers to implement the chess robot. A modification was the addition of heuristics to cut branches of the tree that would not lead to a win. Those heuristics were designed by chess grand masters based on their intuition and previous experience. Vehicles in grand challenge also relied on traditional techniques such as Kalman filters and PID (proportional-integral-derivative) controllers that have been in use for many years.\nTwo distinguishing features of AI algorithms:\nA major difference between modern and historical AI algorithms is that most of the recent AI approaches rely on learning patterns from data. For example, DeepBlue algorithm was “hardcoded” and the human inputs were implemented as if-then statements by the IBM engineers. On the other hand, modern AlphaGo zero algorithm did not use any human inputs whatsoever and learned optimal strategies from a large data sets generated from self-plays. Although handcrafted systems were shown to perform well in some tasks, such as chess playing, the are hard to design for many complex applications, such as self-driving cars. On the other hand large data sets allow us to replace set of rules designed by engineers with a set of rules learned automatically from data. Thus, the learning algorithms, such as deep learning are at the core of the most of modern AI systems.\nThe main driving factor behind the growth of modern AI applications is the availability of massive and often unstructured data sets. Om the other hand, we now have appropriate computing power to develop computationally intensive AI algorithms. The three main modern AI enablers are:\nFitting complicated models to describe complicated patterns without overfitting requires millions or billions of data points. Two key ideas behind pattern-recognition systems are\nIn AI, prediction rules are often referred to as “models”. The process of using data to find a gooo prediction rule is often called “training the model”. With millions (or billions) of datapoints and fast pattern-matching skills, machines can find needles in a haystack proving insights for human health, transportation, ... etc.\nMachine learning (ML) arises from this question: could a computer go beyond “what we know how to order it to perform” and learn on its own how to perform a specified task? Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data? This question opens the door to a new programming paradigm. In classical programming, the paradigm of symbolic AI, humans input rules (a program) and data to be processed according to these rules, and out come answers. With machine learning, humans input data as well as the answers expected from the data, and out come the rules. These rules can then be applied to new data to produce original answers.\nA machine-learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task. For instance, if you wished to automate the task of tagging your vacation pictures, you could present a machine-learning system with many examples of pictures already tagged by humans, and the system would learn statistical rules for associating specific pictures to specific tags.\nAlthough machine learning only started to flourish in the 1990s, it has quickly become the most popular and most successful subfield of AI, a trend driven by the availability of faster hardware and larger datasets. Machine learning is tightly related to mathematical statistics, but it differs from statistics in several important ways. Unlike statistics, machine learning tends to deal with large, complex datasets (such as a dataset of millions of images, each consisting of tens of thousands of pixels) for which classical statistical analysis such as Bayesian analysis would be impractical. As a result, machine learning, and especially deep learning, exhibits comparatively little mathematical theory—maybe too little—and is engineering oriented. It’s a hands-on discipline in which ideas are proven empirically more often than theoretically.\nDeep learning DL is a type of machine learning which performs a sequence of transformations (filters) on a data. Output of each of those filters is called a factor in traditional statistical language and hidden feature in machine learning. Word deep means that there is a large number of filters that process the data. The power of this approach comes from the hierarchical nature of the model.\nThe three main factors driving AI are:\nThe widespread of mobile phones leads to generation of vast amounts of data. Besides images, users generate space and time trajectories, which are currently used to estimate and predict traffic, text messages, website clicking patterns, etc.\nDeep learning with many successful applications, has been frequently discussed in popular media. The popularity of the topic has led to hype people tend to think that deep learning techniques are capable to replace many of the human tasks, such as medical diagnostics, accountings. On the pessimistic side, people think that after a short hype, the DL techniques will disappoint and companies will stop funding R&D work on its development. However, the research on pushing this filed further is slow and it will take time before deep learning penetrates a wide range of industries. At any rate, the demand for data scientists in general and AI specialists has been increasing over the last few years with biggest markets being on silicon valley, NYC and Washington, DC(indeed 2018).\nThe field of predictive analytics was popularized by many famous competitions in which people compete to build the model with lowest prediction error. One of the first of this types of competitions was the Netflix prize. In 2009 Netflix payed $1 million to a team that developed the most accurate model for predicting movies a user would like to watch. At that time Netflix’s recommendation system generated 30 billion predictions per day. The initial goal of improving recommendation algorithm by 10 percent was overachieved by the winning team. The wining team used what is called an ensemble technique, which takes a weighted average from different prediction algorithms. Thus, the first lesson from this competition is that we typically need to build several predictive models to achieve a good results. On the other had, the model developed by the winning team was never used by Netflix due to complexity of those models and the fact that by the end of competition Netflix mostly shifted to streaming movies versus sending DVDs over mail. The second lesson is that simplicity and interpretability of models matters when they are deployed on a large scale. The third lesson, is that models need to adapt accordingly to meet the fast changing business requirements.\nDeep Learning’s (DL) growing popularity is summarized by the grown of products that Google is developing using DL. Figure 2 shows this immense growth. One key differentiating effect is that DL algorithms are scalable and can be implemented across the interned in apps such as YouTube and Gmail.\nApplications of Machine Learning/Deep Learning are endless, you just have to look at the right opportunity! There is a similar dynamics in popularity of deep learning search queries on Google. The growth is again exponential, although it is not yet close to popularity of traditional statistical techniques, such as linear regression analysis.\nMeanwhile, some ethical concurs are being raised as a result of growing popularity of AI. The most discussed thus far is the impact on the job market and many jobs being replaced by deep learning models. Although, some economic analysis (Acemoglu and Restrepo 2018) shows that while jobs displacement leads to reduced demand for labor and wages, it counteracted by a productivity effect and increases in demand for labor in non-automated tasks.\nThe algorithmic aspects of deep learning has existed for decades. In 1956, Kolmogorov has shown that any function can be represented as a superposition of univariate functions (this is exactly what deep learning does). In 1951 Robbins and Monro proposed stochastic approximations algorithms. This is the main technique for finding weights of a deep learning model today.\nBackpropagation algorithm for finding derivatives was first published and implemented by Werbos in 1974. In mid 1980s Schmidhuber studied many practical aspects of applying neural networks to real-life problems. Since the key ingredients of DL has been around for several decades, one could wonder why we observe a recent peak in popularity of those methods.\nOne of the strong driving forces is adoption of DL by internet companies that need to analyze large scale high dimensional datasets, such as human-written text, speech and images. Smartphone photography led to people uploading vast amounts of images to services like Instagram and Facebook. In 2012 more mobile devices were sold than PCs. The number of images shared on the Internet has skyrocketed as well. This can be see in products that Google is developing using DL.\nThe proliferation of smartphones globally has been one of the most dramatic technological adoptions in human history. From just 173 million smartphone users worldwide in 2010, the number exploded to over 6.8 billion users by 2023, representing nearly 86% of the global population. This exponential growth has been particularly pronounced in developing markets, where smartphones often serve as the primary gateway to the internet. Countries like India and China have seen smartphone penetration rates exceed 80%, while regions in Africa and Southeast Asia continue to show rapid adoption curves. The ubiquity of smartphones has fundamentally transformed how data is generated and consumed - these devices produce continuous streams of location data, user interactions, images, messages, and behavioral patterns that form the foundation for modern AI applications. The convergence of increasingly powerful mobile processors, high-resolution cameras, and always-on internet connectivity has created an unprecedented data generation ecosystem that feeds directly into the machine learning models powering everything from recommendation systems to autonomous vehicles.\nTherefore, data generated by Internet users creates a demand for techniques to analyze large scale data sets. Mathematical methodologies were in place for many years. One missing ingredient in the explosive nature of DL popularity is the availability of computing power. DL models are computationally hungry, trial and error process is required to build a useful model. Sometimes hundreds or thousands of different models are required to be evaluated before choosing one to be used in an application. Training models can be computationally expensive, we are usually talking about large amounts of training data that need to be analyzed to build a model.\nThe adoption rate of AI technologies, particularly generative AI like ChatGPT, has shattered all previous records for technology adoption. While it took the internet 7 years to reach 100 million users, the telephone 75 years, and television 13 years, ChatGPT achieved this milestone in just 2 months after its launch in November 2022. This unprecedented speed of adoption reflects not just the accessibility of AI tools, but also their immediate utility across diverse user needs. Unlike previous innovations that required significant infrastructure changes or learning curves, AI chatbots could be accessed through simple web interfaces and provided immediate value for tasks ranging from writing assistance to problem-solving. The viral nature of AI adoption has been further accelerated by social media demonstrations and word-of-mouth sharing of impressive AI capabilities, creating a network effect that compounds the growth rate. This rapid adoption suggests that AI represents a fundamentally different type of technological shift - one that augments human capabilities rather than replacing existing systems entirely. The chart below illustrates the explosive growth potential of AI technologies.\nThe first generation of AI models was fundamentally enabled by the availability of powerful GPU chips, which provided the parallel processing capabilities necessary to train deep neural networks on large datasets. The breakthrough in deep learning around 2012, including innovations like AlexNet for image recognition, would not have been possible without GPUs that could perform thousands of matrix operations simultaneously. Current AI models, including ChatGPT, Claude, and other large language models, continue to rely primarily on GPUs for both training and inference. Modern AI training clusters consist of thousands of interconnected GPUs working together for weeks or months to process the enormous datasets required for today’s sophisticated models. While some companies have developed specialized AI chips like Google’s TPUs, GPUs remain the dominant platform for AI development due to their versatility, widespread availability, and established software ecosystems.\nThe gaming industry was one of the earliest drivers of GPU development, as game developers demanded increasingly sophisticated graphics rendering capabilities to create immersive virtual worlds with realistic lighting, textures, and physics simulations. Companies like NVIDIA and AMD invested heavily in parallel processing architectures optimized for the matrix operations required to render complex 3D scenes in real-time. The rise of cryptocurrency mining, particularly Bitcoin and Ethereum, created an unexpected second wave of GPU demand as miners discovered that graphics cards were far more efficient than traditional CPUs for the repetitive hash calculations required by proof-of-work algorithms. This mining boom drove massive investments in GPU manufacturing capacity and spurred innovations in memory bandwidth and energy efficiency. More recently, the explosion of AI-generated video content has created a third major demand driver, as video generation models require enormous computational power to process and synthesize high-resolution video frames. The convergence of these three use cases - gaming graphics, cryptocurrency mining, and AI video generation - has accelerated GPU development far beyond what any single application could have achieved alone, creating the powerful hardware infrastructure that now enables training of large language models and other AI applications.\nTable 1 illustrates the dramatic evolution of GPU performance over two decades, from early graphics cards to specialized AI accelerators. The data shows exponential growth in computational power: from the modest 0.23 TeraFLOPS of the 2006 GeForce 7900 GTX to the projected 100 PetaFLOPS (FP4) of the 2027 Rubin Ultra - representing a performance increase of over 400,000x. Here FP4 is a lower precision (4-bit) floating-point arithmetic that is used for AI workloads. It is an alternative to FP32 (32-bit) floating-point arithmetic that is used for general purpose computing.\nMemory capacity has similarly exploded from 0.5GB to a projected 1TB. Modern GPUs have evolved from simple graphics processors to sophisticated AI-optimized architectures featuring specialized tensor cores, mixed-precision arithmetic (FP8/FP4), and massive high-bandwidth memory systems. The transition from traditional FP32 floating-point operations to lower-precision AI workloads (FP8/FP4) has enabled unprecedented computational throughput measured in PetaFLOPS and ExaFLOPS scales, making current and future GPUs the primary engines driving the deep learning revolution and large language model training.\nNow AI models are the main consumers of those processors. The more popular of those are ChatGPT-4, Anthropic’s Claude and Perplexity. ChatGPT-4 is based on the transformer architecture. It is able to handle long conversations and maintain better context over multiple turns. It is stronger in creative writing, technical writing, reasoning tasks, and code generation. It has better performance on logic-heavy tasks and answering technical queries. It is mainly used for chatbots, automated content creation, code writing, customer support, and more advanced AI tasks.\nOpenAI, the company behind ChatGPT, has experienced remarkable growth in both valuation and revenue. As of late 2024, OpenAI reached a valuation of $157 billion following its latest funding round, making it one of the most valuable private companies in the world. The company’s annual recurring revenue (ARR) has grown exponentially, reaching approximately $3.7 billion in 2024, driven primarily by ChatGPT subscriptions and API usage. OpenAI has raised over $13 billion in total funding, with major investors including Microsoft, which has invested $13 billion and maintains a strategic partnership that includes exclusive cloud computing arrangements. This rapid financial growth reflects the massive demand for generative AI capabilities across industries and the transformative potential of large language models.\nClaudeis the main competitor of OpenAI. It is supported by Amazon and excels at complex reasoning tasks, problem-solving, and in-depth analysis across a wide range of domains. Claude can write, debug, and explain code in many programming languages. It can analyze images and documents in addition to text and can engage in various conversation styles, from formal analysis to creative writing to casual discussion.\nAmazon has made a significant strategic investment in Anthropic, Claude’s creator, committing up to $4 billion to advance AI safety research and development. This partnership positions Amazon Web Services (AWS) as Anthropic’s primary cloud provider while giving Amazon a minority ownership stake in the company. Unlike ChatGPT, which excels in creative writing and general-purpose conversations, Claude is specifically designed with a focus on safety, harmlessness, and nuanced reasoning. Claude demonstrates superior performance in tasks requiring careful analysis, ethical reasoning, and handling sensitive topics. It employs Constitutional AI training methods that make it more reliable in avoiding harmful outputs and better at acknowledging uncertainty when it doesn’t know something. Recent advances in Claude 3.7 and Claude 4.0 have introduced groundbreaking multimodal capabilities, allowing these models to process and analyze images, documents, and code with unprecedented accuracy. Claude 4.0 represents a significant leap forward in mathematical reasoning, coding assistance, and complex problem-solving tasks, with performance improvements of 40-60% over previous versions in benchmark evaluations. These newer models feature enhanced “thinking” processes that are more transparent, often explaining their reasoning step-by-step with greater depth and clarity, which makes them particularly valuable for educational applications, research assistance, and professional analysis where understanding the AI’s decision-making process is crucial. Claude 4.0 also introduces improved long-context understanding, capable of processing documents up to 200,000 tokens, and demonstrates remarkable advances in scientific reasoning and technical writing. This approach has made Claude increasingly popular among researchers, academics, and professionals who require more thoughtful and contextually aware AI assistance.\nPerplexity synthesizes information from multiple sources and presents it with proper citations. Each response includes references for easy verification. It functions as a conversational search engine. Perplexity has emerged as a formidable competitor to Google Search by offering a fundamentally different approach to information discovery. Unlike traditional search engines that provide links to websites, Perplexity acts as an AI-powered research assistant that directly answers questions while citing sources. The company has attracted significant investment, including backing from Amazon founder Jeff Bezos, who participated in Perplexity’s $74 million Series B funding round in 2024. This strategic investment reflects growing confidence in AI-first search alternatives that could disrupt Google’s longstanding dominance in the search market.\nThe company has also developed innovative partnerships with major brands like Marriott and Nike, demonstrating how AI search can be integrated into enterprise applications. Marriott has explored using Perplexity’s technology to enhance customer service by providing instant, cited answers about hotel amenities, local attractions, and booking policies. Similarly, Nike has experimented with Perplexity’s capabilities to help customers find specific product information, sizing guides, and availability across different locations. These enterprise partnerships showcase Perplexity’s potential to move beyond general web search into specialized, domain-specific applications.\nPerplexity’s advertising model differs significantly from Google’s traditional approach. Rather than displaying ads alongside search results, Perplexity is exploring sponsored answers and branded content integration that maintains the conversational flow while clearly identifying commercial partnerships. This approach could prove less intrusive than traditional search advertising while providing new revenue streams. The company’s growth trajectory and enterprise adoption suggest it could pose a meaningful challenge to Google’s search monopoly, particularly among users who prefer direct answers over browsing multiple websites.\nThe explosive growth of Large Language Models (LLMs) like ChatGPT, Claude, and Perplexity has been fundamentally enabled by the vast repositories of digital text that have accumulated over the past three decades. The “fuel” powering these sophisticated AI systems comes from an unprecedented collection of human knowledge digitized and made accessible through the internet. Wikipedia alone contains over 60 million articles across hundreds of languages, representing one of humanity’s largest collaborative knowledge projects. Web crawling technologies have systematically captured billions of web pages, blog posts, news articles, and forum discussions, creating massive text corpora that encode diverse writing styles, domains of expertise, and forms of human expression. The digitization of literature through projects like Google Books and Internet Archive has made millions of books searchable and processable, from classical literature to technical manuals. Social media platforms have contributed streams of conversational text, while academic databases provide formal scientific and scholarly writing. This digital text explosion created training datasets containing trillions of words - orders of magnitude larger than what any human could read in multiple lifetimes. By processing these enormous text collections through transformer architectures, LLMs learned statistical patterns of language use, absorbing grammar, syntax, semantics, and even reasoning patterns embedded in human writing. The models discovered how words relate to each other, how concepts connect across different contexts, and how to generate coherent, contextually appropriate responses by predicting the most likely next word given preceding text. This approach allowed AI systems to develop surprisingly sophisticated language understanding and generation capabilities without explicit programming of linguistic rules, instead learning the deep structure of human communication from the collective digital footprint of our species.\nThe mathematical operations used for manipulating and rendering images are the same as those used in deep learning models. Researchers started to used graphical processing units (GPUs) (a.k.a graphics cards) to train deep learning models in 2010s. The wide availability of GPUs made deep learning modeling accessible for a large number of researchers and engineers and eventually led to popularity of DL. Recently, several competitive hardware architectures were developed by large companies like Google, which uses its own TPU (Tensor Processing Units) as well as smaller start-ups.\nThis course will focus on practical and theoretical aspects of predicting using deep learning models. Currently, deep learning techniques are almost exclusively used for image analysis and natural language processing and are practiced by a handful number of scientists and engineers with most of them being trained in computer science. However, modern methodologies, software and availability of cloud computing make deep learning accessible to a wide range of data scientists who would typically use more traditional predictive models such as generalized linear regression or tree-based methods.\nA unified approach to analyze and apply deep learning models to a wide range or problems that arise in business and engineering is required. To make this happen, we will bring together ideas from probability and statistics, optimization, scalable linear algebra and high performance computing. Although, deep learning models are very interesting to study from methodological point of view, the most important aspect of those is the predictive power unseen before with more traditional models. Ability to learn very complex patterns in data and generate accurate predictions make the deep learning a useful and exciting methodology to use, we hope to convey that excitement. This set of notes is self-contained and has a set of references for a reader interested in learning further.\nAlthough basics of probability, statistics and linear algebra will be revisited, it is targeted towards students who have completed a course in introductory statistics and high school calculus. We will make extensive use of computational tools, such as R language, as well as PyTorch and TensorFlow libraries for predictive modeling, both for illustration and in homework problems.\nThere are many aspects of data analysis that do not deal with building predictive models, for example data processing and labeling can require significant human resources(Hermann and Balso 2017; Baylor et al. 2017).\nThe landscape of Artificial Intelligence is rapidly being reshaped by the rise of Generative AI (Gen AI). As of 2025, Gen AI has moved beyond hype and into practical application across a multitude of personal and professional domains. A recent article in the Harvard Business Review, “How People Are Really Using Gen AI in 2025” by Marc Zao-Sanders, highlights this shift, noting that user interest has significantly increased and investment in AI is skyrocketing.\nThe article reveals a fascinating trend: a move from purely technical applications towards more emotive and personal uses. The top use cases in 2025 reflect this, with “Therapy/companionship” leading the list. Other prominent uses include “Organizing my life,” “Finding purpose,” “Enhanced learning,” and “Generating code (for pros).” This indicates that individuals are leveraging Gen AI not just for productivity, but also for personal development and well-being.\nSome concrete examples of how people are using Gen AI, as cited in the article, include:\nThe article also points to the increasing sophistication of Gen AI users, who are developing a deeper understanding of the technology’s capabilities and limitations, including concerns around data privacy and the potential for over-reliance.\nBelow is an image from the HBR article summarizing the top 10 use cases:\nSource: Marc Zao-Sanders, “How People Are Really Using Gen AI in 2025,” Harvard Business Review, April 9, 2025, https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025.\nThe continued evolution of Gen AI promises even more sophisticated applications in the future, moving from providing information to taking action (agentic behavior).\nThe computer therapist is not something new. In 1966, Joseph Weizenbaum created ELIZA, a computer program that could simulate a conversation with a psychotherapist. ELIZA used simple pattern matching to respond to user inputs, creating the illusion of understanding. While it was a groundbreaking achievement at the time, it lacked true comprehension and relied on scripted responses.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#generative-ai",
    "href": "00-intro.html#generative-ai",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Therapy/Companionship: Providing accessible mental health support and a sense of connection, especially in regions with limited access to human therapists. Users find AI to be available 24/7 and non-judgmental.\nOrganizing My Life: Creating timelines for tasks, planning daily habits, and managing personal projects.\nEnhanced Learning: Using AI as a study guide to explain complex topics and reinforce learning.\nHealthier Living: Generating meal plans based on specific dietary needs and macro calculations.\nCreating Travel Itineraries: Planning detailed vacations, including finding rustic accommodations and hidden gems while optimizing travel time.\nDisputing Fines: Drafting appeal letters for things like parking tickets.\n\n\n\n\n\n\nTop 10 Gen AI Use Cases in 2025. Source: Harvard Business Review, “How People Are Really Using Gen AI in 2025”, April 9, 2025.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#agi-and-aiq",
    "href": "00-intro.html#agi-and-aiq",
    "title": "Principles of Data Science",
    "section": "AGI and AIQ",
    "text": "AGI and AIQ\n\n“I visualize a time when we will be to robots what dogs are to humans. And I am rooting for the machines.” - Claude Shannon\n\n\n“Let us suppose we have set up a machine with certain initial instruction tables, so constructed that these tables might on occasion, if good reason arose, modify those tables. One can imagine that after the machine had been operating for some time, the instructions would have altered out of all recognition, but nevertheless still be such that one would have to admit that the machine was still doing very worthwhile calculations. Possibly it might still be getting results of the type desired when the machine was first set up, but in a much more efficient manner. In such a case one would have to admit that the progress of the machine had not been foreseen when its original instructions were put in. It would be like a pupil who had learnt much from his master, but had added much more by his own work. When this happens I feel that one is obliged to regard the machine as showing intelligence.” – Alan Turing\n\nPeople, organizations and markets. AI does the organization and hence connects people to markets faster and simplesly. Hence it creates economic values. Most of th recessions in the 19th centurey was a result of not being able to get goods to markets quick enough which led to banking crises. AI accelerates speed to market. It reates growth. The age of abundance is here.\nSkynet and terminator\nTransfer learning\nOlga comments (Toloka)\n\nChat does not know what it does not know\nStill need humans and their skills\nLike co-pilot, we need collaboration between humans and AI, humans became managers\nBefore people would build many classifiers for a specific task. The economics of the model there is one big winner. They combine all the models together.\nNeed humans for ground truth, for labeling data, for training models\nAI is very good at decomposing and planning, and humans are not as good at executing the plan, because it is against their intuition.\n\nAndrej Karpathy’s talk, “Software Is Changing (Again),” explores how large language models (LLMs) are fundamentally transforming the way software is developed and used. He describes this new era as “Software 3.0,” where natural language becomes the primary programming interface and LLMs act as a new kind of computer and compares it to the previous generations of software development approaches sumamrised in the table below.\n\n\n\n\n\n\n\n\n\nParadigm\n“Program” is…\nDeveloper’s main job\nCanonical depot\n\n\n\n\nSoftware 1.0\nHand-written code\nWrite logic\nGitHub\n\n\nSoftware 2.0\nNeural-net weights\nCurate data & train\nHugging Face / Model Atlas\n\n\nSoftware 3.0\nNatural-language prompts\nCompose/police English instructions\nPrompt libraries\n\n\n\nCurrenlty LLMs are collaborative partners that can augment human abilities, democratizing software creation and allowing people without traditional programming backgrounds to build complex applications simply by describing what they want in plain English.\nPolson and Scott (2018) have predicted that human-machine interaction will be the next frontier of AI.\nOlga sais that humans are callable function.\nThe same will happen to university professors. They will become lablers for content. And simply will be responsible for clicking yes, when content is appropriate and no, when it is not.\nHal Varian’s 2010 paper “Computer Mediated Transactions” Varian (2010) provides a foundational framework for understanding how computers can automate routine tasks and decision-making processes, reducing transaction costs and increasing efficiency. This includes automated pricing, inventory management, and customer service systems. He talks about system that can coordinate between multiple parties by providing real-time information sharing and communication platforms. This enables more complex multi-party transactions and supply chain management.\nThis framework remains highly relevant for understanding modern AI and machine learning applications in business, as these technologies represent the next evolution of computer-mediated transactions, enabling even more sophisticated automation, coordination, and communication capabilities.\nIn his talk on “Why are LLMs not Better at Finding Proofs?”, Timothy Gowers discusses that while large language models (LLMs) can display some sensible reasoning—such as narrowing down the search space in a problem—they tend to falter when they get stuck, relying too heavily on intelligent guesswork rather than systematic problem-solving. Unlike humans, who typically respond to a failed attempt with a targeted adjustment based on what went wrong, LLMs often just make another guess that isn’t clearly informed by previous failures. He also highlights a key difference in approach: humans usually build up to a solution incrementally, constructing examples that satisfy parts of the problem and then refining their approach based on the requirements. For example, when trying to prove an existential statement, a human might first find examples satisfying one condition, then look for ways to satisfy additional conditions, adjusting parameters as needed. LLMs, by contrast, are more likely to skip these intermediate steps and try to jump directly to the final answer, missing the structured, iterative reasoning that characterizes human problem-solving.\nWhile there are indeed limitations to what current large language models can solve, particularly in areas requiring systematic mathematical reasoning, they continue to demonstrate remarkable capabilities in solving complex problems through alternative approaches. A notable example is the application of deep learning to the classical three-body problem in physics, a problem that has challenged mathematicians and physicists for centuries. Traditional analytical methods have struggled to find closed-form solutions for the three-body problem, but deep neural networks have shown surprising success in approximating solutions through pattern recognition and optimization techniques. These neural networks can learn the underlying dynamics from training data and generate accurate predictions for orbital trajectories, even when analytical solutions remain elusive. This success demonstrates that the trial-and-error approach, when combined with sophisticated pattern recognition capabilities, can lead to practical solutions for problems that have resisted traditional mathematical approaches. The key insight is that while these methods may not provide the elegant closed-form solutions that mathematicians prefer, they offer valuable computational tools that can advance scientific understanding and enable practical applications in fields ranging from astrophysics to spacecraft navigation.\n\n\n\n\nAcemoglu, Daron, and Pascual Restrepo. 2018. “Artificial Intelligence, Automation and Work.” National Bureau of Economic Research.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, et al. 2017. “Tfx: A Tensorflow-Based Production-Scale Machine Learning Platform.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016. “End to End Learning for Self-Driving Cars.” arXiv Preprint arXiv:1604.07316.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo: Uber’s Machine Learning Platform.”\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in Artificial Intelligence.”\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg Mori. 2017. “Learning Person Trajectory Representations for Team Activity Analysis.” arXiv Preprint arXiv:1706.00893.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How People and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V McConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018. “Prediction of Cardiovascular Risk Factors from Retinal Fundus Photographs via Deep Learning.” Nature Biomedical Engineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.” arXiv.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and TD-Gammon.” Communications of the ACM 38 (3): 58–68.\n\n\nVarian, Hal R. 2010. “Computer Mediated Transactions.” American Economic Review 100 (2): 1–10.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu, Yeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of Structured Information from Street View Imagery.” arXiv Preprint arXiv:1704.03549.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "10-data.html",
    "href": "10-data.html",
    "title": "1  Unreasonable Effectiveness of Data",
    "section": "",
    "text": "It is remarkable that a science which began with the consideration of games of chance should have become the most important object of human knowledge. Laplace, Pierre Simon, 1812\n\nData from telescopes have played a crucial role on the data analysis techniques developed in the 18th century. Internet and mobile devices play a similar role in the 21st century. Large amounts of astronomical data led scientists like Carl Friedrich Gauss, Pierre-Simon Laplace, and Sim{'e}on Denis Poisson to the development of data-driven methods, such as the method of least squares and the Poisson distribution. These methods not only revolutionized astronomy but also are used nowadays in various fields, such as physics, engineering, and economics. The development of these methods marked a significant shift in the scientific approach, enabling more rigorous analysis and interpretation of observational data. The integration of data and statistical methods laid the foundation for modern data science and statistics, demonstrating the power and versatility of data-driven approaches.\nBack in the 18th and 19th century data collection was often limited to manual measurements or observations, and the amount of available data was typically much smaller compared to the massive datasets encountered in modern data science. Scientists like Gauss and Poisson often conducted carefully designed experiments, collected their own data, and performed manual calculations without the aid of computers or advanced statistical software. The focus of their work was often on theoretical developments in mathematics, physics and astronomy, and the data was used to test and validate their theories. Let’s consider one of those studies from early 18th century.\n\nExample 1.1 (Boscovich and Shape of Earth) The 18th century witnessed heated debates surrounding the Earth’s precise shape. While the oblate spheroid model - flattened poles and bulging equator - held sway, inconsistencies in measurements across diverse regions fueled uncertainty about its exact dimensions. The French, based on extensive survey work by Cassini, maintained the prolate view while the English, based on gravitational theory of Newton (1687), maintained the oblate view.\nThe determination of the exact figure of the earth would require very accurate measurements of the length of a degree along a single meridian. The final answer to this debate was given by Roger Boscovich (1711-1787) who used geodetic surveying principles and in collaboration with English Jesuit Christopher Maire, in 1755, they embarked on a bold project: measuring a meridian arc spanning a degree of latitude between Rome and Rimini. He employed ingenious techniques to achieve remarkable accuracy for his era, minimizing errors and ensuring the reliability of his data. In 1755 they published “De litteraria expeditione per pontificiam ditionem” (On the Scientific Expedition through the Papal States) that contained results of their survey and its analysis. For more details about the work of Boscovich, see Altić (2013). Stigler (1981) gives an exhaustive introduction to the history of regression.\nThe data on meridian arcs used by Boscovich was crucial in determining the shape and size of the Earth. He combined data from five locations:\n\nd=read.csv(\"../data/boscovich.csv\")\nknitr::kable(d, digits = 8)\n\n\n\n\nLocation\nLatitude\nArcLength\nsin2Latitude\n\n\n\n\nQuito\n0\n56751\n0\n\n\nCape of Good Hope\n33\n57037\n2987\n\n\nRome\n43\n56979\n4648\n\n\nParis\n49\n57074\n5762\n\n\nLapland\n66\n57422\n8386\n\n\n\n\nplot(d$sin2Latitude,d$ArcLength, ylab=\"Arc Length\", xlab=expression(sin^2~(theta)), pch=16,ylim=c(56700,57450), xlim=c(-30,8590))\ntext(d$sin2Latitude,d$ArcLength-25, labels=d$Location)\n\n\n\n\n\n\n\n\nThe arc length is measured in toises. A toise is a pre-metric unit of length approximately equal to 6.39 feet. It is clear from the table and from the table and from the plot that the the arc length goes up as the latitude increases and the relationship between the arc length and the sine squared of the latitude is approximately linear and the relationship is \\[\n\\text{Arc Length} = \\beta_0 +  \\beta_1 \\sin^2 \\theta\n\\] where \\(\\theta\\) is the latitude. Here \\(\\beta_0\\) is the length of a degree of arc at the equator, and \\(\\beta_1\\) is how much longer a degree of arc is at the pole. The question that Boscovich asked is how can we combine those five data points to estimate the parameters \\(\\beta_0\\) and \\(\\beta_1\\)? His first attempt to answer this question involved calculating ten slopes for each pair of points and then averaging them. The table below shows the ten slopes.\n\nd = read.csv(\"../data/boscovich.csv\")\nsl = matrix(NA,5,5)\nfor (i in 1:5) {\n    for(j in 1:(i-1)) {\n        dx = d$sin2Latitude[i] - d$sin2Latitude[j]\n        dy = d$ArcLength[i] - d$ArcLength[j]\n        sl[i,j]=dy/dx\n    }\n}\nrownames(sl) = d$Location\ncolnames(sl) = d$Location\noptions(knitr.kable.NA = '')\nknitr::kable(sl, digits = 4)\n\n\nTen slopes for each pair of the five cities from the Boscovich data\n\n\n\nQuito\nCape of Good Hope\nRome\nParis\nLapland\n\n\n\n\nQuito\n\n\n\n\n\n\n\nCape of Good Hope\n0.096\n\n\n\n\n\n\nRome\n0.049\n-0.035\n\n\n\n\n\nParis\n0.056\n0.013\n0.085\n\n\n\n\nLapland\n0.080\n0.071\n0.118\n0.13\n\n\n\n\nTen slopes for each pair of the five cities from the Boscovich data\n\nplot(d$sin2Latitude,d$ArcLength, ylab=\"Arc Length\", xlab=expression(sin^2~(theta)), pch=16)\ntext(d$sin2Latitude,d$ArcLength-25, labels=d$Location)\nfor (i in 1:4){\n  for (j in (i+1):5){\n    slope = (d$ArcLength[i] - d$ArcLength[j])/(d$sin2Latitude[i] - d$sin2Latitude[j])\n    intercept = d$ArcLength[i] - slope*d$sin2Latitude[i]\n    abline(a=intercept, b=slope)\n  }\n}\n\n\n\n\nTen slopes for each pair of the five cities from the Boscovich data\n\n\n\n\nThe average of the ten slopes is 0.0667. Notice the slope between Cape of Good Hope and Rome is negative. This is due to the measurement error. Boscovich then calculated an average after removing this outlier. The average of the remaining nine slopes is 0.078. In both cases he used length of the arc at Quito as estimate of the intercept \\(\\beta_0\\). Figure 1.1 (a) shows the line that corresponds to the parameter estimates obtained by Boscovich. Figure 1.1 (b) is the same plot but with the modern least squares line.\nd=read.csv(\"../data/boscovich.csv\")\nplot(d$sin2Latitude,d$ArcLength, ylab=\"Arc Length\", xlab=expression(sin^2~(theta)), pch=16,ylim=c(56700,57450), xlim=c(-30,8590))\nabline(56751,0.06670097, lwd=3, col=\"red\")\nplot(d$sin2Latitude,d$ArcLength, ylab=\"Arc Length\", xlab=expression(sin^2~(theta)), pch=16,ylim=c(56700,57450), xlim=c(-30,8590))\nabline(lm(ArcLength~sin2Latitude, data=d), lwd=3, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Boscovich’s first attempt to estimate the parameters\n\n\n\n\n\n\n\n\n\n\n\n(b) Modern least squares approach\n\n\n\n\n\n\n\nFigure 1.1: Comparison of Boscovich’s first attempt to estimate the parameters and the modern least squares approach\n\n\n\nThis is a very reasonable approach! However, Boscovich was not satisfied with this approach and he wanted to find a better way to combine the data. He was looking for a method that would minimize the sum of the absolute deviations between the data points and the fitted curve. Two years later he developed a pioneering technique called “least absolute deviations,” which revolutionized data analysis. This method, distinct from the prevalent “least squares” approach, minimized the sum of absolute deviations between data points and the fitted curve, proving particularly effective in handling measurement errors and inconsistencies.\nArmed with his meticulous measurements and innovative statistical analysis, Boscovich not only confirmed the oblate spheroid shape of the Earth but also refined its dimensions. His calculations yielded a more accurate value for the Earth’s equatorial radius and the flattening at the poles, providing crucial support for Newton’s theory of gravitation, which predicted this very shape.\n\nMotivated by the analysis of planetary orbits and determining the shape of the Earth, later in 1805, Adrien-Marie Legendre (1752 - 1833) published the first clear and concise explanation of the least squares method in his book “Nouvelles m{'e}thodes pour la d{'e}termination des orbites des cometes”. The method of least squares is a powerful statistical technique used today fit a mathematical model to a set of data points. Its goal is to find the best-fitting curve that minimizes the sum of the squared distances (a.k.a residuals) between the curve and the actual data points. Compared to the approach proposed by Boscovich, the least squares method is less robust to measurement errors and inconsistencies. However, from computational point of view, it is more efficient and there are various algorithms exist for efficient calculation of curve parameters. This computational efficiency is crucial for modern data analysis, where datasets can be massive and complex, making least squares a fundamental tool in statistics and data analysis, offering a powerful and widely applicable approach to data fitting and model building.\nLegendre provided a rigorous mathematical foundation for the least squares method, demonstrating its theoretical underpinnings and proving its optimality under certain conditions. This mathematical basis helped establish the credibility and legitimacy of the method, paving the way for its wider acceptance and application. Legendre actively communicated his ideas and collaborated with other mathematicians, such as Carl Friedrich Gauss (1777-1855), who also contributed significantly to the development of the least squares method. While evidence suggests Gauss used the least squares method as early as 1795, his formal publication came later than Legendre’s in 1809. Despite the delay in publication, Gauss independently discovered the method and applied it to various problems, including celestial mechanics and geodesy. He developed efficient computational methods for implementing the least squares method, making it accessible for practical use by scientists and engineers. While Legendre’s clear exposition and early publication brought the least squares method to the forefront, Gauss’s independent discovery, theoretical development, practical applications, and contributions to computational methods were equally crucial in establishing the method’s significance and impact. Both mathematicians played vital roles in shaping the least squares method into the powerful statistical tool it is today.\nAnother French polymath Pierre-Simon Laplace (1749 - 1827) extended the methods of Boscovich and showed that the curve fitting problem could be solved by ordering the candidate slopes and finding the weighted median. Besides that Laplace made fundamental contributions to probability theory, developing the Bayesian approach to inference. Most of the work of Laplace was in the field of celestial mechanics, where he used data from astronomical observations to develop mathematical models and equations describing the gravitational interactions between celestial bodies. His analytical methods and use of observational data were pioneering in the field of celestial mechanics. Furthermore, developed methods for estimating population parameters from samples, such as the mean and variance and pioneered the use of random sampling techniques, which are essential for ensuring the validity and generalizability of statistical inferences. These contributions helped lay the foundation for modern sampling theory and survey design, which are crucial for conducting reliable and representative studies. Overall, Laplace’s contributions to data analysis were profound and enduring. His work in probability theory, error analysis, sampling methods, and applications significantly advanced the field and laid the groundwork for modern statistical techniques. He also played a crucial role in promoting statistical education and communication, ensuring that these valuable tools were accessible and utilized across various disciplines.\nBoscovich used what we call today a linear regression analysis. This type of analysis relies on the assumption that the relationship between the independent (arc length) and dependent (sinus squared of the latitude) variables is linear. Francis Galton was the person who coined the term “regression” in the context of statistics. One of the phenomena he studied was the relationship between the heights of parents and their children. He found that the heights of children tended to “regress” towards the average height of the population, which led him to use the term “regression” to describe this phenomenon. Galton promoted a data-driven approach to research that continues to shape statistical practice today. Furthermore, he introduced the concept of quantiles, which divide a population into equal-sized subpopulations based on a specific variable. This allowed for a more nuanced analysis of data compared to simply considering the mean and median. He also popularized the use of percentiles, which are specific quantiles used to express the proportion of a population below a certain value.\nGalton used regression analysis to show that the the offspring of exceptionally large or small seed size of sweet peas from tended to be closer to the average size. He also used it for family studies and investigated the inheritance of traits such as intelligence and talent. He used regression analysis to assess the degree to which these traits are passed down from parents to offspring.\nGalton’s overall approach to statistics was highly influential. He emphasized the importance of quantitative analysis, data-driven decision-making, and empirical research, which paved the way for modern statistical methods and helped to establish statistics as a recognized scientific discipline.\n\n\n\n\nAltić, Mirela Slukan. 2013. “Exploring Along the Rome Meridian: Roger Boscovich and the First Modern Map of the Papal States.” In History of Cartography: International Symposium of the ICA, 2012, 71–89. Springer.\n\n\nStigler, Stephen M. 1981. “Gauss and the Invention of Least Squares.” The Annals of Statistics, 465–74.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unreasonable Effectiveness of Data</span>"
    ]
  },
  {
    "objectID": "11-pattern.html",
    "href": "11-pattern.html",
    "title": "2  Pattern Matching",
    "section": "",
    "text": "2.1 Why Pattern Matching?\n“Prediction is very difficult, especially about the future. Niels Bohr, Danish physicist and Nobel laureate”\nThe history of data analysis is closely intertwined with the development of pattern matching techniques. The ability to identify and understand patterns in data has been crucial for scientific discoveries, technological advancements, and decision-making. From the early days of astronomy to modern machine learning, pattern matching has played a pivotal role in advancing our understanding of the world around us. This chapter explores the key concepts of pattern matching, its historical development, and its impact on data analysis.\nData science involves two major steps, collection and cleaning of data and building a model or applying an algorithm. In this chapter we present the process of building predictive models. To illustrate the process think of your data as being generated by a black box on which a set of input variables \\(x\\) go through the box and generate an output variable \\(y\\).\nFor Gauss, Laplace and many other scientist, the main problem was the problem of estimating parameters, while the relationship between the variables was known and was usually linear, like in the shape of the earth example of multiplicative, e.g. Newton’s second law \\(F = ma\\). However, in many cases, the relationship between the variables is unknown and cannot be described by a simple mathematical model. Halevy, Norvig, and Pereira (2009) discuss the problem of human behavior and natural languages. Neither can be described by a simple mathematical model.\nThis is case, the pattern matching approach is a way to use data to find those relations. In data analysis, pattern matching is the process of identifying recurring sequences, relationships, or structures within a dataset. It’s like looking for a specific puzzle piece within a larger picture. By recognizing these patterns, analysts can gain valuable insights into the data, uncover trends, make predictions, and ultimately improve decision-making. Sometimes initial pattern matching analysis leads to a scientific discovery. Consider a case of mammography and early pattern matching.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#why-pattern-matching",
    "href": "11-pattern.html#why-pattern-matching",
    "title": "2  Pattern Matching",
    "section": "",
    "text": "Example 2.1 (Mammography and Early Pattern Matching) The use of mammograms for breast cancer detection relied on simple pattern matching in its initial stages. Radiologists visually examined the X-ray images for specific patterns indicative of cancer, such as: dense areas of tissue appearing different from surrounding breast tissue (a.k.a masses) and small white spots of calcium deposits called microcalcifications. These patterns were associated with early-stage cancer and could be easily missed by visual inspection alone.\nRadiologists relied on their expertise and experience to identify these patterns and distinguish them from normal breast tissue variations. This process was subjective and prone to errors, particularly with subtle abnormalities or in dense breasts. Subtle abnormalities, especially in dense breasts, could be easily missed using visual assessment alone. Despite these limitations, pattern matching played a crucial role in the early detection of breast cancer, saving countless lives. It served as the foundation for mammography as a screening tool.\nAlbert Solomon, a German surgeon, played a pivotal role in the early development of mammography (Nicosia et al. (2023)). His most significant contribution was his 1913 monograph, “Beitr{\"a}ge zur Pathologie und Klinik der Mammakarzinome” (Contributions to the Pathology and Clinic of Breast Cancers). In this work, he demonstrated the potential of X-ray imaging for studying breast disease. He pioneered the use of X-rays, he compared surgically removed breast tissue images with the actual tissue and was able to identify characteristic features of cancerous tumors, such as their size, shape, and borders. He was one of the first to recognize the association between small calcifications appearing on X-rays and breast cancer.\nPresence of calcium deposits is correlated with brest cancer and is still prevailing imaging biomarkers for its detection. Although discovery of the deposit-cancer asosciation induced scientific discoveries, the molecular mechanisms that leads to the formation of these calcium deposits, as well as the significance of their presence in human tissues, have not been completely understood (Bonfiglio et al. (2021)).\n\n\n2.1.1 Richard Feynman on Pattern Matching and Chess\nRichard Feynman, the renowned physicist, was a strong advocate for the importance of pattern matching and its role in learning and problem-solving. He argued that in many scientific discoveries, start with pattern matching. He emphasized that experts in any field, whether it’s chess, science, or art, develop a strong ability to identify and understand relevant patterns in their respective domains.\nHe often used the example of chess to illustrate this concept (Feynman (n.d.)). Feynman argued that a skilled chess player doesn’t consciously calculate every possible move. Instead, they recognize patterns on the board and understand the potential consequences of their actions. For example, a chess player might recognize that having a knight in a certain position is advantageous and will lead to a favorable outcome. This ability to identify and understand patterns allows them to make quick and accurate decisions during the game. Through playing and analyzing chess games, players develop mental models that represent their understanding of the game’s rules, strategies, and potential patterns. These mental models allow them to anticipate their opponent’s moves and formulate effective responses.\nHe emphasized that this skill could be transferred to other domains, such as scientific research, engineering, and even everyday problem-solving.\nHere is a quote from his interview\n\n\n\n\n\n\nRichard Feynman\n\n\n\nLet’s say a chess game. And you don’t know the rules of the game, but you’re allowed to look at the board from time to time, in a little corner, perhaps. And from these observations, you try to figure out what the rules are of the game, what [are] the rules of the pieces moving.\nYou might discover after a bit, for example, that when there’s only one bishop around on the board, that the bishop maintains its color. Later on you might discover the law for the bishop is that it moves on a diagonal, which would explain the law that you understood before, that it maintains its color. And that would be analogous we discover one law and later find a deeper understanding of it.\nAh, then things can happen–everything’s going good, you’ve got all the laws, it looks very good–and then all of a sudden some strange phenomenon occurs in some corner, so you begin to investigate that, to look for it. It’s castling–something you didn’t expect…\nAfter you’ve noticed that the bishops maintain their color and that they go along on the diagonals and so on, for such a long time, and everybody knows that that’s true; then you suddenly discover one day in some chess game that the bishop doesn’t maintain its color, it changes its color. Only later do you discover the new possibility that the bishop is captured and that a pawn went all the way down to the queen’s end to produce a new bishop. That could happen, but you didn’t know it.\n\n\nIn an interview on Artificial General Intelligence (AGI), he compares human and machine intelligence\n\n\n\n\n\n\nRichard Feynman\n\n\n\nFirst of all, do they think like human beings? I would say no and I’ll explain in a minute why I say no. Second, for “whether they be more intelligent than human beings: to be a question, intelligence must first be defined. If you were to ask me are they better chess players than any human being? Possibly can be , yes ,”I’ll get you some day”. They’re better chess players than most human beings right now!\n\n\nBy 1996, computers had become stronger than GMs. With the advent of deep neural networks in 2002, Stockfish15 is way stronger. A turning point on our understanding of AI algorithms was AlphaZero and Chess\nAlphaGo coupled with deep neural networks and Monte Carlo simulation provided a gold standard for chess. AlphaZero showed that neural networks can self-learn by competing against itself. Neural networks are used to pattern match and interpolate both the policy and value function. This implicitly performs “feature selection”. Whilst humans have heuristics for features in chess, such as control center, king safety and piece development, AlphaZero “learns” from experience. With a goal of maximizing the probability of winning, neural networks have a preference for initiative, speed and momentum and space over minor material such as pawns. Thus reviving the old school romantic chess play.\nFeynman discusses how machines show intelligence:\n\n\n\n\n\n\nRichard Feynman\n\n\n\nWith regard to the question of whether we can make it to think like [human beings], my opinion is based on the following idea: That we try to make these things work as efficiently as we can with the materials that we have. Materials are different than nerves, and so on. If we would like to make something that runs rapidly over the ground, then we could watch a cheetah running, and we could try to make a machine that runs like a cheetah. But, it’s easier to make a machine with wheels. With fast wheels or something that flies just above the ground in the air. When we make a bird, the airplanes don’t fly like a bird, they fly but they don’t fly like a bird, okay? They don’t flap their wings exactly, they have in front, another gadget that goes around, or the more modern airplane has a tube that you heat the air and squirt it out the back, a jet propulsion, a jet engine, has internal rotating fans and so on, and uses gasoline. It’s different, right?\nSo, there’s no question that the later machines are not going to think like people think, in that sense. With regard to intelligence, I think it’s exactly the same way, for example they’re not going to do arithmetic the same way as we do arithmetic, but they’ll do it better.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#correlations",
    "href": "11-pattern.html#correlations",
    "title": "2  Pattern Matching",
    "section": "2.2 Correlations",
    "text": "2.2 Correlations\nArguable, the simplest form of pattern matching is correlation. Correlation is a statistical measure that quantifies the strength of the relationship between two variables. It is a measure of how closely two variables move in relation to each other. Correlation is often used to identify patterns in data and determine the strength of the relationship between two variables. It is a fundamental statistical concept that is widely used in various fields, including science, engineering, finance, and business.\nLet’s consider the correlation between returns on Google stock and S&P 500 stock index. The correlation coefficient is a measure of the strength and direction of the linear relationship between two variables. It is a number between -1 and 1.\n\nExample 2.2 (Google Stock Returns) Figure 2.1 shows the scattershot of Google and S&P 500 daily returns\n\ngoog = read.csv(\"../data/GOOG2019.csv\") \nrgoog = goog$Adj.Close[2:251]/goog$Adj.Close[1:250] - 1 \nsp = read.csv(\"../data/SP2019.csv\");   rsp = sp$Adj.Close[2:251]/sp$Adj.Close[1:250] - 1 \nplot(rgoog, rsp, col=\"lightblue\", pch=21, bg=\"grey\", xlab=\"GOOG return\", ylab=\"SP500 return\") \n\n\n\n\n\n\n\nFigure 2.1: Scattershot of Google and S&P 500 daily returns\n\n\n\n\n\nLet’s calculate the covariance and correlation between the daily returns of the Google stock and S&P 500.\n\nvar_goog = mean((rgoog - mean(rgoog))^2) \nvar_sp = mean((rsp - mean(rsp))^2) \ncov = mean((rgoog - mean(rgoog))*(rsp - mean(rsp))); print(cov) \n\n## [1] 7.998625e-05\n\ncor = cov/(sqrt(var_goog)*sqrt(var_sp)); print(cor)\n\n## [1] 0.6673029\n\n\nThe example demonstrates how correlation serves as a fundamental form of pattern recognition, showing the relationship between Google stock returns and S&P 500 returns. We see how correlation coefficients quantify the strength and direction of linear relationships between variables, ranging from -1 to 1. The Google/S&P 500 example shows how correlation analysis is used in finance to understand market relationships and portfolio diversification. The code demonstrates the mathematical calculation of correlation through covariance and variance, showing the underlying statistical mechanics.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#unsupervised-learning",
    "href": "11-pattern.html#unsupervised-learning",
    "title": "2  Pattern Matching",
    "section": "2.3 Unsupervised Learning",
    "text": "2.3 Unsupervised Learning\nPrediction and forecasting is most frequent problem in data analysis and the most common approach to solve the prediction problem is via the pattern matching. Prediction and forecasting are two closely related concepts that are often used interchangeably. In business and engineering the main motivation for prediction and forecasting is to make better decisions. In science, the main motivation is to test and validate theories. Prediction and forecasting help to identify trends and patterns in historical data that would otherwise remain hidden. This allows analysts to make informed decisions about the future based on what they know about the past. By using prediction models, analysts can identify potential risks and opportunities that may lie ahead. This information can then be used to develop proactive strategies to mitigate risks and capitalize on opportunities. In many business applications the concern is improving efficiency of a system. For example to improve logistic chains and to optimally allocate resources, we need to forecast demand and supply and to predict the future prices of the resources. By predicting future sales, businesses can better plan their marketing and sales efforts. This can lead to increased sales and profitability. Prediction and forecasting can be used to identify and mitigate potential risks, such as financial losses, supply chain disruptions, and operational failures.\nBelow we provide an example of an unsupervised learning approach to forecasting. Formally, unsupervised learning is a type of machine learning where the model is not provided with labeled data. The goal is to find patterns in the data without any prior knowledge of the structure of the data. In the example below we use the 2012 US presidential election data to forecast the probability of Obama winning the election.\n\nExample 2.3 (Obama Elections) This example demonstrates a Bayesian approach to election forecasting using polling data from the 2012 US presidential election. The goal is to predict the probability of Barack Obama winning the election by combining polling data across different states.\nThe data used includes polling data from various pollsters across all 50 states plus DC. Each state has polling percentages for Republican (GOP) and Democratic (Dem) candidates along with their electoral vote counts. The data is aggregated by state, taking the most recent polls available.\nThe techniques applied involve Bayesian simulation using a Dirichlet distribution to model uncertainty in polling percentages. Monte Carlo simulation runs 10,000 simulations of the election to estimate win probabilities. The analysis is conducted state-by-state, calculating Obama’s probability of winning each individual state. Electoral college modeling combines state probabilities with electoral vote counts to determine the overall election outcome. The simulation runs the entire election multiple times to account for uncertainty and determines the likelihood of Obama reaching the required 270 electoral votes to win. This approach demonstrates how pattern matching through statistical modeling can be used for prediction, showing how polling data can be transformed into probabilistic forecasts of election outcomes.\nWe start by loading the data and aggregating it by state.\n\nlibrary(plyr)\n# Source: \"http://www.electoral-vote.com/evp2012/Pres/pres_polls.csv\"\nelection.2012 = read.csv(\"../data/pres_polls.csv\")\n# Remove a pollster: elect2012 &lt;- election.2012[!grepl('Rasmussen', election.2012$Pollster),]\nelect2012 &lt;- election.2012\n# Aggregrate the data\nelect2012 &lt;- ddply(elect2012, .(state), subset, Day == max(Day))\nelect2012 &lt;- ddply(elect2012, .(state), summarise, R.pct = mean(GOP), O.pct = mean(Dem), EV = mean(EV))\n\nknitr::kable(elect2012[1:25,], caption = \"Election 2012 Data\",longtable=TRUE)\nknitr::kable(elect2012[26:51,], caption = \"Election 2012 Data\",longtable=TRUE)\n\n\n\n\nElection 2012 Data\n\n\nstate\nR.pct\nO.pct\nEV\n\n\n\n\nAlabama\n61\n38\n9\n\n\nAlaska\n55\n42\n3\n\n\nArizona\n54\n44\n11\n\n\nArkansas\n61\n37\n6\n\n\nCalifornia\n38\n59\n55\n\n\nColorado\n47\n51\n9\n\n\nConnecticut\n40\n58\n7\n\n\nD.C.\n7\n91\n3\n\n\nDelaware\n40\n59\n3\n\n\nFlorida\n49\n50\n29\n\n\nGeorgia\n53\n45\n16\n\n\nHawaii\n28\n71\n4\n\n\nIdaho\n65\n33\n4\n\n\nIllinois\n41\n57\n20\n\n\nIndiana\n54\n44\n11\n\n\nIowa\n47\n52\n6\n\n\nKansas\n60\n38\n6\n\n\nKentucky\n61\n38\n8\n\n\nLouisiana\n58\n41\n8\n\n\nMaine\n41\n56\n4\n\n\nMaryland\n37\n62\n10\n\n\nMassachusetts\n38\n61\n11\n\n\nMichigan\n45\n54\n16\n\n\nMinnesota\n45\n53\n10\n\n\nMississippi\n56\n44\n6\n\n\n\n\n\n\nElection 2012 Data\n\n\n\nstate\nR.pct\nO.pct\nEV\n\n\n\n\n26\nMissouri\n54\n44\n10\n\n\n27\nMontana\n55\n41\n3\n\n\n28\nNebraska\n61\n38\n5\n\n\n29\nNevada\n46\n52\n6\n\n\n30\nNew Hampshire\n46\n52\n4\n\n\n31\nNew Jersey\n41\n58\n14\n\n\n32\nNew Mexico\n43\n53\n5\n\n\n33\nNew York\n36\n63\n29\n\n\n34\nNorth Carolina\n51\n48\n15\n\n\n35\nNorth Dakota\n59\n39\n3\n\n\n36\nOhio\n48\n50\n18\n\n\n37\nOklahoma\n67\n33\n7\n\n\n38\nOregon\n43\n54\n7\n\n\n39\nPennsylvania\n47\n52\n20\n\n\n40\nRhode Island\n36\n63\n4\n\n\n41\nSouth Carolina\n55\n44\n9\n\n\n42\nSouth Dakota\n58\n40\n3\n\n\n43\nTennessee\n60\n39\n11\n\n\n44\nTexas\n57\n41\n38\n\n\n45\nUtah\n73\n25\n6\n\n\n46\nVermont\n31\n67\n3\n\n\n47\nVirginia\n48\n51\n13\n\n\n48\nWashington\n42\n56\n12\n\n\n49\nWest Virginia\n62\n36\n5\n\n\n50\nWisconsin\n46\n53\n10\n\n\n51\nWyoming\n69\n28\n3\n\n\n\n\n\n\nWe then run the simulation and plot probabilities by state.\n\nlibrary(MCMCpack)\nprob.Obama &lt;- function(mydata) {\n    p &lt;- rdirichlet(1000, 500 * c(mydata$R.pct, mydata$O.pct, 100 - mydata$R.pct - \n        mydata$O.pct)/100 + 1)\n    mean(p[, 2] &gt; p[, 1])\n}\nwin.probs &lt;- ddply(elect2012, .(state), prob.Obama)\nwin.probs$Romney &lt;- 1 - win.probs$V1\nnames(win.probs)[2] &lt;- \"Obama\"\nwin.probs$EV &lt;- elect2012$EV\nwin.probs &lt;- win.probs[order(win.probs$EV), ]\nrownames(win.probs) &lt;- win.probs$state\n\nWe then plot the probabilities of Obama winning by state.\n\nlibrary(usmap)\nplot_usmap(data = win.probs, values = \"Obama\") + \n  scale_fill_continuous(low = \"red\", high = \"blue\", name = \"Obama Win Probability\", label = scales::comma) + theme(legend.position = \"right\")\n\n\n\n\nProbabilities of Obama winning by state\n\n\n\n\nWe use those probabilities to simulate the probability of Obama winning the election. First, we calculate the probability of Obama having &gt;270 EV or more\n\nsim.election &lt;- function(win.probs) {\n    winner &lt;- rbinom(51, 1, win.probs$Obama)\n    sum(win.probs$EV * winner)\n}\n\nsim.EV &lt;- replicate(10000, sim.election(win.probs))\noprob &lt;- sum(sim.EV &gt;= 270)/length(sim.EV)\noprob\n\n## [1] 0.9664\n\n\n\nlibrary(lattice)\n# Lattice Graph\ndensityplot(sim.EV, plot.points = \"rug\", xlab = \"Electoral Votes for Obama\", \n    panel = function(x, ...) {\n        panel.densityplot(x, ...)\n        panel.abline(v = 270)\n        panel.text(x = 285, y = 0.01, \"270 EV to Win\")\n        panel.abline(v = 332)\n        panel.text(x = 347, y = 0.01, \"Actual Obama\")\n}, main = \"Electoral College Results Probability\")\n\n\n\n\n\n\n\n\nResults of recent state polls in the 2008 United States Presidential Election between Barack Obama and John McCain.\n\n# Source: LearnBayes library\n#| fig-height: 6\nelection.2008 = read.csv(\"../data/election2008.csv\")\ndata(election.2008)\nattach(election.2008)\n\n##  Dirichlet simulation\n\n\nprob.Obama = function(j)\n {\n p=rdirichlet(5000,500*c(M.pct[j],O.pct[j],100-M.pct[j]-O.pct[j])/100+1)\n mean(p[,2]&gt;p[,1])\n }\n\n## sapply function to compute Obama win prob for all states\n\nObama.win.probs=sapply(1:51,prob.Obama)\n\n##  sim.EV function\n\nsim.election = function()\n {\n winner = rbinom(51,1,Obama.win.probs)\n sum(EV*winner)\n }\n\nsim.EV = replicate(1000,sim.election())\n\n## histogram of simulated election\nhist(sim.EV,min(sim.EV):max(sim.EV),col=\"blue\",prob=T)\nabline(v=365,lwd=3)   # Obama received 365 votes\ntext(375,30,\"Actual \\n Obama \\n total\")\n\n\n\n\n\n\n\n\nThe analysis of the 2008 U.S. Presidential Election data reveals several key insights about the predictive power of state-level polling and the uncertainty inherent in electoral forecasting. The actual result of 365 electoral votes falls within the simulated range, demonstrating the model’s validity. The 270-vote threshold needed to win the presidency is clearly marked and serves as a critical reference point\nWe used relatively simple model to simulate the election outcome. The model uses Dirichlet distributions to capture uncertainty in state-level polling percentages. Obama’s win probabilities vary significantly across states, reflecting the competitive nature of the election. The simulation approach accounts for both sampling uncertainty and the discrete nature of electoral vote allocation. The histogram of simulated results shows the distribution of possible outcomes. The actual Obama total of 365 electoral votes is marked and falls within the reasonable range of simulated outcomes. This validates the probabilistic approach to election forecasting.\nThis analysis demonstrates how Bayesian methods can be effectively applied to complex prediction problems with multiple sources of uncertainty, providing both point estimates and measures of uncertainty around those estimates.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#supervised-learning",
    "href": "11-pattern.html#supervised-learning",
    "title": "2  Pattern Matching",
    "section": "2.4 Supervised Learning",
    "text": "2.4 Supervised Learning\nThe problem of supervised learning is to learn patterns from observed data to make predictions on new, unseen data. The key idea is that we have input-output pairs \\((x_i, y_i)\\) where we know the correct output \\(y_i\\) for each input \\(x_i\\), and we use these examples to learn a function that maps inputs to outputs. Supervised learning has become ubiquitous across modern engineering, business, and technology applications. In manufacturing, predictive maintenance systems use sensor data from industrial equipment to forecast potential failures, enabling proactive maintenance that reduces downtime and costs. Autonomous vehicles rely heavily on supervised learning for object detection, lane recognition, and decision-making systems that process real-time sensor data from cameras, LiDAR, and radar. In healthcare, supervised learning powers diagnostic imaging systems that can detect diseases from X-rays, MRIs, and CT scans with accuracy rivaling human radiologists. Financial institutions employ supervised learning for fraud detection, credit scoring, and algorithmic trading systems that analyze vast amounts of transaction data. Smart cities utilize supervised learning for traffic flow optimization, energy consumption forecasting, and air quality monitoring. Many companies use prediction models for customer churn, it helps identifying early warning signs of dissatisfaction. Marketing teams leverage supervised learning for customer segmentation, campaign optimization, and lead scoring to improve conversion rates. Supply chain optimization uses supervised learning to forecast demand, optimize inventory levels, and predict delivery times. These applications demonstrate how supervised learning has evolved from simple prediction tasks to complex, real-time decision-making systems that operate across diverse domains.\nA typical prediction problem involves building a rule that maps observed inputs \\(x\\) into the output \\(y\\). The inputs \\(x\\) are often called predictors, features, or independent variables, while the output \\(y\\) is often called the response or dependent variable. The goal is to find a predictive rule \\[\ny = f(x).\n\\]\nThe map \\(f\\) can be viewed as a black box which describes how to find the output \\(y\\) from the input \\(x\\). One of the key requirement of \\(f\\) is that we should be able to efficiently find this function using an algorithm. In the simple case \\(y\\) and \\(x\\) are both univariate (scalars) and we can view the map as\n\n\n\n\n.\n\n\nThe goal of machine learning is to reconstruct this this map from observed data. In a multivariate setting \\(x = (x_1,\\ldots,x_p)\\) is a list of \\(p\\) variables. This leads to a model of the form \\(y = f(x_1,\\ldots,x_p)\\). There are a number of possible goals of analysis, such as estimation, inference or prediction. The main one being prediction.\nThe prediction task is to calculate a response that corresponds to a new feature input variable. Example of af an inference is the task of establishing a causation, with the goal of extracting information about the nature of the black box association of the response variable to the input variables.\nIn either case, the goal is to use data to find a pattern that we can exploit. The pattern will be ``statistical” in its nature. To uncover the pattern we use a training dataset, denoted by \\[\nD = (y_i,x_i)_{i=1}^n\n\\]\nwhere \\(x_i\\) is a set of \\(p\\) predictors ans \\(y_i\\) is response variable. Prediction problem is to use a training dataset \\(D\\) to design a rule that can be used for predicting output values \\(y\\) for new observations \\(x\\).\nLet \\(f(x)\\) be predictor of \\(y\\), we will use notation \\[\n\\hat{y} = f(x).\n\\]\nTo summarize, we will use the following notation.\n\n\n\n\\(y\\)\noutput variable (response/outcome)\n\n\n\\(x\\)\ninput variable (predictor/covariate/feature)\n\n\n\\(f(x)\\)\npredictive rule\n\n\n\\(\\hat y\\)\npredicted output value\n\n\n\nWe distinguish several types of input or output variables. First, binary variables that can only have two possible values, e.g. yes/no, left/right, 0/1, up/down, etc. A generalization of binary variable is a categorical variable that can take a fixed number of possible values, for example, marriage status. Additionally, some of the categorical variable can have a natural order to them, for example education level or salary range. Those variables are called ordinal. Lastly, the most common type of a variable is quantitative which is described by a real number.\nDepending on the type of the output variable, there are three types of prediction problems.\n\nTypes of output variables and corresponding prediction problems.\n\n\n\n\n\n\n\nOutput Variable Type\nDescription\nPrediction Problem\n\n\n\n\nBinary\n\\(y\\in \\{0,1\\}\\)\nClassification\n\n\nCategorical\n\\(y\\in \\{0,\\ldots,K\\}\\) for \\(K\\) possible categories\nClassification\n\n\nQuantitative\n\\(y \\in \\mathbb{R}\\) (any real number)\nRegression\n\n\nOrdinal\n\\(y\\) has natural ordering\nRanking\n\n\n\nHere are some examples of prediction problems:\nBinary Classification: Predicting whether an email is spam or not spam\n\nInput variables: Email content, sender information, presence of certain keywords, email length\nOutput variable: \\(y \\in \\{0,1\\}\\) where 0 = not spam, 1 = spam\nGoal: Classify new emails as spam or legitimate\n\nCategorical Classification: Predicting the type of social media content based on text and image features\n\nInput variables: Text content, image features, user engagement metrics, posting time, hashtags\nOutput variable: \\(y \\in \\{0,1,2,3,4\\}\\) where 0 = news, 1 = entertainment, 2 = educational, 3 = promotional, 4 = personal\nGoal: Automatically categorize social media posts for content moderation and recommendation systems\n\nRegression (Quantitative) Problem: Predicting house prices based on features\n\nInput variables: Square footage, number of bedrooms, location, age of house, lot size\nOutput variable: \\(y \\in \\mathbb{R}\\) (house price in dollars)\nGoal: Predict the selling price of a new house\n\nRanking (Ordinal)\nProblem: Predicting customer satisfaction ratings\n\nInput variables: Product quality, customer service experience, delivery time, price\nOutput variable: \\(y \\in \\{1,2,3,4,5\\}\\) where 1 = very dissatisfied, 5 = very satisfied\nGoal: Predict customer satisfaction level for new customers\n\nThere are several simple predictive rules we can use to predict the output variable \\(y\\). For example, in the case of regression problem, the simplest rule is to predict the average value of the output variable. This rule is called the mean rule and is defined as \\[\nf(x) = \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i.\n\\]\nNotice, this model does not depend on the input variable \\(x\\) and will predict the same value for all observations. This rule is simple and easy to implement, but it is not very accurate. In case of binary \\(y\\), we can apply thresholding to the mean rule to obtain a binary classifier. \\[\nf(x) = \\begin{cases}\n1 & \\text{if } \\bar{y} &gt; 0.5, \\\\\n0 & \\text{if } \\bar{y} \\leq 0.5.\n\\end{cases}\n\\]\nA more sophisticated rule is the nearest neighbor rule. This rule predicts the output value \\(y\\) for a new observation \\(x\\) by finding the closest observation in the training dataset and using its output value. The nearest neighbor rule is defined as \\[\nf(x) = y_{i^*},\n\\] where \\[i^* = \\arg\\min_{i=1,\\ldots,n} \\|x_i - x\\|\\] is the index of the closest observation in the training dataset. These two models represent two extreme cases of predictive rules: the mean rule is “stubborn” (it always predicts the same value) and the nearest neighbor rule is “flexible” (can be very sensitive to small changes in the inputs). Using the language of statistics the mean rule is of high bias and low variance, while the nearest neighbor rule is of low bias and high variance. Although those two rules are simple, they sometimes lead to useful models that can be used in practice. Further, those two models represent a trade-off between accuracy and complexity (a.k.a bias-variance trade-off). We will discuss this trade-off in more detail in the later section.\nThe mean model and nearest neighbor model belong to a class of so-called non-parametric models. The non-parametric models do not make explicit assumption about the form of the function \\(f(x)\\). In contrast, parametric models assume that the predictive rule \\(f(x)\\) is a specific function defined by vector of parameters, whcih we will denote as \\(\\theta\\). A typical notation is then \\[f(x) = f_{\\theta}(x).\\]\nTraditional modeling culture employs statistical models characterized by single-layer transformations, where the relationship between input variables and output is modeled through direct, interpretable mathematical formulations. These approaches typically involve linear combinations, additive structures, or simple nonlinear transformations that maintain analytical tractability and statistical interpretability. The list of widely used models includes:\n\n\n\n\n\n\n\n\nModel\nFormula\nParameters\n\n\n\n\nLinear Regression\n\\(y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p\\)\n\\(\\theta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)\\)\n\n\nGeneralized Linear Models (GLM)\n\\(y = f^{-1}(\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p)\\)\n\\(\\theta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)\\)\n\n\nGeneralized Additive Models (GAM)\n\\(y = \\beta_0 + f_1(x_1) + \\dots + f_k(x_k)\\)\n\\(\\theta = (\\beta_0, f_1, \\ldots, f_k)\\)\n\n\nPrincipal Component Regression (PCR)\n\\(y = \\beta^T (W x),\\quad W \\in \\mathbb{R}^{k \\times p},\\quad k &lt; p\\)\n\\(\\theta = (\\beta, W)\\)\n\n\nSliced Inverse Regression (SIR)\n\\(y = f(\\beta_1^T x,\\, \\beta_2^T x,\\, \\ldots,\\, \\beta_k^T x,\\, \\epsilon)\\)\n\\(\\theta = (\\beta_1, \\beta_2, \\ldots, \\beta_k)\\)\n\n\n\nIn contrast to these traditional single-layer approaches, Deep Learning employs sophisticated high-dimensional multi-layer neural network architectures that can capture complex, non-linear relationships in data through hierarchical feature learning. Each layer transforms the input data through by applying an affine transformation and a non-linear activation function. The depth and complexity of these architectures allow deep learning models to automatically discover intricate patterns and representations from raw input data, making them particularly effective for tasks involving high-dimensional inputs such as image recognition, natural language processing, and complex time series analysis. Unlike traditional statistical models that rely on hand-crafted features, deep learning models learn hierarchical representations directly from the data, with early layers capturing simple features (like edges in images) and deeper layers combining these into more complex, abstract representations.\nWe wish to find map \\(f\\) such that \\[\\begin{align*}\ny &= f ( x ) \\\\\ny &=  f ( x_1 , \\ldots , x _p )\n\\end{align*}\\]\nEssentiall, the goal is to perform the pattern matching, also known as nonparametric regression. It involves finding complex relationships in data without assuming a specific functional form. In deep learning, we use composite functions rather than additive functions. We write the superposition of univariate functions as \\[\nf = f_1 \\circ \\ldots \\circ f_L   \\; \\; \\text{versus}  \\; \\; f_1 +  \\ldots + f_L\n\\] where the composition of functions creates a hierarchical structure. Each function \\(f_i\\) is a combination of a linear transformation and a non-linear activation function \\[\nf_i(x) = \\sigma(W_i x + b_i),\n\\] The composition of functions creates a hierarchical structure. Then the set of parameters that we need to find is \\(\\theta = (W_1, b_1, \\ldots, W_L, b_L)\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#model-estimation",
    "href": "11-pattern.html#model-estimation",
    "title": "2  Pattern Matching",
    "section": "2.5 Model Estimation",
    "text": "2.5 Model Estimation\nThere are two main approaches to finding the set of parameters \\(\\theta\\). The first is optimmisation approach that minimizes a loss function. Loss function measure how well predictive rule \\(f\\) captures the relationship between input and output variables. The most common loss function is the mean squared error (MSE). The second approach is to use full Bayesian inference and to calculate the distribution over parameter \\(\\theta\\) given the observed data.\nBoth approaches start with formulating likelihood function Likelihood is a function that tells us how probable the observed data is, given a particular value of the parameter in a statistical model. It is not the same as probability — instead, it’s a function of the parameter, with the data fixed. Suppose you flip a biased coin 10 times and get 7 heads. You want to estimate the probability of getting heads on a single toss. You try different values of \\(\\theta\\) and ask: “How likely is it to get exactly 7 heads out of 10 flips if the true probability is \\(\\theta\\)?” This leads to the likelihood function. Formally, given \\(y_i \\sim f(y_i\\mid x_i,  \\theta)\\): iid samples from a distribution with parameter \\(\\theta\\), the likelihood function is defined as \\[\nL(\\theta) = \\prod_{i=1}^n p(y_i\\mid x_i,  \\theta).\n\\] It treats the data \\(D = (y_i,x_i)_{i=1}^n\\) as fixed and varies \\(\\theta\\).\nLikelihood connects our model to the data generating process by quantifying how likely it is to observe the actual data we have under different parameter values. For example, if we assume our data follows a normal distribution \\(y \\sim N(f_\\theta(x), \\sigma^2)\\) with mean \\(f_\\theta(x)\\) and variance \\(\\sigma^2\\), the likelihood function would be:\n\\[\nL(\\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - f_\\theta(x_i))^2}{2\\sigma^2}\\right).\n\\]\nFor the case of classification problem, we assume that \\(y_i\\) follows a Bernoulli distribution \\(y_i \\sim \\text{Bernoulli}(f_\\theta(x_i))\\). The likelihood function is defined as \\[\nL(\\theta) = \\prod_{i=1}^n f_\\theta(x_i)^{y_i} (1-f_\\theta(x_i))^{1-y_i}.\n\\] Notice, that in this case function \\(f_\\theta(x_i)\\) is restricted to output values in \\((0,1)\\).\nThe optimization-based approach is to find the set of parameters \\(\\theta\\) that maximizes the likelihood function. \\[\n\\theta^* = \\arg\\max_{\\theta} L(\\theta).\n\\]\nAlthough most often, it is easier to optimize the log-likelihood function. We simply take the logarithm of the likelihood function \\[\n\\log L(\\theta) = l(\\theta) = \\sum_{i=1}^n \\log p(y_i\\mid x_i,  \\theta).\n\\] Notice, the log-likelihood function is a sum of log-likelihoods for each data point. This is a more convenient form for optimization algorithms.\nWhy does the solution not change? Since the logarithm is a monotonically increasing function, if \\(L(\\theta_1) &gt; L(\\theta_2)\\), then \\(\\log L(\\theta_1) &gt; \\log L(\\theta_2)\\). This means that the parameter value that maximizes the likelihood function will also maximize the log-likelihood function. The maximum point stays the same, just the function values are transformed.\nThe value of parameters \\(\\theta\\) that maximizes the log-likelihood is called the maximum likelihood estimate (MLE).\nFinally, in machine leanring literature, it is more common to minimize the negative log-likelihood function (same as maximizing the log-likelihood function). \\[\n\\theta^* = \\arg\\min_{\\theta} -l(\\theta).\n\\] Then the negative log-likelihood function is called the loss function. Thus the problem of finding maximum likelihood estimate is equivalent to minimizing the loss function.\n\n2.5.1 Penalized Likelihood\nWhile maximum likelihood estimation provides a principled approach to parameter estimation, we can often find a better estimators using what is called a penalized likelihood. In fact, there are certain cases, when penalized estimator leads to universally better estimators. In statistics, we would say that MLE is inadmissibile, meaning there exists another estimator (James–Stein) that is strictly better in terms of risk. Later is Section ?sec-theoryai we will discuss the theory of penalized estimators in more detail.\nPenalized likelihood addresses overfitting by adding a regularization term to the likelihood function. Instead of maximizing just the likelihood, we maximize:\n\\[\nL_{\\text{penalized}}(\\theta) = L(\\theta) \\cdot \\exp(-\\lambda \\phi(\\theta))\n\\]\nOr equivalently, we minimize the negative log-likelihood plus a penalty: \\[\nl(\\theta) =\\sum_{i=1}^n -l(y_i, f_{\\theta} (x_i)) +\\lambda \\sum_{j=1}^p \\phi(\\theta_j),\n\\] where \\(\\lambda &gt; 0\\) is the regularization parameter that controls the strength of regularization, and \\(\\phi(\\theta)\\) is the penalty function that measures model complexity. In machine learning the technique of adding the penalty term to the loss function is called regularization.\nRegularization can be viewed as constraint on the model space. The techniques were originally applied to solve ill-posed problems where a slight change in the initial data could significantly alter the solution. Regularization techniques were then proposed for parameter reconstruction in a physical system modeled by a linear operator implied by a set of observations. It had long been believed that ill-conditioned problems offered little practical value, until Tikhonov published his seminal paper Andrey Nikolayevich Tikhonov et al. (1943) on regularization. Andrei N. Tikhonov (1963) proposed methods for solving regularized problems of the form \\[\n\\minf_\\beta  ||y- X\\beta||^p_p   + \\lambda||(\\beta - \\beta^{(0)})||^q_q.\n\\] Here \\(\\lambda\\) is the weight on the regularization penalty and the \\(\\ell_q\\)-norm is defined by \\(||\\beta||_q^q = \\sum_i \\beta_i^q\\). This optimization problem is a Lagrangian form of the constrained problem given by \\[\n\\mbox{minimize}_{\\beta}\\quad|y- X\\beta||^2_2\\qquad\\mbox{subject to }\\sum_{i=1}^{p}\\phi(\\beta_i) \\le s.\n\\] with \\(\\phi(\\beta_i) = (\\beta_i - \\beta_i^{(0)})^q\\).\nLater, sparsity became a primary driving force behind new regularization methods Candes and Wakin (2008). The idea is that the vector of parameters \\(\\beta\\) is sparse, meaning that most of its elements are zero. This is a natural assumption for many models, such as the linear regression model. We will discuss the sparsity in more detail later in the book.\nThere are multiple optimisaion algorithms that can be used to find the solution to the penalized likelihood problem. Later in the book we will discuss the Stochastic Gradient Descent (SGD) algorithm, which is a popular tool for training deep learning models.\n\n\n2.5.2 Bayesian Approach\nSimilar to the likelihood maximisation approach, the Bayesin approach to model estimation starts with the likelihood function. The difference is that we assume that the parameters \\(\\theta\\) are random variables and follow some prior distribution. Then we use the Bays rule to find the posterior distribution of the parameters \\[\np(\\theta | D) \\propto l(\\theta) \\cdot p(\\theta)\n\\] where \\(p(\\theta)\\) is the prior distribution and \\(p(y | \\theta)\\) is the likelihood function. The posterior distribution is the distribution of the parameters given the data \\(D = (y_i,x_i)_{i=1}^n\\). It is a distribution over the parameters, not a single value.\nPenalized likelihood has a natural Bayesian interpretation. The penalty term corresponds to a prior distribution on the parameters: \\[\np(\\theta) \\propto \\exp(-\\lambda \\phi(\\theta))\n\\] Then the penalized likelihood is proportional to the posterior distribution: \\[\np(\\theta | y) \\propto p(y | \\theta) \\cdot p(\\theta) = L(\\theta) \\cdot \\exp(-\\lambda \\phi(\\theta))\n\\]\nThis means maximizing the penalized likelihood is equivalent to finding the maximum a posteriori (MAP) estimate, which is the mode of the posterior distribution.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#prediction-accuracy",
    "href": "11-pattern.html#prediction-accuracy",
    "title": "2  Pattern Matching",
    "section": "2.6 Prediction Accuracy",
    "text": "2.6 Prediction Accuracy\nAfter we fit our model and find the optimal value of the parameter \\(\\theta\\), denoted by \\(\\hat \\theta\\), we need to evaluating the accuracy of a predictive model. It involves comparing the model’s predictions to actual outcomes. We can simply use the value of the loss function from the training step to evaluate model’s predictive power. However, this only tells us how well the model fits the training data. It doesn’t tell us how well the model will perform on unseen data. To evaluate the model’s performance on unseen data, we need to use a different approach.\nThe most common approach is to split the data into training and test sets. The training set is used to train the model, while the test set is used to evaluate its performance. This approach is known as the train-test split. It is a simple and effective way to evaluate how well model predicts unseen for unseen inputs.\nAnother approach is to use cross-validation. It involves splitting the data into smaller subsets and using them to train and test the model multiple times. When our sample size is small, this allows for a more robust estimate of the model’s performance than simply splitting the data into a single training and test set. For small data sets, simple train-test split approach will be sensitive to choice of test samples, thus the estimated predicted performance will be unstable (high variance). Cross-validation helps to reduce this variance by averaging the performance across multiple folds. This makes the performance estimate more robust and less sensitive to the choice of test samples.\nCross-validation involves several steps:\n\nSplit the data: The data is randomly divided into \\(k\\) equal-sized chunks (folds).\nTrain and test the model: For each fold, the model is trained on \\(k-1\\) folds and tested on the remaining fold. This process is repeated \\(k\\) times, ensuring each fold is used for testing once.\nEvaluate the model: The performance of the model is evaluated on each fold using a chosen metric, such as accuracy, precision, recall, or F1 score.\nReport the average performance: The average of the performance metrics across all k folds is reported as the final estimate of the model’s performance.\n\nA common choice for \\(k\\) is 5 or 10. When \\(K=n\\), this is known as leave-one-out cross-validation. This method can be computationally expensive but is less likely to overfit the data. Stratified cross-validation ensures that each fold contains approximately the same proportion of each class as in the entire dataset. This is important for imbalanced datasets where one class is significantly larger than the others.\nNotice, that cross-validation requires re-training the model multiple times, which can be computationally expensive. Thus, for large datasets, we typically prefer simple train-test split. However, for small datasets, cross-validation can provide a more robust estimate of the model’s performance.\nEither method is limited to evaluating the model’s performance on data that is available to the modeler. What if we start using our model on data that is different from the training and test sets? Unlike in physics, when a model represents a law that is universal, in data science, we are dealing with data that is generated by a process that is not necessarily universal. For example, if we are building a model to predict the price of a house, we can train and test the model on data from a specific city. However, if we start using the model to predict the price of a house in a different city, the model might not perform as well. This is because the data from the new city might be different from the data used to train and test the model. This is known as the problem of generalization. It refers to the ability of a model to perform well on data that is different from the training and test sets.\n\n2.6.1 Evaluation Metrics for Regression\nThere are several metrics that can be used to evaluate the performance of regression models. We can simply use the same function as we use for fitting the model, e.g. least squares \\[\n\\text{MSE} = \\dfrac{1}{m}\\sum_{i=1}^n (y_i -\\hat y_i)^2,\n\\] here \\(\\hat y_i\\) is the predicted value of the i-th data point by the model \\(\\hat y_i = f(x_i,\\hat\\theta)\\) and \\(m\\) is the total number of data points used for the evaluation. This metric is called the Mean Squared Error (MSE). It is the average squared difference between the actual and predicted values. Lower MSE indicates better model performance, as it means the model’s predictions are closer to the actual values.\nA slight variation of this metric is Root Mean Squared Error (RMSE). This is the square root of MSE and is also commonly used due to its units being the same as the target variable. \\[\n\\text{RMSE} = \\sqrt{\\text{MSE}}.\n\\] However, MSE is sensitive to outliers, as it squares the errors, giving more weight to large errors. This can lead to misleading results when the data contains outliers.\nMedian Absolute Error (MAE) solves the sensetivity to the outliers problem. It is the median of the absolute errors, providing a more robust measure than MAE for skewed error distributions \\[\n\\text{MAE} = \\dfrac{1}{m}\\sum_{i=1}^n |y_i -\\hat y_i|.\n\\] A variation of it is the Mean Absolute Percentage Error (MAPE), which is the mean of the absolute percentage errors \\[\n\\text{MAPE} = \\dfrac{1}{m}\\sum_{i=1}^n \\left | \\dfrac{y_i -\\hat y_i}{y_i} \\right |.\n\\]\nAlternative way to measure the predictive quility is to use the coefficient of determination, also known as the R-squared value, which measures the proportion of variance in the target variable that is explained by the model. Higher R-squared indicates better fit. However, R-squared can be misleading when comparing models with different numbers of features. R-squared is defined as follows \\[\nR^2 = 1 - \\dfrac{\\sum_{i=1}^n (y_i -\\hat y_i)^2}{\\sum_{i=1}^n (y_i -\\bar y_i)^2},\n\\] where \\(\\bar y_i\\) is the mean of the target variable. R-squared is a relative measure of fit, so it can be used to compare different models. However, it is not an absolute measure of fit, so it cannot be used to determine whether a model is good or bad. It is also sensitive to the number of features in the model, so it cannot be used to compare models with different numbers of features.\nFinally, we can use graphics to evaluate the model’s performance. For example, we can scatterplot the actual and predicted values of the target variable to visually compare them. We can also plot the histogram of a boxplot of the residuals (errors) to see if they are normally distributed.\n\n\n2.6.2 Evaluation Metrics for Classification\nAccuracy is the most fundamental metric used to evaluate models. It is defined as the ratio of the number of correct predictions to the total number of predictions. The formula is given by \\[\\text{Accuracy} = \\frac{\\text{TP+TN}}{\\text{TP+TN+FP+FN}},\\] where TP, TN, FP, and FN are the numbers of true positives, true negatives, false positives, and false negatives, respectively. However, it can be misleading for imbalanced datasets where one class is significantly larger than others. For example, if 95% of the data belongs to one class, a model that always predicts this class will be 95% accurate, even though it’s not very useful.\nA more comprehensive understanding of model performance can be achieved by calculaitng the sensitivity (a.k.a precision) and specificity (a.k.a. recall) as well as confusion matrix discussed in ?sec-Sensitivity. The confusion matrix is\n\n\n\nActual/Predicted\nPositive\nNegative\n\n\n\n\nPositive\nTP\nFN\n\n\nNegative\nFP\nTN\n\n\n\nPrecision measures the proportion of positive predictions that are actually positive. It is useful for evaluating how good the model is at identifying true positives. Recall measures the proportion of actual positives that are correctly identified by the model. It is useful for evaluating how good the model is at not missing true positives.\nThen we can use those to calculate F1 Score which is is a harmonic mean of precision and recall, providing a balanced view of both metrics. Higher F1 score indicates better overall performance. If misclassifying certain instances is more costly than others, weighted metrics account for these different costs. For imbalanced datasets, metrics like F1 score or balanced accuracy are important to avoid misleading interpretations.\nSometimes, we use multiple metrics to get a comprehensive assessment of the model’s performance. Additionally, consider comparing the model’s performance to a baseline model or other existing models for the same task. Sometimes, it is hard to beat a “coin flip” classification model, when the model predicts the class randomly with equal probability. In regression, a simple baseline model is \\(f(x_i) = \\bar y\\), which is the mean of the target variable.\n\n\n\n\nBonfiglio, Rita, Annarita Granaglia, Raffaella Giocondo, Manuel Scimeca, and Elena Bonanno. 2021. “Molecular Aspects and Prognostic Significance of Microcalcifications in Human Pathology: A Narrative Review.” International Journal of Molecular Sciences 22 (120).\n\n\nCandes, Emmanuel J, and Michael B Wakin. 2008. “An Introduction To Compressive Sampling. A Sensing/Sampling Paradigm That Goes Against the Common Knowledge in Data Aquisition.” IEEE Signal Processing Magazine 25 (21).\n\n\nFeynman, Richard. n.d. “Feynman :: Rules of Chess.”\n\n\nHalevy, Alon, Peter Norvig, and Fernando Pereira. 2009. “The Unreasonable Effectiveness of Data.” IEEE Intelligent Systems 24 (2): 8–12.\n\n\nNicosia, Luca, Giulia Gnocchi, Ilaria Gorini, Massimo Venturini, Federico Fontana, Filippo Pesapane, Ida Abiuso, et al. 2023. “History of Mammography: Analysis of Breast Imaging Diagnostic Achievements over the Last Century.” Healthcare 11 (1596).\n\n\nTikhonov, Andrei N. 1963. “Solution of Incorrectly Formulated Problems and the Regularization Method.” Sov Dok 4: 1035–38.\n\n\nTikhonov, Andrey Nikolayevich et al. 1943. “On the Stability of Inverse Problems.” In Dokl. Akad. Nauk Sssr, 39:195–98.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "12-regression.html",
    "href": "12-regression.html",
    "title": "3  Linear and Multiple Regression",
    "section": "",
    "text": "3.1 Linear Regression\nThe simplest form of a parametric model is a linear model that assumes a linear relationship between the input variables and the output variable. There are a number of possibilities to specify such a family of functions, for example as a linear combinations of inputs \\[\nf(x)=\\beta_0+\\beta_1x_1 + \\ldots + \\beta_p x_p = \\beta^Tx,\n\\] we assume that vector \\(x\\) has \\(p+1\\) components \\(x = (1,x_1,\\ldots,x_p)\\) and \\(\\beta = (\\beta_0,\\beta_1,\\ldots,\\beta_p)\\) is a vector of parameters.\nA more general form of a linear model is a linear combination of basis functions \\[\nf(x)= \\beta_0 + \\beta_1 \\psi_1(x) + \\ldots + \\beta_M \\psi_M(x) = \\beta^T \\psi(x),\n\\] where \\(\\psi_1,\\ldots,  \\psi_M\\) are the basis functions and \\(\\psi(x) = (1, \\psi_1(x),\\ldots,\\psi_M(x))\\).\nNotice in the later case, the function \\(f\\) is linear in the parameters \\(\\beta = (\\beta_0,\\beta_1,\\ldots, \\beta_p)\\) but not in the input variables \\(x\\). The goal of the modeler to choose an appropriate set of predictors and basis functions that will lead to a good reconstruction of the input-output relations. After we’ve specified what the function \\(f\\) is, we need to find the best possible values of parameters \\(\\beta\\).\nFinding a predictive rule \\(f(x)\\) starts by defining the criterion of what is a good prediction. We assume that the function \\(f(x)\\) is parameterized by a vector of parameters \\(\\beta\\) and we want to find the best value of \\(\\beta\\) that will give us the best prediction. Thus, we will use notation \\(f(x,\\beta)\\).\nWe will us a loss function that quantifies the difference between the predicted and actual values of the output variable. It is closely related to the loss function used in decision theory (thus the name). In decision theory, a loss function is a mathematical representation of the “cost” associated with making a particular decision in a given state of the world. It quantifies the negative consequences of choosing a specific action and helps guide decision-makers towards optimal choices. You can think of the loss function in predictive problems as a “cost” associated with making an inaccurate prediction given the values of the input variables \\(x\\).\nThe least squares loss function is the sum of squared differences between the predicted and actual values. Given observed data set \\(\\{(x_1,y_1),\\ldots,(x_n,y_n)\\}\\), the least squares estimator is the value of \\(\\beta\\) that minimizes the sum of squared errors \\[\n\\mini_{\\beta}\\sum_{i=1}^n (y_i - f(x_i,\\beta))^2\n\\] It is easy to show that in the case of normal distribution, the least squares estimator is the maximum likelihood estimator.\nIn the unconditional case, when we do not observe any inputs \\(x\\), the least squares estimator is the sample mean. We can solve his minimization problem by taking derivative of the loss function and setting it to zero \\[\n\\frac{d}{d\\beta}\\sum_{i=1}^n (y_i - \\beta)^2 = -2\\sum_{i=1}^n (y_i - \\beta) = 0\n\\] which gives us the solution \\[\n\\hat{\\beta} = \\frac{1}{n}\\sum_{i=1}^n y_i\n\\] which is the sample average.\nThe least absolute deviations (Quantile) loss function is the sum of absolute differences between the predicted and actual values. It is used for regression problems with continuous variables. The goal is to minimize the sum of absolute errors (SAE) to improve the predictive performance of the model. Given observed data set the least absolute deviations estimator is the value of \\(\\beta\\) that minimizes the sum of absolute errors \\[\n\\mini_{\\beta}\\sum_{i=1}^n |y_i - f(x_i,\\beta)|\n\\] The least absolute deviations estimator is also known as the quantile estimator, where the quantile is set to 0.5 (the median). This is because the least absolute deviations estimator is equivalent to the median of the data (the 0.5 quantile).\nAgain, in the unconditional case, when we do not observe any inputs \\(x\\), the least absolute deviations estimator is the sample median. We can solve his minimization problem by taking derivative of the loss function and setting it to zero \\[\n\\frac{\\mathrm{d} \\left | x \\right | }{\\mathrm{d} x} = \\operatorname{sign} \\left( x \\right)\n\\] where \\(\\operatorname{sign} \\left( x \\right)\\) is the sign function. Hence, deriving the sum above yields \\[\n\\sum_{i=1}^n \\operatorname{sign}(y_i - \\beta).\n\\] This equals to zero only when the number of positive items equals the number of negative which happens when \\(\\beta\\) is the median.\nA more rigorous and non-calculus proof is due to Schwertman, Gilks, and Cameron (1990). Let \\(y_1,\\ldots,y_n\\) be the observed data and \\(\\hat{\\beta}\\) be the least absolute deviations estimator. Then we have \\[\n\\sum_{i=1}^n |y_i - \\hat{\\beta}| \\leq \\sum_{i=1}^n |y_i - \\beta|\n\\] for any \\(\\beta\\). Let \\(y_{(1)},\\ldots,y_{(n)}\\) be the ordered data. Then we have \\[\n\\sum_{i=1}^n |y_i - \\hat{\\beta}| \\leq \\sum_{i=1}^n |y_i - y_{(i)}|\n\\] Let \\(y_{(n/2)}\\) be the median of the data. Then we have \\[\n\\sum_{i=1}^n |y_i - \\hat{\\beta}| \\leq \\sum_{i=1}^n |y_i - y_{(n/2)}|\n\\] which implies that \\(\\hat{\\beta}\\) is the median of the data.\nThe generalization of the median estimator to the case of estimating value of quantile \\(\\tau\\) is as follows \\[\n\\mini_{\\beta}\\sum_{i=1}^n \\rho_{\\tau}(y_i - \\beta)\n\\] where \\(\\rho_{\\tau}(x) = x(\\tau - \\mathbb{I}(x &lt; 0))\\) is the quantile loss function. If we set \\(\\tau = 0.5\\), the loss function becomes the absolute value function and we get the median estimator. The expected loss is \\[\nE \\rho_{\\tau}(y - \\beta) = (\\tau-1)\\int_{-\\infty}^{\\beta} (y-\\beta)dF(y) + \\tau\\int_{\\beta}^{\\infty} (y-\\beta)dF(y)\n\\] Differentiating the expected loss function with respect to \\(\\beta\\) and setting it to zero gives the quantile estimator \\[\n\\hat{\\beta}_{\\tau} = F^{-1}(\\tau)\n\\] where \\(F^{-1}\\) is the quantile function of the distribution of \\(y\\). Thus, the problem of finding a quantile is solved via optimisation.\nA key difference between the least squares and least absolute deviations estimators is their sensitivity to outliers. The least squares estimator is sensitive to outliers because it squares the errors, giving more weight to large errors. In contrast, the least absolute deviations estimator is less sensitive to outliers because it takes the absolute value of the errors, giving equal weight to all errors. This makes the least absolute deviations estimator more robust to outliers than the least squares estimator.\nAnother difference is the computational complexity. Least squares estimator can be found by solving a linear system of equations. There are fast and efficient algorithms for it, making the least squares estimator computationally efficient. In contrast, the least absolute deviations estimator cannot requires more computationally expensive numerical optimization algorithms.\nThere is also a hybrid loss function, called Huber loss , which combines the advantages of squared errors and absolute deviations. It uses SE for small errors and AE for large errors, making it less sensitive to outliers.\nWe used lm function to fit the linear model for the housing data. This function uses least squares loss function to estimate the parameters of the line. One of the nice properties of the least squares estimator is that it has a closed-form solution. This means that we can find the values of the parameters that minimize the loss function by solving a system of linear equations rather than using an optimisation algorithm. The linear system is obtaind by taking the gradient (multivariate derivative) of the loss function with respect to the parameters and setting it to zero. The loss function \\[\nL(\\beta; ~D) = \\sum_{i=1}^n (y_i - f(x_i,\\beta))^2\n\\] is a quadratic function of the parameters, so the solution is unique and can be found analytically. The gradient of the loss function with respect to the parameters is \\[\n\\Delta L(\\beta; ~D) = -2\\sum_{i=1}^n (y_i - f(x_i,\\beta))\\Delta f(x_i,\\beta).\n\\] Given that \\(f(x_i,\\beta) = \\beta^T \\psi(x_i)\\), the gradient of the loss function with respect to the parameters is \\[\n\\Delta L_{\\beta}  = -2\\sum_{i=1}^n (y_i - \\beta^T \\psi(x_i))\\psi(x_i)^T.\n\\] Setting the gradient to zero gives us the normal equations \\[\n\\sum_{i=1}^n y_i \\psi(x_i) = \\sum_{i=1}^n \\beta^T \\psi(x_i) \\psi(x_i)^T.\n\\] In matrix form, the normal equations are \\[\n\\Psi^Ty = \\Psi^T\\Psi\\beta\n\\] where \\(\\Psi\\) is the design matrix with rows \\(\\psi(x_i)^T\\) and \\(y\\) is the vector of output values. The solution to the normal equations is \\[\n\\hat{\\beta} = (\\Psi^T\\Psi)^{-1}\\Psi^Ty.\n\\]\nThe function solve indeed finds the same values as the lm function. Essentially this is what the lm function does under the hood. The solve function uses the elimination method to solve the system of linear equations. The method we all learned when we were introduced to linear equations. The technique is known in linear algebra as LU decomposition.\nIn out housing example we used a linear model to predict the price of a house based on its square footage. The model is simple and easy to interpret, making it suitable for both prediction and interpretation. The model provides insights into the relationship between house size and price, allowing us to understand how changes in house size affect the price. The model can also be used to make accurate predictions of house prices based on square footage. This demonstrates the versatility of linear models for both prediction and interpretation tasks.\nLet’s consider another example of using a linear model that allows us to understand the relations between the performance of stock portfolio managed by John Maynard Keynes and overall market performance.\nRegression analysis is the most widely used statistical tool for understanding relationships among variables. It provides a conceptually simple method for investigating functional relationships between one or more factors and an outcome of interest. This relationship is expressed in the form of an equation, which we call the model, connecting the response or dependent variable and one or more explanatory or predictor variable.\nIt is convenient to introduce a regression model using the language of probability and uncertainty. A regression model assumes that mean of output variable \\(y\\) depends linearly on predictors \\(x_1,\\ldots,x_p\\) \\[\ny = \\beta_0 +  \\beta_1 x + \\ldots + \\beta_p x_p + \\epsilon,~ \\text{where}~\\epsilon \\sim N(0, \\sigma^2).\n\\] Often, we use simpler dot-product notation \\[\ny = \\beta^Tx + \\epsilon,\n\\] where \\(\\beta = (\\beta_0,\\beta_1,\\ldots,\\beta_p)\\) is the vector regression coefficients and \\(x = (1,x_1,\\ldots,x_p)\\) is the vector of predictors, with 1 appended to the beginning.\nLine coefficients \\(\\beta_i\\)s have the same interpretation as in the deterministic approach. However, the additional term \\(\\epsilon\\) is a random variable that captures the uncertainty in the relationship between \\(y\\) and \\(x\\), it is called the error term or the residual. The error term is assumed to be normally distributed with mean zero and variance \\(\\sigma^2\\). Thus, the linear regression model has a new parameter \\(\\sigma^2\\) that models dispersion of \\(y_i\\) around the mean \\(\\beta^Tx\\), let’s see an example.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "12-regression.html#estimates-and-intervals",
    "href": "12-regression.html#estimates-and-intervals",
    "title": "3  Linear and Multiple Regression",
    "section": "3.2 Estimates and Intervals",
    "text": "3.2 Estimates and Intervals\nIn our housing example, we estimated the parameter \\(\\beta_1\\) to be equal to 113.12 and made a conclusion that the the price of the house goes up by that amount when the lining area goes up by one unit. However, the estimated value is based on a sample. The sample is a result of well-designed data collection procedure and is representative of the population, and we should expect the estimated value to be close to the true value. However, the estimated value is not the true value of the parameter, but an estimate of it. The true value of the parameter is unknown and is the estimated value is subject to sampling error.\nThis means that the estimated value of the parameter is not the true value of the parameter, but an estimate of it. The true value of the parameter is unknown and can only be estimated from the sample data. The estimated value of the parameter is subject to sampling error, which is modeled by the normal distribution. The standard error of the estimate is a measure of the uncertainty in the estimated value of the parameter. The standard error is calculated from the sample data and is used to calculate confidence intervals and p-values for the estimated parameter.\nused the lm function to estimate the parameters of the linear model. The estimated values of the parameters are given in the Estimate column of the output. The estimated value of the intercept is \\(\\hat \\beta_0 = 13.44\\) and the estimated value of the slope is \\(\\hat \\beta_1 = 113.12\\). These values are calculated using the least squares loss function, which minimizes the sum of squared differences between the predicted and actual values of the output variable. The estimated values of the parameters are subject to sampling error, which is modeled by the normal distribution. The standard error of the estimates is given in the Std. Error column of the output. The standard error is a measure of the uncertainty in the estimated values of the parameters. The t-statistic is the ratio of the estimated coefficient to its standard error. The p-value is the probability of observing a value at least as extreme as the one observed, assuming the null hypothesis is true. In this case, the p-value for the livingArea coefficient is less than 0.05, so we conclude that the coefficient is statistically significant. This means that the size of the house is a statistically significant predictor of the price. The Residual standard error is the standard deviation of the residuals \\(\\hat y_i - y_i,~i=1,\\ldots,n\\).\n\nExample 3.3 (House Prices) Let’s go back to the Saratoga Houses dataset\n\nd = read.csv(\"../data/SaratogaHouses.csv\")\nd$price = d$price/1000; d$livingArea = d$livingArea/1000\nl = lm(price ~ livingArea, data=d)\nsummary(l)\n\n\nCall:\nlm(formula = price ~ livingArea, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-277.0  -39.4   -7.7   28.4  553.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    13.44       4.99    2.69   0.0072 ** \nlivingArea    113.12       2.68   42.17   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 69 on 1726 degrees of freedom\nMultiple R-squared:  0.507, Adjusted R-squared:  0.507 \nF-statistic: 1.78e+03 on 1 and 1726 DF,  p-value: &lt;2e-16\n\n\nThe output of the lm function has several components. Besides calculating the estimated values of the coefficients, given in the Estimate column, the method also calculates standard error (Std. Error) and t-statistic (t value) for each coefficient. The t-statistic is the ratio of the estimated coefficient to its standard error. The p-value (Pr(&gt;|t|)) is the probability of observing a value at least as extreme as the one observed, assuming the null hypothesis is true. The null hypothesis is that the coefficient is equal to zero. If the p-value is less than 0.05, we typically reject the null hypothesis and conclude that the coefficient is statistically significant. In this case, the p-value for the livingArea coefficient is less than 0.05, so we conclude that the coefficient is statistically significant. This means that the size of the house is a statistically significant predictor of the price. The Residual standard error the standard deviation of the residuals \\(\\hat y_i - y_i,~i=1,\\ldots,n\\).\n\nThe estimated values of the parameters were calculated using least squares loss function discussed above. The residual standard error is also relatively easy to calculate from the model residuals \\[\ns_e = \\sqrt{ \\frac{1}{n-2} \\sum_{i=1}^n ( \\hat y_i - y_i )^2 }.\n\\] Now the question is, how the p-value for the estimates was calculated? And why did we assume that \\(\\epsilon\\) is normally distributed in the first place? The normality of \\(\\epsilon\\) and as a consequence, the conditional normality of \\(y \\mid x \\overset{iid}{\\sim}  N(\\beta^Tx, \\sigma^2)\\) is easy to explain, it is simply due to mathematical convenience. Plus, this assumption happen to describe the reality well in a wide range of applications. One of those conveniences, is our ability to estimate to calculate mean and variance of the distribution of \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\).\nTo understand how to calculate the p-values, we first notice that there is uncertainty about the values of the parameters \\(\\beta_i\\)s. To get a feeling for the amount of variability in our experimentation. Imagine that we have two sample data sets. For example, we have housing data from two different realtor firms. Do you think the estimated value of price per square foot will be the same for both of those? The answer is no. Let’s demonstrate with an example, we simulate 20 data sets from the same distribution and estimate 20 different linear models.\n\nset.seed(92) #Kuzy\nd = read.csv(\"../data/SaratogaHouses.csv\")\nd$price = d$price/1000; d$livingArea = d$livingArea/1000\nplot(d$livingArea, d$price, pch=20, col=\"blue\", xlab=\"Living Area\", ylab=\"Price\")\nfor (i in 1:10) {\n  # Sample with replacement\n  dnew = d[sample(1:nrow(d),replace=T),]\n  # Fit a linear model\n  l = lm(price ~ livingArea, data=dnew, subset = sample(1:nrow(d),100))\n  abline(l, col=\"green\", lwd=3)\n}\n\n\n\n\n\n\n\nFigure 3.1: Twenty different linear models estimated using randomly selected subsample of the data.\n\n\n\n\n\nFigure 3.1 shows the results of this simulation. We can see that the estimated coefficients \\(\\hat \\beta_i\\) are different for each of the 20 samples. This is due to the sampling error. The sampling error is the difference between the estimated value of a parameter and its true value. The value of \\(\\beta_1\\) will differ from sample to sample. In other words, it will be a random variable. The sampling distribution of \\(\\beta_1\\) describes how it varies over different samples with the \\(x\\) values fixed. Statistical view of linear regression allows us to calculate confidence and prediction intervals for estimated parameters. It turns out that when least squares principle is used, the estimated \\(\\hat\\beta_1\\) is normally distributed: \\(\\hat\\beta_1 \\sim N( \\beta_1 , s_1^2 )\\). Let’s see how we can derive this result.\nThe extension of the central limit theorem, sometimes called the Lindeberg CLT, states that a linear combination of independent random variables that satisfy some mild condition are approximately normally distributed. We can show that estimates of \\((\\beta_0\\ldots,\\beta_p)\\) are linear combinations of the observed values of \\(y\\) and are therefore normally distributed. Indeed, if we write the linear regression model in matrix form \\[\nY = X \\beta + \\epsilon,\n\\] where \\(Y\\) is the vector of observed values of the dependent variable, \\(X\\) is the matrix of observed values of the independent variables, \\(\\beta\\) is the vector of unknown parameters, and \\(\\epsilon\\) is the vector of errors. Then, if we take the derivative of the loss function for linear regression and set it to zero, we get the following expression for the estimated parameters \\[\n\\hat \\beta =  AY,\n\\] where \\(A = (X^TX)^{-1}X^T\\). Due to Lindeberg central limit theorem, \\(\\hat \\beta\\) is normally distributed. This is a useful property that allows us to calculate confidence intervals and p-values for the estimated parameters.\nNow, we need to compute the mean and variance of \\(\\hat \\beta\\). The mean is easy to compute, since the expectation of the sum is the sum of expectations, we have \\[\n\\hat{\\beta} = A(X\\beta + \\epsilon)\n\\] \\[\n\\hat{\\beta} = \\beta + A\\epsilon\n\\]\nThe expectation of \\(\\hat{\\beta}\\) is: \\[\nE[\\hat{\\beta}] = E[\\beta + A\\epsilon] = E[\\beta] + E[A\\epsilon] = \\beta\n\\] Since \\(\\beta\\) is constant, and \\(E[A\\epsilon] = 0\\) (\\(E[\\epsilon] = 0\\)).\nThe variance is given by \\[\nVar(\\hat{\\beta}) = Var(A\\epsilon)\n\\] If we assume that \\(\\epsilon\\) is independent of \\(X\\), we can write and elements of \\(\\epsilon\\) are uncorrelated, we can write: \\[\nVar(\\hat{\\beta}) = AVar(\\epsilon)A^T\n\\] Given \\(Var(\\epsilon) = \\sigma^2 I\\), we have: \\[\nVar(\\hat{\\beta}) = \\sigma^2 (X^TX)^{-1}\n\\]\nPutting together the expectation and variance, and the fact that we get the following distribution for \\(\\hat{\\beta}\\): \\[\n\\hat{\\beta} \\sim N(\\beta, \\sigma^2 (X^TX)^{-1}).\n\\]\nStatistical approach to linear regression is useful. We can think of the estimated coefficients \\(\\hat \\beta_i\\) as an average amount of change in \\(y\\), when \\(x_i\\) goes up by one unit. Since this average was calculated using a sample data, it is subject to sampling error and the sampling error is modeled by the normal distribution. Assuming that residuals \\(\\epsilon\\) are independently normally distributed with a variance that does not depend on \\(x\\) (homoscedasticity), we can calculate the mean and variance of the distribution of \\(\\hat \\beta_i\\). This is a useful property that allows us to calculate confidence intervals and p-values for the estimated parameters.\nIn summary, the statistical view of the linear regression model is useful for understanding the uncertainty associated with the estimated parameters. It also allows us to calculate confidence intervals and prediction intervals for the output variable.\n\nAverage value of output \\(y\\) is a linear function of input \\(x\\) and lie on the straight line of regression \\(\\hat y_i = \\beta^Tx_i\\).\nThe values of \\(y\\) are statistically independent.\nThe true value of \\(y = \\hat y_i + \\epsilon_i\\) is a random variable, and it is normally distributed around the mean with variance \\(\\sigma^2\\). This variance is the same for all values of \\(y\\).\nThe estimated values of the parameters \\(\\hat \\beta_i\\) are calculated from observed data and are subject to the sampling error and we are not certain about them. This uncertainty is modeled by the normally distributed around the true values \\(\\beta\\). Given that errors \\(\\epsilon_i\\) are homoscedastic and independent, we have \\(Var(\\hat{\\beta}) = \\sigma^2 (X^TX)^{-1}\\).\n\nAgain, consider a house example. Say in our data we have 10 houses with the same squire footage, say 2000. Now the third point states, that the prices of those houses should follow a normal distribution and if we are to compare prices of 2000 sqft houses and 2500 sqft houses, they will have the same standard deviation. The second point means that price of one house does not depend on the price of another house.\nAll of the assumptions in the regression model can be written using probabilist notations The looks like: \\[\ny \\mid x \\overset{iid}{\\sim}  N(\\beta^Tx, \\sigma^2).\n\\]\nIn the case when we have only one predictor the variance of the estimated slope \\(\\hat \\beta_1\\) is given by \\[\nVar(\\hat \\beta_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n ( x_i - \\bar{x} )^2 } = \\frac{ \\sigma^2 }{ (n-1) s_x^2 },\n\\] where \\(s_x^2\\) is the sample variance of \\(x\\). Thus, there are three factors that impact the size of standard error for \\(\\beta_1\\): sample size (\\(n\\)), error variance (\\(s^2\\)), and \\(x\\)-spread, \\(s_x\\).\nWe can empirically demonstrate the sampling error by simulating several samples from the same distribution and estimating several linear models. We can see that the estimated coefficients \\(\\hat \\beta_i\\) are normally distributed around the true values \\(\\beta_i\\). If we plot coefficients for 1000 different models, we can see that the empirical distribution resembles a normal distribution.\n\nset.seed(92) #Kuzy\n# Read housing data\nd = read.csv(\"../data/SaratogaHouses.csv\")\nd$price = d$price/1000; d$livingArea = d$livingArea/1000\n# Simulate 1000 samples\nn = 1000\n# Create a matrix to store the results\nbeta = matrix(0, nrow=n, ncol=2)\n# Simulate 1000 samples\nfor (i in 1:n) {\n  # Sample with replacement\n  dnew = d[sample(1:nrow(d),replace=T),]\n  # Fit a linear model\n  l = lm(price ~ livingArea, data=dnew)\n  # Store the coefficients\n  beta[i,] = coef(l)\n}\nind = 2\n# Plot the results\nplot(beta[,1], beta[,2], pch=20, col=\"blue\", xlab=\"Intercept\", ylab=\"Slope\")\nabline(h=coef(l)[2], lwd=3, col=\"red\")\nabline(v=coef(l)[1], lwd=3, col=\"red\")\n\n\n\n\n\n\n\nX = cbind(1, d$livingArea)\n# Calculate the variance of the coefficients\nvar = sigma(l)^2 * solve(t(X) %*% X)\nvarb = var[ind,ind]/0.63\nhist(beta[,ind], col=\"blue\", xlab=\"Intercept\", main=\"\",freq=F)\n\n\n\n\n\n\n\nbt = seq(80,140,0.1)\n\nAccounting for uncertainty in \\(\\hat \\beta\\)s we can calculate confidence intervals for the predicted average \\(\\hat y\\). When we additionally account for the uncertainty in the predicted value \\(\\hat y\\), we can calculate prediction intervals.\nAnother advantage of adopting a statistical view of the linear regression model is ability to quantify information about potential outliers. Outliers are points that are extreme relative to our model predictions. Recall, that residual is \\(e_i  = y_i- \\hat y_i\\). Since our predicted value \\(\\hat y_i\\) follows a normal distribution, the residual also follows a normal distribution, since it is a difference of normal random variable \\(\\hat y_i\\) and a constant \\(y_i\\). It easy to see that \\[e_i \\sim N(0, s_e^2),\\] where \\(s_e^2\\) is an empirical estimate of the error’s variance.\nConsider the relation between the fitted values \\(\\hat y_i\\) and residuals \\(e_i\\). Our predictions are given by the line. The residual \\(e_i\\) and predicted value \\(\\hat y_i\\) for the \\(i\\)th observation are related via \\[\ny_i = \\hat{y}_i + ( y_i - \\hat{y}_i ) = \\hat{y}_i + e_i.\n\\]\nResiduals allow us to define outliers. They simply have large residuals. We re-scale the residuals by their standard errors. This lets us define \\[r_i = \\frac{ e_i }{ s_{ e} } =\\frac{y_i - \\hat{y}_i   }{ s_{ e } }\\] Since residuals follow normal distribution \\(e \\sim N(0,\\sigma^2)\\), in 95% of the time we expect the standardized residuals to satisfy \\(- 2 &lt; r_i &lt; 2\\). Any observation with is an extreme outlier, it is three sigmas away from the mean.\nAnother types of observations we are interested are the influential points. Those are are observations that affect the magnitude of our estimates \\(\\hat{\\beta}\\)’s. They are important to find as they typically have economic consequences. We will use to assess the significance of an influential point. Cook’s distance associated with sample \\(i\\) measure the change in estimated model parameters \\(\\hat \\beta\\) when sample \\(i\\) is removed from the training data set.\nIntuitively, we model regression-back-to-the-mean effect. This is one of the most interesting statistical effects you’ll see in daily life. In statistics, regression does not mean “going backwards”, but rather the tendency for a variable that is extremely high or low to move closer to the average upon subsequent measurement. For example, Francis Galton, who was a cousin of Charles Darwin, in his study on regression to the mean height showed that if your parents are taller than the average, you’ll regress back to the average. While people might expect the children of tall parents to be even taller and the children of short parents to be even shorter, Galton found that this wasn’t the case. Instead, he observed that the heights of the children tended to be closer to the average height for the population. Galton termed this phenomenon “regression towards mediocrity” (now more commonly known as “regression to the mean”). It meant that extreme characteristics (in this case, height) in parents were likely to be less extreme (closer to the average) in their children. It is a classic example that helped introduce and explain this statistical concept. Galton’s finding was one of the first insights into what is now a well-known statistical phenomenon. It doesn’t imply that all individual cases will follow this pattern; rather, it’s a trend observed across a population. It’s important to understand that regression to the mean doesn’t suggest that extreme traits diminish over generations but rather that an extreme measurement is partly due to random variation and is likely to be less extreme upon subsequent measurement.\nAnother example was documented by Daniel Kahneman and Amos Tversky in their book Thinking, Fast and Slow. They found that when a person performs a task, their performance is partly due to skill and partly due to luck. They observed that when a person performs a task and achieves an extreme result, their subsequent performance is likely to be less extreme. Particularly they studied effect of criticism and praise used by Israeli Air Force fighter pilots trainers. After criticism, the low-scoring pilots were retested. Often, their scores improve. At first glance, this seems like a clear effect of feedback from the trainer. However, some of this improvement is likely a statistical artifact and demonstrates the regression to the mean effect.\nWhy? Those pilots who initially scored poorly were, statistically speaking, somewhat unlucky. Their low scores may have been due to a bad day, stress, or other factors. When retested, their scores are likely to be closer to their true skill level, which is closer to the average. This natural movement towards the average can give the illusion that the intervention (praise or criticism) was more effective than it actually was. Conversely, if the top performers were praised and retested, we might find their scores decrease slightly, not necessarily due to the inefficacy of the praise but due to their initial high scores being partly due to good luck or an exceptionally good day. In conclusion, in pilot training and other fields, it’s important to consider regression to the mean when evaluating the effectiveness of interventions. Without this consideration, one might draw incorrect conclusions about the impact of training or other changes.\n\nExample 3.4 (Google vs S&P 500) We will demonstrate how we can use statistical properties of a linear regression model to understand the relationship between returns of a google stock and the S&P 500 index. We will use Capital Asset Pricing Model (CAPM) regression model to estimate the expected return of an investment into Google stock and to price the risk. The CAPM model is\n\\[\n\\mathrm{GOOG} = \\alpha + \\beta \\mathrm{SP500} + \\epsilon\n\\] On the left hand side, we have the return that investors expect to earn from investing into Google stock. In the CAPM model, this return is typically modeled as a dependent variable.\nThe input variable SP500 represents the average return of the entire US market. Beta measures the volatility or systematic risk of a security or a portfolio in comparison to the market as a whole. A beta greater than 1 indicates that the security is more volatile than the market, while a beta less than 1 indicates it is less volatile. Alpha is the intercept of the regression line, it measures the excess return of the security over the market. The error term \\(\\epsilon\\) captures the uncertainty in the relationship between the returns of Google stock and the market.\nIn a CAPM regression analysis, the goal is to find out how well the model explains the returns of a security based on its beta. This involves regressing the security’s excess returns (returns over the risk-free rate) against the excess returns of the market. The slope of the regression line represents the beta, and the intercept should ideally be close to the risk-free rate, although in practice it often deviates. This model helps in understanding the relationship between the expected return and the systematic risk of an investment.\nBased on the uncertainty associated with the estimates for alpha and beta, we can formulate some of hypothesis tests, for example\n\n\\(H_0 :\\) is Google related to the market?\n\\(H_0 :\\) does Google out-perform the market in a consistent fashion?\n\n\ngetSymbols(Symbols = c(\"GOOG\",\"SPY\"),from='2017-01-03',to='2023-12-29')\n\n \"GOOG\" \"SPY\" \n\ngret = as.numeric(dailyReturn(GOOG))\nspyret = as.numeric(dailyReturn(SPY))\nl = lm(gret ~ spyret)\ntidy(l) %&gt;% knitr::kable(digits=4)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0003\n0.0003\n1.1\n0.27\n\n\nspyret\n1.1706\n0.0240\n48.8\n0.00\n\n\n\nGoogle vs S&P 500 returns between 2017-2023\n\n\n\nplot(gret, spyret, pch=20, col=\"blue\", xlab=\"Google Return\", ylab=\"SPY Return\")\nabline(l, lwd=3, col=\"red\")\n\n\n\n\n\n\n\n\nHere’s what we get after we fit the model using function\nOur best estimates are \\[\n\\hat \\alpha = 0.0004 \\; , \\; \\hat{\\beta} = 1.01\n\\]\nNow we can provide the results for hypothesis we set at he beginning. Given that the p-value for \\(H_0: \\beta = 0\\) is &lt;2e-16 we can reject the null hypothesis and conclude that Google is related to the market. The p-value for \\(H_0: \\alpha = 0\\) is 0.06, which is grater than 0.05, so we cannot reject the null hypothesis and conclude that Google does not out-performs the market in a consistent fashion in the 2017-2023 period.\nFurther, we can answer some of the other important questions, such as how much will Google move if the market goes up \\(10\\)%?\n\nalpha = coef(l)[1]\nbeta = coef(l)[2]\n# Calculate the expected return\nalpha + beta*0.1\n\n(Intercept) \n       0.12 \n\n\nHowever, if we look at the earlier period between 2005-2016 (the earlier days of Google) the results will be different.\n\ngetSymbols(Symbols = c(\"GOOG\",\"SPY\"),from='2005-01-03',to='2016-12-29');\n\n \"GOOG\" \"SPY\" \n\ngret = as.numeric(dailyReturn(GOOG))\nspyret = as.numeric(dailyReturn(SPY))\nl = lm(gret ~ spyret)\ntidy(l) %&gt;% knitr::kable(digits=4)\n\n\n\nTable 3.1: Google vs S&P 500 returns between 2005-2016\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0006\n0.0003\n2.2\n0.03\n\n\nspyret\n0.9231\n0.0230\n40.1\n0.00\n\n\n\n\n\n\n\n\n\nplot(gret, spyret, pch=20, col=\"blue\", xlab=\"Google Return\", ylab=\"SPY Return\")\nabline(l, lwd=3, col=\"red\")\n\n\n\n\n\n\n\nFigure 3.2: Google vs S&P 500 returns between 2005-2016\n\n\n\n\n\nIn this period Google did consistently outperform the market. The p-value for \\(H_0: \\alpha = 0\\) is 0.03.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "12-regression.html#capm-model-for-yahoo-stock",
    "href": "12-regression.html#capm-model-for-yahoo-stock",
    "title": "3  Linear and Multiple Regression",
    "section": "3.3 CAPM Model for Yahoo! Stock",
    "text": "3.3 CAPM Model for Yahoo! Stock\nRather than estimate \\(\\mu\\) directly, the CAPM estimates the difference between \\(\\mu\\) and the risk-free rate \\(r_f\\). This quantity \\(\\mu-r_f\\) is known as the expected excess return (excess relative to a risk-free investment). The CAPM relates the expected excess return of a stock to that of an underlying benchmark, typically a broad-based market index. Let \\(\\mu_M\\) and \\(\\sigma_M\\) denote the return and volatility on the market index. The implication of CAPM is that there is a linear relationship between the expected excess return of a stock, \\(\\mu-r_f\\), and the excess return of the market, \\(\\mu_M-r_f\\).\n\\[\n\\text{Excess \\; Return}_{\\text{Stock}} =  \\beta \\;  \\text{Excess \\;\nReturn}_{\\text{Market}}\n\\] \\[\n\\mu-r_f = \\beta(\\mu_M - r_f )\n\\] Put simply, the expected excess return of a stock is \\(\\beta\\) times the excess expected return of the market. Beta (\\(\\beta\\)) is a measure of a stock’s risk in relation to the market. A beta of 1.3 implies that the excess return on the stock is expected to move up or down 30% more than the market. A beta bigger than one implies the stock is riskier than the market and goes up (and down) faster than the market goes up (and down). A beta less than one implies the stock is less risky than the market.\nUsing the CAPM, the expected return of the stock can now be defined as the risk free interest rate plus beta times the expected excess return of the market, \\[\n\\mu = \\text{Expected \\; Return}_{\\text{Stock}} = r_f+\\beta (\\mu_M-r_f)\n\\] Beta is typically estimated from a regression of the individual stock’s returns on those of the market. The other parameters are typically measured as the historical average return on the market \\(\\mu_M\\) and the yield on Treasury Bills \\(r_f\\). Together these form an estimate of \\(\\mu\\). The volatility parameter \\(\\sigma\\) is estimated by the standard deviation of historical returns.\nOur qualitative discussion implicitly took the year as the unit of time. For our example, we make one minor change and consider daily returns so that \\(\\mu\\) and \\(\\sigma\\) are interpreted as a daily rate of return and daily volatility (or standard deviation). We use an annual risk-free rate of 5%; this makes a daily risk-free rate of .019%, \\(r_f = 0.00019\\), assuming there are 252 trading days in a year. A simple historical average is used to estimate the market return (\\(\\mu_M\\)) for the Nasdaq 100. The average annual return is about 23%, with corresponding daily mean \\(\\mu_M = 0.00083\\). A regression using daily returns from 1996-2000 leads to an estimate of \\(\\beta = 1.38\\). Combining these (pieces) leads to an estimated expected return of Yahoo!, \\(\\mu_{Yahoo} = 0.00019+1.38(0.00083-0.00019) = 0.00107\\) on a daily basis. Note that the CAPM model estimates a future return that is much lower than the observed rate over the last three-plus years of .42% per day or 289% per year.\nTo measure the riskiness of Yahoo! notice that the daily historical volatility is 5%, i.e. \\(\\sigma = 0.05\\). On an annual basis this implies a volatility of \\(\\sigma \\sqrt{T} = 0.05 \\sqrt{252} = 0.79\\), that is 79%. For comparison, the benchmark Nasdaq 100 has historical daily volatility 1.9% and an annual historical volatility of 30%. The estimates of all the parameters are recorded in Table 3.2.\n\n\n\nTable 3.2: Key Parameter Estimates Based on Daily Returns 1996–2000\n\n\n\n\n\n\n\n\n\n\n\nAsset\nExpected return\nVolatility\nRegression coefft (s.e.)\n\n\n\n\nYahoo!\n\\(\\mu = 0.00107\\)\n\\(\\sigma = 0.050\\)\n\\(\\beta = 1.38 (.07)\\)\n\n\nNasdaq 100\n\\(\\mu_M = 0.00083\\)\n\\(\\sigma_M = 0.019\\)\n1\n\n\nTreasury Bills\n\\(r_f = 0.00019\\)\n–\n–",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "12-regression.html#factor-regression-and-feature-engineering",
    "href": "12-regression.html#factor-regression-and-feature-engineering",
    "title": "3  Linear and Multiple Regression",
    "section": "3.4 Factor Regression and Feature Engineering",
    "text": "3.4 Factor Regression and Feature Engineering\nA linear model assumes that output variable is proportional to the input variable plus an offset. However, this is not always the case. Often, we need to transform input variables by combining multiple inputs into a single predictor, for example by taking a ratio or putting inputs on a different scale, e.g. log-scale. In machine learning, this process is called feature engineering.\nOne of the classic examples of feature engineering is Fama-French three-factor model which is used in asset pricing and portfolio management. The model states that asset returns depend on (1) market risk, (2) the outperforming of small versus big companies, and (3) the outperformance of high book/market versus small book/market companies, mathematically \\[r = R_f + \\beta(R_m - R_f) + b_s\\cdot SMB + b_v\\cdot HML + \\alpha\\] Here \\(R_f\\) is risk-free return, \\(R_m\\) is the return of market, \\(SMB\\) stands for \"Small market capitalization Minus Big\" and \\(HML\\) for \"High book-to-market ratio Minus Low\"; they measure the historic excess returns of small caps over big caps and of value stocks over growth stocks. These factors are calculated with combinations of portfolios composed by ranked stocks (BtM ranking, Cap ranking) and available historical market data.\n\n3.4.1 Logarithmic and Power Transformations\nConsider, the growth of the Apple stock between 2000 and 2024. With the exception of the 2008 financial crisis period and 2020 COVID 19 related declines, the stock price has been growing exponentially. Figure 3.3 shows the price of the Apple stock between 2000 and 2024. The price is closely related to the company’s growth.\n\ngetSymbols(Symbols = \"AAPL\",from='2000-01-01',to='2023-12-31');\n\n \"AAPL\"\n\nplot(AAPL$AAPL.Adjusted, type='l', col=\"blue\", xlab=\"Date\", ylab=\"Price\")\n\n\n\n\n\n\n\nFigure 3.3: Apple stock price growth in the 2000-2024 period\n\n\n\n\n\nThe 2008 and 2020 declines are more related to extraneous factors, rather than the growth of the company. Thus, we can conclude that the overall growth of the company is exponential. Indeed, if we try to fit a linear model to the time-price data, we will see that the model does not fit the data well\n\ntime = index(AAPL) - start(AAPL)\nplot(time,AAPL$AAPL.Adjusted, type='l', col=\"blue\", xlab=\"Date\", ylab=\"Price\")\nabline(lm(AAPL$AAPL.Adjusted ~ time), col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\nJust by visual inspection we can see that a straight line will not be a good fit for this data. The growth of a successful company typically follows the rule of compounding. Compounding is a fundamental principle that describes how some quantity grows over time when this quantity increases by a fixed percentage periodically. This is a very common phenomenon in nature and business. For example, if two parents have 2.2 children on average, then the population increases by 10% every generation. Another example is growth of investment in a savings account.\nA more intuitive example is probably an investment in a savings account. If you invest \\(1000\\) in a savings account with \\(10\\%\\) annual interest rate and you get payed once a year, then your account value will be \\(1100\\) by the end of the year. However, if you get get payed \\(n\\) times a year, and initially invest \\(y_0\\) final value \\(y\\) of the account after \\(t\\) years will be \\[\ny = y_0 \\times (1 + r/n)^{nt}\n\\] where \\(r\\) is the annual interest rate. When you get payed every month (\\(n=12\\)), a traditional payout schedule used by banks, then \\[\ny = 1000 \\times (1 + 0.1/12)^{12} = 1105.\n\\] A value slightly higher than the annual payout of 1100.\nThe effect of compounding is minimal in the short term. However, the effect of compounding is more pronounced when the growth rate is higher and time periods are longer. For example at \\(r=2\\), \\(n=365\\) and 4-year period \\(t=4\\), you get \\[\ny = 1000 \\times (1 + 2/365)^{3\\times 365} = 2,916,565.\n\\] Your account is close to 3 million dollars! Compared to \\(n=1\\) scenario \\[\ny = 1000 \\times (1 + 2)^{4} = 81,000,\n\\] when you will end up with mealy 81 thousand. This is why compounding is often referred to as the “eighth wonder of the world” in investing contexts, emphasizing its power in growing wealth over time.\nIn general, as \\(n\\) goes up, the growth rate of the quantity approaches the constant\n\nn = 1:300\nr = 1\nplot(n, (1+r/n)^n, type='l', col=\"blue\", xlab=\"n\", ylab=\"Future Value\")\nabline(h=exp(r), col=\"red\", lwd=3)\n\n\n\n\n\n\n\nFigure 3.4: Growth of an investment in a savings account when n increases and return rate is 100% per year\n\n\n\n\n\nFigure 3.4 shows the growth of an investment in a savings account when \\(n\\) increases and return rate is \\(100\\%\\) per year. We can see that the growth rate approaches the constant \\(e \\approx 2.72\\) as \\(n\\) increases. \\[\n(1+r/n)^n \\rightarrow e^r,~\\text{as}~n \\rightarrow \\infty.\n\\] This limit was first delivered by Leonhard Euler and the number \\(e\\) is known as Euler’s number.\nComing back to the growth of the Apple company, we can think of it growing at small constant rate every day. The relation between the time and size of Apple is multiplicative. Meaning when when time increases by one day, the size of the company increases by a small constant percentage. This is a multiplicative relation. In contrast, linear relation is additive, meaning that when time increases by one day, the size of the company increases by a constant amount. The exponential growth model is given by the formula \\[\ny = y_0 \\times e^{\\beta^Tx}.\n\\] There are many business and natural science examples where multiplicative relation holds. IF we apply the \\(\\log\\) function to both sides of the equation, we get \\[\n\\log y = \\log y_0 + \\beta^Tx.\n\\] This is a linear relation between \\(\\log y\\) and \\(x\\). Thus, we can use linear regression to estimate the parameters of the exponential growth model by putting the output variable \\(y\\) on the log-scale.\nAnother example of nonlinear relation that can be analyzed using linear regression is when variables are related via a power law. This concept helps us modeling proportional relationships or ratios. In a multiplicative relationship, when one variable changes on a percent scale, the other changes in a directly proportional manner, as long as the multiplying factor remains constant. For example, the relation between the size of a city and the number of cars registered in the city is given by a power law. When the size of the city doubles, the number of cars registered in the city is also expected to double. The power law model is given by the formula \\[\ny = \\beta_0 x^{\\beta_1}.\n\\] If we apply the \\(\\log\\) function to both sides of the equation, we get \\[\n\\log y = \\log \\beta_0 + \\beta_1 \\log x.\n\\] This is a linear relation between \\(\\log y\\) and \\(\\log x\\). Thus, we can use linear regression to estimate the parameters of the power law model by putting the output variable \\(y\\) and input variable \\(x\\) on the log-scale.\nHowever, there are several caveats when putting variables on the log-scale. We need to make sure that the variable is positive. It means that we cannot apply log transformations to dummy or count variables.\n\nExample 3.5 (World’s Smartest Mammal) We will demonstrate the power relation using the data on the brain (measured in grams) and body (measured in kilograms) weights for 62 mammal species. The data was collected by Harry J. Jerison in 1973. The data set contains the following variables:\n\nmammals = read.csv(\"../data/mammals.csv\")\nknitr::kable(head(mammals))\n\n\n\n\nMammal\nBrain\nBody\n\n\n\n\nAfrican_elephant\n5712.0\n6654.00\n\n\nAfrican_giant_pouched_rat\n6.6\n1.00\n\n\nArctic_Fox\n44.5\n3.38\n\n\nArctic_ground_squirrel\n5.7\n0.92\n\n\nAsian_elephant\n4603.0\n2547.00\n\n\nBaboon\n179.5\n10.55\n\n\n\n\n\nLet’s build a linear model.\n\nattach(mammals)\nmodel = lm(Brain~Body)\nplot(Body,Brain, pch=21, bg=\"lightblue\")\nabline(model, col=\"red\", lwd=3)\n\n\n\n\nBrain vs Body weight for 62 mammal species\n\n\n\n\nWe see a few outliers with suggests that normality assumption is violated. We can check the residuals by plotting residuals against fitted values and plotting fitted vs true values.\nplot(model$fitted.values, model$residuals, pch=21, bg=\"lightblue\", xlab=\"Fitted y\", ylab=\"Residuals\")\nabline(h=0, col=\"red\", lwd=3)\nplot(Brain, model$fitted.values, pch=21, bg=\"lightblue\", xlab=\"True y\", ylab=\"Fitted y\")\nabline(a=0,b=1, col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\n\nFigure 3.5: Fitted y vs Residuals\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.6: Fitted y vs True y\n\n\n\n\n\n\nRemember, that residuals should roughly follow a normal distribution with mean zero and constant variance. We can see that the residuals are not normally distributed and the variance increases with the fitted values. This is a clear indication that we need to transform the data. We can try a log-log transformation.\n\nmodel = lm(log(Brain)~log(Body))\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.18\n0.11\n20\n0\n\n\nlog(Body)\n0.74\n0.03\n23\n0\n\n\n\n\n\nplot(model$fitted.values, model$residuals, pch=21, bg=\"lightblue\")\nabline(h=0, col=\"red\", lwd=3)\nplot(log(Brain), model$fitted.values, pch=21, bg=\"lightblue\")\nabline(a=0,b=1, col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\n\nFigure 3.7: Fitted y vs Residuals\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.8: Fitted y vs True y\n\n\n\n\n\n\nThat is much better! The residuals variance is constant and the plot of fitted vs true values shows a linear relationship. The log-log model is given by the formula \\[\n\\log \\mathrm{Brain} = 2.18 + 0.74 \\log \\mathrm{Body}.\n\\] seem to achieve two important goals, namely linearity and constant variance. The coefficients are highly significant.\nAlthough the log-log model fits the data rather well, there are a couple of outliers there. Let us print the observations with the largest residuals.\n\n# res = rstudent(model)\nres = model$residuals/sd(model$residuals)\noutliers = order(res,decreasing = T)[1:10]\ncbind(mammals[outliers,],\n      Std.Res = res[outliers], Residual=model$residuals[outliers],\n      Fit = exp(model$fitted.values[outliers])) %&gt;% knitr::kable(digits=2)\n\n\n\n\n\nMammal\nBrain\nBody\nStd.Res\nResidual\nFit\n\n\n\n\n11\nChinchilla\n64\n0.42\n3.41\n2.61\n4.7\n\n\n34\nMan\n1320\n62.00\n2.53\n1.93\n190.7\n\n\n50\nRhesus_monkey\n179\n6.80\n2.06\n1.58\n36.9\n\n\n6\nBaboon\n180\n10.55\n1.64\n1.26\n51.1\n\n\n42\nOwl_monkey\n16\n0.48\n1.44\n1.10\n5.1\n\n\n10\nChimpanzee\n440\n52.16\n1.26\n0.96\n167.7\n\n\n27\nGround_squirrel\n4\n0.10\n1.18\n0.91\n1.6\n\n\n43\nPatas_monkey\n115\n10.00\n1.11\n0.85\n49.1\n\n\n60\nVervet\n58\n4.19\n1.06\n0.81\n25.7\n\n\n3\nArctic_Fox\n44\n3.38\n0.92\n0.71\n22.0\n\n\n\n\n\nThere are two outliers, the Chinchilla and the Human, both have disproportionately large brains!\nIn fact, the Chinchilla has the largest standartised residual of 3.41. Meaning that predicted value of 4.7 g is 3.41 standard diviaitons away from the recorded value of 64 g. This suggests that the Chinchilla is a master race of supreme intelligence! However, afer checking more carefully we realized that there was a recording error and the acual weight of an average Chinchilla’s brain is 6.4. We mistyped the decimal separator! Thus the actual residual is 0.4.\n\nabs(model$fitted.values[11] - log(6.4))/sd(model$residuals)\n\n 11 \n0.4 \n\n\nIn reality Chinchilla’s brain is not far from an average mamal of this size!\n\n\nExample 3.6 (Newfood) A six month market test has been performed on the Newfood product, which is a breakfast cereal. The goal is to build a multiple regression model that provides accurate sales forecasts. This dataset represents the outcome of a controlled experiment in which the values of the independent variables that affect sales were carefully chosen by the analyst.\nThe analysis aims to identify the factors that contribute to sales of a new breakfast cereal and to quantify the effects of business decisions such as the choice of advertising level, location in store, and pricing strategies.\n\n\n\nvariable\ndescription\n\n\n\n\nsales\nnew cereal sales\n\n\nprice\nprice\n\n\nadv\nlow or high advertising (\\(0\\) or \\(1\\))\n\n\nlocat\nbread or breakfast section (\\(0\\) or \\(1\\))\n\n\ninc\nneighborhood income\n\n\nsvol\nsize of store\n\n\n\nFirst, we need to understand which variables need to be transformed. We start by running the “kitchen-sink” regression with all variables. Then we perform diagnostic checks to assess model assumptions and identify potential issues. Based on these diagnostics, we decide which variables should be transformed. After running the new model with transformations, we perform additional diagnostics and variable selection to refine the model. Using the final model after transformations and eliminating variables, we examine what the largest Cook’s distance is to identify influential observations. Finally, we provide a summary of coefficients and their statistical significance.\nFirst, let’s examine the correlation matrix to understand the relationships between all variables in the dataset. This will help us identify potential multicollinearity issues and understand the strength and direction of associations between variables before building our regression model.\n\nnewfood = read.csv(\"../data/newfood.csv\")\nattach(newfood)\nnames(newfood)\n\n \"sales\"  \"price\"  \"adv\"    \"locat\"  \"income\" \"svol\"   \"city\"   \"indx\"  \n\n# knitr::kable()\nhead(newfood)\n\n\n\n\n\nsales\nprice\nadv\nlocat\nincome\nsvol\ncity\nindx\n\n\n\n\n225\n24\n0\n0\n7.3\n34\n3\n1\n\n\n190\n24\n0\n0\n7.3\n34\n3\n2\n\n\n205\n24\n0\n0\n7.3\n34\n3\n3\n\n\n323\n24\n0\n0\n8.3\n41\n4\n1\n\n\n210\n24\n0\n0\n8.3\n41\n4\n2\n\n\n241\n24\n0\n0\n8.3\n41\n4\n3\n\n\n\n\n\n# correlation matrix\ncm = cor(cbind(sales,price,adv,locat,income,svol))\ncm[upper.tri(cm, diag = TRUE)] = NA\n# knitr::kable(as.table(round(cm, 3)))\nas.table(round(cm, 3)) %&gt;% knitr::kable()\n\n\n\n\n\nsales\nprice\nadv\nlocat\nincome\nsvol\n\n\n\n\nsales\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nprice\n-0.66\nNA\nNA\nNA\nNA\nNA\n\n\nadv\n0.00\n0.00\nNA\nNA\nNA\nNA\n\n\nlocat\n0.00\n0.00\n0.00\nNA\nNA\nNA\n\n\nincome\n0.16\n-0.13\n-0.75\n0.00\nNA\nNA\n\n\nsvol\n0.38\n-0.18\n-0.74\n-0.04\n0.81\nNA\n\n\n\n\n\nRemember, correlations between variables are not the same as regression coefficients (\\(\\beta\\)’s)! Looking at the correlation matrix, we can see that total sales volume (svol) is negatively correlated with advertising (adv), and income (income) is also negatively correlated with advertising (adv). The wuesiton is how might these negative correlations impact our ability to estimate the true advertising effects in our regression model?\n\nas.table(round(cm[2:4, 1:3], 3)) %&gt;% knitr::kable()\n\n\n\n\n\nsales\nprice\nadv\n\n\n\n\nprice\n-0.66\nNA\nNA\n\n\nadv\n0.00\n0\nNA\n\n\nlocat\n0.00\n0\n0\n\n\n\n\n\nThere’s no correlation in the \\(X\\)’s by design! Let’s start by only including price, adv, locat\n\nmodel = lm(sales~price+adv+locat)\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n562.31\n53.1\n10.58\n0.00\n\n\nprice\n-12.81\n1.8\n-7.20\n0.00\n\n\nadv\n0.22\n14.5\n0.02\n0.99\n\n\nlocat\n-0.22\n14.5\n-0.02\n0.99\n\n\n\n\n\nWhy is the marketer likely to be upset by this regression?! Why is the economist happy? Let’s add income and svol to the regression and use log-log model.\n\nmodel = lm(log(sales)~log(price)+adv+locat+log(income)+log(svol))\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.41\n1.39\n6.06\n0.00\n\n\nlog(price)\n-1.74\n0.22\n-7.90\n0.00\n\n\nadv\n0.15\n0.10\n1.49\n0.14\n\n\nlocat\n0.00\n0.06\n0.02\n0.99\n\n\nlog(income)\n-0.52\n0.50\n-1.06\n0.29\n\n\nlog(svol)\n1.03\n0.26\n4.04\n0.00\n\n\n\n\n\nWhy no logs for adv and locat variables? The log(svol) coefficient is close to one!\nThe reason we don’t apply logarithms to adv and locat variables is because they are binary categorical variables (taking values 0 or 1). Taking the logarithm of 0 is undefined, and taking the logarithm of 1 equals 0, which would not provide any meaningful transformation. For binary variables, the exponential transformation in the final model interpretation directly gives us the multiplicative effect on sales when the variable changes from 0 to 1.\nRegarding the log(svol) coefficient being close to one (1.03), this suggests that sales scale approximately proportionally with store volume. A coefficient of 1.0 would indicate perfect proportional scaling, meaning a 1% increase in store volume would lead to a 1% increase in sales. Our coefficient of 1.03 indicates slightly more than proportional scaling - a 1% increase in store volume leads to a 1.03% increase in sales, suggesting some economies of scale or network effects in larger stores.\nOn the transformed scale (log-log model), \\[\n\\log sales=8.41 - 1.74 \\log price + 0.150 {\\text{adv}} + 0.001 {\\text{locat}} - 0.524 \\log inc  + 1.03 \\log svol\n\\] On the un-transformed scale, \\[\n\\text{sales} = e^{8.41} ( \\text{price} )^{-1.74} e^{ 0. 15 \\text{adv} } e^{ 0.001 \\text{locat}} ( \\text{inc} )^{-0.524}  ( \\text{svol} )^{1.03}\n\\] In the log-log regression model, the relationship between sales and the continuous variables (price, income, and store volume) follows a power function relationship. This means that a 1% change in these variables leads to a proportional change in sales according to their respective coefficients. Specifically, a 1% increase in price leads to a 1.74% decrease in sales, a 1% increase in income leads to a 0.524% decrease in sales, and a 1% increase in store volume leads to a 1.03% increase in sales.\nIn contrast, the binary variables (advertising and location) follow an exponential relationship with sales. When advertising is present (adv=1), sales increase by a factor of e^0.15 = 1.16, representing a 16% improvement. Similarly, when a store is in a good location (locat=1), sales increase by a factor of e^0.001 = 1.001, representing a 0.1% improvement. This exponential relationship arises because these variables are binary (0 or 1) and cannot be log-transformed, so their effects are multiplicative on the original sales scale.\nThe log-log regression model reveals several important relationships between the independent variables and sales performance.\n\nPrice elasticity is \\(\\hat{\\beta}_{\\text{price}} = - 1.74\\). A \\(1\\)% increase in price will drop sales \\(1.74\\)%\n\\(\\mathrm{adv}=1\\) increases sales by a factor of \\(e^{0.15} = 1.16\\). That’s a \\(16\\)% improvement\n\nWe should delete the locat variable from our regression model because it is statistically insignificant. The coefficient for locat has a very small magnitude (0.001) and a high p-value, indicating that there is insufficient evidence to reject the null hypothesis that this variable has no effect on sales. Including statistically insignificant variables in a model can lead to overfitting and reduce the model’s predictive accuracy on new data. By removing locat, we create a more parsimonious model that focuses only on the variables that have meaningful relationships with the outcome variable.\nNow, we are ready to use our model for prediction. predict.lm provides a \\(\\hat{Y}\\)-prediction given a new \\(X_f\\)\n\nmodelnew = lm(log(sales)~log(price)+adv+log(income)+log(svol))\nmodelnew  %&gt;% tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.41\n1.37\n6.1\n0.00\n\n\nlog(price)\n-1.74\n0.22\n-8.0\n0.00\n\n\nadv\n0.15\n0.10\n1.5\n0.14\n\n\nlog(income)\n-0.52\n0.49\n-1.1\n0.29\n\n\nlog(svol)\n1.03\n0.25\n4.1\n0.00\n\n\n\n\nnewdata=data.frame(price=30,adv=1,income=8,svol=34)\npredict.lm(modelnew,newdata,se.fit=T,interval=\"confidence\",level=0.99,) %&gt;% knitr::kable(digits=2)\n\n\n\n\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n5.2\n4.9\n5.5\n\n\n\n\n\n\n\n\nx\n\n\n\n\n0.1\n\n\n\n\n\n\n\n\nx\n\n\n\n\n67\n\n\n\n\n\n\n\n\nx\n\n\n\n\n0.25\n\n\n\n\n\n\n\n\n\nExponentiate-back to find \\(\\text{sales} = e^{5.1739} = 176.60\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "12-regression.html#interactions",
    "href": "12-regression.html#interactions",
    "title": "3  Linear and Multiple Regression",
    "section": "3.5 Interactions",
    "text": "3.5 Interactions\nIn many situations, \\(X_1\\) and \\(X_2\\) interact when predicting \\(Y\\). An interaction occurs when the effect of one independent variable on the dependent variable changes at different levels of another independent variable. For example, consider a study analyzing the effect of study hours \\(X_1\\) and a tutoring program \\(X_2\\), a binary variable where 0 = no tutoring, 1 = tutoring) on test scores \\(Y\\). Without an interaction term, we assume the effect of study hours on test scores is the same regardless of tutoring. With an interaction term, we can explore whether the effect of study hours on test scores is different for those who receive tutoring compared to those who do not. Here are a few more examples when there is potential interraction.\n\nDoes gender change the effect of education on wages?\nDo patients recover faster when taking drug A?\nHow does advertisement affect price sensitivity?\nInteractions are useful. Particularly with dummy variables.\nWe build a kitchen-sink model with all possible dummies (day of the week, gender,...)\n\nIf we think that the effect of \\(X_1\\) on \\(Y\\) depends on the value of \\(X_2\\), we model it using a liner relation \\[\n\\beta_1 = \\beta_{10} + \\beta_{11} X_2\n\\] and the model without interaction \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\) becomes \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\epsilon.\n\\] The interaction term captures the effect of \\(X_1\\) on \\(Y\\) when \\(X_2=1\\). The coefficient \\(\\beta_3\\) is the difference in the effect of \\(X_1\\) on \\(Y\\) when \\(X_2=1\\) and \\(X_2=0\\). If \\(\\beta_3\\) is significant, then there is an interaction effect. If \\(\\beta_3\\) is not significant, then there is no interaction effect.\nIn R:\n\nmodel = lm(y = x1 * x2)\n\ngives \\(X_1+X_2+X_1X_2\\), and\n\nmodel = lm(y = x1:x2)\n\ngives only \\(X_1 X_2\\)\nThe coefficients \\(\\beta_1\\) and \\(\\beta_2\\) are marginal effects.\nIf \\(\\beta_3\\) is significant there’s an interaction effect and ee leave \\(\\beta_1\\) and \\(\\beta_2\\) in the model whether they are significant or not.\n\\(X_1\\) and \\(D\\) dummy\n\n\\(X_2 = D\\) is a dummy variable with values of zero or one.\nModel: typically we run a regression of the form \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1 \\star D + \\epsilon\\]\nThe coefficient \\(\\beta_1 + \\beta_2\\) is the effect of \\(X_1\\) when \\(D=1\\). The coefficient \\(\\beta_1\\) is the effect when \\(D=0\\).\n\n\nExample 3.7 (Orange Juice)  \n\noj = read.csv(\"./../data/oj.csv\")\nknitr::kable(oj[1:5,1:10], digits=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstore\nbrand\nweek\nlogmove\nfeat\nprice\nAGE60\nEDUC\nETHNIC\nINCOME\n\n\n\n\n2\ntropicana\n40\n9.0\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n2\ntropicana\n46\n8.7\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n2\ntropicana\n47\n8.2\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n2\ntropicana\n48\n9.0\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n2\ntropicana\n50\n9.1\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n\n\nmodel = lm(logmove ~ log(price)*feat, data=oj)\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.66\n0.02\n588\n0\n\n\nlog(price)\n-0.96\n0.02\n-51\n0\n\n\nfeat\n1.71\n0.03\n56\n0\n\n\nlog(price):feat\n-0.98\n0.04\n-23\n0\n\n\n\n\nmodel = lm(log(price)~ brand-1, data = oj)\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nbranddominicks\n0.53\n0\n254\n0\n\n\nbrandminute.maid\n0.79\n0\n382\n0\n\n\nbrandtropicana\n1.03\n0\n500\n0\n\n\n\n\n\nbrandcol &lt;- c(\"green\",\"red\",\"gold\")\noj$brand = factor(oj$brand)\nboxplot(log(price) ~ brand, data=oj, col=brandcol)\nplot(logmove ~ log(price), data=oj, col=brandcol[oj$brand], pch=20)\n\n\n\n\n\n\n\n\n\n\n\n83 Chicagoland Stores (Demographic info for each)\nPrice, sales (log units moved), and whether advertised (feat)\n\nOrange Juice: Price vs Sales\n\n\n\n\n\n\n\n\n\nOrange Juice: Price vs log(Sales)\n\nplot(logmove ~ price, data=oj, col=brandcol[oj$brand], pch=16, cex=0.5, ylab=\"log(move)\")\n\n\n\n\n\n\n\n\nOrange Juice: Price vs log(Sales)\n\nl1 &lt;- loess(logmove ~ price, data=oj, span=2)\nsmoothed1 &lt;- predict(l1) \nind = order(oj$price)\nplot(logmove ~ price, data=oj, col=brandcol[oj$brand], pch=16, cex=0.5, ylab=\"log(move)\")\nlines(smoothed1[ind], x=oj$price[ind], col=\"blue\", lwd=2)\n\n\n\n\n\n\n\n\nOrange Juice: log(Price) vs log(Sales)\n\nplot(logmove ~ log(price), data=oj, col=brandcol[oj$brand], pch=16, cex=0.5, ylab=\"log(move)\")\nl2 &lt;- lm(logmove ~ log(price), data=oj)\nsmoothed2 &lt;- predict(l2) \nind = order(oj$price)\nlines(smoothed2[ind], x=log(oj$price[ind]), col=\"blue\", lwd=2)\n\n\n\n\n\n\n\n\nWhy? Multiplicative (rather than additive) change.\nNow we are interested in how does advertisement affect price sensitivity?\nOriginal model \\[\n\\log(\\mathrm{sales}) = \\beta_0 + \\beta_1\\log(\\mathrm{price}) + \\beta_2 \\mathrm{feat}.\n\\] If we feature the brand (in-store display promo or flyer ad), does it affect price sensitivity \\(\\beta_1\\)? If we assume it does \\[\n\\beta_1 = \\beta_3 +  \\beta_4\\mathrm{feat}.\n\\] The new model is \\[\n\\log(\\mathrm{sales}) = \\beta_0 + (\\beta_3 +  \\beta_4\\mathrm{feat})\\log(\\mathrm{price}) + \\beta_2 \\mathrm{feat}.\n\\] After expanding \\[\n\\log(\\mathrm{sales}) = \\beta_0 + \\beta_3\\log(\\mathrm{price}) +  \\beta_4\\mathrm{feat}*\\log(\\mathrm{price}) + \\beta_2 \\mathrm{feat}.\n\\]\n\n## and finally, consider 3-way interactions\nojreg &lt;- lm(logmove ~ log(price)*feat, data=oj)\nojreg %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.66\n0.02\n588\n0\n\n\nlog(price)\n-0.96\n0.02\n-51\n0\n\n\nfeat\n1.71\n0.03\n56\n0\n\n\nlog(price):feat\n-0.98\n0.04\n-23\n0\n\n\n\n\nlm(logmove ~ log(price), data=oj) %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.4\n0.02\n679\n0\n\n\nlog(price)\n-1.6\n0.02\n-87\n0\n\n\n\n\n\n\nlm(logmove ~ log(price)+feat + brand, data=oj) %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.28\n0.01\n708\n0\n\n\nlog(price)\n-2.53\n0.02\n-116\n0\n\n\nfeat\n0.89\n0.01\n85\n0\n\n\nbrandminute.maid\n0.68\n0.01\n58\n0\n\n\nbrandtropicana\n1.30\n0.01\n88\n0\n\n\n\n\n\n\nlm(logmove ~ log(price)*feat, data=oj) %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.66\n0.02\n588\n0\n\n\nlog(price)\n-0.96\n0.02\n-51\n0\n\n\nfeat\n1.71\n0.03\n56\n0\n\n\nlog(price):feat\n-0.98\n0.04\n-23\n0\n\n\n\n\n\n\nlm(logmove ~ brand-1, data=oj) %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nbranddominicks\n9.2\n0.01\n885\n0\n\n\nbrandminute.maid\n9.2\n0.01\n889\n0\n\n\nbrandtropicana\n9.1\n0.01\n879\n0\n\n\n\n\n\nAdvertisement increases price sensitivity from -0.96 to -0.958 - 0.98 = -1.94!\nWhy?\nOne of the reasons is that the price was lowered during the Ad campaign!\n\ndoj = oj %&gt;% filter(brand==\"dominicks\")\npar(mfrow=c(1,3), mar=c(4.2,4.6,2,1))\nboxplot(price ~  feat, data = oj[oj$brand==\"dominicks\",], col=c(2,3), main=\"dominicks\", ylab=\"Price ($)\")\nboxplot(price ~  feat, data = oj[oj$brand==\"minute.maid\",], col=c(2,3), main=\"minute.maid\")\nboxplot(price ~  feat, data = oj[oj$brand==\"tropicana\",], col=c(2,3), main=\"tropicana\")\n\n\n\n\n\n\n\n\n0 = not featured, 1 = featured",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "12-regression.html#dummies",
    "href": "12-regression.html#dummies",
    "title": "3  Linear and Multiple Regression",
    "section": "3.6 Dummies",
    "text": "3.6 Dummies\nWe want to understand effect of the brand on the sales \\[\\log(\\mathrm{sales}) = \\beta_0 + \\beta_1\\log(\\mathrm{price}) + \\xcancel{\\beta_2\\mathrm{brand}}\\]\nBut brand is not a number!\nHow can you use it in your regression equation?\nWe introduce dummy variables\n\n\n\nBrand\nIntercept\nbrandminute.maid\nbrandtropicana\n\n\n\n\nminute.maid\n1\n1\n0\n\n\ntropicana\n1\n0\n1\n\n\ndominicks\n1\n0\n0\n\n\n\n\\[\\log(\\mathrm{sales}) = \\beta_0 + \\beta_1\\log(\\mathrm{price}) + \\beta_{21}\\mathrm{brandminute.maid} + \\beta_{22}\\mathrm{brandtropicana}\\]\nR will automatically do it it for you\n\nprint(lm(logmove ~ log(price)+brand, data=oj))\n\n\nCall:\nlm(formula = logmove ~ log(price) + brand, data = oj)\n\nCoefficients:\n     (Intercept)        log(price)  brandminute.maid    brandtropicana  \n           10.83             -3.14              0.87              1.53  \n\n\n\\[\\log(\\mathrm{sales}) = \\beta_0 + \\beta_1\\log(\\mathrm{price}) + \\beta_3\\mathrm{brandminute.maid} + \\beta_4\\mathrm{brandtropicana}\\]\n\\(\\beta_3\\) and \\(\\beta_4\\) are “change relative to reference\" (dominicks here).\nHow does brand affect price sensitivity?\nInteractions: logmove ~ log(price) * brand\nNo Interactions: logmove ~ log(price) + brand\n\n\n\nParameter\nInteractions\nNo Interactions\n\n\n\n\n(Intercept)\n10.95\n10.8288\n\n\nlog(price)\n-3.37\n-3.1387\n\n\nbrandminute.maid\n0.89\n0.8702\n\n\nbrandtropicana\n0.96239\n1.5299\n\n\nlog(price):brandminute.maid\n0.057\n\n\n\nlog(price):brandtropicana\n0.67\n\n\n\n\n\nExample 3.8 (Golf Performance Data) Dave Pelz has written two best-selling books for golfers, Dave Pelz’s Short Game Bible, and Dave Pelz’s Putting Bible. These books have become essential reading for serious golfers looking to improve their performance through data-driven analysis and scientific methodology.\nDave Pelz was formerly a “rocket scientist” (literally) at NASA, where he worked on the Apollo space program. His background in physics and engineering provided him with the analytical skills to revolutionize golf instruction through data analytics. His systematic approach to analyzing golf performance helped him refine his teaching methods and develop evidence-based strategies for improving players’ games. Through his research, Pelz discovered that it’s the short-game that matters most for overall scoring performance.\nOne of Pelz’s most famous findings concerns the optimal speed for a putt. Through extensive data collection and analysis, he determined that the best chance to make a putt is one that will leave the ball \\(17\\) inches past the hole, if it misses. This counterintuitive result challenges the common belief that golfers should aim to leave putts just short of the hole. Pelz’s research showed that putts hit with this specific speed have the highest probability of going in, as they account for the natural variations in green speed, slope, and other factors that affect putt trajectory.\nNow, we demonstrate how to use data to improve your golf game. We analyze the dataset that contains comprehensive year-end performance statistics for 195 professional golfers from the 2000 PGA Tour season. This rich dataset captures technicical abilities of the players as well as financial success (measured by the amount of prize money they made). Each observation represents season’s averages of the players’ performance and total prize money. List below shows the variables in the dataset.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nnevents\nThe number of official PGA events included in the statistics\n\n\nmoney\nThe official dollar winnings of the player\n\n\ndrivedist\nThe average number of yards driven on par 4 and par 5 holes\n\n\ngir\nGreens in regulation, measured as the percentage of time that the first (tee) shot on a par 3 hole ends up on the green, or the second shot on a par 4 hole ends up on the green, or the third shot on a par 5 hole ends up on the green\n\n\navgputts\nThe average number of putts per round\n\n\n\nWe will analyze these data to determine which of the variables nevents, drivedist, gir, and avgputts is most important for winning money on the PGA Tour. We begin by performing a regression of Money on all explanatory variables:\n\nd00 = read_csv(\"../data/pga-2000.csv\")\nd18 = read_csv(\"../data/pga-2018.csv\")\n\n\nmodel18 = lm(money ~ nevents + drivedist + gir + avgputts, data=d18)\nmodel00 = lm(money ~ nevents + drivedist + gir + avgputts, data=d00)\nmodel00 %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.5e+07\n4206466\n3.5\n0.00\n\n\nnevents\n-3.0e+04\n11183\n-2.7\n0.01\n\n\ndrivedist\n2.1e+04\n6913\n3.1\n0.00\n\n\ngir\n1.2e+05\n17429\n6.9\n0.00\n\n\navgputts\n-1.5e+07\n2000905\n-7.6\n0.00\n\n\n\n\n\nLet’s look at the residuals:\n\narrows(x0 = 7.5,y0 = 20,x1 = 8.5,y1 = 2,length = 0.1)\ntext(x = 7,y = 22,labels = \"Tiger Woods\", cex=1.5)\n\n\n\n\n\n\n\n\nIt seems like we need to measure money on a log scale. Let’s transform with log(Money) as it has much better residual diagnostic plots.\n\nm = lm(formula = log(money) ~ nevents + drivedist + gir + avgputts, data = d00)\nm %&gt;% tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n36.15\n3.58\n10.10\n0.00\n\n\nnevents\n-0.01\n0.01\n-0.94\n0.35\n\n\ndrivedist\n0.01\n0.01\n2.40\n0.02\n\n\ngir\n0.17\n0.01\n11.18\n0.00\n\n\navgputts\n-21.13\n1.70\n-12.42\n0.00\n\n\n\n\n\n\npar(mar = c(4,4.5,0,0),mfrow=c(1,1))\nmodel00log %&gt;% tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n36.15\n3.58\n10.10\n0.00\n\n\nnevents\n-0.01\n0.01\n-0.94\n0.35\n\n\ndrivedist\n0.01\n0.01\n2.40\n0.02\n\n\ngir\n0.17\n0.01\n11.18\n0.00\n\n\navgputts\n-21.13\n1.70\n-12.42\n0.00\n\n\n\n\nhist(rstandard(model00log), breaks=20, col=\"lightblue\", xlab = \"Standartized Residual\", main=\"\")\narrows(x0 = 3,y0 = 20,x1 = 3.2,y1 = 2,length = 0.1)\ntext(x = 3,y = 22,labels = \"Tiger Woods\", cex=1.5)\n\n\n\n\n\n\n\n\nUsing log scale for money gives us a better model. We will keep it for now. How about selectng variables. Notice, that \\(t\\)-stats for nevents is \\(&lt;1.5\\). Thus, we can remove it.\n\nm1 = lm(formula = log(money) ~ drivedist + gir + avgputts, data = d00)\nm1 %&gt;% tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n36.17\n3.58\n10.1\n0.00\n\n\ndrivedist\n0.01\n0.01\n2.5\n0.01\n\n\ngir\n0.17\n0.01\n11.2\n0.00\n\n\navgputts\n-21.37\n1.68\n-12.7\n0.00\n\n\n\n\n\nIt is obvious that fewer putts indicate a better golfer. However, decreasing the average number of putts per round by one is extremely difficult to achieve.\nEvaluating the Coefficients\n\nGreens in Regulation (GIR) has a \\(\\hat{\\beta} = 0.17\\). If I can increase my GIR by one, I’ll earn \\(e^{0.17} = 1.18\\)% An extra \\(18\\)%\nDriveDis has a \\(\\hat{\\beta} = 0.014\\). A \\(10\\) yard improvement, I’ll earn \\(e^{0.014 \\times 10} =e^{0.14}  = 1.15\\)% An extra \\(15\\)%\n\nCaveat: Everyone has gotten better since 2000!\nTiger Woods was nine standard deviations better than what the model predicted, while taking the natural logarithm of money earnings significantly improves the residual diagnostics and an exponential model appears to fit the data well as evidenced by the good residual diagnostic plots; furthermore, the t-ratios for the number of events variable are consistently under 1.5, indicating it may not be a significant predictor.\nThe outliers represent the biggest over and under-performers in terms of money winnings when compared with their performance statistics, and Tiger Woods, Phil Mickelson, and Ernie Els won major championships by performing exceptionally well during tournaments with substantial prize money available.\nWe can see the over-performers and under-performers in the data.\n\n\n\nOver-Performers\n\n\n\nname\nmoney\nPredicted\nError\n\n\n\n\n1\nTiger Woods\n9188321\n3584241\n5604080\n\n\n2\nPhil Mickelson\n4746457\n2302171\n2444286\n\n\n3\nErnie Els\n3469405\n1633468\n1835937\n\n\n4\nHal Sutton\n3061444\n1445904\n1615540\n\n\n20\nNotah Begay III\n1819323\n426061\n1393262\n\n\n182\nSteve Hart\n107949\n-1186685\n1294634\n\n\n\n\n\nNow, let’s extract the list of underperformers, which are given by large negative residuals. According to our model, Glasson and Stankowski should win more money based on their performance statistics, but they are not achieving the expected earnings. This could be due to several factors: they might be performing well in practice rounds but struggling under tournament pressure, they could be playing in fewer high-payout events, or their performance metrics might not capture other important aspects of tournament success like clutch putting or mental toughness during critical moments.\n\n\n\nUnder-Performers\n\n\n\nname\nmoney\nPredicted\nError\n\n\n\n\n47\nFred Couples\n990215\n1978477\n-988262\n\n\n52\nKenny Perry\n889381\n1965740\n-1076359\n\n\n70\nPaul Stankowski\n669709\n1808690\n-1138981\n\n\n85\nBill Glasson\n552795\n1711530\n-1158735\n\n\n142\nJim McGovern\n266647\n1397818\n-1131171\n\n\n\n\n\nLets look at 2018 data, the highest earners are\n\n\n\nHighest earners 2018\n\n\nname\nnevents\nmoney\ndrivedist\ngir\navgputts\n\n\n\n\nJustin Thomas\n23\n8694821\n312\n69\n1.7\n\n\nDustin Johnson\n20\n8457352\n314\n71\n1.7\n\n\nJustin Rose\n18\n8130678\n304\n70\n1.7\n\n\nBryson DeChambeau\n26\n8094489\n306\n70\n1.8\n\n\nBrooks Koepka\n17\n7094047\n313\n68\n1.8\n\n\nBubba Watson\n24\n5793748\n313\n68\n1.8\n\n\n\n\n\nOverperformers\n\n\n\nOverperformers 2018\n\n\n\nname\nmoney\nPredicted\nError\n\n\n\n\n1\nJustin Thomas\n8694821\n5026220\n3668601\n\n\n2\nDustin Johnson\n8457352\n6126775\n2330577\n\n\n3\nJustin Rose\n8130678\n4392812\n3737866\n\n\n4\nBryson DeChambeau\n8094489\n3250898\n4843591\n\n\n5\nBrooks Koepka\n7094047\n4219781\n2874266\n\n\n6\nBubba Watson\n5793748\n3018004\n2775744\n\n\n9\nWebb Simpson\n5376417\n2766988\n2609429\n\n\n11\nFrancesco Molinari\n5065842\n2634466\n2431376\n\n\n12\nPatrick Reed\n5006267\n2038455\n2967812\n\n\n84\nSatoshi Kodaira\n1471462\n-1141085\n2612547\n\n\n\n\n\nUnderperformers\n\n\n\nUnderperformers 2018\n\n\n\nname\nmoney\nPredicted\nError\n\n\n\n\n102\nTrey Mullinax\n1184245\n3250089\n-2065844\n\n\n120\nJ.T. Poston\n940661\n3241369\n-2300708\n\n\n135\nTom Lovelady\n700783\n2755854\n-2055071\n\n\n148\nMichael Thompson\n563972\n2512330\n-1948358\n\n\n150\nMatt Jones\n538681\n2487139\n-1948458\n\n\n158\nHunter Mahan\n457337\n2855898\n-2398561\n\n\n168\nCameron Percy\n387612\n3021278\n-2633666\n\n\n173\nRicky Barnes\n340591\n3053262\n-2712671\n\n\n176\nBrett Stegmaier\n305607\n2432494\n-2126887\n\n\n\n\n\nOur analysis reveals three particularly interesting effects from the golf performance data, with Tiger Woods demonstrating exceptional performance as an outlier that is eight standard deviations above the model’s predictions, indicating his extraordinary success relative to his statistical metrics, while the model shows that increasing driving distance by ten yards corresponds to a fifteen percent increase in earnings, suggesting that power off the tee provides a significant competitive advantage in professional golf, and additionally, improving greens in regulation (GIR) by one percentage point leads to an eighteen percent increase in earnings, highlighting the importance of approach shot accuracy in determining financial success on the PGA Tour, with the model also successfully identifying both under-performers and over-performers, players whose actual earnings significantly differ from what their statistical performance would predict, providing valuable insights into which players may be exceeding or falling short of expectations based on their measurable skills, demonstrating the practical applications of statistical modeling in sports analytics and performance evaluation.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "12-regression.html#bayesian-regression",
    "href": "12-regression.html#bayesian-regression",
    "title": "3  Linear and Multiple Regression",
    "section": "3.7 Bayesian Regression",
    "text": "3.7 Bayesian Regression\nConsider a linear regression \\[\nf(x) = x^T\\beta + \\epsilon,~~~\\epsilon \\sim N(0,\\sigma_e).\n\\] We put a zero mean Gaussian prior on the model parameters \\[\n\\beta \\sim N(0,\\Sigma).\n\\] Bayesian inference is to calculate posterior given the data \\[\np(\\beta\\mid y,X) = \\dfrac{p(y\\mid X,\\beta)p(\\beta)}{p(y\\mid X)}.\n\\] Product of two Gaussian density functions lead to another Gaussian \\[\n\\begin{aligned}\np(\\beta\\mid y,X)  & \\propto \\exp\\left(-\\dfrac{1}{2\\sigma_e^2}(y-\\beta^TX)^T(y-\\beta^TX)\\right)\\exp\\left(-\\dfrac{1}{2}\\beta^T\\Sigma^{-1}\\beta\\right)\\\\\n& \\propto \\exp\\left(-\\dfrac{1}{2}(\\beta - \\bar\\beta)^T\\left(\\dfrac{1}{\\sigma_e^2XX^T + \\Sigma^{-1}}\\right)(\\beta-\\bar\\beta)\\right)\n\\end{aligned}\n\\]\nThus, the posterior is \\[\n\\beta\\mid X,y \\sim N(\\bar\\beta,A^{-1}),\n\\] where \\(A = \\left(\\sigma_e^{-2}XX^T + \\Sigma\\right)\\), and \\(\\bar\\beta = \\sigma_e^{-2}A^{-1}Xy\\).\n\nExample 3.9 (Posterior) Consider a model with \\(p = 1\\) \\[\ny = \\beta_0 + \\beta_1 x + \\epsilon, ~~ \\beta_i \\sim N(0,1),~~~\\sigma_e = 1\n\\] Let’s plot a sample from the prior set of functions\n\n\n\nSample from prior distribution over possible linear models\n\n\nNow, say we observed two points \\((1,1)\\) and \\((2,2)\\), we can calculate the posterior \\(\\beta \\mid X,y \\sim N(0.833, 0.166)\\)\n\n\n\nSample from posterior distribution over possible linear models\n\n\nWhy our posterior mean is not 1?\n\n\n3.7.1 Horseshoe for Linear regression\nThe linear regression model is given by \\[Y = X\\beta + \\varepsilon,\\]\nwhere \\(Y\\) and \\(\\varepsilon\\) are vectors of length \\(n\\), \\(\\beta\\) is a vector of length \\(p\\) and \\(X\\) is an \\(n \\times p\\)-matrix. We assume \\(\\varepsilon \\sim \\mathcal{N}(0, I_n)\\). The main function for the horseshoe for the linear regression model is horseshoe and it implements the algorithm of Bhattacharya et al (2016).\nThe options of horseshoe are the same as for HS.normal.means (discussed above, although in case of linear regression it is less clear which prior to use for \\(\\tau\\)). We illustrate the use of horseshoe via an example.\nWe create a 50 by 100 design matrix \\(X\\) filled with realizations of independent normal random variables. The first 10 entries of the vector \\(\\beta\\) are set equal to six (the signals) and the remaining 90 entries are set equal to zero (the noise).\n\nX &lt;- matrix(rnorm(50*100), 50)\nbeta &lt;- c(rep(6, 10), rep(0, 90))\ny &lt;- X %*% beta + rnorm(50)\n\nWe use the horseshoe and plot the posterior mean and marginal 95% credible interval per parameter in red. The true parameter values are shown in black.\n\nlibrary(horseshoe)\nhs.object &lt;- horseshoe(y, X, method.tau = \"truncatedCauchy\", method.sigma =\"Jeffreys\")\n\n 1000\n 2000\n 3000\n 4000\n 5000\n 6000\n\ndf &lt;- data.frame(index = 1:100,\n                 truth = beta,\n                 post.mean = hs.object$BetaHat,\n                 lower.CI &lt;- hs.object$LeftCI,\n                 upper.CI &lt;- hs.object$RightCI\n                 )\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean), size = 2, col = \"red\") +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI), width = .1, col = \"red\") +\n  ggtitle(\"Black = truth, Red = estimates with 95% credible intervals\")\n\n\n\n\n\n\n\n\nWe again perform variable selection. The function, HS.var.select, is the same as described above for the normal means problem. Here we show how it works when variables are selected by checking whether 0 is in the credible interval. For the thresholding procedure, please refer to the normal means example above.\nWe perform variable selection:\n\ndf$selected.CI &lt;- HS.var.select(hs.object, df$y, method = \"intervals\")\n\nThe result is a vector of zeroes and ones, with the ones indicating that the observations is suspected to correspond to an actual signal. We now plot the results, coloring the estimates/intervals blue if a signal is detected and red otherwise.\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean, col = factor(selected.CI)), \n             size = 2) +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI, col = factor(selected.CI)),\n                width = .1) +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Black = truth, Blue = selected as signal, Red = selected as noise\")\n\n\n\n\n\n\n\n\n\n\n\n\nSchwertman, Neil C, AJ Gilks, and J Cameron. 1990. “A Simple Noncalculus Proof That the Median Minimizes the Sum of the Absolute Deviations.” The American Statistician 44 (1): 38–39.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear and Multiple Regression</span>"
    ]
  },
  {
    "objectID": "13-logistic.html",
    "href": "13-logistic.html",
    "title": "4  Classification: Logistic Regression",
    "section": "",
    "text": "4.1 Logistic Regression\nClassification is a type of predictive modeling where the goal is to predict a categorical variable based on a set of input variables. The categorical variable is often called the response variable and the input variables are called the predictors. The response variable is typically binary, meaning it can take on only two values, such as 0 or 1, or it can be a multi-class variable, meaning it can take on more than two values.\nWe start by assuming a binomial likelihood function for the response variable. The binomial likelihood function is a function of the probability of the response variable taking on a value of 1, given the input variables. The binomial likelihood function is defined as follows \\[\nP(y_i = 1\\mid p_i) = p_i^{y_i} (1-p_i)^{1-y_i},\n\\] where \\(p_i\\) is the funciton of the inputs \\(x_i\\) and coefficients \\(\\beta\\) that gives us the probability of the response variable taking on a value of 1, given the input variables. A typical approach to calculate \\(p_i\\) is to use logistic function \\[\np_i  = f(x_i,\\beta)= \\frac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}},\n\\] where \\(\\beta\\) is a vector of parameters. The logistic function is a sigmoid function that maps any real number to a number between zero and one.\nGiven the observed data set \\(\\{(x_i,y_i)\\}_{i=1}^n\\), where each \\(y_i\\) is either 0 or 1, the goal is to predict a probability that the next observation will be 1. Binomial log-likelihood minimisation leads to the maximum likelihood estimator for parameters \\(\\beta\\) (a.k.a cross-entropy estimator). The maximum likelihood estimator is defined as follows \\[\n\\mini_{\\beta} -\\sum_{i=1}^n y_i \\log \\left ( f(x_i,\\beta) \\right ) + (1-y_i) \\log \\left ( 1-f(x_i,\\beta) \\right ).\n\\]\nIn the unconditional case, when we do not observe any inputs \\(x\\), the cross-entropy estimator is again, the sample mean. If we take the derivative of the above expression with respect to \\(\\beta\\) and set it to zero, we get \\[\n\\frac{d}{d\\beta} -\\sum_{i=1}^n y_i \\log \\left ( \\beta \\right ) + (1-y_i) \\log \\left ( 1-\\beta \\right ) = -\\sum_{i=1}^n \\frac{y_i}{\\beta} - \\frac{1-y_i}{1-\\beta} = 0\n\\] which gives us the solution \\[\n\\hat{\\beta} = \\frac{1}{n}\\sum_{i=1}^n y_i.\n\\] which is the sample mean.\nUnlike the least squares estimator, there is no analytical solution to the problem of minimizing cross-entropy. However, there are efficient numerical optimization algorithms that can be used to find the optimal solution. As we will show later, the cross-entropy estimator is equivalent to the maximum likelihood estimator, assuming that \\(y\\) is the Bernoulli random variable.\nIn the case when we have more than two classes \\(y \\in \\{1,\\ldots,K\\}\\), we simply build \\(K\\) models \\(f_1(x_i,\\beta),\\ldots, f(x_K,\\beta)\\), one for each class and then use the softmax function to convert the output of each model into a number between zero and one. The softmax function is defined as follows \\[\n\\mathrm{softmax}\\left(f_j(x,\\beta)\\right) = \\frac{\\exp(f_j(x,\\beta))}{\\sum_{i=1}^K \\exp(f_i(x,\\beta))}.\n\\] The softmax function is a generalization of the sigmoid function to the case of more than two classes. It is often used as the activation function in the output layer of neural networks for multi-class classification problems. It converts the output of each model into a probability distribution over the classes, making it suitable for multi-class classification with probabilistic outputs.\nIn summary, choosing the right loss function for your predictive rule depends on several factors, including type of prediction task: regression vs classification and importance of sensitivity to outliers.\nFor that, we need to specify the loss function that will measure the mismatch between an observed and a predicted values. The loss function is a measure of how well the model fits the data and is used to estimate the parameters of the model. The goal is to find the values of the parameters that minimize the loss function. A most popular choice for the loss function is the squared error loss function. \\[\nL(\\beta; ~D) = \\sum_{i=1}^n (y_i - f(x_i))^2 \\rightarrow \\mathrm{minimize}_{\\beta}\n\\]\nWhen the value \\(y\\) we are trying to predict is categorical (or qualitative) we have a classification problem. For a binary output we predict the probability its going to happen \\[\np ( Y=1 | X = x ),\n\\] where \\(X = (x_1,\\ldots,x_p)\\) is our usual list of predictors.\nSuppose that we have a binary response, \\(y\\) taking the value \\(0\\) or \\(1\\)\nThe goal is to predict the probability that \\(y\\) equals \\(1\\). You can then do and categorize a new data point. Assessing credit risk and default data is a typical problem. - \\(y\\): whether or not a customer defaults on their credit card (No or Yes). - \\(x\\): The average balance that customer has remaining on their credit card after making their monthly payment, plus as many other features you think might predict \\(Y\\).\nA linear model is a powerful tool to find relations among different variables \\[\ny = \\beta^Tx + \\epsilon.\n\\] It works assuming that \\(y\\) variable is contentious and ranges in \\((-\\infty,+\\infty)\\). Another assumption is that conditional distribution of \\(y\\) is normal \\(p(y\\mid \\beta^Tx) \\sim N(\\beta^Tx, \\sigma^2)\\)\nWhat do we do when assumptions about conditional normal distributions do not hold. For example \\(y\\) can be a binary variable with values 0 and 1. For example \\(y\\) is\nWe model response \\(\\{0,1\\}\\), using a continuous variable \\(y \\in [0,1]\\) which is interpreted as the probability that response equals to 1. \\[\np( y= 1 | x_1, \\ldots , x_p  ) = F \\left ( \\beta_1 x_1 + \\ldots + b_p x_p   \\right )\n\\] where \\(f\\) is increasing and \\(0&lt; f(x)&lt;1\\).\nIt seems logical to find a transformation \\(F\\) so that \\(F(\\beta^Tx + \\epsilon) \\in [0,1]\\). Then we can predict using \\(F(\\beta^Tx)\\) and intercepting interpret the result as a probability, i.e if \\(F(\\beta^Tx) = z\\) then we interpret it as \\(p(y=1) = z\\). Such function \\(F\\) is called a link function.\nDo we know a function that maps any real number to a number in \\([0,1]\\) interval? What about commutative distribution function \\(F(x) = p(Z \\le x)\\)? If we choose CDF \\(\\Phi(x)\\) for \\(N(0,1)\\) then we have \\[\\begin{align*}\n\\hat y = p(y=1) &= \\Phi(\\beta^Tx) \\\\\n\\Phi^{-1}(\\hat y) = & \\beta^Tx + \\epsilon\n\\end{align*}\\] This is a linear model for \\(\\Phi^{-1}(\\hat y)\\), but not for \\(y\\)! You can thing of this as a change of units for variable \\(y\\). In this specific case, when we use normal CDF, the resulting model is called probit, it stands for probability unit. The resulting link function is \\(\\Phi^{-1}\\) and now \\(\\Phi^{-1}(Y)\\) follows a normal distribution! This term was coined in the 1930’s by biologists studying the dosage-cure rate link. We can fit a probit model using glm function in R.\nset.seed(92) # Kuzy\nx = seq(-3,3,length.out = 100)\ny = pnorm(x+rnorm(100))&gt;0.5\nprobitModel = glm(y~x, family=binomial(link=\"probit\"))\nmc = as.double(coef(probitModel))\n# we want to predict outcome for x = -1\nxnew = -1\n(yt = mc[1] + mc[2]*xnew)\n\n -0.86\n\n(pnorm(yt))\n\n 0.19\n\n(pred = predict(probitModel, list(x = c(xnew)), type=\"response\"))\n\n   1 \n0.19\nnd = dnorm(mc[1] + mc[2]*x)\nplot(x,nd, type='l', col=\"red\", xlab=\"x\", ylab = \"P(y=1)\")\npolygon(c(-3,x[x&lt; -1],-1),c(0,nd[x&lt; -1],0), col=\"blue\")\nOur prediction is the blue area which is equal to 0.195.\nplot(x,y, type='p', col=\"red\", xlab=\"x\", ylab = \"P(y=1)\")\npred_probit = predict(probitModel, list(x=x), type=\"response\")\nlines(x,pred_probit, type='l')\nA couple of observations: (i) this fits the data much better than the linear estimation, and (i) it always lies between 0 and 1. Instead of thinking of \\(y\\) as a probability and transforming right hand side of the linear model we can think of transforming \\(y\\) so that transformed variable lies in \\((-\\infty,+\\infty)\\). We can use odds ratio, that we talked about before \\[\n\\dfrac{y}{1-y}\n\\]\nOdds ration lies in the interval \\((0,+\\infty)\\). Almost what we need, but not exactly. Can we do another transform that maps \\((0,+\\infty)\\) to \\((-\\infty,+\\infty)\\)? \\[\n\\log\\left(\\dfrac{y}{1-y}\\right)\n\\] will do the trick! This function is called a logit function and it is This function is called a logit function and it is the inverse of the sigmoidal \"logistic\" function or logistic transform. The is linear in \\[\n\\log \\left ( \\frac{ p \\left ( y=1|x \\right ) }{ 1 -  p \\left ( Y=1|x \\right ) } \\right ) = \\beta_0 + \\beta_1 x_1 + \\ldots + x_p.\n\\] These model are easy to fit in R:\nglm( y ~ x1 + x2,  family=\"binomial\")\nOutside of specific field, i.e. behavioral economics, the logistic function is much more popular of a choice compared to probit model. Besides that fact that is more intuitive to work with logit transform, it also has several nice properties when we deal with multiple classes (more then 2). Also, it is computationally easier then working with normal distributions. The density function of the logit is very similar to the probit one.\nlogitModel  = glm(y~x, family=binomial(link=\"logit\"))\npred_logit = predict(logitModel, list(x = x), type=\"response\")\nplot(x,pred_probit, pch=20, col=\"red\", cex=0.9, ylab=\"y\")\nlines(x,pred_logit, type='p', pch=20, cex=0.5, col=\"blue\")\nlines(x,y, type='p', pch=21, cex=0.5, bg=\"lightblue\")\nlegend(\"bottomright\",pch=20, legend=c(\"Logit\", \"Probit\"), col=c(\"blue\",\"red\"),y.intersp = 2)\nWe can easily derive an inverse of the logit function to get back the original \\(y\\) \\[\n\\log\\left(\\dfrac{y}{1-y}\\right) = \\beta^Tx;~~ y=\\dfrac{e^{\\beta^Tx}}{1+e^{\\beta^Tx}}\n\\]",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classification: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#logistic-regression",
    "href": "13-logistic.html#logistic-regression",
    "title": "4  Classification: Logistic Regression",
    "section": "",
    "text": "Win or lose\nSick or healthy\nBuy or not buy\nPay or default\n\n\n\n\n\nOutcome of an election\nResult of spam filter\nDecision variable about loan approval\n\n\n\n\n\n\n\n\n\n\n\n\nis for indicates \\(y=0\\) or \\(1\\)\nhas a bunch of other options.\n\n\n\n\n\nExample 4.1 (Example: NBA point spread)  \nNBA = read.csv(\"../data/NBAspread.csv\")\nattach(NBA)\nn = nrow(NBA)\nhist(NBA$spread[favwin==1], col=5, main=\"\", xlab=\"spread\")\nhist(NBA$spread[favwin==0], add=TRUE, col=6)\nlegend(\"topright\", legend=c(\"favwin=1\", \"favwin=0\"), fill=c(5,6), bty=\"n\")\nboxplot(NBA$spread ~ NBA$favwin, col=c(6,5), horizontal=TRUE, ylab=\"favwin\", xlab=\"spread\")\n\n\n\n\n\n\n\n\n\n\nDoes the Vegas point spread predict whether the favorite wins or not? Turquoise = Favorites does win, Purple = Favorite does not win. In R: the output gives us\n\nnbareg = glm(favwin~spread-1, family=binomial)\nsummary(nbareg)\n\n\nCall:\nglm(formula = favwin ~ spread - 1, family = binomial)\n\nCoefficients:\n       Estimate Std. Error z value Pr(&gt;|z|)    \nspread   0.1560     0.0138    11.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 766.62  on 553  degrees of freedom\nResidual deviance: 527.97  on 552  degrees of freedom\nAIC: 530\n\nNumber of Fisher Scoring iterations: 5\n\ns = seq(0,30,length=100)\nfit = exp(s*nbareg$coef[1])/(1+exp(s*nbareg$coef[1]))\nplot(s, fit, typ=\"l\", col=4, lwd=2, ylim=c(0.5,1), xlab=\"spread\", ylab=\"P(favwin)\")\n\n\n\n\n\n\n\n\nThe \\(\\beta\\) measures how our log-odds change! \\(\\beta = 0.156\\)\nLet’s do the NBA Point Spread Prediction. “Plug-in” the values for the new game into our logistic regression \\[\n{ P \\left ( \\mathrm{ favwin}  \\mid  \\mathrm{ spread} \\right ) = \\frac{ e^{ \\beta x } }{ 1 + e^{\\beta x} } }\n\\] Check that when \\(\\beta =0\\) we have \\(p= \\frac{1}{2}\\).\nGiven our new values spread\\(=8\\) or spread\\(=4\\), the win probabilities are \\(77\\)% and \\(65\\)%, respectively. Clearly, the bigger spread means a higher chance of winning.\n\n\nExample 4.2 (Logistic Regression for Tennis Classification) Data science plays a major role in tennis, you can learn about recent AI tools developed by IBM from this This Yahoo Article.\nWe will analyze the Tennis Major Tournament Match Statistics Data Set from the UCI ML repository. The data set has one per each game from four major Tennis tournaments in 2013 (Australia Open, French Open, US Open, and Wimbledon).\nLet’s load the data and familiarize ourselves with it\n\nd = read.csv(\"./../data/tennis.csv\")\ndim(d)\n\n 943  44\n\nstr(d[,1:5])\n\n'data.frame':   943 obs. of  5 variables:\n $ Player1: chr  \"Lukas Lacko\" \"Leonardo Mayer\" \"Marcos Baghdatis\" \"Dmitry Tu\"..\n $ Player2: chr  \"Novak Djokovic\" \"Albert Montanes\" \"Denis Istomin\" \"Michael \"..\n $ Round  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Result : int  0 1 0 1 0 0 0 1 0 1 ...\n $ FNL1   : int  0 3 0 3 1 1 2 2 0 3 ...\n\n\nLet’s look at the few coluns of the randomly selected five rows of the data\n\nd[sample(1:943,size = 5),c(\"Player1\",\"Player2\",\"Round\",\"Result\",\"gender\",\"surf\")]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer1\nPlayer2\nRound\nResult\ngender\nsurf\n\n\n\n\n532\nFlorian Mayer\nJuan Monaco\n1\n1\nM\nHard\n\n\n816\nL.Kubot\nJ.Janowicz\n5\n0\nM\nGrass\n\n\n431\nSvetlana Kuznetsova\nEkaterina Makarova\n1\n1\nW\nClay\n\n\n568\nMarcos Baghdatis\nGo Soeda\n1\n1\nM\nHard\n\n\n216\nMandy Minella\nAnastasia Pavlyuchenkova\n2\n0\nW\nHard\n\n\n\n\n\n\nWe have data for 943 matches and for each match we have 44 columns, including names of the players, their gender, surface type and match statistics. Let’s look at the number of break points won by each player. We will plot BPW (break points won) by each player on the scatter plot and will colorize each dot according to the outcome\n\nn = dim(d)[1]\nplot(d$BPW.1+rnorm(n),d$BPW.2+rnorm(n), pch=21, col=d$Result+2, cex=0.6, bg=\"yellow\", lwd=0.8,\n     xlab=\"BPW by Player 1\", ylab=\"BPW by Player 2\")\nlegend(\"bottomright\", c(\"P1 won\", \"P2 won\"), col=c(3,2), pch=21, bg=\"yellow\", bty='n')\n\n\n\n\n\n\n\n\nWe can clearly see that number of the break points won is a clear predictor of the match outcome. Which is obvious and follows from the rules, to win a match, a player must win break points. Now, we want to understand the impact of a winning a break point on the overall match outcome. We do it by building a logistic regression model\n\nwhich(is.na(d$BPW.1)) # there is one row with NA value for the BPW.1 value and we remove it\n\n 171\n\nd = d[-171,]; n = dim(d)[1]\nm = glm(Result ~ BPW.1 + BPW.2-1, data=d, family = \"binomial\" )\nsummary(m)\n\n\nCall:\nglm(formula = Result ~ BPW.1 + BPW.2 - 1, family = \"binomial\", \n    data = d)\n\nCoefficients:\n      Estimate Std. Error z value Pr(&gt;|z|)    \nBPW.1   0.4019     0.0264    15.2   &lt;2e-16 ***\nBPW.2  -0.4183     0.0277   -15.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1305.89  on 942  degrees of freedom\nResidual deviance:  768.49  on 940  degrees of freedom\nAIC: 772.5\n\nNumber of Fisher Scoring iterations: 5\n\n\nR output does not tell us how accurate our model is but we can quickly check it by using the table function. We will use \\(0.5\\) as a threshold for our classification.\n\ntable(d$Result, as.integer(m$fitted.values&gt;0.5))\n\n   \n      0   1\n  0 416  61\n  1  65 400\n\n\nThus, our model got (416+416)/942 = 0.88% of the predictions correctly!\nEssentially, the logistic regression is trying to draw a line that separates the red observations from the green one. In out case, we have two predictors \\(x_1\\) = BPW.1 and \\(x_2\\) = BPW.2 and our model is \\[\n\\log\\left(\\dfrac{p}{1-p}\\right) = \\beta_1x_1 + \\beta_2 x_2,\n\\] where \\(p\\) is the probability of player 1 winning the match. We want to find the line along which the probability is 1/2, meaning that \\(p/(1-p) = 1\\) and log-odds \\(\\log(p/(1-p)) = 0\\), thus the equation for the line is \\(\\beta_1x_1 + \\beta_2 x_2 = 0\\) or \\[\nx_2 = \\dfrac{-\\beta_1}{\\beta_2}x_1\n\\]\nLet’s see the line found by the glm function\n\nlegend(\"bottomright\", c(\"P1 won\", \"P2 won\"), col=c(3,2), pch=21, bg=\"yellow\", bty='n')\n\nx = seq(0,30,length.out = 200)\ny  =  -m$coefficients[1]*x/m$coefficients[2]\nlines(x,y, lwd=2, col=\"red\") \n\n\n\n\n\n\n\n\nThere are a couple of observations. First, effect of a break point on the game outcome is significant and symmetric, effect of loosing break point is the same as the effect of winning one. We also can interpret the effect of winning a break point in the following way. We will keep BPW.2 = 0 and will calculate what happens to the probability of winning when BPW.1 changes from 0 to 1. The odds ration for player 1 winning when BPW.1 = 0 is exp(0) which is 1, meaning that the probability that P1 wins is 1/2. Now when BPW.1 = 1, the odds ratio is 1.5\n\nexp(0.4019)\n\n 1.5\n\n\nWe can calculate probability of winning from the regression equation \\[\n\\dfrac{p}{1-p} = 1.5,~~~p = 1.5(1-p),~~~2.5p = 1.5,~~~p = 0.6\n\\] Thus probability of winning goes from 50% to 60%, we can use predict function to get this result\n\npredict.glm(m,newdata = data.frame(BPW.1 = c(0), BPW.2 = c(0)), type=\"response\")\n\n  1 \n0.5 \n\npredict.glm(m,newdata = data.frame(BPW.1 = c(1), BPW.2 = c(0)), type=\"response\")\n\n  1 \n0.6 \n\n\nWhat happens to the chances of winning when P1 wins three more break points compared to the opponent\n\npredict.glm(m,newdata = data.frame(BPW.1 = c(0), BPW.2 = c(0)), type=\"response\")\n\n  1 \n0.5 \n\npredict.glm(m,newdata = data.frame(BPW.1 = c(3), BPW.2 = c(0)), type=\"response\")\n\n   1 \n0.77 \n\n\nChances go up by 27%.\nTennis is arguably the sport in which mean and women are treated equally. Both man and women matches are shown during the prime-time on TV, they both have the same prize money. However, one of the comments you hear often is that Women’s matches are “less predictable”, meaning that an upset (when the favorite looses) is more likely to happen in a women’s match compared to man Matches. We can test thus statement by looking at the residuals. The large the residual the less accurate our prediction was.\n\noutlind = which(d$res&lt;2)\nboxplot(d$res[outlind] ~ d$gender[outlind], col=c(2,3), xlab=\"Gender\",ylab=\"Residual\")\n\n\n\n\n\n\n\n\nLet’s do a formal T-test on the residuals foe men’s and women’s matches\n\nmen = d %&gt;% filter(res&lt;2, gender==\"M\") %&gt;% pull(res)\nwomen = d %&gt;% filter(res&lt;2, gender==\"W\") %&gt;% pull(res)\nt.test(men, women, alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  men and women\nt = -5, df = 811, p-value = 3e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.105 -0.043\nsample estimates:\nmean of x mean of y \n      1.2       1.3 \n\n\nLooks like the crowd wisdom that Women’s matches are less predictable is correct.\n\n\nExample 4.3 (Credit Card Default) We have 10,000 observations\n\nDefault = read.csv(\"../data/CreditISLR.csv\", stringsAsFactors = T)\nhead(Default)\n\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n730\n44362\n\n\nNo\nYes\n817\n12106\n\n\nNo\nNo\n1074\n31767\n\n\nNo\nNo\n529\n35704\n\n\nNo\nNo\n786\n38464\n\n\nNo\nYes\n920\n7492\n\n\n\n\n\n\nLet’s build a logistic regression model\n\nglm.fit=glm(default~balance,data=Default,family=binomial)\nsummary(glm.fit)\n\n\nCall:\nglm(formula = default ~ balance, family = binomial, data = Default)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -10.65133    0.36116   -29.5   &lt;2e-16 ***\nbalance       0.00550    0.00022    24.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600\n\nNumber of Fisher Scoring iterations: 8\n\n\nWe use it now to predict default\n\npredict.glm(glm.fit,newdata = list(balance=1000))\n\n   1 \n-5.2 \n\n-1.065e+01 + 5.499e-03*1000\n\n -5.2\n\npredict.glm(glm.fit,newdata = list(balance=1000), type=\"response\")\n\n     1 \n0.0058 \n\nexp(-1.065e+01 + 5.499e-03*1000)/(1+exp(-1.065e+01 + 5.499e-03*1000))\n\n 0.0058\n\n\nPredicting default\n\ny = predict.glm(glm.fit,newdata = x, type=\"response\")\nplot(x$balance,y, pch=20, col=\"red\", xlab = \"balance\", ylab=\"Default\")\nlines(Default$balance, as.integer(Default$default)-1, type='p',pch=20)",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classification: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#confusion-matrix",
    "href": "13-logistic.html#confusion-matrix",
    "title": "4  Classification: Logistic Regression",
    "section": "4.2 Confusion Matrix",
    "text": "4.2 Confusion Matrix\nWe can use accuracy rate: \\[\n\\text{accuracy} = \\dfrac{\\text{\\# of Correct answers}}{n}\n\\] or its dual, error rate \\[\n\\text{error rate} = 1 - \\text{accuracy}.\n\\] You remember, we haw two types of errors. We can use confusion matrix to quantify those\n\n\n\n\nPredicted: YES\nPredicted: NO\n\n\n\n\nActual: YES\nTPR\nFNR\n\n\nActual: NO\nFPR\nTNR\n\n\n\nTrue positive rate (TPR) is the sensitivity and false positive rate (FPR) is the specificity of our predictive model\nExample: Evolute the previous model Accuracy = 0.96\n\n\n\n\nPredicted: YES\nPredicted: NO\n\n\n\n\nActual: YES\nTPR=0.6\nFNR=0.4\n\n\nActual: NO\nFPR=0.03\nTNR=0.97\n\n\n\nI used \\(p=0.2\\) as a cut-off. What if I use smaller or larger \\(p\\), e.g. \\(p=0\\)?\nROC Curve Shows what happens for different cut-off values\nFirst, we define a function, that calculates the ROC\n\nroc &lt;- function(p,y, ...){\n  y &lt;- factor(y)\n  n &lt;- length(p)\n  p &lt;- as.vector(p)\n  Q &lt;- p &gt; matrix(rep(seq(0,1,length=100),n),ncol=100,byrow=TRUE)\n  specificity &lt;- colMeans(!Q[y==levels(y)[1],])\n  sensitivity &lt;- colMeans(Q[y==levels(y)[2],])\n  plot(1-specificity, sensitivity, type=\"l\", ...)\n  abline(a=0,b=1,lty=2,col=8)\n}\n\n\n## roc curve and fitted distributions\npred = predict.glm(glm.fit,newdata = Default, type=\"response\")\ndefault = y\nroc(p=pred, y=Default$default, bty=\"n\", main=\"in-sample\")\n# our 1/5 rule cutoff\npoints(x= 1-mean((pred&lt;.2)[default==0]), \n       y=mean((pred&gt;.2)[default==1]), \n       cex=1.5, pch=20, col='red') \n## a standard `max prob' (p=.5) rule\npoints(x= 1-mean((pred&lt;.5)[default==0]), \n       y=mean((pred&gt;.5)[default==1]), \n       cex=1.5, pch=20, col='blue') \nlegend(\"bottomright\",fill=c(\"red\",\"blue\"),\n       legend=c(\"p=1/5\",\"p=1/2\"),bty=\"n\",title=\"cutoff\")\n\n\n\n\n\n\n\n\nLook at other predictors\n\nglm.fit=glm(default~balance+income+student,data=Default,family=binomial)\nsummary(glm.fit)\n\n\nCall:\nglm(formula = default ~ balance + income + student, family = binomial, \n    data = Default)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.09e+01   4.92e-01  -22.08   &lt;2e-16 ***\nbalance      5.74e-03   2.32e-04   24.74   &lt;2e-16 ***\nincome       3.03e-06   8.20e-06    0.37   0.7115    \nstudentYes  -6.47e-01   2.36e-01   -2.74   0.0062 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.5  on 9996  degrees of freedom\nAIC: 1580\n\nNumber of Fisher Scoring iterations: 8\n\n\nStudent is significant!?\nStudent vs Balance\n\nboxplot(balance~student,data=Default, col = Default$student, ylab = \"balance\")\n\n\n\n\n\n\n\n\nLet’s adjust for balance\n\nx2 = data.frame(balance = seq(1000,2500,length.out = 100), student = as.factor(rep(\"No\",100)), income=rep(40,100))\ny1 = predict.glm(glm.fit,newdata = x1, type=\"response\")\ny2 = predict.glm(glm.fit,newdata = x2, type=\"response\")\nplot(x1$balance,y1, type='l', col=\"red\", xlab=\"Balance\", ylab = \"P(Default)\")\nlines(x2$balance,y2, type='l', col=\"black\")\nlegend(\"topleft\",bty=\"n\", legend=c(\"Not Student\", \"Student\"), col=c(\"black\",\"red\"), lwd=2)\n\n\n\n\n\n\n\n\n\n4.2.1 Estimating the Regression Coefficients\nThe coefficients \\(\\beta = (\\beta_0, \\beta_1)\\) can be estimated using maximum likelihood method we discussed when talked about linear regression.\nThe model we derived above, gives us probability of \\(y\\), given \\(x\\) \\[\np(y\\mid x) = \\dfrac{e^{\\beta^Tx}}{1+e^{\\beta^Tx}}\n\\]\nNow the problem is as follows \\[\n\\underset{\\beta}{maximize} \\prod_{i:y_i = 1}p(x_i)\\prod_{j:y_j = 0}(1-p(x_j)).\n\\]\nMaximum likelihood is a very general approach that is used to fit many of the non-linear models.\n\n\n4.2.2 Choosing \\(p\\) and Evaluating Quality of Classifier\nIn logistic regression we use the logistic function to calculate a probability of \\(y = 1\\) \\[\np(y=1\\mid x) = \\dfrac{e^{\\beta^Tx}}{1+e^{\\beta^Tx}}.\n\\] Then, to predict a label we use a rule if \\(y&lt;p\\), predict 0, and predict 1 otherwise. Now we answer the question of how to choose the cut-off value \\(p\\). We show it through an example.\n\nExample 4.4 (Load Default) Assume a bank is using a logistic regression model to predict probability of a loan default and would issue a loan if \\(a = p(y=1) &lt; p\\). Here \\(p\\) is the level of risk bank is willing to take. If bank chooses \\(p=1\\) and gives loans to everyone it is likely to loose a lot of money from defaulted accounts. If it chooses \\(p = 0\\) it will not issue loan to anyone and wont make any money. In order to choose an appropriate \\(p\\), we need to know what are the risks. Assume, bank makes $0.25 on every $1 borrowed in interest in fees and loose the entire amount of $1 if account defaults. This leads to the following pay-off matrix\n\nPay-off matrix for a loan\n\n\n\npayer\ndefaulter\n\n\n\n\nloan\n-0.25\n1\n\n\nno load\n0\n0\n\n\n\nThen, given \\(a = p(y=1)\\), the expected profit is profit = \\(0.25(1-a) - a\\) to maintain a positive profit we need to choose \\[\n0.25(1-a) - a &gt;0 \\iff -1.25a &gt; -0.25 \\iff a &lt; 0.25 /1.25= 0.2\n\\] Thus, by choosing cutoff to be 0.2 or less, we guarantee to make profit on our loans.\nTo evaluate a binary classification predictor, we will use confusion matrix. It is shows numbers of correct predictions by the model (true positives and true negatives) and incorrect ones (false positive and false negatives). Say, we have a model that predicts weather person has a disease or not and we evaluate this model using 200 samples (\\(n=200\\)) with 60 being labeled as 0 (NO) and 140 labeled as 1 (YES) and model predicted correctly 130 YES labeled observations and 50 NOs.\n\n\n\n\nPredicted: YES\nPredicted: NO\n\n\n\n\nActual: YES\nTP = 130\nFN = 10\n\n\nActual: NO\nFP = 10\nTN = 50\n\n\n\nSometimes, it is convenient to used rates rather than absolute counts and we compute\n\n\n\n\nPredicted: YES\nPredicted: NO\n\n\n\n\nActual: YES\nTPR = 130/140\nFNR = 10/140\n\n\nActual: NO\nFPR = 10/60\nTNR = 50/60\n\n\n\nTrue positive rate (TPR) is nothing but the sensitivity and false positive rate (FPR) is the specificity of our predictive model. Accuracy, which is the percent of correct predictions is another metric can be used to evaluate a classifier. \\[\n\\mbox{Accuracy} = \\dfrac{\\mbox{TP + TN}}{n}.\n\\] The error rate is opposite to accuracy \\[\n\\mbox{Error rate}  = 1- \\mbox{Accuracy}\n\\]\nFor a logistic regression, the confusion matrix will be different for different choices of the cut-off values \\(p\\). If we would like to understand the performance of the model for different values of \\(p\\) we can split an ROC curve, which plots pairs of TPR and FPR for different values of \\(p\\). Saw we take a sequence of 11 values \\(p \\in {0, 0.1, 0.2,\\ldots,1}\\) and we evaluate TPR and FPR for those 10 values and plot those pairs on a 2D plot then we will get the ROC curve. A few facts about the ROC curve:\n\nIf we set \\(p=0\\), then any model will always predict NO, this leads to FPR=0 and TPR=0\nIf we set \\(p=1\\) and model always predicts YES, then we get FPR = 1 and TPR = 1\nIf we have an “ideal\" model then for any \\(0&lt;p&lt;1\\) we will have FPR = 0 and TPR = 1.\nA naive model that uses coin flip to classify will have FPR = 1/2 and TPR = 1/2\nAn ROC curve for an model will lie in-between the ideal curve and naive curve. If your model is worse then naive, it is not a good model. And your model cannot be better than an ideal model.\n\n\n\nExample 4.5 (Default) Let’s consider an example. We want to predict default given attributes of the loan applicant. We have 1000 observations of 9 variables\n\ncredit = read.csv(\"../data/credit.csv\")\ncredit$history = factor(credit$history, levels=c(\"A30\",\"A31\",\"A32\",\"A33\",\"A34\"))\nlevels(credit$history) = c(\"good\",\"good\",\"poor\",\"poor\",\"terrible\")\ncredit$foreign &lt;- factor(credit$foreign, levels=c(\"A201\",\"A202\"), labels=c(\"foreign\",\"german\"))\ncredit$rent &lt;- factor(credit$housing==\"A151\")\ncredit$purpose &lt;- factor(credit$purpose, levels=c(\"A40\",\"A41\",\"A42\",\"A43\",\"A44\",\"A45\",\"A46\",\"A47\",\"A48\",\"A49\",\"A410\"))\nlevels(credit$purpose) &lt;- c(\"newcar\",\"usedcar\",rep(\"goods/repair\",4),\"edu\",NA,\"edu\",\"biz\",\"biz\")\n\ncredit &lt;- credit[,c(\"Default\", \"duration\", \"amount\",\n                    \"installment\", \"age\", \"history\",\n                    \"purpose\", \"foreign\", \"rent\")]\nknitr::kable(head(credit))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefault\nduration\namount\ninstallment\nage\nhistory\npurpose\nforeign\nrent\n\n\n\n\n0\n6\n1169\n4\n67\nterrible\ngoods/repair\nforeign\nFALSE\n\n\n1\n48\n5951\n2\n22\npoor\ngoods/repair\nforeign\nFALSE\n\n\n0\n12\n2096\n2\n49\nterrible\nedu\nforeign\nFALSE\n\n\n0\n42\n7882\n2\n45\npoor\ngoods/repair\nforeign\nFALSE\n\n\n1\n24\n4870\n3\n53\npoor\nnewcar\nforeign\nFALSE\n\n\n0\n36\n9055\n2\n35\npoor\nedu\nforeign\nFALSE\n\n\n\n\n\nWe build a logistic regression model using all of the 8 predictors and their interactions\ncredscore = glm(Default~.^2,data=credit,family=binomial)\nThen we plot ROC curve (FPR vs TPR) for different values of \\(p\\) and compare the curve with the naive\n\n\n\n\n\nROC curve for logistic regression model for repearting defualts",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classification: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#multinomial-logistic-regression",
    "href": "13-logistic.html#multinomial-logistic-regression",
    "title": "4  Classification: Logistic Regression",
    "section": "4.3 Multinomial logistic regression",
    "text": "4.3 Multinomial logistic regression\nSoftmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: \\(y_i \\in \\{0,1\\}\\) . We used such a classifier to distinguish between two kinds of hand-written digits. Softmax regression allows us to handle \\(y_i \\in \\{1,\\ldots ,K\\}\\) where \\(K\\) is the number of classes. Our model took the form: \\[\nf(x\\mid \\beta)=\\dfrac{1}{1+\\exp(-\\beta^Tx)}~,\n\\] and the model parameters \\(\\beta\\) were trained to minimize the loss function (negative log-likelihood) \\[\nJ(\\beta) = -\\left[ \\sum_{i=1}^m y_i \\log f(x\\mid \\beta) + (1-y_i) \\log (1-f(x\\mid \\beta)) \\right]\n\\]\nGiven a test input \\(x\\), we want our model to estimate the probability that \\(p(y=k|x)\\) for each value of \\(k=1,\\ldots ,K\\) Thus, our model will output a \\(K\\)-dimensional vector (whose elements sum to 1) giving us our K estimated probabilities. Concretely, our model \\(f(x\\mid \\beta)\\) takes the form: \\[\n\\begin{aligned}\nf(x\\mid \\beta) =\n\\begin{bmatrix}\np(y = 1 | x; \\beta) \\\\\np(y = 2 | x; \\beta) \\\\\n\\vdots \\\\\np(y = K | x; \\beta)\n\\end{bmatrix}\n=\n\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\beta_k^T x) }}\n\\begin{bmatrix}\n\\exp(\\beta_1^{T} x ) \\\\\n\\exp(\\beta_2^{T} x ) \\\\\n\\vdots \\\\\n\\exp(\\beta_k^T x ) \\\\\n\\end{bmatrix}\\end{aligned}\n\\]\nHere \\(\\beta_i \\in R^n, i=1,\\ldots,K\\) are the parameters of our model. Notice that the term \\(1/ \\sum_{j=1}^{K}{\\exp(\\beta_k^T x) }\\) normalizes the distribution, so that it sums to one.\nFor convenience, we will also write \\(\\beta\\) to denote all the parameters of our model. When you implement softmax regression, it is usually convenient to represent \\(\\beta\\) as an \\(n\\)-by-\\(K\\) matrix obtained by concatenating \\(\\beta_1,\\beta_2,\\ldots ,\\beta_K\\) into columns, so that \\[\n\\beta = \\left[\\begin{array}{cccc}| & | & | & | \\\\\n\\beta_1 & \\beta_2 & \\cdots & \\beta_K \\\\\n| & | & | & |\n\\end{array}\\right].\n\\]\nWe now describe the cost function that we’ll use for softmax regression. In the equation below, \\(1\\) is the indicator function, so that \\(1\\)(a true statement)=1, and \\(1\\)(a false statement)=0. For example, 1(2+3 &gt; 4) evaluates to 1; whereas 1(1+1 == 5) evaluates to 0. Our cost function will be: \\[\n\\begin{aligned}\nJ(\\beta) = - \\left[ \\sum_{i=1}^{m} \\sum_{k=1}^{K}  1\\left\\{y_i = k\\right\\} \\log \\frac{\\exp(\\beta_k^T x_i)}{\\sum_{j=1}^K \\exp(\\beta_k^T x_i)}\\right]\n\\end{aligned}\n\\]\nNotice that this generalizes the logistic regression cost function, which could also have been written: \\[\n\\begin{aligned}\nJ(\\beta) &= - \\left[ \\sum_{i=1}^m   (1-y_i) \\log (1-f(x\\mid \\beta)) + y_i \\log f(x\\mid \\beta) \\right] \\\\\n&= - \\left[ \\sum_{i=1}^{m} \\sum_{k=0}^{1} 1\\left\\{y_i = k\\right\\} \\log p(y_i = k | x_i ; \\beta) \\right]\\end{aligned}\n\\] The softmax cost function is similar, except that we now sum over the \\(K\\) different possible values of the class label. Note also that in softmax regression, we have that \\[\np(y_i = k | x_i ; \\beta) = \\frac{\\exp(\\beta_k^T x_i)}{\\sum_{j=1}^K \\exp(\\beta_k^T x_i) }.\n\\]\nSoftmax regression has an unusual property that it has a redundant set of parameters. To explain what this means, suppose we take each of our parameter vectors \\(\\beta_j\\), and subtract some fixed vector \\(\\psi\\). Our model now estimates the class label probabilities as \\[\n\\begin{aligned}\np(y_i = k | x_i ; \\beta)\n&= \\frac{\\exp((\\beta_k-\\psi)^T x_i)}{\\sum_{j=1}^K \\exp( (\\beta_j-\\psi)^T x_i)}  \\\\\n&= \\frac{\\exp(\\beta_k^T x_i) \\exp(-\\psi^T x_i)}{\\sum_{j=1}^K \\exp(\\beta_k^T x_i) \\exp(-\\psi^T x_i)} \\\\\n&= \\frac{\\exp(\\beta_k^T x_i)}{\\sum_{j=1}^K \\exp(\\beta_k^T x_i)}.\\end{aligned}\n\\] In other words, subtracting \\(\\psi\\) does not affect our model’ predictions at all! This shows that softmax regression’s parameters are redundant. More formally, we say that our softmax model is overparameterized, meaning that for any model we might fit to the data, there are multiple parameter settings that give rise to exactly the same model function \\(f(x \\mid \\beta)\\) mapping from inputs \\(x\\) to the predictions.\nFurther, if the cost function \\(J(\\beta)\\) is minimized by some setting of the parameters \\((\\beta_1,\\ldots,\\beta_K)\\), then it is also minimized by \\((\\beta_1-\\psi,\\ldots,\\beta_K-\\psi)\\) for any value of \\(\\psi\\). Thus, the minimizer of \\(J(\\beta)\\) is not unique. Interestingly, \\(J(\\beta)\\) is still convex, and thus gradient descent will not run into local optima problems. But the Hessian is singular/non-invertible, which causes a straightforward implementation of Newton’s method to run into numerical problems. We can just set \\(\\psi\\) to \\(\\beta_i\\) and remove \\(\\beta_i\\).\n\nExample 4.6 (LinkedIn Study) How to Become an Executive(Irwin 2016; Gan and Fritzler 2016)?\nLogistic regression was used to analyze the career paths of about \\(459,000\\) LinkedIn members who worked at a top 10 consultancy between 1990 and 2010 and became a VP, CXO, or partner at a company with at least 200 employees. About \\(64,000\\) members reached this milestone, \\(\\hat{p} = 0.1394\\), conditional on making it into the database. The goals of the analysis were the following\n\nLook at their profiles – educational background, gender, work experience, and career transitions.\nBuild a predictive model of the probability of becoming an executive\nProvide a tool for analysis of “what if” scenarios. For example, if you are to get a master’s degree, how your jobs perspectives change because of that.\n\nLet’s build a logistic regression model with \\(8\\) key features (a.k.a. covariates): \\[\n\\log\\left ( \\frac{p}{1-p} \\right ) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_8x_8\n\\]\n\n\\(p\\): Probability of “success” – reach VP/CXO/Partner seniority at a company with at least 200 employees.\nFeatures to predict the “success” probability: \\(x_i (i=1,2,\\ldots,8)\\).\n\n\n\nVariable\nParameters\n\n\n\n\n\\(x_1\\)\nMetro region: whether a member has worked in one of the top 10 largest cities in the U.S. or globally.\n\n\n\\(x_2\\)\nGender: Inferred from member names: ‘male’, or ‘female’\n\n\n\\(x_3\\)\nGraduate education type: whether a member has an MBA from a top U.S. program / a non-top program / a top non-U.S. program / another advanced degree\n\n\n\\(x_4\\)\nUndergraduate education type: whether a member has attended a school from the U.S. News national university rankings / a top 10 liberal arts college /a top 10 non-U.S. school\n\n\n\\(x_5\\)\nCompany count: # different companies in which a member has worked\n\n\n\\(x_6\\)\nFunction count: # different job functions in which a member has worked\n\n\n\\(x_7\\)\nIndustry sector count: # different industries in which a member has worked\n\n\n\\(x_8\\)\nYears of experience: # years of work experience, including years in consulting, for a member.\n\n\n\n\nThe following estimated \\(\\hat\\beta\\)s of features were obtained. With a sample size of 456,000 thy are measured rather accurately. Recall, given each location/education choice in the “Choice and Impact” is a unit change in the feature.\n\nLocation: Metro region: 0.28\nPersonal: Gender(Male): 0.31\nEducation: Graduate education type: 1.16, Undergraduate education type: 0.22\nWork Experience: Company count: 0.14, Function count: 0.26, Industry sector count: -0.22, Years of experience: 0.09\n\nHere are three main findings\n\nWorking across job functions, like marketing or finance, is good. Each additional job function provides a boost that, on average, is equal to three years of work experience. Switching industries has a slight negative impact. Learning curve? lost relationships?\nMBAs are worth the investment. But pedigree matters. Top five program equivalent to \\(13\\) years of work experience!!!\nLocation matters. For example, NYC helps.\n\nWe can also personalize the prediction for predict future possible future executives. For example, Person A (p=6%): Male in Tulsa, Oklahoma, Undergraduate degree, 1 job function for 3 companies in 3 industries, 15-year experience.\nPerson B (p=15%): Male in London, Undergraduate degree from top international school, Non-MBA Master, 2 different job functions for 2 companies in 2 industries, 15-year experience.\nPerson C (p=63%): Female in New York City, Top undergraduate program, Top MBA program, 4 different job functions for 4 companies in 1 industry, 15-year experience.\nLet’s re-design Person B.\nPerson B (p=15%): Male in London, Undergraduate degree from top international school, Non-MBA Master, 2 different job functions for 2 companies in 2 industries, 15-year experience.\n\nWork in one industry rather than two. Increase \\(3\\)%\nUndergrad from top \\(10\\) US program rather than top international school. \\(3\\)%\nWorked for \\(4\\) companies rather than \\(2\\). Another \\(4\\)%\nMove from London to NYC. \\(4\\)%\nFour job functions rather than two. \\(8\\)%. A \\(1.5\\)x effect.\nWorked for \\(10\\) more years. \\(15\\)%. A \\(2\\)X effect.\n\nChoices and Impact (Person B) are shown below",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classification: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#imbalanced-data",
    "href": "13-logistic.html#imbalanced-data",
    "title": "4  Classification: Logistic Regression",
    "section": "4.4 Imbalanced Data",
    "text": "4.4 Imbalanced Data\nOften, you have much more observations with a specific label, such a sample is called imbalanced. You should avoid using accuracy as a metric to choose a model. Say, you have a binary classification problem with 95% of samples labeled as 1. Then a naive classifier that assigns label 1 for each input will be 95% accurate. An ROC curve and area under the curve (AUC) metric derived from it should be used. Alternatively, you can use F1 sore, which combines precision and recall \\[\nF1 = 2\\dfrac{\\mathrm{precision} \\times \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}\n\\]\nA modeler should consider synthetically generating more samples of class with small number of observation, e.g. bootstrap or using a generative model or subsampling observations with major label if data set is large enough.\n\n4.4.1 Kernel Trick\nKernel trick is a method of using a linear classifier to solve a non-linear problem. The idea is to map the data into a higher dimensional space, where it becomes linearly separable. The kernel trick is to use a kernel function \\(K(x_i,x_j)\\) to calculate the inner product of two vectors in the higher dimensional space without explicitly calculating the mapping \\(\\phi(x_i)\\) and \\(\\phi(x_j)\\). The kernel function is defined as \\(K(x_i,x_j) = \\phi(x_i)^T\\phi(x_j)\\). The most popular kernel functions are polynomial kernel \\(K(x_i,x_j) = (x_i^Tx_j)^d\\) and Gaussian kernel \\(K(x_i,x_j) = \\exp(-\\gamma||x_i-x_j||^2)\\). The kernel trick is used in Support Vector Machines (SVM) and Gaussian Processes (GP).\n\n\nCode\ngencircledata = function(numSamples,radius,noise) {\n    d = matrix(0,ncol = 3, nrow = numSamples); # matrix to store our generated data\n    # Generate positive points inside the circle.\n    for (i in 1:(numSamples/2) ) {\n    r = runif(1,0, radius * 0.4);\n    angle = runif(1,0, 2 * pi);\n    x = r * sin(angle);\n    y = r * cos(angle);\n    noiseX = runif(1,-radius, radius) * noise;\n    noiseY = runif(1,-radius, radius) * noise;\n    d[i,] = c(0,x,y)\n    }\n\n    # Generate negative points outside the circle.\n    for (i in (numSamples/2+1):numSamples ) {\n    r = runif(1,radius * 0.8, radius);\n    angle = runif(1,0, 2 * pi);\n    x = r * sin(angle);\n    y = r * cos(angle);\n    noiseX = runif(1,-radius, radius) * noise;\n    noiseY = runif(1,-radius, radius) * noise;\n    d[i,] = c(1,x,y)\n    }\n    colnames(d) = c(\"label\", \"x1\", \"x2\")\n    return(d)\n}\n\n\n\nd = gencircledata(numSamples=200, radius=1, noise=0.001)\nplot(d[,2],d[,3], col=d[,1]+1, pch=19, xlab=\"x\", ylab=\"y\")\n\n\n\n\n\n\n\nFigure 4.1\n\n\n\n\n\nThe data on the left in Figure Figure 4.1 is clearly not linearly separable. However, if we map it to a three-dimensional space using the transformation: \\[\n\\begin{aligned}\n\\phi: R^{2} & \\longrightarrow R^{3} \\\\\n\\left(x_{1}, x_{2}\\right) & \\longmapsto\\left(z_{1}, z_{2}, z_{3}\\right)=\\left(x_{1}^{2}, \\sqrt{2} x_{1} x_{2}, x_{2}^{2}\\right),\n\\end{aligned}\n\\] and attempt to linearly separate the transformed data, the decision boundaries become hyperplanes in \\(R^{3}\\), expressed as \\(\\omega^{T} z + b = 0\\). In terms of the original variables \\(x\\), these boundaries take the form: \\[\n\\omega_{1} x_{1}^{2} + \\omega_{2} \\sqrt{2} x_{1} x_{2} + \\omega_{3} x_{2}^{2} = 0,\n\\] which corresponds to the equation of an ellipse. This demonstrates that we can apply a linear algorithm to transformed data to achieve a non-linear decision boundary with minimal effort.\nNow, consider what the algorithm is actually doing. It relies solely on the Gram matrix \\(K\\) of the data. Once \\(K\\) is computed, the original data can be discarded: \\[\n\\begin{aligned}\nK & = \\left[\\begin{array}{ccc}\nx_{1}^{T} x_{1} & x_{1}^{T} x_{2} & \\cdots \\\\\nx_{2}^{T} x_{1} & \\ddots & \\\\\n\\vdots & &\n\\end{array}\\right]_{n \\times n} = X X^{T}, \\\\\n\\text{where} \\quad X & = \\left[\\begin{array}{c}\nx_{1}^{T} \\\\\n\\vdots \\\\\nx_{n}^{T}\n\\end{array}\\right]_{n \\times d}.\n\\end{aligned}\n\\] Here, \\(X\\), which contains all the data, is referred to as the design matrix.\nWhen we map the data using \\(\\phi\\), the Gram matrix becomes: \\[\nK = \\left[\\begin{array}{ccc}\n\\phi\\left(x_{1}\\right)^{T} \\phi\\left(x_{1}\\right) & \\phi\\left(x_{1}\\right)^{T} \\phi\\left(x_{2}\\right) & \\cdots \\\\\n\\phi\\left(x_{2}\\right)^{T} \\phi\\left(x_{1}\\right) & \\ddots & \\\\\n\\vdots & &\n\\end{array}\\right].\n\\]\nLet us compute these inner products explicitly. For vectors \\(r\\) and \\(s\\) in \\(R^{3}\\) corresponding to \\(a\\) and \\(b\\), respectively: \\[\n\\begin{aligned}\n\\langle r, s \\rangle & = r_{1} s_{1} + r_{2} s_{2} + r_{3} s_{3} \\\\\n& = a_{1}^{2} b_{1}^{2} + 2 a_{1} a_{2} b_{1} b_{2} + a_{2}^{2} b_{2}^{2} \\\\\n& = \\langle a, b \\rangle^{2}.\n\\end{aligned}\n\\]\nThus, instead of explicitly mapping the data via \\(\\phi\\) and then computing the inner product, we can compute it directly in one step, leaving the mapping \\(\\phi\\) implicit. In fact, we do not even need to know \\(\\phi\\) explicitly; all we require is the ability to compute the modified inner product. This modified inner product is called a kernel, denoted \\(K(x, y)\\). The matrix \\(K\\), which contains the kernel values for all pairs of data points, is also referred to as the kernel matrix.\nSince the kernel itself is the primary object of interest, rather than the mapping \\(\\phi\\), we aim to characterize kernels without explicitly relying on \\(\\phi\\). Mercer’s Theorem provides the necessary framework for this characterization.\nLet’s implement it\n\nlibrary(\"scatterplot3d\")\nphi &lt;- function(x1, x2) {\n    z1 &lt;- x1^2\n    z2 &lt;- sqrt(2) * x1 * x2\n    z3 &lt;- x2^2\n    return(cbind(z1, z2, z3))\n}\n\n# Generate sample 2D data (you can replace this with your actual data)\n\n\n# Apply the transformation\ntransformed_data &lt;- phi(d[,2], d[,3])\nscatterplot3d(transformed_data, color = ifelse(d[,1] == 0, \"red\", \"blue\"), pch = 19,\n                xlab = \"z1 (x1^2)\", ylab = \"z2 (sqrt(2) * x1 * x2)\", zlab = \"z3 (x2^2)\",\n                main = \"3D Scatter Plot of Transformed Data\", angle=222, grid=FALSE, box=FALSE)\n\n\n\n\n\n\n\n\n\nExample 4.7 (Formula One) As described in the Artificial Intelligence in Formula 1 article, Formula One teams are increasingly leveraging AI and machine learning to optimize race strategies. The article highlights how teams collect massive amounts of data during races - with 300 sensors per car generating millions of data points over 200-mile races. This data includes critical variables like fuel load, tire degradation, weight effects, and pit stop timing that must be optimized in real-time.\nThe key innovation is moving from pre-race strategy planning to in-race dynamic optimization using cloud computing platforms like AWS. Teams run Monte Carlo simulations of all cars and traffic situations to continuously update their strategy recommendations. This allows them to make optimal decisions about when to pit, which tires to use, and how to manage fuel consumption based on real-time race conditions rather than static pre-race plans.\nThe article emphasizes that the best strategies can vary dramatically from moment to moment during a race, making real-time AI-powered decision making crucial for competitive advantage. Teams are limited to 60 data scientists, so they must rely heavily on automated machine learning systems to process the vast amounts of sensor data and generate actionable insights during races.\nThe CNBC article highlights how Formula One championships are increasingly being determined by technological innovation rather than just driver skill. F1 success depends heavily on advanced data analytics, machine learning algorithms, and real-time computing capabilities. Key technological factors driving F1 success include real-time data processing where teams process millions of data points from hundreds of sensors per car during races. AI-powered strategy optimization uses machine learning algorithms to continuously analyze race conditions and recommend optimal pit stop timing, tire choices, and fuel management. Cloud computing infrastructure allows teams to rely on platforms like AWS to run complex simulations and data analysis during races. Predictive modeling employs advanced algorithms to predict tire degradation, fuel consumption, and competitor behavior. Simulation capabilities enable teams to run thousands of Monte Carlo simulations to optimize race strategies.\nThe technological arms race in Formula One has led to significant regulatory challenges. To maintain competitive balance and prevent larger teams from gaining insurmountable advantages through unlimited technological investment, F1 has implemented strict caps on the number of engineers and data scientists that teams can employ. Currently, teams are limited to 60 data scientists and engineers, which forces them to be highly strategic about their technological investments and resource allocation. This cap creates an interesting dynamic where teams must balance the need for sophisticated AI and machine learning capabilities with the constraint of limited human resources. As a result, teams are increasingly turning to automated systems and cloud-based solutions to maximize their technological capabilities within these constraints. The cap also levels the playing field somewhat, ensuring that success depends more on the efficiency and innovation of the technology rather than simply having more engineers and data scientists than competitors.\n\n\n\n\n\nGan, Link, and Alan Fritzler. 2016. “How to Become an Executive.”\n\n\nIrwin, Neil. 2016. “How to Become a C.E.O.? The Quickest Path Is a Winding One.” The New York Times, September.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classification: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html",
    "href": "13-theoryai.html",
    "title": "5  Theory of AI",
    "section": "",
    "text": "5.1 Penalty and Regularisation\nWe use observed input-output pairs \\((x_i,y_i)\\) to learn a function \\(f\\) that maps \\(x_i\\) to \\(y_i\\). The goal is to learn a function \\(f\\) that generalizes well to unseen data. We can measure the quality of a function \\(f\\) by its empirical risk, which is the expected loss of \\(f\\) on a new input-output pair \\((x,y)\\):\n\\[\nR(f) = \\sum_{i=1}^N  l(y_i,f(x_i)) + \\lambda \\phi(f)\n\\] where \\(l\\) is a loss function, \\(\\phi\\) is a regularization function, and \\(\\lambda\\) is a regularization parameter. The loss function \\(l\\) measures the difference between the output of the function \\(f\\) and the true output \\(y\\). The regularization function \\(\\phi\\) measures the complexity of the function \\(f\\). The regularization parameter \\(\\lambda\\) controls the tradeoff between the loss and the complexity.\nThe loss is simply a negative log-likelihood from a probabilistic model specified for the data generating process. For example, when \\(y\\) is numeric and \\(y_i \\mid x_i \\sim N(f(x_i),\\sigma^2)\\), we get the squared loss \\(l(y,f(x)) = (y-f(x))^2\\). When \\(y_i\\in \\{0,1\\}\\) is binary, we use the logistic loss \\(l(y,f(x)) = \\log(1+\\exp(-yf(x)))\\).\nThe Bayesian version of the risk is the Bayes risk, which is the expected loss of \\(f\\) on a new input-output pair \\((x,y)\\) under the posterior distribution of \\(f\\): \\[\nR(f) = \\int\\int\\int l(y,f(x)) p(y|x,f) p(f)dydfdx\n\\] where \\(p(y|x,f)\\) is the predictive posterior distribution of \\(y\\) given \\(x\\) and \\(f\\), and \\(p(f)\\) is the prior distribution of \\(f\\). When \\(f\\) is a parametric model parametrized by \\(\\theta\\), \\(p(f)\\) becomes the prior distribution of the parameters of the model \\(p(\\theta)\\).\nTo find the optimal predictor that minimizes this risk (the Bayes predictor), we focus on minimizing the expected loss for a specific input \\(x\\).\n\\[\n\\arg\\min_f \\int l(y,f(x)) p(f\\mid x,y) df\n\\]\nThere is a duality between using regularization term in optimisation problem and assuming a prior distribution over the parameters of the model \\(f\\). The regularization parameter \\(\\lambda\\) is related to the variance of the prior distribution. When \\(\\lambda=0\\), the function \\(f\\) is the maximum likelihood estimate of the parameters. When \\(\\lambda\\) is large, the function \\(f\\) is the prior mean of the parameters. When \\(\\lambda\\) is infinite, the function \\(f\\) is the prior mode of the parameters. When \\(\\lambda\\) is negative, the function \\(f\\) is the posterior mean of the parameters. When \\(\\lambda\\) is very negative, the function \\(f\\) is the posterior mode of the parameters.\nThe goal is to find a function \\(f\\) that minimizes the risk \\(R(f)\\). This is called the empirical risk minimization problem. Finding minimum is a difficult problem when the risk function \\(R(f)\\) is non-convex. In practice, we often use gradient descent to find a local minimum of the risk function \\(R(f)\\).\nThere are multiple ways to choose the penalty term \\(\\phi(f)\\). Sections below describe the most popular approaches.\nThe problem of finding a good model boils down to finding \\(\\phi\\) that minimize some form of Bayes risk for the problem at hand.\nThere are a number of commonly used penalty functions (a.k.a. log prior density). For example, the $ l^2$-norm corresponds to s normal prior. The resulting Bayes rule will take the form of a shrinkage estimator, a weighted combination between data and prior beliefs about the parameter. An $ l^1 $-norm will induce a sparse solution in the estimator and can be used an a variable selection operator. The $ l_0 $-norm directly induces a subset selection procedure.\nThe amount of regularisation $ $ gauges the trade-off between the compromise between the observed data and the initial prior beliefs.\nThere are two main approaches to finding a good model:\nThe posterior is given by Bayes rule \\[\np( \\theta | y ) = \\frac{ f( y | \\theta ) p( \\theta ) }{ m(y) } \\; \\; {\\rm where} \\; \\; m(y) = \\int f( y| \\theta ) p( \\theta ) d \\theta\n\\] Here $ m(y) $ is the marginal beliefs about the data. This can also be used to choose the amount of regularisation via the type II maximum likelihood estimator (MMLE) defined by \\[\n\\hat{\\tau} = \\arg \\max \\log m( y | \\tau )\n\\] where again $ m( y | ) = f( y | ) p( | ) $.\nFor example, in the normal-normal model, with $ $, we can integrate out the high dimensional $ $ and find $ m( y | ) $ in closed form as $ y_i N ( 0 , ^2 + ^2 ) $ \\[\nm( y | \\tau ) = ( 2 \\pi)^{-n/2} ( \\sigma^2 + \\tau^2 )^{- n/2}  \\exp \\left ( - \\frac{ \\sum y_i^2 }{ 2 ( \\sigma^2 + \\tau^2) }\n\\] The original JS estimator shrinks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want o shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS is \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), which serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate proper from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\nRather than having to perform high dimensional integration with the likes of MCMC etc, a common approach is to use a maximum a posteriori (MAP) estimator defined by \\[\n\\hat{\\theta} = \\arg \\max \\log p ( \\theta | y )\n\\] This can directly lead to sparsity as in the case of $ _1 $-norm optimisation.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#penalty-and-regularisation",
    "href": "13-theoryai.html#penalty-and-regularisation",
    "title": "5  Theory of AI",
    "section": "",
    "text": "Full Bayes: This approach places a prior distribution on the parameters and computes the full posterior distribution.\n\n\n\n\nRegularization Methods: These approaches add penalty terms to the objective function to control model complexity. Common examples include ridge regression (L2 penalty), lasso regression (L1 penalty), and elastic net (combination of L1 and L2 penalties).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#normal-means-problem",
    "href": "13-theoryai.html#normal-means-problem",
    "title": "5  Theory of AI",
    "section": "5.2 Normal Means Problem",
    "text": "5.2 Normal Means Problem\nThe canonical problem is estimation in the normal means problem. First, consider the univariate case where the signal, \\(y\\), has mean, $ $, and Gaussian error, $ \\(. The model is\\)$ y = + ; ; {} ; ; N ( 0 , ^2 ) \\[\nFor the prior distribution  we assume a conjugate $ \\theta \\sim N ( \\mu , \\sigma^2 ) $. The optimal Bayes rule under squared error loss\nis given by the posterior mean, $ E( \\theta | y ) $, where\n\\] ( y ) = is only a global shrinkage rule–in the sense that there are no local parameters to learn about sparsity. A simple sparsity example shows the issue with \\(L^2\\)-regularisation. Consider the sparse \\(r\\)-spike shows the problem with focusing solely on rules with the same shrinkage weight (albeit benefiting from pooling of information).\nLet the true parameter value be \\(\\theta_p = \\left ( \\sqrt{d/p} , \\ldots , \\sqrt{d/p} , 0 , \\ldots , 0 \\right )\\). James-Stein is equivalent to the model \\[\ny_i = \\theta_i + \\epsilon_i \\; \\mathrm{ and} \\; \\theta_i \\sim \\mathcal{N} \\left ( 0 , \\tau^2 \\right )\n\\] This dominates the plain MLE but loses admissibility! This is due to the fact that a “plug-in” estimate of global shrinkage \\(\\hat{\\tau}\\) is used.\nFrom a risk perspective \\(E \\Vert \\hat{\\theta}^{JS} - \\theta \\Vert \\leq p , \\forall \\theta\\) showing the inadmissibility of the MLE. At origin the risk is \\(2\\).\n\nExample 5.1 (Stein’s Paradox) Stein’s paradox, as explained Efron and Morris (1977), is a phenomenon in statistics that challenges our intuitive understanding of estimation. The paradox arises when trying to estimate the mean of a multivariate normal distribution. Traditionally, the best guess about the future is usually obtained by computing the average of past events. However, Charles Stein showed that there are circumstances where there are estimators better than the arithmetic average. This is what’s known as Stein’s paradox.\nIn 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution that has uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein’s rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson’s chi-square test with results from a computer simulation.\nIn each of these examples, the mean square error of these rules is less than half that of the sample mean. This result is paradoxical because it contradicts the elementary law of statistical theory. The philosophical implications of Stein’s paradox are also significant. It has influenced the development of shrinkage estimators and has connections to Bayesianism and model selection criteria.\nSuppose that we have \\(n\\) independent observations \\(y_{1},\\ldots,y_{n}\\) from a \\(N\\left(  \\theta,\\sigma^{2}\\right)\\) distribution. The maximum likelihood estimator is \\(\\widehat{\\theta}=\\bar{y}\\), the sample mean. The Bayes estimator is the posterior mean, \\[\n\\widehat{\\theta}=\\mathbb{E}\\left[  \\theta\\mid y\\right]  =\\frac{\\sigma^{2}}{\\sigma^{2}+n}% \\bar{y}.\n\\] The Bayes estimator is a shrinkage estimator, it shrinks the MLE towards the prior mean. The amount of shrinkage is determined by the ratio of the variance of the prior and the variance of the likelihood. The Bayes estimator is also a function of the MLE \\[\n\\widehat{\\theta}=\\frac{\\sigma^{2}}{\\sigma^{2}+n}\\bar{y}+\\frac{n}{\\sigma^{2}+n}\\widehat{\\theta}.\n\\] This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior. The Bayes estimator is a function of the MLE \\[\n\\widehat{\\theta}=\\frac{\\sigma^{2}}{\\sigma^{2}+n}\\bar{y}+\\frac{n}{\\sigma^{2}+n}\\widehat{\\theta}.\n\\] This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior.\n\n\nExample 5.2 (Example: James-Stein for Baseball Batting Averages) We reproduce the baseball batting average example from Efron and Morris (1977). Data below has the number of hits for 18 baseball player after 45 at-beat in 1970 season\n\n# Data source: https://www1.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt\nbaseball = read.csv(\"../data/EfronMorrisBB.txt\", sep = \"\\t\", stringsAsFactors = FALSE) %&gt;% select(LastName,AtBats,BattingAverage,SeasonAverage)\n\nNow, we can eatimate overall mean and variance\n\nmu_hat &lt;- mean(baseball$BattingAverage)\nsigma2_hat &lt;- var(baseball$BattingAverage)\n\nAs well as the posterior mean for each player (James-Stein estimator)\n\nbaseball &lt;- baseball %&gt;%\n  mutate(\n    JS = (sigma2_hat / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * mu_hat +\n      ((BattingAverage * (1 - BattingAverage) / AtBats) / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * BattingAverage\n  )\nkable(baseball)\n\n\n\n\nLastName\nAtBats\nBattingAverage\nSeasonAverage\nJS\n\n\n\n\nClemente\n45\n0.400\n0.352\n0.3358644\n\n\nRobinson\n45\n0.378\n0.306\n0.3237686\n\n\nHoward\n45\n0.356\n0.283\n0.3117929\n\n\nJohnstone\n45\n0.333\n0.238\n0.2994785\n\n\nBerry\n45\n0.311\n0.276\n0.2879767\n\n\nSpencer\n45\n0.311\n0.274\n0.2879767\n\n\nKessinger\n45\n0.289\n0.266\n0.2768343\n\n\nAlvarado\n45\n0.267\n0.224\n0.2661503\n\n\nSanto\n45\n0.244\n0.267\n0.2555955\n\n\nSwaboda\n45\n0.244\n0.233\n0.2555955\n\n\nPetrocelli\n45\n0.222\n0.261\n0.2462289\n\n\nRodriguez\n45\n0.222\n0.225\n0.2462289\n\n\nScott\n45\n0.222\n0.296\n0.2462289\n\n\nUnser\n45\n0.222\n0.258\n0.2462289\n\n\nWilliams\n45\n0.222\n0.251\n0.2462289\n\n\nCampaneris\n45\n0.200\n0.279\n0.2377410\n\n\nMunson\n45\n0.178\n0.302\n0.2303313\n\n\nAlvis\n45\n0.156\n0.183\n0.2242473\n\n\n\n\n\nPlot below shows the observed averages vs. James-Stein estimate\n\nggplot(baseball, aes(x = BattingAverage, y = JS)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"Observed Batting Average\",\n    y = \"James-Stein Estimate\",\n    title = \"Empirical Bayes Shrinkage of Batting Averages (2016)\"\n  )\n\n\n\n\n\n\n\n\nCalculate mean squared error (MSE) for observed and James-Stein estimates\n\nmse_observed &lt;- mean((baseball$BattingAverage - mu_hat)^2)\nmse_js &lt;- mean((baseball$JS - mu_hat)^2)\n\ncat(sprintf(\"MSE (Observed): %.6f\\n\", mse_observed))\n\n## MSE (Observed): 0.004584\n\ncat(sprintf(\"MSE (James-Stein): %.6f\\n\", mse_js))\n\n## MSE (James-Stein): 0.001031\n\n\nWe can see that the James-Stein estimator has a lower MSE than the observed batting averages. This is a demonstration of Stein’s paradox, where the James-Stein estimator, which shrinks the estimates towards the overall mean, performs better than the naive sample mean estimator.\n\na = matrix(rep(1:3, nrow(baseball)), 3, nrow(baseball))\nb = matrix(c(baseball$BattingAverage, baseball$SeasonAverage, baseball$JS),    3, nrow(baseball), byrow=TRUE)\n\nmatplot(a, b, pch=\" \", ylab=\"predicted average\", xaxt=\"n\", xlim=c(0.5, 3.1), ylim=c(0.13, 0.42))\nmatlines(a, b)\ntext(rep(0.7, nrow(baseball)), baseball$BattingAverage, baseball$LastName, cex=0.6)\ntext(1, 0.14, \"First 45\\nat bats\", cex=0.5)\ntext(2, 0.14, \"Average\\nof remainder\", cex=0.5)\ntext(3, 0.14, \"J-S\\nestimator\", cex=0.5)\n\n\n\n\n\n\n\n\nNow if we look at the season dynamics for Clemente\n\n# Data source: https://www.baseball-almanac.com/players/hittinglogs.php?p=clemero01&y=1970\ncl = read.csv(\"../data/clemente.csv\")\nx = cumsum(cl$AB)\ny = cumsum(cl$H)/cumsum(cl$AB)\n# Plot x,y startind from index 2\nind = c(1,2)\nplot(x[-ind],y[-ind], type='o', ylab=\"Betting Average\", xlab=\"Number at Bats\")\n# Add horizontal line for season average 145/412 and add text above line `Seaosn Average`\ntext(200, 145/412 + 0.005, \"Season Average\", col = \"red\")\nabline(h = 145/412, col = \"red\", lty = 2)\n# Ted williams record is .406 in in 1941, so you know the first data points are noise\ntext(200, baseball$JS[1] + 0.005, \"JS\", col = \"red\")\nabline(h = baseball$JS[1], col = \"red\", lty = 2)\ntext(200, baseball$BattingAverage[1] + 0.005, \"After 45 Bets\", col = \"red\")\nabline(h = baseball$BattingAverage[1], col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nThe motivation for the shrinkage prior rather than a flat uniform prior are the following probabilistic arguments.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#ell_2-shrinkage.",
    "href": "13-theoryai.html#ell_2-shrinkage.",
    "title": "5  Theory of AI",
    "section": "5.3 \\(\\ell_2\\) Shrinkage.",
    "text": "5.3 \\(\\ell_2\\) Shrinkage.\nThe original JS estimator shranks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want o shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS uses \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), which serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate proper prior from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\n\n5.3.1 Sparse \\(r\\)-spike problem\nFor the sparse \\(r\\)-spike problem we require a different rule. For a sparse signal, however, \\(\\hat \\theta_{JS}\\) performs poorly when the true parameter is an \\(r\\)-spike where \\(\\theta_r\\) has \\(r\\) coordinates at \\(\\sqrt{p/r}\\) and the rest set at zero with norm \\({\\Vert \\theta_r \\Vert}^2 =p\\).\nThe classical risk satisfies \\(R \\left ( \\hat \\theta_{JS} , \\theta_r \\right ) \\geq p/2\\) where the simple thresholding rule \\(\\sqrt{2 \\ln p}\\) performs with risk \\(\\sqrt{\\ln p}\\) in the \\(r\\)-spike sparse case even though it is inadmissible in MSE for a non-sparse signal. Then is due to the fact that for $ _p $ we have \\[\n\\frac{p \\Vert \\theta \\Vert^2}{p + \\Vert \\theta \\Vert^2} \\leq R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\leq\n2 + \\frac{p \\Vert \\theta \\Vert^2}{ d + \\Vert \\theta \\Vert^2}.\n\\] This implies that \\(R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\geq (p/2)\\). Hence, simple thresholding rule beats James-Stein this with a risk given by \\(\\sqrt{\\log p }\\). This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.\nA Bayes rule that inherits good MSE properties but also simultaneously provides asymptotic minimax estimation risk for sparse signals. HS estimator uniformly dominates the traditional sample mean estimator in MSE and has good posterior concentration properties for nearly black objects. Specifically, the horseshoe estimator attains asymptotically minimax risk rate \\[\n\\sup_{ \\theta \\in l_0[p_n] } \\;\n\\mathbb{E}_{ y | \\theta } \\|\\hat y_{hs} - \\theta \\|^2 \\asymp\np_n \\log \\left ( n / p_n \\right ).\n\\] The “worst’’ \\(\\theta\\) is obtained at the maximum difference between \\(\\left| \\hat \\theta_{HS} - y \\right|\\) where \\(\\hat \\theta_{HS} = \\mathbb{E}(\\theta|y)\\) can be interpreted as a Bayes posterior mean (optimal under Bayes MSE).\nOne such estimator that achieves the optimal minimax rate is the horseshoe estimator proposed by Carvalho, Polson, and Scott (2010).\n\n\n5.3.2 Efron Example\nEfron provide an example which shows the importance of specifying priors in high dimensions. The key idea behind James-Stein shrinkage is that one when one can “borrow strength” across components. In this sense the multivariate parameter estimation problem is easier than the univariate one.\nStein’s phenomenon where \\(y_i | \\theta_i \\sim N(\\theta_i, 1)\\) and \\(\\theta_i \\sim N(0, \\tau^2)\\) where $ $ illustrates this point well. This leads to the improper “non-informative” uniform prior. The corresponding generalized Bayes rule is the vector of means—which we know is inadmissible. so no regularisation leads to an estimator with poor risk property.\nLet $ || y || = _{i=1}^p y_i^2 \\(. Then, we can make the following probabilistic statements from the model,\\)$ P( | y | &gt; | | ) &gt; \\[\nNow for the posterior, this inequallty is reversed under a flat Lebesgue measure,\n\\] P( | | &gt; | y | ; | ; y ) &gt; $$ which is in conflict with the classical statement. This is a property of the prior which leads to a poor rule (the overall average) and risk.\nThe shrinkage rule (a.k.a. normal prior) where $ ^2 $ is “estimated” from the data avoids this conflict. More precisely, we have \\[\n\\hat{\\theta}(y) = \\left( 1 - \\frac{k-2}{\\|y\\|^2} \\right) y \\quad \\text{and} \\quad E\\left( \\| \\hat{\\theta} - \\theta \\| \\right) &lt; k, \\; \\forall \\theta.\n\\] Hence, when \\(\\|y\\|^2\\) is small the shrinkage factor is more extreme. For example, if \\(k=10\\), \\(\\|y\\|^2=12\\), then \\(\\hat{\\theta} = (1/3) y\\). Now we have the more intuitive result that $ P( | | &gt; | y | ; | ; y ) &lt; $.\nThis shows that careful specification of default priors matter in high dimensions is necessary.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#ell_1-sparsity",
    "href": "13-theoryai.html#ell_1-sparsity",
    "title": "5  Theory of AI",
    "section": "5.4 \\(\\ell_1\\) Sparsity",
    "text": "5.4 \\(\\ell_1\\) Sparsity",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#ell_0-subset-selection",
    "href": "13-theoryai.html#ell_0-subset-selection",
    "title": "5  Theory of AI",
    "section": "5.5 \\(\\ell_0\\) Subset Selection",
    "text": "5.5 \\(\\ell_0\\) Subset Selection\nThe canonical problem is estimaiton of the normal means problem. Here we have \\(y_i = \\theta_i + e_i,~i=1,\\ldots,p\\) and \\(e_i \\sim N(0, \\sigma^2)\\). The goal is to estimate the vector of means \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\). This is also a proxy for non-parametric regression, where \\(\\theta_i = f(x_i)\\). Aslo typically \\(y_i\\) is a mean of \\(n\\) observations, i.e. \\(y_i = \\frac{1}{n} \\sum_{j=1}^n x_{ij}\\). Much has been written on the properties of the Bayes risk as a function of \\(n\\) and \\(p\\). Much work has also been done on the asymptotic properties of the Bayes risk as \\(n\\) and \\(p\\) grow to infinity. We now sumamrise some of the standard risk results.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#james-stein-estimator",
    "href": "13-theoryai.html#james-stein-estimator",
    "title": "5  Theory of AI",
    "section": "5.6 James-Stein Estimator",
    "text": "5.6 James-Stein Estimator\nThe classic James-Stein shrinkage rule, \\(\\hat y_{js}\\), uniformly dominates the traditional sample mean estimator, \\(\\hat{\\theta}\\), for all values of the true parameter \\(\\theta\\). In classical MSE risk terms: \\[\nR(\\hat y_{js}, \\theta) \\defeq E_{y|\\theta} {\\Vert \\hat y_{js} - \\theta \\Vert}^2 &lt; p\n    = E_{y|\\theta} {\\Vert y - \\theta \\Vert}^2, \\;\\;\\; \\forall \\theta\n\\] For a sparse signal, however, \\(\\hat y_{js}\\) performs poorly when the true parameter is an \\(r\\)-spike where \\(\\theta_r\\) has \\(r\\) coordinates at \\(\\sqrt{p/r}\\) and the rest set at zero with norm \\({\\Vert \\theta_r \\Vert}^2 =p\\).\nThe classical risk satisfies \\(R \\left ( \\hat y_{js} , \\theta_r \\right ) \\geq p/2\\) where the simple thresholding rule \\(\\sqrt{2 \\ln p}\\) performs with risk \\(\\sqrt{\\ln p}\\) in the \\(r\\)-spike sparse case even though it is inadmissible in MSE for a non-sparse signal.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#r-spike-problem",
    "href": "13-theoryai.html#r-spike-problem",
    "title": "5  Theory of AI",
    "section": "5.7 R-spike Problem",
    "text": "5.7 R-spike Problem\nFrom a historical perspective, James-Stein (a.k.a \\(L^2\\)-regularisation)(Stein 1964) is only a global shrinkage rule–in the sense that there are no local parameters to learn about sparsity. A simple sparsity example shows the issue with \\(L^2\\)-regularisation. Consider the sparse \\(r\\)-spike shows the problem with focusing solely on rules with the same shrinkage weight (albeit benefiting from pooling of information).\nLet the true parameter value be \\(\\theta_p = \\left ( \\sqrt{d/p} , \\ldots , \\sqrt{d/p} , 0 , \\ldots , 0 \\right )\\). James-Stein is equivalent to the model \\[\ny_i = \\theta_i + \\epsilon_i \\; \\mathrm{ and} \\; \\theta_i \\sim \\mathcal{N} \\left ( 0 , \\tau^2 \\right )\n\\] This dominates the plain MLE but loses admissibility! This is due to the fact that a “plug-in” estimate of global shrinkage \\(\\hat{\\tau}\\) is used. Tiao and Tan’s original “closed-form” analysis is particularly relevant here. They point out that the mode of \\(p(\\tau^2|y)\\) is zero exactly when the shrinkage weight turns negative (their condition 6.6). From a risk perspective \\(E \\Vert \\hat{\\theta}^{JS} - \\theta \\Vert \\leq p , \\forall \\theta\\) showing the inadmissibility of the MLE. At origin the risk is \\(2\\), but! \\[\n\\frac{p \\Vert \\theta \\Vert^2}{p + \\Vert \\theta \\Vert^2} \\leq R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\leq\n2 + \\frac{p \\Vert \\theta \\Vert^2}{ d + \\Vert \\theta \\Vert^2}.\n\\] This implies that \\(R \\left ( \\hat{\\theta}^{JS} , \\theta_p \\right ) \\geq (p/2)\\). Hence, simple thresholding rule beats James-Stein this with a risk given by \\(\\sqrt{\\log p }\\). This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.\nThe horseshoe estimator, which we will discuss in more detail later, \\(\\hat y_{hs}\\), was proposed by Carvalho, Polson, and Scott (2010) to provide a Bayes rule that inherits good MSE properties but also simultaneously provides asymptotic minimax estimation risk for sparse signals. HS estimator uniformly dominates the traditional sample mean estimator in MSE and has good posterior concentration properties for nearly black objects. Specifically, the horseshoe estimator attains asymptotically minimax risk rate \\[\n\\sup_{ \\theta \\in l_0[p_n] } \\;\n\\mathbb{E}_{ y | \\theta } \\|\\hat y_{hs} - \\theta \\|^2 \\asymp\np_n \\log \\left ( n / p_n \\right ).\n\\] The “worst’’ \\(\\theta\\) is obtained at the maximum difference between \\(\\left| \\hat y_{hs} - y \\right|\\) where \\(\\hat y_{hs} = \\mathbb{E}(\\theta|y)\\) can be interpreted as a Bayes posterior mean (optimal under Bayes MSE).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#ell_2-shrinkage",
    "href": "13-theoryai.html#ell_2-shrinkage",
    "title": "5  Theory of AI",
    "section": "5.8 \\(\\ell_2\\) Shrinkage",
    "text": "5.8 \\(\\ell_2\\) Shrinkage\n\nExample 5.3 (Stein’s Paradox) Stein’s paradox, as explained Efron and Morris (1977), is a phenomenon in statistics that challenges our intuitive understanding of estimation. The paradox arises when trying to estimate the mean of a multivariate normal distribution. Traditionally, the best guess about the future is usually obtained by computing the average of past events. However, Charles Stein showed that there are circumstances where there are estimators better than the arithmetic average. This is what’s known as Stein’s paradox.\nIn 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution that has uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein’s rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson’s chi-square test with results from a computer simulation.\nIn each of these examples, the mean square error of these rules is less than half that of the sample mean. This result is paradoxical because it contradicts the elementary law of statistical theory. The philosophical implications of Stein’s paradox are also significant. It has influenced the development of shrinkage estimators and has connections to Bayesianism and model selection criteria.\nSuppose that we have \\(n\\) independent observations \\(y_{1},\\ldots,y_{n}\\) from a \\(N\\left(  \\theta,\\sigma^{2}\\right)\\) distribution. The maximum likelihood estimator is \\(\\widehat{\\theta}=\\bar{y}\\), the sample mean. The Bayes estimator is the posterior mean, \\[\n\\widehat{\\theta}=\\mathbb{E}\\left[  \\theta\\mid y\\right]  =\\frac{\\sigma^{2}}{\\sigma^{2}+n}% \\bar{y}.\n\\] The Bayes estimator is a shrinkage estimator, it shrinks the MLE towards the prior mean. The amount of shrinkage is determined by the ratio of the variance of the prior and the variance of the likelihood. The Bayes estimator is also a function of the MLE \\[\n\\widehat{\\theta}=\\frac{\\sigma^{2}}{\\sigma^{2}+n}\\bar{y}+\\frac{n}{\\sigma^{2}+n}\\widehat{\\theta}.\n\\] This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior. The Bayes estimator is a function of the MLE \\[\n\\widehat{\\theta}=\\frac{\\sigma^{2}}{\\sigma^{2}+n}\\bar{y}+\\frac{n}{\\sigma^{2}+n}\\widehat{\\theta}.\n\\] This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior.\nThe original JS estimator shranks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and MOrris and Lindley showed that you want o shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS is \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), whcih serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate propr from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\nWe reproduce the baseball bartting average example from Efron and Morris (1977). Data below has the number of hits for 18 baseball player after 45 at-beat in 1970 season\n\n# Data source: https://www1.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt\nbaseball = read.csv(\"../data/EfronMorrisBB.txt\", sep = \"\\t\", stringsAsFactors = FALSE) %&gt;% select(LastName,AtBats,BattingAverage,SeasonAverage)\n\nNow, we can eatimate overall mean and variance\n\nmu_hat &lt;- mean(baseball$BattingAverage)\nsigma2_hat &lt;- var(baseball$BattingAverage)\n\nAs well as the osterior mean for each player (James-Stein estimator)\n\nbaseball &lt;- baseball %&gt;%\n  mutate(\n    JS = (sigma2_hat / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * mu_hat +\n      ((BattingAverage * (1 - BattingAverage) / AtBats) / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * BattingAverage\n  )\nkable(baseball)\n\n\n\n\nLastName\nAtBats\nBattingAverage\nSeasonAverage\nJS\n\n\n\n\nClemente\n45\n0.400\n0.352\n0.3358644\n\n\nRobinson\n45\n0.378\n0.306\n0.3237686\n\n\nHoward\n45\n0.356\n0.283\n0.3117929\n\n\nJohnstone\n45\n0.333\n0.238\n0.2994785\n\n\nBerry\n45\n0.311\n0.276\n0.2879767\n\n\nSpencer\n45\n0.311\n0.274\n0.2879767\n\n\nKessinger\n45\n0.289\n0.266\n0.2768343\n\n\nAlvarado\n45\n0.267\n0.224\n0.2661503\n\n\nSanto\n45\n0.244\n0.267\n0.2555955\n\n\nSwaboda\n45\n0.244\n0.233\n0.2555955\n\n\nPetrocelli\n45\n0.222\n0.261\n0.2462289\n\n\nRodriguez\n45\n0.222\n0.225\n0.2462289\n\n\nScott\n45\n0.222\n0.296\n0.2462289\n\n\nUnser\n45\n0.222\n0.258\n0.2462289\n\n\nWilliams\n45\n0.222\n0.251\n0.2462289\n\n\nCampaneris\n45\n0.200\n0.279\n0.2377410\n\n\nMunson\n45\n0.178\n0.302\n0.2303313\n\n\nAlvis\n45\n0.156\n0.183\n0.2242473\n\n\n\n\n\nPlot below shows the observed averages vs. James-Stein estimate\n\nggplot(baseball, aes(x = BattingAverage, y = JS)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"Observed Batting Average\",\n    y = \"James-Stein Estimate\",\n    title = \"Empirical Bayes Shrinkage of Batting Averages (2016)\"\n  )\n\n\n\n\n\n\n\n\nCalculate mean squared error (MSE) for observed and James-Stein estimates\n\nmse_observed &lt;- mean((baseball$BattingAverage - mu_hat)^2)\nmse_js &lt;- mean((baseball$JS - mu_hat)^2)\n\ncat(sprintf(\"MSE (Observed): %.6f\\n\", mse_observed))\n\n## MSE (Observed): 0.004584\n\ncat(sprintf(\"MSE (James-Stein): %.6f\\n\", mse_js))\n\n## MSE (James-Stein): 0.001031\n\n\nWe can see that the James-Stein estimator has a lower MSE than the observed batting averages. This is a demonstration of Stein’s paradox, where the James-Stein estimator, which shrinks the estimates towards the overall mean, performs better than the naive sample mean estimator.\n\na = matrix(rep(1:3, nrow(baseball)), 3, nrow(baseball))\nb = matrix(c(baseball$BattingAverage, baseball$SeasonAverage, baseball$JS),    3, nrow(baseball), byrow=TRUE)\n\nmatplot(a, b, pch=\" \", ylab=\"predicted average\", xaxt=\"n\", xlim=c(0.5, 3.1), ylim=c(0.13, 0.42))\nmatlines(a, b)\ntext(rep(0.7, nrow(baseball)), baseball$BattingAverage, baseball$LastName, cex=0.6)\ntext(1, 0.14, \"First 45\\nat bats\", cex=0.5)\ntext(2, 0.14, \"Average\\nof remainder\", cex=0.5)\ntext(3, 0.14, \"J-S\\nestimator\", cex=0.5)\n\n\n\n\n\n\n\n\nNow if we look at the season dynamics for Clemente\n\n# Data source: https://www.baseball-almanac.com/players/hittinglogs.php?p=clemero01&y=1970\ncl = read.csv(\"../data/clemente.csv\")\nx = cumsum(cl$AB)\ny = cumsum(cl$H)/cumsum(cl$AB)\n# Plot x,y startind from index 2\nind = c(1,2)\nplot(x[-ind],y[-ind], type='o', ylab=\"Betting Average\", xlab=\"Number at Bats\")\n# Add horizontal line for season average 145/412 and add text above line `Seaosn Average`\ntext(200, 145/412 + 0.005, \"Season Average\", col = \"red\")\nabline(h = 145/412, col = \"red\", lty = 2)\n# Ted williams record is .406 in in 1941, so you know the first data points are noise\ntext(200, baseball$JS[1] + 0.005, \"JS\", col = \"red\")\nabline(h = baseball$JS[1], col = \"red\", lty = 2)\ntext(200, baseball$BattingAverage[1] + 0.005, \"After 45 Bets\", col = \"red\")\nabline(h = baseball$BattingAverage[1], col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nThe motivation for the shrinkage prior rather than a flat uniform prior are the following probabilistic arguments.\n\nExample 5.4 (Efron Example) Efron shows the importance of priors in high dimensions when one can “borrow strength” (a.k.a. regularisation) across components.\nStein’s phenomenon where \\(y_i | \\theta_i \\sim N(\\theta_i, 1)\\) and \\(\\theta_i \\sim N(0, \\tau^2)\\) illustrates this point well. From the model,\n\\[\nP\\left( \\| y \\| &gt; \\| \\theta \\| \\right) &gt; \\frac{1}{2}\n\\]\nUnder a flat Lebesgue measure, this inequality is reversed in the posterior, namely\n\\[\nP\\left( \\| \\theta \\| &gt; \\| y \\| \\; | \\; y \\right) &gt; \\frac{1}{2}\n\\]\nIn conflict with the classical statement. However, if we use Stein’s rule (posterior where \\(\\tau^2\\) is estimated via empirical Bayes) we have\n\\[\n\\hat{\\theta}(y) = \\left( 1 - \\frac{k-2}{\\|y\\|^2} \\right) y \\quad \\text{and} \\quad E\\left( \\| \\hat{\\theta} - \\theta \\| \\right) &lt; k, \\; \\forall \\theta.\n\\]\nHence, when \\(\\|y\\|^2\\) is small the shrinkage factor is more extreme.\nFor example, if \\(k=10\\), \\(\\|y\\|^2=12\\), then \\(\\hat{\\theta} = (1/3) y\\). Now we have the more intuitive result:\n\\[\nP\\left( \\| \\theta \\| &gt; \\| y \\| \\; | \\; y \\right) &lt; \\frac{1}{2}\n\\]\nShowing that default priors matter in high dimensions.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#ell_1-sparsity-1",
    "href": "13-theoryai.html#ell_1-sparsity-1",
    "title": "5  Theory of AI",
    "section": "5.9 \\(\\ell_1\\) Sparsity",
    "text": "5.9 \\(\\ell_1\\) Sparsity",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#ell_0-subset-selection-1",
    "href": "13-theoryai.html#ell_0-subset-selection-1",
    "title": "5  Theory of AI",
    "section": "5.10 \\(\\ell_0\\) Subset Selection",
    "text": "5.10 \\(\\ell_0\\) Subset Selection",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#bayesain-model-selection-via-regularisation",
    "href": "13-theoryai.html#bayesain-model-selection-via-regularisation",
    "title": "5  Theory of AI",
    "section": "5.11 Bayesain Model Selection via Regularisation",
    "text": "5.11 Bayesain Model Selection via Regularisation\nFrom Bayesian perspective regularization is nothing but incorporation of prior information into the model. Remember, that a Bayesian model is specified by likelihood and prior distributions. Bayesian regularization methods include the Bayesian bridge, horseshoe regularization, Bayesian lasso, Bayesian elastic net, spike-and-slab lasso, and global-local shrinkage priors. Bayesian \\(\\ell_0\\) regularization is an attractive solution for high dimensional variable selection as it directly penalizes the number of predictors. The caveat is the need to search over all possible model combinations, as a full solution requires enumeration over all possible models which is NP-hard.\nIn Bayesian approach, regularization requires the specification of a loss, denoted by \\(l\\left(\\beta\\right)\\) and a penalty function, denoted by \\(\\phi_{\\lambda}(\\beta)\\), where \\(\\lambda\\) is a global regularization parameter. From a Bayesian perspective, \\(l\\left(\\beta\\right)\\) and \\(\\phi_{\\lambda}(\\beta)\\) correspond to the negative logarithms of the likelihood and prior distribution, respectively. Regularization leads to an maximum a posteriori (MAP) optimization problem of the form \\[\n\\underset{\\beta \\in R^p}{\\mathrm{minimize}\\quad}\nl\\left(\\beta\\right) + \\phi_{\\lambda}(\\beta) \\; .\n\\] Taking a probabilistic approach leads to a Bayesian hierarchical model \\[\np(y \\mid \\beta) \\propto \\exp\\{-l(\\beta)\\} \\; , \\quad p(\\beta) \\propto \\exp\\{ -\\phi_{\\lambda}(\\beta) \\} \\ .\n\\] The solution to the minimization problem estimated by regularization corresponds to the posterior mode, \\(\\hat{\\beta} = \\mathrm{ arg \\; max}_\\beta \\; p( \\beta|y)\\), where \\(p(\\beta|y)\\) denotes the posterior distribution. Consider a normal mean problem with \\[\n\\label{eqn:linreg}\ny = \\theta+ e \\ , \\ \\  \\text{where } e \\sim N(0, \\sigma^2),~-\\infty \\le \\theta \\le \\infty \\ .\n\\] What prior \\(p(\\theta)\\) should we place on \\(\\theta\\) to be able to separate the “signal” \\(\\theta\\) from “noise” \\(e\\), when we know that there is a good chance that \\(\\theta\\) is sparse (i.e. equal to zero). In the multivariate case we have \\(y_i = \\theta_i + e_i\\) and sparseness is measured by the number of zeros in \\(\\theta = (\\theta_1\\ldots,\\theta_p)\\). The Bayesan Lasso assumes double exponential (a.k.a Laplace) prior distribution where \\[\np(\\theta_i \\mid b) = 0.5b\\exp(-|\\theta|/b).\n\\] We use Bayes rule to calculate the posterior as a product of Normal likelihood and Laplace prior \\[\n\\log p(\\theta \\mid y, b) \\propto ||y-\\theta||_2^2 + \\dfrac{2\\sigma^2}{b}||\\theta||_1.\n\\] For fixed \\(\\sigma^2\\) and \\(b&gt;0\\) the posterior mode is equivalent to the Lasso estimate with \\(\\lambda = 2\\sigma^2/b\\). Large variance \\(b\\) of the prior is equivalent to the small penalty weight \\(\\lambda\\) in the Lasso objective function.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#shrinkage-ell_2-norm",
    "href": "13-theoryai.html#shrinkage-ell_2-norm",
    "title": "5  Theory of AI",
    "section": "5.12 Shrinkage (\\(\\ell_2\\) Norm)",
    "text": "5.12 Shrinkage (\\(\\ell_2\\) Norm)\nWe can estimate the risk bounds of \\(\\ell_2\\) Norm regularisation. The classic bias-variance tradeoff is given by the MSE risk bound. \\[\nR(\\theta,\\hat \\theta) = E_{y|\\theta} \\left [ \\Vert \\hat \\theta - \\theta \\Vert^2 \\right ] = \\Vert \\hat \\theta - \\theta \\Vert^2 + E_{y|\\theta} \\left [ \\Vert \\hat \\theta - \\mathbb{E}(\\hat \\theta) \\Vert^2 \\right ]\n\\]\nIn a case of multiple parameters, the Stein bound is \\[\nR(\\theta,\\hat \\theta_{JS}) &lt; R(\\theta,\\hat \\theta_{MLE}) \\;\\;\\; \\forall \\theta \\in \\mathbb{R}^p, \\;\\;\\; p \\geq 3.\n\\] In an applied problem the gap in risk between MLE and JS estimators can be large. For example, in the normal means problem with \\(p=100\\) and \\(n=100\\), the risk of the MLE is \\(R(\\theta,\\hat \\theta_{MLE}) = 100\\) while the risk of the JS estimator is \\(R(\\theta,\\hat \\theta_{JS}) = 1.5\\). The JS estimator is 67 times more efficient than the MLE. The JS estimator is also minimax optimal in the sense that it attains the minimax risk bound for the normal means problem. The minimax risk bound is the smallest risk that can be attained by any estimator.\nJS is a specific estimate and it motivates the ridge estimator. The ridge estimator is a shrinkage estimator with the penalty function being the squared norm of the parameter vector. The ridge estimator is \\[\n\\hat \\theta_{ridge} = \\left (  X^T X + \\lambda I \\right )^{-1} X^T y\n\\] where \\(\\lambda\\) is the regularization parameter.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#sparsity-ell_1-norm",
    "href": "13-theoryai.html#sparsity-ell_1-norm",
    "title": "5  Theory of AI",
    "section": "5.13 Sparsity (\\(\\ell_1\\) Norm)",
    "text": "5.13 Sparsity (\\(\\ell_1\\) Norm)\nHigh-dimensional predictor selection and sparse signal recovery are routine statistical and machine learning tasks. There is a vast and growing literator with lasso (leat absolute selection) being the gold standard. Horseshoe priors are the Bayesian counterpart. Sparsity relies on the property of a few large signals among many (nearly zero) noisy observations. The goal is to find a niddle in the haystack. Suppose that we observe data from Normal means model \\[\ny_i \\mid \\theta_i \\sim N(\\theta_i,1),~ 1\\le i\\le p, ~ \\theta = (\\theta_1,\\ldots,\\theta_p),\n\\] where parameter \\(\\theta\\) lies in the ball \\[\n||\\theta||_{\\ell_0} = \\{\\theta : \\text{number of  }\\theta_i \\ne 0 \\le p_n\\}.\n\\]\nEven threshholding can beat MLE, when the signal is sparse. The thresholding estimator is \\[\n\\hat \\theta_{thr} = \\left \\{ \\begin{array}{ll} \\hat \\theta_i & \\mbox{if} \\; \\hat \\theta_i &gt; \\sqrt{2 \\ln p} \\\\ 0 & \\mbox{otherwise} \\end{array} \\right .\n\\]\nSparse signal detection provides a challenge to statistical methodology; consider the classical normal means inference problem. Suppose that we observe data from the probability model \\(( y_i | \\theta_i ) \\sim N( \\theta_i,1)\\). We wish to provide an estimator \\(\\hat y_{hs}\\) for the vector of normal means \\(\\theta = ( \\theta_1, \\ldots , \\theta_p )\\). Sparsity occurs when a large portion of the parameter vector contains zeroes. The ’’ultra-sparse`` or “nearly black” vector case occurs when \\(p_n\\), denoting the number of non-zero parameter values, and for \\(\\theta \\in l_0 [ p_n]\\), which denotes the set \\(\\# ( \\theta_i \\neq 0 ) \\leq p_n\\) where \\(p_n = o(n)\\) where \\(p_n \\rightarrow \\infty\\) as \\(n \\rightarrow \\infty\\).\nThe predictive rule is often represented by a mathematical model, such as a linear regression model or a neural network. The model is trained using historical data, which consists of observed inputs and outputs \\((x_1,y_1),\\ldots, (x_n,y_n)\\).\nThe model is then used to predict the output \\(y\\) for new inputs \\(x\\). The accuracy of the model is evaluated using a performance metric, such as the mean squared error or the mean absolute error. The model is then updated and retrained using new data to improve its accuracy. This process is repeated until the model achieves the desired level of accuracy.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#lasso",
    "href": "13-theoryai.html#lasso",
    "title": "5  Theory of AI",
    "section": "5.14 LASSO",
    "text": "5.14 LASSO\nThe Laplace distribution can be represented as scale mixture of Normal distribution(Andrews and Mallows 1974) \\[\n\\begin{aligned}\n\\theta_i \\mid \\sigma^2,\\tau \\sim &N(0,\\tau^2\\sigma^2)\\\\\n\\tau^2  \\mid \\alpha \\sim &\\exp (\\alpha^2/2)\\\\\n\\sigma^2 \\sim & \\pi(\\sigma^2).\\end{aligned}\n\\] We can show equivalence by integrating out \\(\\tau\\) \\[\np(\\theta_i\\mid \\sigma^2,\\alpha) =  \\int_{0}^{\\infty} \\dfrac{1}{\\sqrt{2\\pi \\tau}}\\exp\\left(-\\dfrac{\\theta_i^2}{2\\sigma^2\\tau}\\right)\\dfrac{\\alpha^2}{2}\\exp\\left(-\\dfrac{\\alpha^2\\tau}{2}\\right)d\\tau = \\dfrac{\\alpha}{2\\sigma}\\exp(-\\alpha/\\sigma|\\theta_i|).\n\\] Thus it is a Laplace distribution with location 0 and scale \\(\\alpha/\\sigma\\). Representation of Laplace prior is a scale Normal mixture allows us to apply an efficient numerical algorithm for computing samples from the posterior distribution. This algorithms is called a Gibbs sample and it iteratively samples from \\(\\theta \\mid a,y\\) and \\(b\\mid \\theta,y\\) to estimate joint distribution over \\((\\hat \\theta, \\hat b)\\). Thus, we so not need to apply cross-validation to find optimal value of \\(b\\), the Bayesian algorithm does it “automatically”. We will discuss Gibbs algorithm later in the book.\nWhen prior is Normal \\(\\theta_i \\sim N(0,\\sigma_{\\theta}^2)\\), the posterior mode is equivalent to the ridge estimate. The relation between variance of the prior and the penalty weight in ridge regression is inverse proportional \\(\\lambda\\propto 1/\\sigma_{\\theta}^2\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#subset-selection-ell_0-norm",
    "href": "13-theoryai.html#subset-selection-ell_0-norm",
    "title": "5  Theory of AI",
    "section": "5.15 Subset Selection (\\(\\ell_0\\) Norm)",
    "text": "5.15 Subset Selection (\\(\\ell_0\\) Norm)\nSkike-and-slab (George and McCulloh) or Bernoulli-Gaussian (Polson Sun)",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#bridge-ell_alpha",
    "href": "13-theoryai.html#bridge-ell_alpha",
    "title": "5  Theory of AI",
    "section": "5.16 Bridge (\\(\\ell_{\\alpha}\\))",
    "text": "5.16 Bridge (\\(\\ell_{\\alpha}\\))\nThis is a non-convex penalty when \\(0&lt;\\alpha&lt;1\\). It is an NP-hard problem. When \\(\\alpha=1\\) or \\(\\alpha=2\\) we have optimisation problems that are “solvable” for large scale cases. However, when \\(0\\le \\alpha&lt;1\\) the current optimisation algorithms won’t work.\nThe real killer is that you can use data to estimate \\(\\alpha\\) and \\(\\lambda\\) (let the data speak for itself) Box and Tiao (1992).\nBayesian analogue of the bridge estimator in regression is \\[\ny = X\\beta + \\epsilon\n\\]\nfor some unknown vector \\(\\beta = (\\beta_1, \\ldots, \\beta_p)'\\). Given choices of \\(\\alpha \\in (0,1]\\) and \\(\\nu \\in \\mathbb{R}^+\\), the bridge estimator \\(\\hat{\\beta}\\) is the minimizer of\n\\[\nQ_y(\\beta) = \\frac{1}{2} \\|y - X\\beta\\|^2 + \\nu \\sum_{j=1}^p |\\beta_j|^\\alpha.\n\\tag{5.1}\\]\nThis bridges a class of shrinkage and selection operators, with the best-subset-selection penalty at one end, and the \\(\\ell_1\\) (or lasso) penalty at the other. An early reference to this class of models can be found in Frank and Friedman (1993), with recent papers focusing on model-selection asymptotics, along with strategies for actually computing the estimator (Huang, Horowitz, and Ma (2008), Mazumder, and and Hastie (2011)).\nBridge approach differs from this line of work in adopting a Bayesian perspective on bridge estimation. Specifically, we treat\n\\[\np(\\beta \\mid y) \\propto \\exp\\{-Q_y(\\beta)\\}\n\\]\nas a posterior distribution having the minimizer of Equation 5.1 as its global mode. This posterior arises in assuming a Gaussian likelihood for \\(y\\), along with a prior for \\(\\beta\\) that decomposes as a product of independent exponential-power priors (Box and Tiao (1992)):\n\\[\np(\\beta \\mid \\alpha, \\nu) \\propto \\prod_{j=1}^p \\exp\\left(-\\left|\\frac{\\beta_j}{\\tau}\\right|^\\alpha\\right), \\quad \\tau = \\nu^{-1/\\alpha}. \\tag{2}\n\\]\nRather than minimizing (1), we proceed by constructing a Markov chain having the joint posterior for \\(\\beta\\) as its stationary distribution.\n\n5.16.1 Spike-and-Slab Prior\nOur Bayesian formulation of allows to specify a wide range of range of regularized formulations for a regression problem. In this section we consider a Bayesian model for variable selection. Consider a linear regression problem \\[\ny = \\beta_1x_1+\\ldots+\\beta_px_p + e \\ , \\ \\  \\text{where } e \\sim N(0, \\sigma^2),~-\\infty \\le \\beta_i \\le \\infty \\ .\n\\] We would like to solve the problem of variable selections, i.e. identify which input variables \\(x_i\\) to be used in our model. The gold standard for Bayesian variable selection are spike-and-slab priors, or Bernoulli-Gaussian mixtures. Whilst spike-and-slab priors provide full model uncertainty quantification, they can be hard to scale to very high dimensional problems and can have poor sparsity properties. On the other hand, techniques like proximal algorithms can solve non-convex optimization problems which are fast and scalable, although they generally don’t provide a full assessment of model uncertainty.\nTo perform a model selection, we would like to specify a prior distribution \\(p\\left(\\beta\\right)\\), which imposes a sparsity assumption on \\(\\beta\\), where only a small portion of all \\(\\beta_i\\)’s are non-zero. In other words, \\(\\|\\beta\\|_0 = k \\ll p\\), where \\(\\|\\beta\\|_0 \\defeq \\#\\{i : \\beta_i\\neq0\\}\\), the cardinality of the support of \\(\\beta\\), also known as the \\(\\ell_0\\) (pseudo)norm of \\(\\beta\\). A multivariate Gaussian prior (\\(l_2\\) norm) leads to poor sparsity properties in this situation. Sparsity-inducing prior distributions for \\(\\beta\\) can be constructed to impose sparsity include the double exponential (lasso).\nUnder spike-and-slab, each \\(\\beta_i\\) exchangeably follows a mixture prior consisting of \\(\\delta_0\\), a point mass at \\(0\\), and a Gaussian distribution centered at zero. Hence we write,\n\\[\n\\label{eqn:ss}\n\\beta_i | \\theta, \\sigma_\\beta^2 \\sim (1-\\theta)\\delta_0 + \\theta N\\left(0, \\sigma_\\beta^2\\right) \\ .\n\\] Here \\(\\theta\\in \\left(0, 1\\right)\\) controls the overall sparsity in \\(\\beta\\) and \\(\\sigma_\\beta^2\\) accommodates non-zero signals. This family is termed as the Bernoulli-Gaussian mixture model in the signal processing community.\nA useful re-parameterization, the parameters \\(\\beta\\) is given by two independent random variable vectors \\(\\gamma = \\left(\\gamma_1, \\ldots, \\gamma_p\\right)'\\) and \\(\\alpha = \\left(\\alpha_1, \\ldots, \\alpha_p\\right)'\\) such that \\(\\beta_i  =  \\gamma_i\\alpha_i\\), with probabilistic structure \\[\n\\label{eq:bg}\n\\begin{array}{rcl}\n\\gamma_i\\mid\\theta & \\sim & \\text{Bernoulli}(\\theta) \\ ;\n\\\\\n\\alpha_i \\mid \\sigma_\\beta^2 &\\sim & N\\left(0, \\sigma_\\beta^2\\right) \\ .\n\\\\\n\\end{array}\n\\] Since \\(\\gamma_i\\) and \\(\\alpha_i\\) are independent, the joint prior density becomes \\[\np\\left(\\gamma_i, \\alpha_i \\mid \\theta, \\sigma_\\beta^2\\right) =\n\\theta^{\\gamma_i}\\left(1-\\theta\\right)^{1-\\gamma_i}\\frac{1}{\\sqrt{2\\pi}\\sigma_\\beta}\\exp\\left\\{-\\frac{\\alpha_i^2}{2\\sigma_\\beta^2}\\right\\}\n\\ , \\ \\ \\ \\text{for } 1\\leq i\\leq p \\ .\n\\] The indicator \\(\\gamma_i\\in \\{0, 1\\}\\) can be viewed as a dummy variable to indicate whether \\(\\beta_i\\) is included in the model.\nLet \\(S = \\{i: \\gamma_i = 1\\} \\subseteq \\{1, \\ldots, p\\}\\) be the “active set\" of \\(\\gamma\\), and \\(\\|\\gamma\\|_0 = \\sum\\limits_{i = 1}^p\\gamma_i\\) be its cardinality. The joint prior on the vector \\(\\{\\gamma, \\alpha\\}\\) then factorizes as \\[\n\\begin{array}{rcl}\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2\\right) & = & \\prod\\limits_{i = 1}^p p\\left(\\alpha_i, \\gamma_i \\mid \\theta, \\sigma_\\beta^2\\right) \\\\\n& = &\n\\theta^{\\|\\gamma\\|_0}\n\\left(1-\\theta\\right)^{p - \\|\\gamma\\|_0}\n\\left(2\\pi\\sigma_\\beta^2\\right)^{-\\frac p2}\\exp\\left\\{-\\frac1{2\\sigma_\\beta^2}\\sum\\limits_{i = 1}^p\\alpha_i^2\\right\\} \\ .\n\\end{array}\n\\]\nLet \\(X_\\gamma \\defeq \\left[X_i\\right]_{i \\in S}\\) be the set of “active explanatory variables\" and \\(\\alpha_\\gamma \\defeq \\left(\\alpha_i\\right)'_{i \\in S}\\) be their corresponding coefficients. We can write \\(X\\beta = X_\\gamma \\alpha_\\gamma\\). The likelihood can be expressed in terms of \\(\\gamma\\), \\(\\alpha\\) as \\[\np\\left(y \\mid \\gamma, \\alpha, \\theta, \\sigma_e^2\\right)\n=\n\\left(2\\pi\\sigma_e^2\\right)^{-\\frac n2}\n\\exp\\left\\{\n-\\frac1{2\\sigma_e^2}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n\\right\\} \\ .\n\\]\nUnder this re-parameterization by \\(\\left\\{\\gamma, \\alpha\\right\\}\\), the posterior is given by\n\\[\n\\begin{array}{rcl}\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2, \\sigma_e^2, y\\right) & \\propto &\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2\\right)\np\\left(y \\mid \\gamma, \\alpha, \\theta, \\sigma_e^2\\right)\\\\\n& \\propto &\n\\exp\\left\\{-\\frac1{2\\sigma_e^2}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n-\\frac1{2\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2\n-\\log\\left(\\frac{1-\\theta}{\\theta}\\right)\n\\left\\|\\gamma\\right\\|_0\n\\right\\} \\ .\n\\end{array}\n\\] Our goal then is to find the regularized maximum a posterior (MAP) estimator \\[\n\\arg\\max\\limits_{\\gamma, \\alpha}p\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2, \\sigma_e^2, y \\right) \\ .\n\\] By construction, the \\(\\gamma\\) \\(\\in\\left\\{0, 1\\right\\}^p\\) will directly perform variable selection. Spike-and-slab priors, on the other hand, will sample the full posterior and calculate the posterior probability of variable inclusion. Finding the MAP estimator is equivalent to minimizing over \\(\\left\\{\\gamma, \\alpha\\right\\}\\) the regularized least squares objective function\n\\[\n\\min\\limits_{\\gamma, \\alpha}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n+ \\frac{\\sigma_e^2}{\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2\n+ 2\\sigma_e^2\\log\\left(\\frac{1-\\theta}{\\theta}\\right)\n\\left\\|\\gamma\\right\\|_0 \\ .\n\\tag{5.2}\\] This objective possesses several interesting properties:\n\nThe first term is essentially the least squares loss function.\nThe second term looks like a ridge regression penalty and has connection with the signal-to-noise ratio (SNR) \\(\\sigma_\\beta^2/\\sigma_e^2\\). Smaller SNR will be more likely to shrink the estimates towards \\(0\\). If \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the prior uncertainty on the size of non-zero coefficients is much larger than the noise level, that is, the SNR is sufficiently large, this term can be ignored. This is a common assumption in spike-and-slab framework in that people usually want \\(\\sigma_\\beta \\to \\infty\\) or to be “sufficiently large\" in order to avoid imposing harsh shrinkage to non-zero signals.\nIf we further assume that \\(\\theta &lt; \\frac12\\), meaning that the coefficients are known to be sparse a priori, then \\(\\log\\left(\\left(1-\\theta\\right) / \\theta\\right) &gt; 0\\), and the third term can be seen as an \\(\\ell_0\\) regularization.\n\nTherefore, our Bayesian objective inference is connected to \\(\\ell_0\\)-regularized least squares, which we summarize in the following proposition.\n(Spike-and-slab MAP & \\(\\ell_0\\) regularization)\nFor some \\(\\lambda &gt; 0\\), assuming \\(\\theta &lt; \\frac12\\), \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the Bayesian MAP estimate defined by Equation 5.2 is equivalent to the \\(\\ell_0\\) regularized least squares objective, for some \\(\\lambda &gt; 0\\), \\[\n\\min\\limits_{\\beta}\n\\frac12\\left\\|y - X\\beta\\right\\|_2^2\n+ \\lambda\n\\left\\|\\beta\\right\\|_0 \\ .\n\\tag{5.3}\\]\nFirst, assuming that \\[\n\\theta &lt; \\frac12, \\ \\ \\  \\sigma_\\beta^2 \\gg \\sigma_e^2, \\ \\ \\  \\frac{\\sigma_e^2}{\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2 \\to 0 \\ ,\n\\] gives us an objective function of the form \\[\n\\min\\limits_{\\gamma, \\alpha}\n\\frac12 \\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n+ \\lambda\n\\left\\|\\gamma\\right\\|_0,  \\ \\ \\ \\  \\text{where } \\lambda \\defeq \\sigma_e^2\\log\\left(\\left(1-\\theta\\right) / \\theta\\right) &gt; 0 \\ .\n\\tag{5.4}\\]\nEquation Equation 5.4 can be seen as a variable selection version of equation Equation 5.3. The interesting fact is that Equation 5.3 and Equation 5.4 are equivalent. To show this, we need only to check that the optimal solution to Equation 5.3 corresponds to a feasible solution to Equation 5.4 and vice versa. This is explained as follows.\nOn the one hand, assuming \\(\\hat\\beta\\) is an optimal solution to Equation 5.3, then we can correspondingly define \\(\\hat\\gamma_i \\defeq I\\left\\{\\hat\\beta_i \\neq 0\\right\\}\\), \\(\\hat\\alpha_i \\defeq \\hat\\beta_i\\), such that \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) is feasible to Equation 5.4 and gives the same objective value as \\(\\hat\\beta\\) gives Equation 5.3.\nOn the other hand, assuming \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) is optimal to Equation 5.4, implies that we must have all of the elements in \\(\\hat\\alpha_\\gamma\\) should be non-zero, otherwise a new \\(\\tilde\\gamma_i \\defeq I\\left\\{\\hat\\alpha_i \\neq 0\\right\\}\\) will give a lower objective value of Equation 5.4. As a result, if we define \\(\\hat\\beta_i \\defeq \\hat\\gamma_i\\hat\\alpha_i\\), \\(\\hat\\beta\\) will be feasible to Equation 5.3 and gives the same objective value as \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) gives Equation 5.4.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#horseshoe-prior",
    "href": "13-theoryai.html#horseshoe-prior",
    "title": "5  Theory of AI",
    "section": "5.17 Horseshoe Prior",
    "text": "5.17 Horseshoe Prior\n\n\n\n\n\n\n\n\n\nThe sparse normal means problem is concerned with inference for the parameter vector \\(\\theta = ( \\theta_1 , \\ldots , \\theta_p )\\) where we observe data \\(y_i = \\theta_i + \\epsilon_i\\) where the level of sparsity might be unknown. From both a theoretical and empirical viewpoint, regularized estimators have won the day. This still leaves open the question of how does specify a penalty, denoted by \\(\\pi_{HS}\\), (a.k.a. log-prior, \\(- \\log p_{HS}\\))? Lasso simply uses an \\(L^1\\)-norm, \\(\\sum_{i=1}^K | \\theta_i |\\), as opposed to the horseshoe prior which (essentially) uses the penalty \\[\n\\pi_{HS} ( \\theta_i | \\tau ) = - \\log p_{HS} ( \\theta_i | \\tau ) = - \\log \\log \\left ( 1 + \\frac{2 \\tau^2}{\\theta_i^2} \\right ) .\n\\] The motivation for the horseshoe penalty arises from the analysis of the prior mass and influence on the posterior in both the tail and behaviour at the origin. The latter is the key determinate of the sparsity properties of the estimator.\nThe horseshoe Carvalho, Polson, and Scott (2010) is a Bayesian method for ‘needle-in-a-haystack’ type problems where there is some sparsity, meaning that there are some signals amid mostly noise.\nWe introduce the horseshoe in the context of the normal means model, which is given by \\[Y_i = \\beta_i + \\varepsilon_i, \\quad i = 1, \\ldots, n,\\] with \\(\\varepsilon_i\\) i.i.d. \\(\\mathcal{N}(0, \\sigma^2)\\). The horseshoe prior is given by \\[\\begin{align*}\n\\beta_i &\\sim \\mathcal{N}(0, \\sigma^2 \\tau^2 \\lambda_i^2)\\\\\n\\lambda_i &\\sim C^+(0, 1),\n\\end{align*}\\] where \\(C^+\\) denotes the half-Cauchy distribution. Optionally, hyperpriors on \\(\\tau\\) and \\(\\sigma\\) may be specified, as is described further in the next two sections.\nTo illustrate the shrinkage behaviour of the horseshoe, let’s plot the posterior mean for \\(\\beta_i\\) as a function of \\(y_i\\) for three different values of \\(\\tau\\).\n\nlibrary(horseshoe)\nlibrary(ggplot2)\ntau.values &lt;- c(0.005, 0.05, 0.5)\ny.values &lt;- seq(-5, 5, length = 100)\ndf &lt;- data.frame(tau = rep(tau.values, each = length(y.values)),\n                 y = rep(y.values, 3),\n                 post.mean = c(HS.post.mean(y.values, tau = tau.values[1], Sigma2=1), \n                               HS.post.mean(y.values, tau = tau.values[2], Sigma2=1), \n                               HS.post.mean(y.values, tau = tau.values[3], Sigma2=1)) )\n\nggplot(data = df, aes(x = y, y = post.mean, group = tau, color = factor(tau))) + \n  geom_line(size = 1.5) + \n  scale_color_brewer(palette=\"Dark2\") + \n  geom_abline(lty = 2) + geom_hline(yintercept = 0, colour = \"grey\") + \n  theme_classic() + ylab(\"\") + labs(color = \"Tau\") +\n  ggtitle(\"Horseshoe posterior mean for three values of tau\") \n\n\n\n\n\n\n\n\nSmaller values of \\(\\tau\\) lead to stronger shrinkage behaviour of the horseshoe. Observations that are in absolute value at most equal to \\(\\sqrt{2\\sigma^2\\log(1/\\tau)}\\) are shrunk to values close to zero (Van der Pas et al (2014)). For larger observed values, the horseshoe posterior mean will tend to the identity (that is, barely any shrinkage, the estimate will be very close to the observed value). The optimal value of \\(\\tau\\) is the proportion of true signals. This value is typically not known in practice but can be estimated, as described further in the next sections.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "13-theoryai.html#the-normal-means-problem",
    "href": "13-theoryai.html#the-normal-means-problem",
    "title": "5  Theory of AI",
    "section": "5.18 The normal means problem",
    "text": "5.18 The normal means problem\nThe normal means model is: \\[Y_i = \\beta_i + \\varepsilon_i, \\quad i = 1, \\ldots, n,\\] with \\(\\varepsilon_i\\) i.i.d. \\(\\mathcal{N}(0, \\sigma^2)\\).\nFirst, we will be computing the posterior mean only, with known variance \\(\\sigma^2\\) The function HS.post.mean computes the posterior mean of \\((\\beta_1, \\ldots, \\beta_n)\\). It does not require MCMC and is suitable when only an estimate of the vector \\((\\beta_1, \\ldots, \\beta_n)\\) is desired. In case uncertainty quantification or variable selection is also of interest, or no good value for \\(\\sigma^2\\) is available, please see below for the function HS.normal.means.\nThe function HS.post.mean requires the observed outcomes, a value for \\(\\tau\\) and a value for \\(\\sigma\\). Ideally, \\(\\tau\\) should be equal to the proportion of nonzero \\(\\beta_i\\)’s. Typically, this proportion is unknown, in which case it is recommended to use the function HS.MMLE to find the marginal maximum likelihood estimator for \\(\\tau\\).\nAs an example, we generate 50 data points, the first 10 of which are coming from true signals. The first 10 \\(\\beta_i\\)’s are equal to five and the remaining \\(\\beta_i\\)’s are equal to zero. Let’s first plot the true parameters (black) and observations (blue).\n\ndf &lt;- data.frame(index = 1:50,\n                 truth &lt;- c(rep(5, 10), rep(0, 40)),\n                 y &lt;- truth + rnorm(50) #observations\n                 )\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  ggtitle(\"Black = truth, Blue = observations\")\n\n\n\n\n\n\n\n\nWe estimate \\(\\tau\\) using the MMLE, using the known variance.\n\n(tau.est &lt;- HS.MMLE(df$y, Sigma2 = 1))\n\n## [1] 0.8944264\n\n\nWe then use this estimate of \\(\\tau\\) to find the posterior mean, and add it to the plot in red.\n\npost.mean &lt;- HS.post.mean(df$y, tau.est, 1)\ndf$post.mean &lt;- post.mean\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean), size = 2, col = \"red\") +\n  ggtitle(\"Black = truth, Blue = observations, Red = estimates\")\n\n\n\n\n\n\n\n\nIf the posterior variance is of interest, the function HS.post.var can be used. It takes the same arguments as HS.post.mean.\n\n5.18.1 Posterior mean, credible intervals and variable selection, possibly unknown \\(\\sigma^2\\)\nThe function HS.normal.means is the main function to use for the normal means problem. It uses MCMC and results in an object that contains all MCMC samples as well as the posterior mean for all parameters (\\(\\beta_i\\)’s, \\(\\tau\\), \\(\\sigma\\)), the posterior median for the \\(\\beta_i\\)’s, and credible intervals for the \\(\\beta_i\\)’s.\nThe key choices to make are:\n\nHow to handle \\(\\tau\\). The recommended option is “truncatedCauchy” (a half-Cauchy prior truncated to \\([1/n, 1]\\)). See the manual for other options.\nHow to handle \\(\\sigma\\). The recommended option is “Jeffreys” (Jeffrey’s prior). See the manual for other options.\n\nOther options that can be set by the user are the level of the credible intervals (default is 95%), and the number of MCMC samples (default is 1000 burn-in samples and then 5000 more).\nLet’s continue the example from the previous section. We first create a ‘horseshoe object’.\n\nhs.object &lt;- HS.normal.means(df$y, method.tau = \"truncatedCauchy\", method.sigma = \"Jeffreys\")\n\nWe extract the posterior mean of the \\(\\beta_i\\)’s and plot them in red.\n\ndf$post.mean.full &lt;- hs.object$BetaHat\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full), size = 2, col = \"red\") +\n  ggtitle(\"Black = truth, Blue = observations, Red = estimates\")\n\n\n\n\n\n\n\n\nWe plot the marginal credible intervals (and remove the observations from the plot for clarity).\n\ndf$lower.CI &lt;- hs.object$LeftCI\ndf$upper.CI &lt;- hs.object$RightCI\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full), size = 2, col = \"red\") +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI), width = .1, col = \"red\") +\n  ggtitle(\"Black = truth, Red = estimates with 95% credible intervals\")\n\n\n\n\n\n\n\n\nFinally, we perform variable selection using HS.var.select. In the normal means problem, we can use two decision rules. We will illustrate them both. The first method checks whether zero is contained in the credible interval, as studied by Van der Pas et al (2017).\n\ndf$selected.CI &lt;- HS.var.select(hs.object, df$y, method = \"intervals\")\n\nThe result is a vector of zeroes and ones, with the ones indicating that the observations is suspected to correspond to an actual signal. We now plot the results, coloring the estimates/intervals blue if a signal is detected and red otherwise.\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full, col = factor(selected.CI)), \n             size = 2) +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI, col = factor(selected.CI)),\n                width = .1) +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Black = truth, Blue = selected as signal, Red = selected as noise\")\n\n\n\n\n\n\n\n\nThe other variable selection method is the thresholding method of Carvalho et al (2010). The posterior mean can be written as \\(c_iy_i\\) where \\(y_i\\) is the observation and \\(c_i\\) some number between 0 and 1. A variable is selected if \\(c_i \\geq c\\) for some user-selected threshold \\(c\\) (default is \\(c = 0.5\\)). In the example:\n\ndf$selected.thres &lt;- HS.var.select(hs.object, df$y, method = \"threshold\")\n\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full, col = factor(selected.thres)), \n             size = 2) +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI, col = factor(selected.thres)),\n                width = .1) +\n  theme(legend.position=\"none\") +\n  ggtitle(\"Black = truth, Blue = selected as signal, Red = selected as noise\")\n\n\n\n\n\n\n\n\n\n\n\n\nAndrews, D. F., and C. L. Mallows. 1974. “Scale Mixtures of Normal Distributions.” Journal of the Royal Statistical Society. Series B (Methodological) 36 (1): 99–102.\n\n\nBox, George E. P., and George C. Tiao. 1992. Bayesian Inference in Statistical Analysis. New York: Wiley-Interscience.\n\n\nCarvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010. “The Horseshoe Estimator for Sparse Signals.” Biometrika, asq017.\n\n\nDiaconis, P., and D. Ylvisaker. 1983. “Quantifying Prior Opinion.”\n\n\nEfron, Bradley, and Carl Morris. 1977. “Stein’s Paradox in Statistics.” Scientific American 236 (5): 119–27.\n\n\nFrank, Ildiko E., and Jerome H. Friedman. 1993. “A Statistical View of Some Chemometrics Regression Tools.” Technometrics 35 (2): 109–35.\n\n\nHuang, Jian, Joel L. Horowitz, and Shuangge Ma. 2008. “Asymptotic Properties of Bridge Estimators in Sparse High-Dimensional Regression Models.” The Annals of Statistics 36 (2): 587–613.\n\n\nMazumder, Rahul, Friedman, and Trevor and Hastie. 2011. “SparseNet: Coordinate Descent With Nonconvex Penalties.” Journal of the American Statistical Association 106 (495): 1125–38.\n\n\nStein, Charles. 1964. “Inadmissibility of the Usual Estimator for the Variance of a Normal Distribution with Unknown Mean.” Annals of the Institute of Statistical Mathematics 16 (1): 155–60.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Theory of AI</span>"
    ]
  },
  {
    "objectID": "14-rct.html",
    "href": "14-rct.html",
    "title": "6  Randomized Controlled Trials",
    "section": "",
    "text": "6.1 The Question of Causation\nFlorence Nightingale (1820 - 1910) was a heroine of the Crimean War, Patron Saint of Nurses, admirer of Quetelet, and champion of the statistical, study of society. She can be called the mother of observational studies. To her every piece of legislation was an experiment in the laboratory of society deserving study and demanding evaluation. Nightingale recognized the importance of collecting accurate and reliable data to understand healthcare outcomes. She developed standardized methods for collecting data on hospital admissions, deaths, causes of death, and other relevant factors. This systematic data collection allowed for more rigorous and reliable analysis of healthcare practices and their impact on patient outcomes. During the Crimean War (1853-1856), she collected and analyzed data on mortality rates among soldiers. She created statistical diagrams, such as the famous polar area diagram or “coxcomb,” to illustrate the causes of mortality. These visual representations helped to convey complex information in a clear and understandable way. Nightingale’s observations and statistical analyses led her to emphasize the importance of sanitation and hygiene in healthcare settings. She advocated for improvements in cleanliness, ventilation, and sanitation in hospitals, recognizing the impact of these factors on the health and well-being of patients. Beyond the battlefield, Nightingale continued her work in public health. She used statistical evidence to advocate for healthcare reforms and improvements in public health infrastructure. Her efforts played a crucial role in shaping public health policies and practices.\nThe work of Nightingale would be nowadays called an observational study. An observational study is a research design where researchers observe and collect data on existing groups of people or phenomena without intervening or manipulating any variables. Unlike randomized controlled trials, researchers do not assign participants to different groups and do not directly influence the outcome.\nGeorge Washington (1732-1799) on the other hand has made an enormous fortune in farming and one of the distinguishing features of his farming practices was the use of what we nowadays call a controlled experiment. He was deeply interested in improving agricultural techniques and conducted numerous experiments at his Mount Vernon estate. One of his most notable experiments involved dividing his land into plots and testing different crop rotations and fertilization methods. Washington recognized the importance of sustainable agriculture and the detrimental effects of monoculture (growing the same crop year after year) on soil fertility. He observed how tobacco, his primary cash crop at the time, depleted the soil nutrients, leading to diminishing yields. To address this issue and improve the long-term health of his land, he began experimenting with crop rotation and soil management techniques.\nWashington divided his land into several plots, each receiving different treatments. He experimented with various crop rotations, including wheat-fallow, wheat-rye-fallow, and corn-wheat-fallow. These rotations aimed to prevent soil depletion and promote its natural restoration by planting nitrogen-fixing crops like rye and clover. He also tested different fertilizer applications on various plots. He used manure, compost, and even imported materials like gypsum and marl to improve soil fertility and crop yields.\nWashington meticulously documented his experiments in his agricultural diaries. He recorded planting dates, yields, weather conditions, and observations on crop growth and soil health. This meticulous record-keeping allowed him to analyze the effectiveness of different treatments and compare their impact on crop yields and soil quality.\nWashington’s experiments yielded valuable insights into sustainable agricultural practices. He discovered that crop rotation and fertilization improved soil health and increased crop yields over time. He abandoned tobacco as his primary crop and shifted towards wheat, which was less soil-depleting and offered a more stable income source.\nThe historic trades staff at Mount Vernon have recreated Washington’s experiment at the Pioneer Farm, using the same plot layout, crops, and fertilization methods described in his diaries. This allows visitors to learn about his innovative farming techniques and their impact on the land. Figure 6.1 shows the plot layout at the Pioneer Farm.\nGeorge Washington’s commitment to experimentation and innovation made him a pioneer in American agriculture. His plot-based experiments demonstrated the effectiveness of crop rotation and soil management in promoting sustainable farming practices. His work continues to inspire farmers today and serves as a valuable resource for understanding agricultural history and best practices.\nLater, at the turn of the 20th century, Ronald Fisher (1890 - 1962) developed the theory of experimental design which allowed for controlled experiments, known as randomized controlled trials (RCT). Fisher’s work laid the foundation for modern experimental design and analysis, providing a rigorous statistical framework for conducting randomized controlled trials. His contributions to experimental design and ANOVA were crucial in establishing the importance of randomized trials in research. He emphasized the importance of randomization and control groups in experimental design, recognizing their crucial role in establishing causal relationships.\nThe modern randomized controlled trial (RCT) in medicine is most often attributed to Sir Austin Bradford Hill. In 1948, Hill published a landmark paper titled “Streptomycin Treatment of Pulmonary Tuberculosis” in the British Medical Journal, which described the first fully randomized, double-blind clinical trial. This study is considered a turning point in the history of medical research and established the RCT as the gold standard for evaluating the effectiveness of medical treatments.\nRandomized trials and observational studies are two distinct approaches to gathering and analyzing data in research studies. Here’s a breakdown of their key differences:\nIf you happen to have a choice between randomized trials and observational data (often you do not have that choice), which one should you choose? Here are a few things to consider:\nUltimately, both randomized trials and observational data play crucial roles in research. Combining these two approaches can provide a more comprehensive understanding of the relationship between interventions and outcomes.\nRothmstead t-rations split/pop designs\nRandomized controlled trials (RCTs) and field experiments are considered the gold standard for establishing causation because they allow researchers to isolate the effect of a specific intervention or treatment from other confounding factors. The main principle of RCTs and field experiments is randomization, which ensures that the treatment and control groups are similar in all respects except for the treatment. This allows researchers to attribute any differences in outcomes between the two groups to the treatment, rather than to other factors. Randomization helps to control for confounding variables, which are factors that are associated with both the treatment and the outcome variable. By randomly assigning participants to groups, researchers can ensure that any confounding variables are evenly distributed between the groups. The control group serves as a baseline for comparison. It is a group that is not exposed to the treatment or intervention being studied. By comparing the outcomes of the treatment group and the control group, researchers can isolate the effect of the treatment. Any differences in the outcomes between the two groups can be attributed to the treatment, rather than to other factors.\nFor many years, the main area of applications of randomized trials were medicine and agriculture. In medicine, randomized trials are used to test the effectiveness of new drugs and treatments. In agriculture, randomized trials are used to test the effectiveness of new fertilizers, pesticides, and other agricultural inputs. However, with the rise of internet, randomized trials have become increasingly popular for testing the effectiveness of online interventions, such as email campaigns, website designs, and social media ads. However, then applied to user experience and marketing, randomized trials are often called A/B tests. The idea of A/B testing is the same: randomly assign users to different versions of a website or an email campaign and compare the outcomes. However, the level or “rigor” of designing the experiment is often lower than in medicine or agriculture. There are less strict rules about ethics, randomization, sample size, and statistical analysis. For example, randomization sometimes completely ignored in A/B testing. Instead of assigning users randomly to groups, they are divided into groups based on factors like time of day, location, or browsing history. This can introduce bias into the results, as the groups may not be comparable. As a result, A/B testing is cheap and quick to conduct, as it can be done online without the need for IRB approval or recruitment of participants. Participants most of the times do not even know that they are participating in an A/B experiment. Futhermore, in A/B testing the primarily focused on measuring the comparative performance of variations without necessarily establishing a causal relationship. It answers questions like, “Which version of our web page leads to more clicks?”\nYet another area where RCTs are becoming popular are the economic studies. For example, randomized trials are used to test the effectiveness of educational interventions, such as tutoring programs and online courses. In recent years, randomized trials have been used to study a wide range of other phenomena, including education, economics, and public policy.\nWhile RCTs and field experiments are powerful tools for establishing causation, they are not always feasible or ethical. In some cases, observational studies may be the best way to study a particular phenomenon. However, even in observational studies, researchers can use techniques such as matching and instrumental variables to try to control for confounding variables. It is important to remember that even RCTs and field experiments cannot definitively prove causation. However, they provide the strongest evidence possible for a causal relationship between a treatment or intervention and an outcome.\nDiane Lambert was the original statistitian who promoted the ides of proper statistical usage of observational data among internet companies. She has presented on how to detect selection bias in data streams drawn from transaction logs, ad-systems, etc.; diagnostics to judge when bias overwhelms signal; and practical fixes such as propensity-score weighting, doubly-robust estimators, post-stratification and simulation so that credible causal conclusions can still be drawn from field/observational data. For example, at JSM 2011 (Miami) she co-authored the session “The Effectiveness of Display Ads,” contrasting large-scale field experiments with observational analyses of ad-click logs.\nThe advent of digital data has fundamentally transformed the practice of statistics, shifting the field from a discipline focused on small, carefully collected samples to one that must grapple with massive, often messy datasets generated as byproducts of digital systems. In the pre-digital era, statisticians worked primarily with structured, purposefully collected data through surveys, experiments, and clinical trials, where the sample size was a critical constraint and every observation was expensive to obtain. Today, organizations routinely collect terabytes of data from web traffic, sensor networks, financial transactions, and social media interactions, creating what some have called a “data deluge.” This shift has necessitated new statistical approaches that can handle high-dimensional data, complex dependencies, and the computational challenges of scale. Machine learning algorithms, once considered separate from traditional statistics, have become essential tools for extracting patterns from these vast datasets. However, this transition has also introduced new challenges: the need to distinguish correlation from causation in observational data, the importance of addressing selection bias in non-random samples, and the ethical considerations of privacy and algorithmic fairness. The field has evolved to embrace both the opportunities presented by big data—such as the ability to detect subtle patterns and make real-time predictions—and the responsibility to develop robust methods that can provide reliable insights despite the inherent noise and complexity of digital data sources.\nMarkets give signals in a form of betting odds, e.g. polymarket uses collective thought to produce a signal. It often happens that the collectife though outperforms the best experts.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Randomized Controlled Trials</span>"
    ]
  },
  {
    "objectID": "14-rct.html#bart-for-causal-inference",
    "href": "14-rct.html#bart-for-causal-inference",
    "title": "6  Randomized Controlled Trials",
    "section": "6.2 BART For Causal Inference",
    "text": "6.2 BART For Causal Inference\nEstimating the causal effect of an intervention, such as a new drug, a marketing campaign, or a public policy, is a central goal across science and industry. While the gold standard for causal inference is the Randomized Controlled Trial (RCT), it is often infeasible, unethical, or too expensive to conduct. Researchers must therefore turn to observational data, where the assignment of treatment is not controlled by the investigator. This introduces a fundamental challenge: individuals who receive the treatment may be systematically different from those who do not, a problem known as confounding. Separating the true effect of the treatment from these pre-existing differences is the primary task of causal inference from observational data.\nTo formalize causal questions, we rely on the Rubin Causal Model (RCM), also known as the potential outcomes framework. For a binary treatment \\(Z\\) (where \\(Z_i=1\\) if individual \\(i\\) receives the treatment and \\(Z_i=0\\) otherwise), we posit that each individual \\(i\\) has two potential outcomes: * \\(Y_i(1)\\): The outcome that would be observed if individual \\(i\\) were exposed to the treatment. * \\(Y_i(0)\\): The outcome that would be observed if individual \\(i\\) were exposed to the control (no treatment).\nThis framework leads directly to what Holland (1986) termed the “fundamental problem of causal inference”: for any given individual, we can only ever observe one of these two potential outcomes. The outcome we do not see is the counterfactual. Causal inference can thus be viewed as a missing data problem, where the goal is to estimate the values of the unobserved potential outcomes.\nFrom this foundation, we can define several key causal quantities, or estimands:\n\nIndividual Treatment Effect (ITE): The effect for a single individual, defined as \\[\\tau_i = Y_i(1) - Y_i(0).\\] This is typically unobservable.\nAverage Treatment Effect (ATE): The average effect across the entire population, \\[\\tau_{ATE} = E[Y(1) - Y(0)].\\] This is often the primary estimand of interest for broad policy questions.\nAverage Treatment Effect on the Treated (ATT): The average effect for those who actually received the treatment, \\[\\tau_{ATT} = E[Y(1) - Y(0) | Z=1].\\]\nConditional Average Treatment Effect (CATE): The average effect for a subpopulation defined by a set of covariates \\(X=x\\), \\[\\tau(x) = E[Y(1) - Y(0) | X=x].\\] Understanding the CATE allows for the exploration of treatment effect heterogeneity.\n\nTo estimate these causal estimands from observational data, we must rely on a set of critical, untestable assumptions that connect the observed data to the unobserved potential outcomes. These are known as identification assumptions.\n\nStable Unit Treatment Value Assumption (SUTVA): This assumption has two parts. First, it assumes there is no interference between units, meaning one individual’s treatment status does not affect another’s outcome. Second, it assumes there are no hidden variations of the treatment; the treatment assigned to one individual is the same as the treatment assigned to any other.\nIgnorability (or Unconfoundedness): This is the most crucial assumption. It states that, conditional on a set of observed pre-treatment covariates \\(X\\), treatment assignment \\(Z\\) is independent of the potential outcomes: \\[(Y(0), Y(1)) \\perp Z | X\\]. In essence, it assumes that we have measured all the common causes of both treatment selection and the outcome. If this holds, then within any stratum defined by the covariates \\(X\\), the treatment assignment is “as-if” random.\nPositivity (or Overlap/Common Support): This assumption requires that for any set of covariate values \\(x\\) present in the population, there is a non-zero probability of being in either the treatment or the control group: \\(0 &lt; P(Z=1 | X=x) &lt; 1\\). This ensures that we can find both treated and control individuals with similar characteristics, making comparison meaningful and avoiding extrapolation to regions with no data.\n\nTo demonstrate the application of Bayesian methods to this challenge, we use the famous Lalonde dataset, a canonical benchmark in the causal inference literature. The dataset addresses a real-world policy question: evaluating the effectiveness of the National Supported Work (NSW) Demonstration, a federally funded job training program implemented in the US from 1975-1979. The program was designed to help individuals facing significant social and economic barriers (e.g., former drug addicts, ex-convicts, high school dropouts) improve their labor market prospects. The treatment (\\(treat\\)) is participation in this program, and the primary outcome (\\(re78\\)) is the individual’s real earnings in 1978, after the program.\nThe historical importance of this dataset stems from Robert Lalonde’s 1986 paper, which delivered a powerful critique of the non-experimental methods used at the time. Lalonde started with data from an actual RCT, which provided an unbiased estimate of the program’s effect. He then took the treated group from the experiment but replaced the experimental control group with a non-experimental comparison group drawn from large public surveys—the Panel Study of Income Dynamics (PSID) and the Current Population Survey (CPS). He showed that the standard econometric models of the era failed to replicate the experimental benchmark when applied to this new, confounded dataset, casting serious doubt on their reliability for policy evaluation. Our task is to see if a modern, flexible Bayesian method—Bayesian Additive Regression Trees (BART)—can succeed where these earlier methods failed.\nThe challenge posed by the Lalonde dataset becomes immediately apparent when we examine the pre-treatment characteristics of the treated group versus the non-experimental control group. A naive comparison of their 1978 earnings would be deeply misleading because the groups were profoundly different before the program even began. Table 6.1 illustrates this imbalance for key covariates, including age, education, race, marital status, and earnings in the years prior to the intervention (1974 and 1975).\nThe Standardized Mean Difference (SMD) provides a scale-free measure of the difference between the group means. A common rule of thumb suggests that an absolute SMD greater than 0.1 indicates a potentially meaningful imbalance. As the table shows, the groups differ substantially on nearly every measured characteristic. The treated individuals were younger, less educated, more likely to be from minority groups, and had drastically lower earnings in the years before the program. This severe selection bias is precisely what makes the Lalonde dataset such a difficult and important test case for causal inference methods. Any credible method must be able to adjust for these vast pre-existing differences to isolate the true causal effect of the job training program.\n\n\n\nTable 6.1: Covariate Balance in the Lalonde Non-Experimental Dataset. Note: Data corresponds to the widely used Dehejia and Wahba (1999) sample of the Lalonde dataset. Standardized Mean Difference is calculated as the difference in means divided by the pooled standard deviation.\n\n\n\n\n\nCovariate\nTreated Mean\nControl Mean\nStd. Mean Diff.\n\n\n\n\nAge (years)\n25.82\n28.04\n-0.31\n\n\nEducation (years)\n10.35\n10.23\n0.06\n\n\nBlack (indicator)\n0.84\n0.20\n1.84\n\n\nHispanic (indicator)\n0.06\n0.14\n-0.32\n\n\nMarried (indicator)\n0.19\n0.51\n-0.81\n\n\nNo Degree (indicator)\n0.71\n0.60\n0.25\n\n\nEarnings 1974\n2095.57\n5630.71\n-0.63\n\n\nEarnings 1975\n1532.06\n5205.52\n-0.65\n\n\n\n\n\n\nTo address the challenge of confounding, we need a method that can flexibly model the relationship between the outcome, the treatment, and the many covariates shown to be imbalanced. Bayesian Additive Regression Trees (BART) is a powerful non-parametric machine learning algorithm that is exceptionally well-suited for this task.It combines the predictive power of ensemble methods with a rigorous Bayesian framework for regularization and uncertainty quantification.\nAt its core, BART models the expected value of an outcome \\(Y\\) as a sum of many individual regression trees. For a set of predictors \\(x\\), the model is:\n\\[Y = \\sum_{j=1}^{m} g(x; T_j, M_j) + \\epsilon, \\quad \\text{where} \\quad \\epsilon \\sim N(0, \\sigma^2)\\]\nHere, \\(m\\) is the number of trees in the ensemble (typically around 200), and each function \\(g(x; T_j, M_j)\\) represents a single regression tree. The structure of the tree is denoted by \\(T_j\\), and \\(M_j\\) is the set of parameter values in its terminal nodes (or leaves).\nCrucially, each individual tree is designed to be a “weak learner”. It is kept shallow and simple, meaning it explains only a small fraction of the variation in the outcome. The final, powerful prediction comes from summing up the contributions of all these simple components. This sum-of-trees structure allows BART to automatically capture very complex relationships, including high-order interactions and non-linearities, without the user needing to specify them in advance. For example, an interaction between age and education is implicitly modeled if a tree splits on education within a branch that has already been split on age. This flexibility is a major advantage in observational studies where the true functional form of the relationship between the outcome and the confounders is unknown.\nIn most machine learning algorithms, overfitting is controlled through techniques like cross-validation or complexity penalties. BART, being a fully Bayesian method, achieves this through a carefully specified set of regularization priors. These priors are designed to keep each tree simple and prevent any single tree from dominating the overall fit.\nThe key priors are:\n\nPrior on Tree Structure: This prior strongly encourages shallow trees. It is defined by a rule governing the probability that a node at a certain depth \\(d\\) will be split further. This probability is typically modeled as \\[p(T_j) = \\alpha(1+d)^{-\\beta},\\] where \\(\\alpha \\in (0,1)\\) and \\(\\beta \\ge 0\\) are hyperparameters. Setting \\(\\beta\\) to a value like 2 ensures that the probability of splitting decreases rapidly with depth, keeping the trees small.\nPrior on Terminal Node Parameters: After the response variable \\(Y\\) is centered and scaled, the values \\(\\mu_{jk}\\) in the terminal nodes of each tree are given a Normal prior, \\[\n\\mu_{jk} \\sim N(0, \\sigma_{\\mu}^2).\n\\] This prior shrinks the predictions within each leaf towards zero. Because the final prediction is a sum over \\(m\\) trees, this shrinkage ensures that the contribution of each individual tree is small.\nPrior on Error Variance: The residual variance \\(\\sigma^2\\) is typically given a conjugate Inverse-Gamma prior. This prior is usually chosen to be weakly informative, allowing the data to dominate the posterior estimate of the noise level, but it still constrains the variance to be reasonable.\n\nTogether, these priors act as a sophisticated regularization mechanism that allows BART to fit complex functions while being highly resistant to overfitting.\nBART models are fit using a Markov chain Monte Carlo (MCMC) algorithm, specifically a form of Gibbs sampler known as Bayesian backfitting. The algorithm does not find a single “best” model. Instead, it generates thousands of samples from the joint posterior distribution of all model parameters: \\(p(T_1,\\ldots,T_m, M_1,\\ldots,M_m, \\sigma | Y, X)\\).\nThe fitting process works iteratively :\n\nInitialize all \\(m\\) trees and \\(\\sigma\\).\nFor each tree \\(j\\) from 1 to \\(m\\):\n\nCalculate the “partial residual” by subtracting the predictions of all other trees from the outcome: \\[R_j = Y - \\sum_{k \\neq j} g(x; T_k, M_k)\\].\nDraw a new tree structure \\(T_j\\) and its leaf parameters \\(M_j\\) from their posterior distribution conditional on this partial residual, \\[p(T_j, M_j | R_j, \\sigma).\\]\n\nAfter iterating through all trees, draw a new value for \\(\\sigma\\) from its posterior conditional on the current residuals.\nRepeat steps 2 and 3 for thousands of iterations.\n\nThe output of this process is not one set of trees, but a collection of (e.g., 5000) sets of trees, where each set represents a plausible regression function drawn from the posterior distribution. This collection of draws is the key to quantifying uncertainty in a Bayesian way.\nThe power of BART for causal inference lies in how it leverages the full posterior distribution to estimate counterfactuals. The strategy aligns perfectly with the Bayesian view of causal inference as a missing data problem, as articulated by Rubin (1978).\nThe standard approach for causal inference with BART is to model the outcome \\(Y\\) as a function of both the covariates \\(X\\) and the treatment indicator \\(Z\\). The model learns a single, flexible response surface:\n\\[E[Y | X, Z] = f(X, Z)\\]\nHere, the treatment \\(Z\\) is included as if it were “just another covariate” in the set of predictors fed to the BART algorithm. The model is free to discover how the effect of \\(Z\\) varies with \\(X\\) through the tree-splitting process. The Conditional Average Treatment Effect (CATE) is then simply the difference in the predictions from this learned function:\n\\[\\tau(x) = f(x, Z=1) - f(x, Z=0)\\]\nThe core of the estimation process is a predictive step that is repeated for each draw from the MCMC sampler. Suppose the MCMC algorithm has produced \\(S\\) posterior draws of the function \\(f\\). For each draw \\(s = 1,\\ldots, S\\):\n\nWe take the full dataset of \\(n\\) individuals with their observed covariates \\(X\\).\nWe create two hypothetical, or counterfactual, datasets:\n\nTreated World: The observed covariates \\(X\\) for all \\(n\\) individuals, but with the treatment indicator set to \\(Z=1\\) for everyone.\nControl World: The observed covariates \\(X\\) for all \\(n\\) individuals, but with the treatment indicator set to \\(Z=0\\) for everyone.\n\nUsing the fitted BART model corresponding to posterior draw \\(s\\) (i.e., \\(f^{(s)}\\)), we predict the outcome for every individual under both scenarios. This gives us a full set of posterior predictive draws for the potential outcomes: \\(\\tilde{Y}_i(1)^{(s)}\\) and \\(\\tilde{Y}_i(0)^{(s)}\\) for each individual \\(i\\).\n\nThis process is a direct implementation of the missing data analogy. For an individual \\(i\\) who was actually treated (\\(Z_i=1\\)), their observed outcome \\(Y_i\\) is their potential outcome \\(Y_i(1)\\). The BART model provides a posterior predictive draw for their missing counterfactual outcome, \\(\\tilde{Y}_i(0)^{(s)}\\). Conversely, for a control subject, we use the model to predict their missing \\(\\tilde{Y}_i(1)^{(s)}\\).\nOnce we have the posterior draws of the potential outcomes for every individual at each MCMC iteration, we can compute a posterior draw for any causal estimand of interest. For example, at each iteration \\(s\\):\n\nITE draw: \\[\\tau_i^{(s)} = \\tilde{Y}_i(1)^{(s)} - \\tilde{Y}_i(0)^{(s)}\\]\nATE draw: \\[\\tau_{ATE}^{(s)} = \\frac{1}{n} \\sum_{i=1}^{n} \\tau_i^{(s)}\\]\n\nBy collecting these values across all \\(S\\) MCMC iterations, we obtain \\[\\{\\tau_{ATE}^{(1)}, \\tau_{ATE}^{(2)},\\ldots, \\tau_{ATE}^{(S)}\\}.\\] This set is a Monte Carlo approximation of the entire posterior distribution of the Average Treatment Effect.\nThis is a profoundly powerful result. Instead of a single point estimate and a standard error, the Bayesian approach yields a full probability distribution for the unknown causal effect. From this posterior distribution, we can easily calculate a posterior mean (our best point estimate) and a 95% credible interval. Unlike a frequentist confidence interval, the Bayesian credible interval has a direct and intuitive probabilistic interpretation: given our data and model, there is a 95% probability that the true value of the ATE lies within this range. This propagation of uncertainty from the model parameters all the way to the final causal estimate is a hallmark of the Bayesian approach.\nWe now apply this framework to the Lalonde dataset to estimate the causal effect of the NSW job training program on 1978 earnings.\nThe analysis is streamlined by using the bartCause package in R, which is specifically designed for causal inference with BART. The package provides a wrapper around the core dbarts implementation, simplifying the process of fitting the model and generating counterfactuals. A typical function call would look like this:\n\n# Load the package and data\nlibrary(bartCause)\ndata(lalonde)\n\n# Define confounders\nconfounders &lt;- c('age', 'educ', 'black', 'hisp', 'married', 'nodegr', 're74', 're75')\n\n# Fit the BART model\nfit &lt;- bartc(\n  response = lalonde$re78,\n  treatment = lalonde$treat,\n  confounders = lalonde[, confounders],\n  estimand = \"ate\",\n  commonSup.rule = \"sd\" # Rule to handle poor overlap\n)\n\nIn this call, we specify the outcome (re78), the binary treatment (treat), and the matrix of pre-treatment confounders. We set estimand = ate to target the Average Treatment Effect\nBefore interpreting the causal estimates, it is essential to perform MCMC diagnostics to ensure the algorithm has converged to a stable posterior distribution. The bartCause package provides plotting functions for this purpose. Trace plots for key parameters, such as the posterior draws of the ATE and the residual standard deviation (\\(\\sigma\\)), should be examined. These plots should show the chains mixing well and exploring a consistent region of the parameter space, without long-term drifts or stuck periods, indicating that the sampler has converged.\nThe primary result can be obtained by calling summary(fit). This provides the posterior mean of the ATE, which serves as our point estimate, along with a 95% credible interval. For a richer view, we can plot the entire posterior distribution of the ATE, which visualizes our uncertainty about the treatment effect.\nThe true power of this result is seen when placed in the context of other estimates, as shown in Table 6.2. The naive difference in means between the treated and control groups in the non-experimental data is large and negative, a direct consequence of the severe confounding. The experimental benchmark from the original RCT for this subset of treated individuals is an earnings gain of approximately $886. The BART estimate, after adjusting for the observed confounders, is remarkably close to this benchmark. This result demonstrates that a flexible, non-parametric Bayesian model like BART can successfully overcome the severe selection bias that plagued earlier econometric methods, effectively “solving” the problem posed by\n\n\n\nTable 6.2: Comparison of ATE Estimates for the NSW Program. Note: Estimates are for the non-experimental Lalonde sample (treated units from NSW, control units from PSID). The experimental benchmark is the difference-in-means estimate from the randomized trial for the same treated units. Uncertainty for BART is the posterior standard deviation\n\n\n\n\n\n\n\n\n\n\nMethod\nATE Estimate\nUncertainty (Std. Dev. / Interval)\n\n\n\n\nExperimental Benchmark\n886.3\n-277.37\n\n\nNaive Difference-in-Means\n-8492.24\n-633.91\n\n\nPropensity Score Matching\n1079.13\n-158.59\n\n\nDouble Machine Learning\n370.94\n-394.68\n\n\nCausal BART\n818.79\n-184.46\n\n\n\n\n\n\nWhile the ATE provides a useful summary, it can mask important variations in how the treatment affects different people. A policy might be beneficial on average but ineffective or even harmful for certain subgroups. A key advantage of BART is its ability to move beyond the average and explore this Heterogeneous Treatment Effect (HTE), which is critical for developing more targeted and effective policies.\nEstimating HTE allows us to answer questions like: “For whom does this program work best?” or “Are there individuals for whom the program is detrimental?” In settings with limited resources, this information is vital for allocating the intervention to those most likely to benefit. The flexibility of BART, which does not assume a constant treatment effect, makes it an ideal tool for this task.\nBecause BART provides a posterior predictive distribution of potential outcomes for every individual in the dataset, we can estimate an Individual Conditional Average Treatment Effect (ICATE) for each person. By plotting a histogram of the posterior means of these ICATEs, we can visualize the distribution of effects across the sample. This reveals whether the effect is consistent for everyone or if there is substantial variation, with some individuals benefiting much more than others.\nTo understand what drives this heterogeneity, we can examine how the estimated CATE varies as a function of key pre-treatment covariates. These relationships are often visualized using partial dependence plots. For the Lalonde data, such analyses have revealed that the effect of the job training program is not constant but varies non-linearly with characteristics like age and pre-treatment income (re74). For instance, the program’s benefit might increase with age up to a certain point and then decline, or it might be most effective for individuals with low-to-moderate prior earnings but less so for those with very low or higher earnings. These are nuanced, data-driven insights that would be completely missed by a standard linear regression model that only estimates a single average effect.\nA subtle but important issue can arise when using flexible regularized models like BART for causal inference in the presence of strong confounding, as is the case here. The regularization priors, which are designed to prevent overfitting, can shrink the estimated effects of the confounders towards zero. Because the treatment Z is highly correlated with these confounders, the model may mistakenly attribute some of the effect of the confounders to the treatment, leading to a bias known as Regularization-Induced Confounding (RIC).\nA powerful solution, proposed by Hahn, Murray, and Carvalho (2020), is to first estimate the propensity score, \\(\\pi(x) = P(Z=1|X)\\), which is the probability of receiving treatment given the covariates X. This score serves as a one-dimensional summary of all confounding information. This estimated propensity score is then included as an additional predictor in the BART outcome model. By providing this confounding summary directly to the model, we help the BART algorithm differentiate between the prognostic effects of the covariates (captured by \\(\\pi(x)\\)) and the causal effect of the treatment Z, thereby mitigating RIC. This “ps-BART” approach is considered state-of-the-art and is easily implemented in the bartCause package by setting the argument p.scoreAsCovariate = TRUE\n\n6.2.1 BART versus Propensity Score Matching (PSM)\nBART is one of several methods for causal inference from observational data. It is instructive to compare its philosophy with that of another widely used technique: Propensity Score Matching (PSM). BART and PSM represent two different philosophies for tackling confounding.Propensity Score Matching (PSM): This approach focuses on the design of the study. The goal is to use the observed data to construct a new sample in which the treatment and control groups are balanced on their observed covariates, thereby mimicking the properties of an RCT. The propensity score is the central tool used to achieve this balance. The analysis of the outcome is then performed on this newly created, “balanced” dataset.\nBART focuses on the analysis stage. The goal is to build a highly flexible and accurate predictive model for the outcome that explicitly includes the treatment and confounders, \\(E[Y|X,Z]\\). It uses the full dataset and relies on the model’s ability to correctly adjust for the confounding variables to isolate the causal effect.\nEach approach has its own set of advantages and disadvantages. PSM is often praised for its transparency; one can assess the quality of the covariate balance achieved by the matching procedure before ever looking at the outcome variable, reducing the risk of “p-hacking” or specification searching. However, PSM can be inefficient, as it often requires discarding a significant portion of the control group that does not have good matches in the treated group (i.e., poor overlap). It can also suffer from residual confounding if the matches are not sufficiently close. BART, on the other hand, is highly efficient as it uses all available data. Its main strengths are its flexibility in capturing unknown functional forms and interactions, its ability to easily estimate heterogeneous effects, and its principled framework for uncertainty quantification. Its primary weakness is that it can be perceived as a “black box” if not diagnosed carefully. Its validity, like all modeling approaches, depends on the untestable ignorability assumption, and as discussed, it can be susceptible to regularization-induced confounding if not applied with care.\nIn modern practice, the line between these two philosophies is blurring. It is now common to see them used in conjunction. For example, many practitioners use flexible machine learning models, including BART itself, to estimate the propensity scores used for matching or weighting, which can improve the quality of the covariate balance over simpler logistic regression models. Conversely, the state-of-the-art application of BART for causal inference (ps-BART) incorporates the propensity score directly into the outcome model. This convergence reflects a mature understanding that both balancing the data structure and flexibly modeling the outcome are complementary and powerful tools for robust causal inference.\n\nConceptual Comparison of BART and Propensity Score Matching\n\n\n\n\n\n\n\nFeature\nPropensity Score Matching (PSM)\nBayesian Additive Regression Trees (BART)\n\n\n\n\nPrimary Goal\nCreate balanced treatment/control groups (Design)\nFlexibly model the outcome-covariate relationship (Analysis)\n\n\nUse of Data\nOften discards unmatched units, reducing sample size\nUses all available data\n\n\nConfounding Control\nAchieved by balancing covariates via matching/weighting\nAchieved by conditioning on covariates in a flexible model\n\n\nKey Assumption\nCorrect specification of the propensity score model\nCorrect specification of the outcome model (though BART is very flexible)\n\n\nTreatment Effect\nPrimarily estimates ATT; ATE can be harder to estimate\nEasily estimates ATE, ATT, and CATE/HTE\n\n\nUncertainty\nOften requires bootstrapping for standard errors\nProvides full posterior distributions and credible intervals naturally\n\n\nFlexibility\nLimited by the PS model; main effect is assumed constant after matching\nHighly flexible; automatically models non-linearities and interactions\n\n\n\nThis example shows that BART, a flexible non-parametric method, can successfully adjust for severe confounding and recover a causal estimate that is remarkably close to the experimental benchmark, a feat that eluded many of the methods available when Lalonde first published his critique. It is crucial to remember that BART is not a panacea. Its validity, like that of any non-experimental method, rests on the untestable assumption of ignorability—that we have measured and adjusted for all relevant confounding variables. However, given that assumption, BART offers a suite of powerful advantages that make it a top-tier method in the modern causal inference landscape, a status confirmed by its consistent high performance in data analysis competitions. For the Bayesian statistician, the key takeaways are threefold:\n\nPhilosophical Coherence: BART provides a method for causal inference that is deeply integrated with Bayesian principles. It seamlessly frames the estimation of causal effects as a posterior predictive imputation of missing potential outcomes, propagating all sources of parameter uncertainty into the final result.\nRobustness to Misspecification: By using a flexible sum-of-trees ensemble, BART avoids the need for strong parametric assumptions about the functional form of the relationship between the covariates and the outcome. This provides robust protection against model misspecification bias, which is a major concern in observational studies where these relationships are complex and unknown.\nRichness of Inference: BART naturally yields a full posterior distribution for any causal estimand of interest. This allows for a more complete and intuitive quantification of uncertainty through credible intervals and facilitates the exploration of heterogeneous treatment effects, moving the analysis from a single average number to a nuanced understanding of for whom an intervention works.\n\n\n\n\n\nEnikolopov, Ruben, Vasily Korovkin, Maria Petrova, Konstantin Sonin, and Alexei Zakharov. 2013. “Field Experiment Estimate of Electoral Fraud in Russian Parliamentary Elections.” Proceedings of the National Academy of Sciences 110 (2): 448–52.\n\n\nHahn, P. Richard, Jared S. Murray, and Carlos M. Carvalho. 2020. “Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects (with Discussion).” Bayesian Analysis 15 (3): 965–1056.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Randomized Controlled Trials</span>"
    ]
  },
  {
    "objectID": "15-select.html",
    "href": "15-select.html",
    "title": "7  Model Selection",
    "section": "",
    "text": "7.1 Prediction vs Interpretation\nHere are a few important considerations when building predictive models:\n1. Model Selection: Choosing the right model for the relationship between \\(x\\) and \\(y\\) is crucial and involves navigating a fundamental trade-off between model complexity and generalization ability. If the chosen model is too simple (e.g., linear regression when the true relationship is polynomial), it might underfit the data and fail to capture important relationships, leading to high bias and poor performance on both training and test data. Conversely, a model that is too complex (e.g., high-degree polynomials or deep neural networks with insufficient data) might overfit the data by memorizing training examples rather than learning the underlying pattern, resulting in excellent training performance but poor generalization to unseen examples. This problem becomes even more complex when dealing with non-linear relationships, high-dimensional data, or noisy data, where the optimal model complexity is not immediately obvious and may require systematic experimentation with different model architectures, regularization techniques, and hyperparameter tuning to find the right balance between capturing the true signal while avoiding noise.\n2. Overfitting and Underfitting: Overfitting occurs when the model fits the training data too closely, capturing not only the true underlying relationship but also random noise and idiosyncrasies specific to the training dataset. This phenomenon typically manifests when a model has too many parameters relative to the amount of training data available, allowing it to essentially “memorize” the training examples rather than learning the generalizable patterns. The model may achieve excellent performance metrics on the training data (low training error) but will perform poorly on new, unseen data (high generalization error). This is because the model has learned to recognize specific noise patterns in the training data that don’t exist in the broader population. Common signs of overfitting include a large gap between training and validation/test performance, or performance that improves on training data while degrading on validation data during training iterations.\nUnderfitting occurs when the model is too simple and fails to capture the true relationship between x and y, often due to insufficient model complexity or inadequate training. This can happen when using a model that is inherently too simple for the problem at hand (e.g., linear regression for a highly non-linear relationship), when the model hasn’t been trained for enough iterations, or when regularization is applied too aggressively. Underfitting results in poor performance on both training and test data, as the model lacks the capacity to learn the underlying patterns in the data. The model essentially misses important features or relationships that are necessary for accurate predictions. Unlike overfitting, underfitting typically shows similar poor performance across training, validation, and test sets, indicating that the model is not capturing the signal in the data regardless of the dataset.\n3. Data Quality and Quantity: The accuracy of predictions heavily relies on the quality and quantity of the available data. If the data is noisy, inaccurate, or incomplete, it can lead to misleading predictions. A sufficient amount of data is also crucial to ensure the model can learn the underlying relationship effectively. Insufficient data can result in underfitting and poor generalization.\nData quality issues can manifest in various forms, including missing values, inconsistent formatting, labeling errors, and biased sampling. These problems are particularly acute in machine learning applications where large volumes of labeled data are required for training. To address these challenges, companies have emerged that specialize in data quality improvement and annotation services.\nCompanies like Scale AI and Toloka provide platforms that help organizations improve data quality through human-in-the-loop annotation and validation processes. These platforms employ large networks of human annotators who can perform tasks such as image labeling, text classification, data validation, and quality control. Scale AI, for example, offers services for creating high-quality training datasets through human annotation, with built-in quality control mechanisms that include multiple annotators per task and consensus-based validation. Their platform can handle various data types including images, text, and video, making it suitable for computer vision, natural language processing, and other AI applications.\nToloka, similarly, provides a crowdsourcing platform that connects businesses with a global network of contributors who can perform data labeling, content moderation, and quality assessment tasks. Their platform includes quality control features such as skill-based routing, where tasks are assigned to annotators based on their demonstrated expertise, and dynamic overlap, where multiple workers verify the same data to ensure accuracy.\nThese platforms help address several key data quality challenges: they can identify and correct labeling errors through consensus mechanisms, handle missing data through targeted collection efforts, and ensure consistency in data formatting and annotation standards. By leveraging human expertise at scale, these services enable organizations to create more reliable training datasets, which in turn leads to better-performing machine learning models and more accurate predictions.\n4. Model Explainability: In many applications, it is crucial to understand how the model arrives at its predictions. This is particularly important in areas like healthcare or finance, where transparency and interpretability are essential. Some models, particularly complex ones like deep neural networks, can be difficult to interpret, making it challenging to understand the rationale behind their predictions. However, modern machine learning has developed several techniques to address this challenge and make complex models more interpretable.\nThe importance of explainability extends beyond mere curiosity about model behavior. In healthcare applications, doctors need to understand why a model recommended a particular diagnosis or treatment plan to make informed decisions and maintain trust in the system. A model that predicts a patient has a 90% chance of having cancer but cannot explain which symptoms or test results led to this conclusion would be of limited clinical value. Similarly, in financial services, regulators require explanations for credit decisions to ensure compliance with fair lending laws and to prevent discriminatory practices. When a loan application is denied, both the applicant and regulatory bodies need to understand the specific factors that influenced this decision.\nIn legal and compliance contexts, explainability becomes a legal requirement. The European Union’s General Data Protection Regulation (GDPR) includes a “right to explanation” that allows individuals to request information about automated decisions that affect them. This has created a legal imperative for organizations to develop explainable AI systems. In criminal justice applications, where AI systems might be used for risk assessment or sentencing recommendations, the stakes are particularly high. Judges, lawyers, and defendants all need to understand how these systems arrive at their conclusions to ensure fair and just outcomes.\nOne prominent approach is the use of interpretable surrogate models, such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations). These methods work by approximating the complex model’s behavior in the vicinity of a specific prediction using simpler, more interpretable models like linear regression or decision trees. LIME, for instance, creates local explanations by sampling points around the prediction of interest and fitting a linear model to explain the model’s behavior in that neighborhood. This allows us to understand which features contributed most to a particular prediction, even for complex models like deep neural networks.\nAnother powerful technique is attention mechanisms, which have become increasingly popular in natural language processing and computer vision. Attention mechanisms allow models to “focus” on specific parts of the input when making predictions, providing a form of built-in interpretability. For example, in image classification tasks, attention maps can highlight which regions of an image the model is focusing on when making its prediction, making it easier to understand the model’s decision-making process.\nGradient-based methods offer another approach to model interpretability. Techniques like Grad-CAM (Gradient-weighted Class Activation Mapping) use gradients to identify which parts of the input are most important for the model’s prediction. By computing the gradient of the model’s output with respect to the input features, these methods can create heatmaps that show which features or regions contributed most to the final prediction.\nFor tree-based models like random forests and gradient boosting machines, built-in feature importance measures provide natural interpretability. These methods can rank features based on their contribution to the model’s predictive performance, offering insights into which variables are most important for making predictions.\nModel distillation techniques represent another approach, where a complex model (the teacher) is used to train a simpler, more interpretable model (the student) that mimics the teacher’s behavior. The student model, being simpler, is easier to interpret while maintaining much of the teacher’s predictive performance.\nFinally, counterfactual explanations provide a different type of interpretability by showing what changes to the input would be needed to change the model’s prediction. For example, if a loan application is rejected, a counterfactual explanation might show that the application would have been approved if the applicant’s income were $10,000 higher or if their credit score were 50 points better.\nThese modern interpretability techniques have made it possible to understand and explain the behavior of even the most complex models, addressing the “black box” problem that has historically limited the adoption of advanced machine learning methods in critical applications where transparency is essential.\n5. Computational Cost: Training and using prediction models can be computationally expensive, especially for complex models with large datasets. This can limit their applicability in resource-constrained environments. Finding a balance between model complexity, accuracy, and computational cost is critical for practical applications.\nThe computational demands of machine learning models have been significantly addressed through the development of specialized hardware, particularly Graphics Processing Units (GPUs). Originally designed for rendering graphics in video games, GPUs have become essential for deep learning due to their parallel processing architecture. Unlike traditional Central Processing Units (CPUs) that process tasks sequentially, GPUs can perform thousands of mathematical operations simultaneously, making them ideal for the matrix multiplications and tensor operations that are fundamental to neural network training. This parallel processing capability has reduced training times from weeks to hours or even minutes for many deep learning models, democratizing access to advanced machine learning techniques.\nHowever, the computational cost challenge extends beyond just training to the deployment phase, where models need to run efficiently in production environments. This has led to the emergence of edge computing as a crucial solution. Edge computing involves processing data and running models closer to where the data is generated, rather than sending everything to centralized cloud servers. This approach offers several advantages for machine learning applications: reduced latency for real-time predictions, lower bandwidth costs by processing data locally, and improved privacy by keeping sensitive data on local devices.\nEdge computing is particularly important for applications requiring real-time decision making, such as autonomous vehicles, industrial IoT systems, and mobile applications. For example, a self-driving car cannot afford the latency of sending sensor data to a cloud server and waiting for predictions to return; it needs to process information and make decisions locally within milliseconds. Similarly, smart manufacturing systems use edge computing to monitor equipment and predict maintenance needs in real-time without the delays associated with cloud processing.\nQuantization and lower precision calculations have emerged as powerful techniques for reducing computational costs while maintaining model performance. Traditional neural networks use 32-bit floating-point numbers (FP32) for all calculations, which provides high precision but requires significant computational resources and memory. Quantization reduces the precision of these numbers, typically to 16-bit (FP16), 8-bit integers (INT8), or even 4-bit integers (INT4), dramatically reducing both memory usage and computational requirements. For example, converting from FP32 to INT8 can reduce memory usage by 75% and computational cost by 2-4x, while often maintaining acceptable accuracy levels. This is particularly valuable for deployment on edge devices with limited resources, such as smartphones, IoT devices, and embedded systems. Modern hardware, including specialized AI accelerators like Google’s Tensor Processing Units (TPUs) and NVIDIA’s Tensor Cores, are specifically designed to handle these lower precision calculations efficiently, further reducing the computational cost barrier.\nThe trade-offs between computational cost and model performance are becoming increasingly sophisticated. Techniques like model pruning, which removes unnecessary connections from neural networks, can create smaller, faster models. Knowledge distillation allows large, complex models to transfer their knowledge to smaller, more efficient models that can run on resource-constrained devices.\nThese developments have created a spectrum of deployment options, from powerful cloud-based systems that can run the most complex models to lightweight edge devices that can perform basic predictions locally. The choice depends on the specific requirements of the application, including latency requirements, accuracy needs, privacy concerns, and cost constraints. As hardware continues to improve and optimization techniques become more sophisticated, the computational cost barrier to deploying machine learning models continues to decrease, opening up new possibilities for AI applications in previously inaccessible domains.\n6. Ethical Considerations: Predictions can have significant real-world consequences, raising ethical concerns about bias, fairness, and potential misuse. It is crucial to consider the potential harms and unintended consequences of predictions and implement safeguards to mitigate them.\nThe ethical implications of predictive models have become increasingly prominent as these systems are deployed in critical domains such as healthcare, criminal justice, employment, and financial services. One of the most significant concerns is algorithmic bias, which can perpetuate or amplify existing societal inequalities. For example, facial recognition systems have been shown to have higher error rates for people of color, potentially leading to wrongful arrests or surveillance. Similarly, hiring algorithms trained on historical data may perpetuate gender or racial biases present in past hiring decisions, creating a feedback loop that reinforces discrimination.\nFairness in machine learning has emerged as a critical research area, with multiple definitions and approaches to ensure equitable treatment across different demographic groups. Statistical parity, equalized odds, and individual fairness are among the various fairness metrics that can be applied depending on the specific context and requirements of the application. However, achieving fairness often involves trade-offs with model accuracy, and different fairness definitions may conflict with each other, requiring careful consideration of which definition is most appropriate for a given use case.\nThe potential for misuse of predictive models is another significant concern. Models designed for legitimate purposes can be repurposed for harmful applications, such as using facial recognition for mass surveillance or employing predictive policing algorithms that disproportionately target certain communities. Additionally, the increasing sophistication of deepfake technology, which uses predictive models to generate realistic but fake images, videos, or audio, raises concerns about misinformation and manipulation.\nPrivacy concerns arise when predictive models require access to sensitive personal data. The collection, storage, and processing of personal information for training and deploying these models can violate individual privacy rights and create risks of data breaches. Differential privacy techniques, which add carefully calibrated noise to data or model outputs, have emerged as a promising approach to protect individual privacy while maintaining model utility.\nTransparency and accountability are essential for addressing ethical concerns. Organizations deploying predictive models must be able to explain their decisions and be held accountable for any harms that result. This includes maintaining audit trails, implementing human oversight mechanisms, and establishing clear procedures for addressing complaints or errors. The concept of “algorithmic impact assessments” has been proposed as a framework for evaluating the potential social impacts of automated decision-making systems before deployment.\nRegulatory frameworks are evolving to address these ethical challenges. The European Union’s General Data Protection Regulation (GDPR) includes provisions for automated decision-making and profiling, while various jurisdictions are developing specific regulations for AI systems. These regulations often require transparency, human oversight, and the ability to contest automated decisions.\nTechnical approaches to addressing ethical concerns include adversarial training to reduce bias, interpretability techniques to increase transparency, and robust testing procedures to identify potential harms before deployment. Regular monitoring and updating of deployed models is also crucial, as societal norms and legal requirements evolve over time.\nAddressing these challenges requires careful consideration of the specific problem, selection of appropriate techniques, and continuous evaluation and improvement of the prediction model. It also requires collaboration between technical experts, domain specialists, ethicists, and stakeholders to ensure that predictive models serve the public good while minimizing potential harms.\nAs we have discussed at the beginning of this chapter the predictive rule can be used for two purposes: prediction and interpretation. The goal of interpretation is to understand the relationship between the input and output variables. The two goals are not mutually exclusive, but they are often in conflict. For example, a model that is good at predicting the target variable might not be good at interpreting the relationship between the input and output variables. A nice feature of a linear model is that it can be used for both purposes, unlike more complex predictive rules with many parameters that can be difficult to interpret.\nTypically the problem of interpretation requires a simpler model. We prioritize models that are easy to interpret and explain, even if they have slightly lower predictive accuracy. Also, evaluation metrics are different, we typically use coefficient of determination (R-squared) or p-values, which provide insights into the model’s fit and the significance of the estimated relationships.\nThe choice between using a model for prediction or interpretation depends on the specific task and desired outcome. If the primary goal is accurate predictions, a complex model with high predictive accuracy might be preferred, even if it is less interpretable. However, if understanding the underlying relationships and causal mechanisms is crucial, a simpler and more interpretable model might be chosen, even if it has slightly lower predictive accuracy. Typically interpretive models are used in scientific research, social sciences, and other fields where understanding the underlying causes and relationships is crucial.\nIn practice, it’s often beneficial to consider both prediction and interpretation when building and evaluating models. However, it is not unusual to build two different models, one for prediction and one for interpretation. This allows for a more nuanced analysis of the data and can lead to better insights than using a single model for both purposes.\nBreiman’s Two Cultures\nStatistical prediction problems are of great practical and theoretical interest. The deep learning predictor has a number of advantages over traditional predictors, including that\nLet \\(x\\) be a high dimensional input containing a large set of potentially relevant data. Let \\(y\\) represent an output (or response) to a task which we aim to solve based on the information in \\(x\\). Brieman [2000] summaries the difference between statistical and machine learning philosophy as follows.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#prediction-vs-interpretation",
    "href": "15-select.html#prediction-vs-interpretation",
    "title": "7  Model Selection",
    "section": "",
    "text": "input data can include all data of possible relevance to the prediction problem at hand\nnonlinearities and complex interactions among input data are accounted for seamlessly\noverfitting is more easily avoided than traditional high dimensional procedures\nthere exists fast, scale computational frameworks (TensorFlow)\n\n\n\n“There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.”\n\n\n“The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.”\n\n\n“Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.”",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#what-makes-a-good-model",
    "href": "15-select.html#what-makes-a-good-model",
    "title": "7  Model Selection",
    "section": "7.2 What makes a good model?",
    "text": "7.2 What makes a good model?\nWhat makes a good model? If the goal is prediction, then the model is as good as its prediction. The easiest way to visualize the quality of the prediction is to plot \\(y\\) vs \\(\\hat y\\). In the case of the linear regression model, the prediction interval is defined by \\[\ns\\sqrt{1+\\frac{1}{n}+\\frac{(x-\\bar x)^2}{\\sum_{i=1}^n(x_i-\\bar x)^2}}\n\\] where \\(s\\) is the standard deviation of the residuals. The prediction interval is the confidence interval for the prediction. The prediction interval is wider than the confidence interval because it includes the uncertainty in the prediction.\nThe bias-variance tradeoff is a fundamental property of statistical models. The bias is the difference between the expected value of the prediction and the true value \\(y-\\hat y\\). The variance is the variance of the prediction. The bias-variance tradeoff says that the bias and variance are inversely related. A model with high bias has low variance and a model with low bias has high variance. The bias-variance tradeoff is a consequence of the fact that the expected value of the prediction is the sum of the bias and the variance.\n\\[\nMSE = E(y-\\hat y)^2 = E(y-\\mathbb{E}(\\hat y))^2 + E(\\mathbb{E}(\\hat y)-\\hat y)^2\n\\] The bias-variance tradeoff is a consequence of the fact that the expected value of the prediction is the sum of the bias and the variance.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#exploratory-data-analysis",
    "href": "15-select.html#exploratory-data-analysis",
    "title": "7  Model Selection",
    "section": "7.3 Exploratory Data Analysis",
    "text": "7.3 Exploratory Data Analysis\nBefore deciding on a parametric model for a dataset. There are several tools that we use to choose the appropriate model. These include\n\nTheoretical assumptions underlying the distribution (our prior knowledge about the data)\nExploratory data analysis\nFormal goodness-of-fit tests\n\nThe two most common tools for exploratory data analysis are Q-Q plot, scatter plots and bar plots/histograms.\nA Q-Q plot simply compares the quantiles of your data with the quantiles of a theoretical distribution (like normal, exponential, etc.). Quantile is the fraction (or percent) of points below the given value. That is, the \\(i\\)-th quantile is the point \\(x\\) for which \\(i\\)% of the data lies below \\(x\\). On a Q-Q plot, if the two data sets come from a population with the same distribution, we should see the points forming a line that’s roughly straight. More precisely, if the two data sets \\(x\\) and \\(y\\) come from the same distribution, then the points \\((x_{(i)}, y_{(i)})\\) should lie roughly on the line \\(y = x\\). If \\(y\\) comes from a distribution that’s linear in \\(x\\), then the points \\((x_{(i)}, y_{(i)})\\) should lie roughly on a line, but not necessarily on the line \\(y = x\\).\n\nExample 7.1 (Normal Q-Q plot) Figure 7.1 shows the normal Q-Q plot for the Data on birth weights of babies born in a Brisbane hospital on December 18, 1997. The data set contains 44 records. A more detailed description of the data set can be found in UsingR manual.\n\nbabyboom = read.csv(\"../data/babyboom.csv\")\nqqnorm(babyboom$wt)\nqqline(babyboom$wt)\n\n\n\n\n\n\n\nFigure 7.1: Normal Q-Q plot of baby weights\n\n\n\n\n\nVisually, the answer to answer the question “Are Birth Weights Normally Distributed?” is no. We can see that on the left side of the plot the points are below the line. This indicates that the data is skewed to the left. The data is not normally distributed.\nThe Q-Q plots look different if we split the data based on the gender\ng = babyboom %&gt;% filter(gender==\"girl\") %&gt;% pull(wt) \nb = babyboom %&gt;% filter(gender==\"boy\")  %&gt;% pull(wt) \nqqnorm(g); qqline(g)\nqqnorm(b); qqline(b)\n\n\n\n\n\n\nGirls\n\n\n\n\n\n\n\nBoys\n\n\n\n\n\n\nHistogram of baby weights by gender\n\n\n\nHow about the times in hours between births of babies?\n\nhr = ceiling(babyboom$running.time/60)\nBirthsByHour = tabulate(hr)\n# Number of hours with 0, 1, 2, 3, 4 births\nObservedCounts = table(BirthsByHour) \n# Average number of births per hour\nBirthRate=sum(BirthsByHour)/24    \n# Expected counts for Poisson distribution\nExpectedCounts=dpois(0:4,BirthRate)*24    \n# bind into matrix for plotting\nObsExp &lt;- rbind(ObservedCounts,ExpectedCounts) \nbarplot(ObsExp,names=0:4, beside=TRUE,legend=c(\"Observed\",\"Expected\"))\n\n\n\n\n\n\n\n\nWhat about the Q-Q plot?\n\n# birth intervals\nbirthinterval=diff(babyboom$running.time) \n # quantiles of standard exponential distribution (rate=1)   \nexponential.quantiles = qexp(ppoints(43)) \nqqplot(exponential.quantiles, birthinterval)\nlmb=mean(birthinterval)\nlines(exponential.quantiles,exponential.quantiles*lmb) # Overlay a line\n\n\n\n\n\n\n\n\nHere\n\nppoints function computes the sequence of probability points\nqexp function computes the quantiles of the exponential distribution\ndiff function computes the difference between consecutive elements of a vector",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#out-of-sample-performance",
    "href": "15-select.html#out-of-sample-performance",
    "title": "7  Model Selection",
    "section": "7.4 Out of Sample Performance",
    "text": "7.4 Out of Sample Performance\nA parametric model that we choose to fit to data is chosen from a family of functions. Then, we use optimization to find the best model from that family. To find the best model we either minimize empirical loss or maximize the likelihood. We also established, that when \\(y \\sim N(f(x\\mid \\beta),\\sigma^2)\\) then mean squared error loss and negative log-likelihood are the same function. \\[\n\\E{y | x} = f(\\beta^Tx)\n\\]\nFor a regression model, an empirical loss measures a distance between fitted values and measurements and the goal is to minimize it. A typical choice of loss function for regression is \\[\nL (y,\\hat y) =  \\dfrac{1}{n}\\sum_{i=1}^n |y_i -  f(\\beta^Tx_i)|^p\n\\] When \\(p=1\\) we have MAE (mean absolute error), \\(p=2\\) we have MSE (mean squared error).\nFinding an appropriate family of functions is a major problem and is called model selection problem. For example, the choice of input variables to be included in the model is part of the model selection process. Model selection involves determining which predictors, interactions, or transformations should be included in the model to achieve the best balance between complexity and predictive accuracy. In practice, we often encounter several models for the same dataset that perform nearly identically, making the selection process challenging.\nIt is important to note that a good model is not necessarily the one that fits the data perfectly. Overfitting can occur when a model is overly complex, capturing noise rather than the underlying pattern. A good model strikes a balance between fitting the data well and maintaining simplicity to ensure generalizability to new, unseen data. For instance, including too many parameters can lead to a perfect fit when the number of observations equals the number of parameters, but such a model is unlikely to perform well on out-of-sample data.\nThe goal of model selection is not only to achieve a good fit but also to reduce complexity by excluding unnecessary parameters. This process typically involves selecting a model from a relevant class of functions while keeping in mind the trade-offs between bias, variance, and model complexity. Techniques such as cross-validation, information criteria (e.g., AIC, BIC), and regularization methods are commonly used to guide the model selection process.\nThe model selection task is sometimes one of the most consuming parts of the data analysis. Unfortunately, there is no single rule to find the best model. One way to think about the model choice problem as yet another optimization problem, with the goal to find best family of functions that describe the data. With a small number of predictors we can do brute force (check all possible models). For example, with \\(p\\) predictors there are \\(2^p\\) possible models with no interactions. Thus, the number of potential family functions is huge even for modest values of \\(p\\). One cannot consider all transformations and interactions.\nOur goal is to build a model that predicts well for out-of-sample data, e.g. the data that was not used for training. Eventually, we are interested in using our models for prediction and thus, the out of sample performance is the most important metric and should be used to choose the final model. In-sample performance is of little interest when predictive model need to be chosen, as one of the winners of Netflix prize put it, “It’s like predicting how much someone will like a movie, having them watch it and tell you how much they really liked it”. The out-of-sample performance is the final judge of the quality of our model. The goal is to use data to find a pattern that we can exploit. The pattern will be “statistical” in its nature. To uncover the pattern we start with a training dataset, denoted by \\[\nD = (y_i,x_i)_{i=1}^n\n\\] and to test the validity of our mode we use out-of-sample testing dataset \\[\nD^* = (y_j^*, x_j^*)_{j=1}^m,\n\\] where \\(x_i\\) is a set of \\(p\\) predictors ans \\(y_i\\) is response variable.\nA good predictor will “generalize” well and provide low MSE out-of-sample. These are a number of methods/objective functions that we will use to find, \\(\\hat f\\). In a parameter-based style we will find a black box. There are a number of ways to build our black box model. Our goal is to find the map \\(f\\) that approximates the process that generated the data. For example data could be representing some physical observations and our goal is recover the “laws of nature\" that led to those observations. One of the pitfalls is to find a map \\(f\\) that does not generalize. Generalization means that our model actually did learn the”laws of nature\" and not just identified patterns presented in training. The lack of generalization of the model is called over-fitting. It can be demonstrated in one dimension by remembering the fact from calculus that any set of \\(n\\) points can be approximated by a polynomial of degree \\(n\\), e.g we can alway draw a line that connects two points. Thus, in one dimension we can always find a function with zero empirical risk. However, such a function is unlikely to generalize to the observations that were not in our training data. In other words, the empirical risk measure for \\(D^*\\) is likely to be very high. Let us illustrate that in-sample fit can be deceiving.\n\nExample 7.2 (Hard Function) Say we want to approximate the following function \\[\nf(x) = \\dfrac{1}{1+25x^2}.\n\\] This function is simply a ratio of two polynomial functions and we will try to build a liner model to reconstruct this function\n\nx = seq(-2,2,by=0.01)\ny = 1/(1+25*x^2)\n# Approximate with polynomial of degree 1 and 2\nm1 = lm(y~x)\nm2 = lm(y~poly(x,2))\n# Approximate with polynomial of degree 20 and 5\nm20 = lm(y~poly(x,20))\nm5 = lm(y~poly(x,5))\nx = seq(-3,3,by=0.01)\ny = 1/(1+25*x^2)\nplot(x,y,type='l',col='black',lwd=2)\nlines(x,predict(m1,list(x=x)),lwd=2, col=1)\nlines(x,predict(m2,poly(x,2)),lwd=2, col=2)\nlines(x,predict(m5,poly(x,5)),lwd=2, col=3)\nlines(x,predict(m20,poly(x,20)),lwd=2, col=4)\nlegend(\"topright\", legend=c(\"f(x)\",\"m1\",\"m2\",\"m5\",\"m20\"), col=c(\"black\",1:4), lty=1, cex=0.8, bty='n')\n\n\n\n\n\n\n\nFigure 7.2: Runge-Kutta function\n\n\n\n\n\nFigure 7.2 shows the function itself (black line) on the interval \\([-3,3]\\). We used observations of \\(x\\) from the interval \\([-2,2]\\) to train the data (solid line) and from \\([-3,-2) \\cup (2,3]\\) (dotted line) to test the model and measure the out-of-sample performance. We tried four different linear functions to capture the relations. We see that linear model \\(\\hat y = \\beta_0 + \\beta_1 x\\) is not a good model. However, as we increas the degree of the polynomial to 20, the resulting model \\(\\hat y = \\beta_0 + \\beta_1x + \\beta_2 x^2 +\\ldots+\\beta_{20}x^{20}\\) does fit the training data set quite well, but does very poor job on the test data set. Thus, while in-sample performance is good, the out-of sample performance is unsatisfactory. We should not use the degree 20 polynomial function as a predictive model. In practice in-sample out-of-simple loss or classification rates provide us with a metric for providing horse race between different predictors. It is worth mentioning here there should be a penalty for overly complex rules which fits extremely well in sample but perform poorly on out-of-sample data. As Einstein famous said “model should be simple, but not simpler.”\n\nTo a Bayesian, the solution to these decision problems are rather obvious: compute posterior distributions, and then make decisions by maximizing expected utility, where the posterior distribution is used to calculate the expectations. Classical solutions to these problems are different, and use repeated sampling ideas, whereby the performance of a decision rule is judged on its performance if the same decision problem were repeated infinitely. Thus, the decisions are made based on their population properties. One of the main uses of statistical decision theory is to compare different estimators or hypothesis testing procedures. This theory generates many important findings, most notably that many of the common classical estimators are “bad”,in some sense, and that Bayesian estimators are always “good”.\nThese results have major implications for empirical work and practical applications, as they provide a guide for forecasting.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#bias-variance-trade-off",
    "href": "15-select.html#bias-variance-trade-off",
    "title": "7  Model Selection",
    "section": "7.5 Bias-Variance Trade-off",
    "text": "7.5 Bias-Variance Trade-off\nType II MLE: Marginal MLE (MMLE)\nOne approach to find “plug-in” estimates of hyper-parameters (a.k.a. amount of regularisation) is to use the marginal likelihood defined by\n\\[\nm(y | \\lambda) = \\int p(y | \\theta) p(\\theta | \\lambda) d\\theta.\n\\]\nWe then select\n\\[\n\\hat{\\lambda} = \\arg\\max_\\lambda \\; \\log m(y | \\lambda)\n\\]\nEssentially, we have a new objective function for finding the hyper-parameters (tuning parameters).\nWe can add a further regularisation penalty \\(-\\log p(\\lambda)\\) too.\nAs J.W.Tukey stated at the 1972 American Statistical Association meeting:\n\n“A feeling that any gains possible from a complicated procedurelike Stein’s could not be worththe extratroubl”",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#full-bayes",
    "href": "15-select.html#full-bayes",
    "title": "7  Model Selection",
    "section": "7.6 Full Bayes",
    "text": "7.6 Full Bayes\nShould also place a prior on hyper-parameters \\(p(\\lambda)\\). The optimal Bayes estimator under quadratic loss is\n\\[\n\\hat{\\theta}(y) = E(\\theta | y) = E_{\\lambda | y} \\left( E(\\theta | \\lambda, y) \\right).\n\\]\nwhere \\(E_{\\lambda | y}\\) is taken with respect to the marginal posterior\n\\[\np(\\lambda | y) = \\frac{m(y | \\lambda) p(\\lambda)}{m(y)}\n\\]\nThe choice of \\(p(\\lambda)\\) is an issue. Jeffreys, Polson and Scott propose the use of half-Cauchy priors \\(C^+(0,1)\\)-priors.\nFor any predictive model we seek to achieve best possible results, i.e. smallest MSE or misclassification rate. However, a model performance can be different as data used in one training/validation split may produce results dissimilar to another random split. In addition, a model that performed well on the test set may not produce good results given additional data. Sometimes we observe a situation, when a small change in the data leads to large change in the final estimated model, e.g. parameters of the model. These results exemplify the bias/variance tradeoff, where increasing model bias produces large variance in the final results. Similarly, low bias results in low variance, but can also produce an oversimplification of the final model. While Bias/variance concept is depicted below.\n\n\n\n\n\n\nFigure 7.3: Bias-variance trade-off\n\n\n\n\nExample 7.3 (Bias-variance) We demonstrate bias-variance concept using Boston housing example. We fit a model \\(\\mathrm{medv} = f(\\mathrm{lstat})\\). We use polynomial functions to approximate this relation. We fitted twelve polynomial functions with degree \\(1,\\ldots,12\\) ten time. Each time we randomly selected 20% of sample for testing and the rest for training. We estimated in-of-sample performance (bias) and out-of-sample performance by calculating MSE on training and testing sets correspondingly. For each polynomial \\(f\\) we averaged MSE from each of the ten models.\nFigure 7.4 (a) shows bias and variance for our twelve different models. As expected, bias increases while variance increases as model complexity grows. On the other hand out-of-sample MSE is a U-shaped curve. The optimal model is the one that has smallest out-of-sample MSE. In our case it is polynomial of degree 5!\n\n\n\n\n\n\n\n\n\n\n\n(a) Metrics for twelve polynomial functions fitted into Boston housing data set. As model complexity (degree of the polynomial function) increases, model variance increase and bias decreases. Out-of-sample MSE is smallest for 5th degree polynomial function, which is the optimal model in terms of bias-variance trade-off.\n\n\n\n\n\n\n\n\n\n\n\n(b) Optimal complexity model, which is 5th degree polynomial used to predict observations from testing data set. Model predictions (red line) are compared to actual observed values of medv variable (dots)\n\n\n\n\n\n\n\nFigure 7.4: Metrics for 12 models\n\n\n\nLet’s take another, a more formal, look at bias-variance trade-off for a linear regression problem. We are interested in the decomposition of the error \\(\\E{(y-\\hat y)^2}\\) as a function of bias \\(\\E{y-\\hat y}\\) and variance \\(\\Var{\\hat y}\\).\nHere \\(\\hat y = \\hat f_{\\beta}(x)\\) prediction from the model, and \\(y = f(x) + \\epsilon\\) is the true value, which is measured with noise \\(\\Var{\\epsilon} = \\sigma^2\\), \\(f(x)\\) is the true unknown function. The expectation above measures squared error of our model on a random sample \\(x\\). \\[\n\\begin{aligned}\n\\E{(y - \\hat{y})^2}\n& = \\E{y^2 + \\hat{y}^2 - 2 y\\hat{y}} \\\\\n& = \\E{y^2} + \\E{\\hat{y}^2} - \\E{2y\\hat{y}} \\\\\n& = \\Var{y} + \\E{y}^2 + \\Var{\\hat{y}} + \\E{\\hat{y}}^2 - 2f\\E{\\hat{y}} \\\\\n& = \\Var{y} + \\Var{\\hat{y}} + (f^2 - 2f\\E{\\hat{y}} + \\E{\\hat{y}}^2) \\\\\n& = \\Var{y} + \\Var{\\hat{y}} + (f - \\E{\\hat{y}})^2 \\\\\n& = \\sigma^2 + \\Var{\\hat{y}} + \\mathrm{Bias}(\\hat{y})^2\\end{aligned}\n\\] Here we used the following identity: \\(\\Var{X} = \\E{X^2} - \\E{X}^2\\) and the fact that \\(f\\) is deterministic and \\(\\E{\\epsilon} = 0\\), thus \\(\\E{y} = \\E{f(x)+\\epsilon} = f + \\E{\\epsilon} = f\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#cross-validation",
    "href": "15-select.html#cross-validation",
    "title": "7  Model Selection",
    "section": "7.7 Cross-Validation",
    "text": "7.7 Cross-Validation\nIf the data set at-hand is small and we cannot dedicate large enough sample size for testing, simply measuring error on test data set can lead to wrong conclusions. When size of the testing set \\(D^*\\) is small, the estimated out-of-sample performance is of high variance, depending on precisely which observations are included in the test set. On the other hand, when training set \\(D^*\\) is a large fraction of the entire sample available, estimated out-of-sample performance will be underestimated. Why?\nA trivial solution is to perform the training/testing split randomly several times and then use average out-of-sample errors. This procedure has two parameters, the fraction of samples to be selected for testing \\(p\\) and number of estimates to be performed \\(K\\). The resulting algorithm is as follows\nfsz = as.integer(p*n)\nerror = rep(0,K)\nfor (k in 1:K)\n{\n    test_ind = sample(1:n,size = fsz)\n    training = d[-test_ind,]\n    testing  = d[test_ind,]\n    m = lm(y~x, data=training)\n    yhat = predict(m,newdata = testing)\n    error[k] = mean((yhat-testing$y)^2)\n}\nres = mean(error)\nFigure 7.5 shows the process of splitting data set randomly five times.\nCross validation modifies the random splitting approach uses more “disciplined” way to split data set for training and testing. Instead of randomly selecting training data points, CV chooses consecutive observations and thus, each data point is used once for testing. As the random approach, CV helps addressing the high variance issue of out-of-sample performance estimation when data set available is small. Figure 7.6 shows the process of splitting data set five times using cross-validation approach.\n\n\n\n\n\n\n\n\n\nFigure 7.5: Bootstrap\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.6: Cross-validation\n\n\n\n\n\n\n\nTraining set (red) and testing set (green)\n\n\n\n\nExample 7.4 (Simulated) We use simulated data set to demonstrate difference between estimated out-of-sample performance using random 20/80 split, 5-fold cross-validation and random split. We used \\(x=-2,-1.99,-1.98,\\ldots,2\\) and \\(y = 2+3x + \\epsilon, ~ \\epsilon \\sim N(0,\\sqrt{3})\\). We simulated 35 datasets of size 100. For each of the simulated data sets, we fitted a linear model and estimated out-of-sample performance using three different approaches. Figure 7.7 compares empirical distribution of errors estimated from 35 samples.\n\n\n\n\n\n\nFigure 7.7: Empirical comparison of simple split, cross-validation, and bootstrap approaches to estimate out-of sample performance.\n\n\n\nAs we can see the estimated out-of-sample performance by a training set approach is of high variance. While, both cross-validation and bootstrap approaches lead to better estimates, they require model to be fitted 5 times, which can be computationally costly for a complex model. On the other hand, estimate from cross-validation is of lower variance and less bias compared to the bootstrap estimate. Thus, we should prefer cross-validation.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#small-sample-size",
    "href": "15-select.html#small-sample-size",
    "title": "7  Model Selection",
    "section": "7.8 Small Sample Size",
    "text": "7.8 Small Sample Size\nWhen sample size is small and it is not feasible to divide your data into training and validation data sets, an information criterion could be used to assess a model. We can think of information criterion as a metric that “approximates” out-os-sample performance of the model. Akaike’s Information Criterion (AIC) takes the form \\[\n\\mathrm{AIC} = log(\\sigma_k^2) + \\dfrac{n+2k}{n}\n\\] \\[\n\\hat{\\sigma}_k^2 = \\dfrac{SSE_k}{n}\n\\] Here \\(k\\) = number of coefficients in regression model, \\(SSE_k\\) = residual sum of square, \\(\\hat{\\sigma}_k^2\\) = MLE estimator for variance. We do not need to proceed sequentially, each model individually evaluated\nAIC is derived using the Kullback-Leibler information number. It is a ruler to measure the similarity between the statistical model and the true distribution. \\[\nI(g ; f) = E_g\\left(\\log \\left\\{\\dfrac{g(y)}{f(y)}\\right\\}\\right) = \\int_{-\\infty}^{\\infty}\\log \\left\\{\\dfrac{g(y)}{f(y)}\\right\\}g(y)dy.\n\\] Here - \\(I(g ; f) &gt; 0\\) - \\(I(g ; f) = 0 \\iff g(u) = f(y)\\) - \\(f \\rightarrow g\\) as \\(I(g ; f) \\rightarrow 0\\)\nTo estimate \\(I(g ; f)\\), we write \\[\nI(g ; f) = E_g\\left(\\log \\left\\{\\dfrac{g(y)}{f(y)}\\right\\}\\right) = E_g (\\log g(y)) - E_g(\\log f(y))\n\\] Only the second term is important in evaluating the statistical model \\(f(y)\\). Thus we need to estimate \\(E_g(\\log f(y))\\). Given sample \\(z_1,...,z_n\\), and estimated parameters \\(\\hat{\\theta}\\) a naive estimate is \\[\n\\hat{E}_g(\\log f(y)) =  \\dfrac{1}{n} \\sum_{i=1}^n \\log f(z_i) = \\dfrac{\\ell(\\hat{\\theta})}{n}\n\\] where \\(\\ell(\\hat{\\theta})\\) is the log-likelihood function for model under test.\n\nthis estimate is very biased\ndata used used twice: to get the MLE and second to estimate the integral\nit will favor those model that overfit\n\nAkaike showed that the bias is approximately \\(k/n\\) where \\(k\\) is the number of parameters \\(\\theta\\). Therefore we use \\[\n\\hat{E}_g(\\log f(y)) = \\dfrac{\\ell(\\hat{\\theta})}{n} - \\dfrac{k}{n}\n\\] Which leads to AIC \\[\nAIC = 2n \\hat{E}_g(\\log f(y)) = 2 \\ell(\\hat{\\theta}) - 2k\n\\]\nAkaike’s Information Criterion (AIC) \\[\n\\mathrm{AIC} = \\log(\\sigma_k^2) + \\dfrac{n+2k}{n}\n\\] Controls for balance between model complexity (\\(k\\)) and minimizing variance. The model selection process involve trying different \\(k\\), chose model with smallest AIC.\nA slightly modified version designed for small samples is the bias corrected AIC (AICc). \\[\n\\mathrm{AICc} = \\log(\\hat{\\sigma}_k^2) + \\dfrac{n+k}{n-k-2}\n\\] This criterion should be used for regression models with small samples\nYet, another variation designed for larger datasets is the Bayesian Information Criterion (BIC). \\[\n\\mathrm{BIC} = \\log(\\hat{\\sigma}_k^2) + \\dfrac{k \\log(n)}{n}\n\\] Is is the same as AIC but harsher penalty, this chooses simpler models. It works better for large samples when compared to AICc. The motivation fo BIC is from the posterior distribution over model space. Bayes rule lets you calculate the joint probability of parameter and models as \\[\np(\\theta,M\\mid D) = \\dfrac{p(D\\mid \\theta,M(p(M,\\theta)}{p(D)},~~ p(M\\mid D) = \\int p(\\theta,M\\mid D)d\\theta \\approx n^{p/2}p(D\\mid \\hat \\theta M)p(M).\n\\]\nConsider a problem of predicting mortality rates given pollution and temperature measurements. Let’s plot the data.\n\n\n\n\n\n\n\n\n\nFigure 7.8: Time Series Plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.9: Scatter Plot\n\n\n\n\n\n\nRegression Model 1, which just uses the trend: \\(M_t = \\beta_1 + \\beta_2 t + w_t\\). We fit by calling lm(formula = cmort ~ trend) to get the following coefficients\n                Estimate    Std. Error t value  \n    (Intercept) 3297.6062   276.3132   11.93\n    trend         -1.6249     0.1399  -11.61\nRegression Model 2 regresses to time (trend) and temperature: \\(M_t = \\beta_1 + \\beta_2 t + \\beta_t(T_t - T)+ w_t\\). The R call is lm(formula = cmort ~ trend + temp)\n                Estimate    Std. Error t value \n    (Intercept) 3125.75988  245.48233   12.73 \n    trend         -1.53785    0.12430  -12.37 \n    temp          -0.45792    0.03893  -11.76 \nRegression Model 3, uses trend, temperature and mortality: \\(M_t = \\beta_1 + \\beta_2 t + \\beta_3(T_t - T)+ \\beta_4(T_t - T)^2 + w_t\\). The R call is lm(formula = cmort ~ trend + temp + I(temp^2)\n                Estimate    Std. Error t value \n    (Intercept)  3.038e+03  2.322e+02  13.083 \n    trend       -1.494e+00  1.176e-01 -12.710 \n    temp        -4.808e-01  3.689e-02 -13.031 \n    temp2        2.583e-02  3.287e-03   7.858 \nRegression Model 4 adds temperature squared: \\[M_t = \\beta_1 + \\beta_2 t + \\beta_3(T_t - T)+ \\beta_4(T_t - T)^2 + \\beta_5 P_t+ w_t.\\]\nThe R call is lm(formula = cmort ~ trend + temp +  I(temp^2) + part)\n                Estimate    Std. Error t value \n    (Intercept)  2.831e+03  1.996e+02   14.19 \n    trend       -1.396e+00  1.010e-01  -13.82 \n    temp        -4.725e-01  3.162e-02  -14.94 \n    temp2        2.259e-02  2.827e-03    7.99 \n    part         2.554e-01  1.886e-02   13.54 \nTo choose the model, we look at the information criterion\n\n\n\nModel\n\\(k\\)\nSSE\ndf\nMSE\n\\(R^2\\)\nAIC\nBIC\n\n\n\n\n1\n2\n40,020\n506\n79.0\n.21\n5.38\n5.40\n\n\n2\n3\n31,413\n505\n62.2\n.38\n5.14\n5.17\n\n\n3\n4\n27,985\n504\n55.5\n.45\n5.03\n5.07\n\n\n4\n5\n20,508\n503\n40.8\n.60\n4.72\n4.77\n\n\n\n\\(R^2\\) always decreases with number of covariates (that is what MLE does). Thus, cannot be used as a selection criteria. \\(R^2\\) for out-of-sample data is useful!\nThe message to take home on model selection\n\n\\(R^2\\) is NOT a good metric for model selection\nValue of likelihood function is NOT a god metric\nare intuitive and work very well in practice (you should use those)\nAIC is good for big \\(n/df\\), so it overfits in high dimensions\nShould prefer AICc over AIC\nBIC underfits for large \\(n\\)\nCross-validation is important, we will go over it later",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#regularization",
    "href": "15-select.html#regularization",
    "title": "7  Model Selection",
    "section": "7.9 Regularization",
    "text": "7.9 Regularization\nRegularization is a technique to incorporate some prior knowledge about parameters of the model into the estimation process. Consider an example when regularization allows us to solve a hard problem of filtering noisy traffic data.\n\nExample 7.5 (Traffic) Consider traffic flow speed measured by an in-ground sensor installed on interstate I-55 near Chicago. Speed measurements are noisy and prone to have outliers. Figure 7.10 shows speed measured data, averaged over five minute intervals on one of the weekdays.\n\n\n\n\n\n\nFigure 7.10: Speed profile over 24 hour period on I-55, on October 22, 2013\n\n\n\nSpeed measurements are noisy and prone to have outliers. There are two sources of noise. The first is the measurement noise, caused by inhalant nature of the sensor’s hardware. The second source is due to sampling error, vehicles observed on a specific lane where senor installed might not represent well traffic in other lanes.\nTrend filtering, which is a variation of a well-know Hodrick-Prescott filter. In this case, the trend estimate is the minimizer of the weighted sum objective function \\[\n(1/2) \\sum_{t=1}^{n}(y_t - x_t)^2 + \\lambda \\sum_{t=1}^{n-1}|x_{t-1} - 2x_t + x_{t+1}|,\n\\]\n\n\n\n\n\n\nTrend filter for different penalty\n\n\n\n\n\n\n\nTrend filtering with optimal penalty\n\n\n\n\n\n\nTrend Filtering for Traffic Speed Data",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#ridge-regression",
    "href": "15-select.html#ridge-regression",
    "title": "7  Model Selection",
    "section": "7.10 Ridge Regression",
    "text": "7.10 Ridge Regression\nGauss invented the concept of least squares and developed algorithms to solve the the optimization problem \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad ||y- X\\beta||_2^2\n\\] where \\(\\beta = (\\beta_1 , \\ldots , \\beta_p )\\), we can use linear algebra algorithms, the solution given by \\[\n\\hat{\\beta} = ( X^T X )^{-1} X^T y\n\\] This can be numerically unstable when \\(X^T X\\) is ill-conditioned, and happens when \\(p\\) is large. Ridge regression addresses this problem by adding an extra term to the \\(X^TX\\) matrix \\[\n\\hat{\\beta}_{\\text{ridge}} = ( X^T X + \\lambda I )^{-1} X^T y.\n\\] The corresponding optimization problem is \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad ||y- X\\beta||_2^2   + \\lambda||\\beta||_2^2.\n\\] We can think of the constrain is of a budget on the size of \\(\\beta\\). Ridge reguralization was first proposed in solving inverse problems to “discover” physical laws from observations and the norm of the \\(\\beta\\) vector would usually represent amount of energy required, and the 2-norm penalty term allows to find find a balance between data fidelity and solution simplicity. The regularization term acts to constrain the solution space, preventing it from reaching high-energy (overly complex) states, most of the times nature chooses the path of least resistance, thus minimal energy solutions are practical.\nThe we choose \\(\\lambda\\) over a regularisation path. The penalty in ridge regression forces coefficients \\(\\beta\\) to be close to 0. Penalty is large for large values and very small for small ones. Tuning parameter \\(\\lambda\\) controls trade-off between how well model fits the data and how small \\(\\beta\\)s are. Different values of \\(\\lambda\\) will lead to different models. We select \\(\\lambda\\) using cross validation.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#kernel-view",
    "href": "15-select.html#kernel-view",
    "title": "7  Model Selection",
    "section": "7.11 Kernel view",
    "text": "7.11 Kernel view\nAnother interesting view stems from what is called the push-through matrix identity: \\[\n(aI + UV)^{-1}U = U(aI + VU)^{-1}\n\\] for \\(a\\), \\(U\\), \\(V\\) such that the products are well-defined and the inverses exist. We can obtain this from \\(U(aI + VU) = (aI + UV)U\\), followed by multiplication by \\((aI + UV)^{-1}\\) on the left and the right. Applying the identity above to the ridge regression solution with \\(a = \\lambda\\), \\(U = X^T\\), and \\(V = X\\), we obtain an alternative form for the ridge solution: \\[\n\\hat{\\beta} = X^T (XX^T + \\lambda I)^{-1} Y.\n\\] This is often referred to as the kernel form of the ridge estimator. From this, we can see that the ridge fit can be expressed as \\[\nX\\hat{\\beta} = XX^T (XX^T + \\lambda I)^{-1} Y.\n\\] What does this remind you of? This is precisely \\(K(K + \\lambda I)^{-1}Y\\) where \\(K = XX^T\\), which, recall, is the fit from RKHS regression with a linear kernel \\(k(x, z) = x^T z\\). Therefore, we can think of RKHS regression as generalizing ridge regression by replacing the standard linear inner product with a general kernel. (Indeed, RKHS regression is often called kernel ridge regression.) \\[\n(aI + UV)^{-1}U = U(aI + VU)^{-1}\n\\] for \\(a\\), \\(U\\), \\(V\\) such that the products are well-defined and the inverses exist. We can obtain this from \\(U(aI + VU) = (aI + UV)U\\), followed by multiplication by \\((aI + UV)^{-1}\\) on the left and the right. Applying the identity above to the ridge regression solution with \\(a = \\lambda\\), \\(U = X^T\\), and \\(V = X\\), we obtain an alternative form for the ridge solution: \\[\n\\hat{\\beta} = X^T (XX^T + \\lambda I)^{-1} Y.\n\\] This is often referred to as the kernel form of the ridge estimator. From this, we can see that the ridge fit can be expressed as \\[\nX\\hat{\\beta} = XX^T (XX^T + \\lambda I)^{-1} Y.\n\\] What does this remind you of? This is precisely \\(K(K + \\lambda I)^{-1}Y\\) where \\(K = XX^T\\), which, recall, is the fit from RKHS regression with a linear kernel \\(k(x, z) = x^T z\\). Therefore, we can think of RKHS regression as generalizing ridge regression by replacing the standard linear inner product with a general kernel. (Indeed, RKHS regression is often called kernel ridge regression.)\n\nExample 7.6 (Shrinkage) Consider a simulated data with \\(n=50\\), \\(p=30\\), and \\(\\sigma^2=1\\). The true model is linear with \\(10\\) large coefficients between \\(0.5\\) and \\(1\\).\nOur approximators \\(\\hat f_{\\beta}\\) is a linear regression. We can empirically calculate the bias by calculating the empirical squared loss \\(1/n||y -\\hat y||_2^2\\) and variance can be empirically calculated as \\(1/n\\sum  (\\bar{\\hat{y}} - \\hat y_i)\\)\nBias squared \\(\\mathrm{Bias}(\\hat{y})^2=0.006\\) and variance \\(\\Var{\\hat{y}} =0.627\\). Thus, the prediction error = \\(1 + 0.006 + 0.627 = 1.633\\)\nWe’ll do better by shrinking the coefficients to reduce the variance. Let’s estimate, how big a gain will we get with Ridge?\n\n\n\nTrue model coefficients\n\n\nNow we see the accuracy of the model as a function of \\(\\lambda\\)\n\n\n\nPrediction error as a function of \\(\\lambda\\)\n\n\nRidge Regression At best: Bias squared \\(=0.077\\) and variance \\(=0.402\\).\nPrediction error = \\(1 + 0.077 + 0.403 = 1.48\\)\n\n\n\nRidge\n\n\n\nThe additional term \\(\\lambda||\\beta||_2^2\\) in the optimization problem is called the regularization term. There are several ways to regularize an optimization problem. All of those techniques were developed in the middle of last century and were applied to solve problems of fitting physics models into observed data, those frequently arise in physics and engineering applications. Here are a few examples of such regularization techniques.\nIvanov regularization \\[\n\\underset{x \\in \\mathbb{R^n}}{\\mathrm{minimize}}\\quad ||y - X\\beta||_2^2~~~~ \\mbox{s.t.}~~||\\beta||_l \\le k\n\\]\nMorozov regularization \\[\n\\underset{x \\in \\mathbb{R^n}}{\\mathrm{minimize}}\\quad ||\\beta||_l~~~~ \\mbox{s.t.}~~ ||y - X\\beta||_2^2 \\le \\tau\n\\] Here \\(\\tau\\) reflects the so called noise level, i.e. an estimate of the error which is made during the measurement of \\(b\\).\nTikhonov regularization \\[\n\\underset{\\beta\\in \\mathbb{R^n}}{\\mathrm{minimize}}\\quad ||y - X\\beta||_2^2 + \\lambda||\\beta||_l\n\\] - Tikhonov regularization with \\(l=1\\) is lasso - Tikhonov regularization with \\(l=2\\) is ridge regression - lasso + ridge = elastic net",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#ell_1-regularization-lasso",
    "href": "15-select.html#ell_1-regularization-lasso",
    "title": "7  Model Selection",
    "section": "7.12 \\(\\ell_1\\) Regularization (LASSO)",
    "text": "7.12 \\(\\ell_1\\) Regularization (LASSO)\nThe Least Absolute Shrinkage and Selection Operator (LASSO) uses \\(\\ell_1\\) norm penalty and in case of linear regression leads to the following optimization problem \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad ||y- X\\beta||_2^2   + \\lambda||\\beta||_1\n\\]\nIn one dimensional case solves the following optimization problem \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad  \\frac{1}{2} (y-\\beta)^2 + \\lambda | \\beta |\n\\] The solution is given by the soft-thresholding operator defined by \\[\n\\hat{\\beta} = \\mathrm{soft} (y; \\lambda) = ( y - \\lambda ~\\mathrm{sgn}(y) )_+.\n\\] Here sgn is the sign function and \\(( x )_+ = \\max (x,0)\\). To demonstrate how this solution is derived, we can define a slack variable \\(z = | \\beta |\\) and solve the joint constrained optimisation problem which is differentiable.\nGraphically, the soft-thresholding operator is\n\n\n\nSoft-threshold operator.\n\n\nLASSO has a nice feature that it forces some of the \\(\\hat{\\beta}\\)’s to zero. It is an automatic variable selection! Finding optimal solution is computationally fast, it is a convex optimisation problem, though, it is non-smooth. As in ridge regression, we still have to pick \\(\\lambda\\) via cross-validation. Visually the process can be represented using regularization path graph, as in the following example Example: We model prostate cancer using LASSO\n\n\n\n\n\n\nMSE.\n\n\n\n\n\n\n\nPath\n\n\n\n\n\n\nMSE and Regularization path for Prostate Cancer data using LASSO\n\n\n\nNow with ridge regression\n\n\n\n\n\n\nMSE\n\n\n\n\n\n\n\nPath\n\n\n\n\n\n\nMSE and Regularization path for Prostate Cancer data using Ridge\n\n\n\n\nExample 7.7 (Horse race prediction using logistic regression) We use the run.csv data from Kaggle (https://www.kaggle.com/gdaley/hkracing). Thhis dataset contains the condition of horse races in Hong Kong, including race course, distance, track condition and dividends paid. We want to use individual variables to predict the chance of winning of a horse. For the simplicity of computation, we only consider horses with id \\(\\leq 500\\), and train the model with \\(\\ell_1\\)-regularized logistic regression.\nAnd we include lengths_behind, horse_age, horse_country, horse_type, horse_rating, horse_gear, declared_weight, actual_weight, draw, win_odds, place_odds as predicting variables in our model.\nSince most of the variables, such as country, gear, type, are categorical, after spanning them into binary indictors, we have more than 800 columns in the design matrix.\nWe try two logistic regression model. The first one includes win_odds given by the gambling company. The second one does not include the win_odds and we use win_odds to test the power of our model. We tune both models with a 10-fold cross-validation to find the best penalty parameter \\(\\lambda\\).\nIn this model, we fit the logistic regression with full dataset. The best \\(\\lambda\\) we find is \\(5.699782e-06\\).\n\n\n\n\n\n\nNumber of variables vs lambda\n\n\n\n\n\n\n\nCoefficient Ranking\n\n\n\n\n\n\nLogistic regression for full data\n\n\n\nIn this model, we randomly partition the dataset into training(70%) and testing(30%) parts. We fit the logistic regression with training dataset. The best \\(\\lambda\\) we find is \\(4.792637e-06\\).\n\n\n\n\n\n\nNumber of variables vs lambda\n\n\n\n\n\n\n\nCoefficient Ranking\n\n\n\n\n\n\nLogistic regression for test data\n\n\n\nThe out-of-sample mean squared error for win_odds is 0.0668.\n\nElastic Net combines Ridge and Lasso and chooses coefficients \\(\\beta_1,\\ldots,\\beta_p\\) for the input variables by minimizing the sum-of-squared residuals plus a penalty of the form \\[\n\\lambda||\\beta||_1 + \\alpha||\\beta||_2^2.\n\\]",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#bayesian-model-selection",
    "href": "15-select.html#bayesian-model-selection",
    "title": "7  Model Selection",
    "section": "7.13 Bayesian Model Selection",
    "text": "7.13 Bayesian Model Selection\nWhen analyzing data, we deal with three types of quantities\n\n\\(X\\) = observed variables\n\\(Y\\) = hidden variable\n\\(\\theta\\) = parameters of the model that describes the relation between \\(X\\) and \\(Y\\).\n\nA probabilistic models of interest are the joint probability distribution \\(p(D,\\theta)\\) (called a generative model) and \\(P(Y,\\theta \\mid X)\\) (discriminative model). Discriminative models are easier to build and are more frequently used in practice. Generative model requires modeling a distribution over the set of observed variables, which makes our model more complicated. Text analysis provides an illustrative example. The task of identifying a topic of an article can be solved using discriminative distribution. The problem of generating a new article requires generative model.\nWhile performing data analysis using learning algorithms, we perform two tasks, namely training and inference which are summarized in the table below\n\n\n\nStep\nGiven\nHidden\nWhat to find\n\n\n\n\nTraining\n\\(D = (X,Y) = \\{x_i,y_i\\}_{i=1}^n\\)\n\\(\\theta\\)\n\\(p(\\theta \\mid D)\\)\n\n\nPrediction\n\\(x_{\\text{new}}\\)\n\\(y_{\\text{new}}\\)\n\\(p(y_{\\text{new}}  \\mid  x_{\\text{new}}, D)\\)\n\n\n\nThe training can be performed via the Bayes rule \\[\np(\\theta \\mid D) = \\dfrac{p(Y \\mid \\theta,X)p(\\theta)}{\\int p(Y \\mid \\theta,X)p(\\theta)d\\theta}.\n\\] Now to perform the second step (prediction), we calculate \\[\np(y_{\\text{new}}  \\mid  x_{\\text{new}}, D) = \\int p(y_{\\text{new}}  \\mid  x_{\\text{new}},\\theta)p(\\theta \\mid D)d\\theta\n\\] Thus, full Bayesian inference requires calculating two integrals, which might be difficult. We mentioned earlier that MAP allows us to avoid those calculations by approximating the posterior with \\[\np(\\theta \\mid D) \\approx \\delta(\\theta_{\\text{MAP}}),~~\\theta_{\\text{MAP}} \\in \\argmax_{\\theta}p(\\theta \\mid D)\n\\] To calculate \\(\\theta_{\\text{MAP}}\\), we do not need to know the normalizing constant for calculating posterior, since the solution of optimization problem does not depend on this constant. Further, the second integral for inference becomes degenerate and get approximated by \\[\np(y_{\\text{new}}  \\mid  x_{\\text{new}}, D) = \\int p(y_{\\text{new}}  \\mid  x_{\\text{new}},\\theta)p(\\theta \\mid D)d\\theta \\approx p(y_{\\text{new}}  \\mid  x_{\\text{new}},\\theta_{\\text{MAP}}).\n\\]\nNow we consider a case, when we have several candidate density functions for performing the prediction \\[\np_1(Y,\\theta  \\mid  X), ~~p_2(Y,\\theta \\mid X),\\ldots\n\\] How do we choose the better model? We can choose the model with highest evidence value (due to David MacKay) \\[\nj = \\argmax_j p_j(Y  \\mid  X) = \\argmax_j \\int p_j(Y  \\mid  X,\\theta)p(\\theta)d\\theta.\n\\] Note, formally instead of \\(p(\\theta)\\) we need to write \\(p(\\theta \\mid X)\\), however since \\(\\theta\\) does not depend on \\(X\\) we omit it.\n\n\n\nModel Selection\n\n\nCan you think of how the prior \\(p(\\theta)\\), posterior \\(p(\\theta \\mid D)\\) and the evidence \\(p(Y \\mid X)\\) distributions will look like? Which model is the best? Which model will have the highest \\(\\theta_{\\text{MAP}}\\)?\n\nExample 7.8 (Racial discrimination) Say we want to analyze racial discrimination by the US courts. We have three variables:\n\nMurderer: \\(m \\in {0,1}\\) (black/white)\nVictim: \\(v \\in \\{0,1\\}\\) (black/white)\nVerdict: \\(d \\in \\{0,1\\}\\) (prison/death penalty)\n\nSay we have the data\n\n\n\nm\nv\nd\nn\n\n\n\n\n0\n0\n0\n132\n\n\n0\n0\n1\n19\n\n\n0\n1\n0\n9\n\n\n0\n1\n1\n0\n\n\n1\n0\n0\n52\n\n\n1\n0\n1\n11\n\n\n1\n1\n0\n97\n\n\n1\n1\n1\n6\n\n\n\nWe would like to establish a causal relations between the race and verdict variables. For this, we consider several models\n\n\\(p(d \\mid m,v) = p(d) = \\theta\\)\n\\(p(d \\mid m,v) = p(d \\mid v)\\); \\(p(d \\mid v=0) = \\alpha, ~p(d \\mid v=1)=\\beta\\)\n\\(p(d \\mid v,m) = p(d \\mid m)\\); \\(p(d \\mid m=1) = \\gamma,~p(d \\mid m=1) = \\delta\\)\n\\(p(d|v,m)\\) cannot be reduced, and\n\n\n\n\n\\(p(d=1 \\mid m,v)\\)\n\\(m=0\\)\n\\(m=1\\)\n\n\n\n\n\\(v=0\\)\n\\(\\tau\\)\n\\(\\chi\\)\n\n\n\\(v=1\\)\n\\(\\nu\\)\n\\(\\zeta\\)\n\n\n\n\nWe calculate which model describes data the best, we calculate the evidences. We need to describe the discriminative model \\[\np(Y ,\\theta \\mid X) = p(Y \\mid X,\\theta)p(\\theta \\mid X)\n\\] Here \\(X\\) is the number of cases, and \\(Y\\) is the number of death penalties. We use uninformative prior \\(\\theta \\sim U[0,1]\\). To specify the likelihood, we use Binomial distribution \\[\nY \\mid X,\\theta \\sim B(X,\\theta),~~B(Y \\mid X,\\theta) = C_Y^Xp^Y(1-\\theta)^{X-Y}\n\\] We assume \\(p(\\theta)\\sim Uniform\\). Now lets calculate the evidence \\[\np(Y, \\theta \\mid X) = \\int p(Y  \\mid  X,\\theta)p(\\theta)d\\theta\n\\] for each of the four models\n\n\\(p(Y \\mid X) = \\int B(19 \\mid 151,\\theta)B(0 \\mid 9,\\theta)B(11 \\mid 63,\\theta)B(6 \\mid 103,\\theta)d\\theta\\) \\(\\propto \\int_0^{1} \\theta^{36}(1-\\theta)^{290}d\\theta = B(37,291) = 2.8\\times 10^{-51}\\)\n\\(p(Y \\mid X) = \\int\\int B(19 \\mid 151,\\alpha)B(0 \\mid 9,\\beta)B(11 \\mid 63,\\alpha)B(6 \\mid 103,\\beta)d\\alpha d\\beta \\propto 4.7\\times 10^{-51}\\)\n\\(p(d \\mid v,m) = p(d \\mid m)=\\int\\int B(19 \\mid 151,\\gamma)B(0 \\mid 9,\\gamma)B(11 \\mid 63,\\delta)B(6 \\mid 103,\\delta)d\\gamma d\\delta \\propto 0.27\\times10^{-51}\\)\n\\(p(d \\mid v,m) = \\int\\int\\int\\int B(19 \\mid 151,\\tau)B(0 \\mid 9,\\nu)B(11 \\mid 63,\\chi)B(6 \\mid 103,\\zeta)d\\tau d\\nu d\\chi d\\zeta \\propto 0.18\\times10^{-51}\\)\n\nThe last model is too complex, it can explain any relations in the data and this, has the lowest evidence score! However, if we are to use ML estimates, the fourth model will have the highest likelihood. Bayesian approach allows to avoid over-fitting! You can also see that this data set contains the Simpson’s paradox. Check it! A related problem is Bertrand’s gold box problem.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#bayesian-ell_0-regularization",
    "href": "15-select.html#bayesian-ell_0-regularization",
    "title": "7  Model Selection",
    "section": "7.14 Bayesian \\(\\ell_0\\) regularization",
    "text": "7.14 Bayesian \\(\\ell_0\\) regularization\nBayesian \\(\\ell_0\\) regularization is an attractive solution for high dimensional variable selection as it directly penalizes the number of predictors. The caveat is the need to search over all possible model combinations, as a full solution requires enumeration over all possible models which is NP-hard. The gold standard for Bayesian variable selection are spike-and-slab priors, or Bernoulli-Gaussian mixtures.\nConsider a standard Gaussian linear regression model, where \\(X = [X_1, \\ldots, X_p] \\in \\mathbb{R}^{n \\times p}\\) is a design matrix, \\(\\beta = (\\beta_1, \\ldots, \\beta_p)^T \\in \\mathbb{R}^p\\) is a \\(p\\)-dimensional coefficient vector, and \\(e\\) is an \\(n\\)-dimensional independent Gaussian noise. After centralizing \\(y\\) and all columns of \\(X\\), we ignore the intercept term in the design matrix \\(X\\) as well as \\(\\beta\\), and we can write\n\\[\ny = X\\beta + e, \\quad \\text{where } e \\sim N(0, \\sigma_e^2 I_n)\n\\tag{7.1}\\]\nTo specify a prior distribution \\(p(\\beta)\\), we impose a sparsity assumption on \\(\\beta\\), where only a small portion of all \\(\\beta_i\\)’s are non-zero. In other words, \\(\\|\\beta\\|_0 = k \\ll p\\), where \\(\\|\\beta\\|_0 := \\#\\{i : \\beta_i \\neq 0\\}\\), the cardinality of the support of \\(\\beta\\), also known as the \\(\\ell_0\\) (pseudo)norm of \\(\\beta\\). A multivariate Gaussian prior (\\(l_2\\) norm) leads to poor sparsity properties in this situation. (See, for example, Polson and Scott (2011).)\nSparsity-inducing prior distributions for \\(\\beta\\) can be constructed to impose sparsity. The gold standard is a spike-and-slab prior (Jeffreys 1998; Mitchell and Beauchamp 1988; George and and McCulloch 1993). Under these assumptions, each \\(\\beta_i\\) exchangeably follows a mixture prior consisting of \\(\\delta_0\\), a point mass at \\(0\\), and a Gaussian distribution centered at zero. Hence we write\n\\[\n\\beta_i | \\theta, \\sigma_\\beta^2 \\sim (1-\\theta)\\delta_0 + \\theta N(0, \\sigma_\\beta^2)\n\\tag{7.2}\\]\nHere \\(\\theta \\in (0, 1)\\) controls the overall sparsity in \\(\\beta\\) and \\(\\sigma_\\beta^2\\) accommodates non-zero signals. This family is termed as the Bernoulli-Gaussian mixture model in the signal processing community.\nA useful re-parameterization: the parameters \\(\\beta\\) are given by two independent random variable vectors \\(\\gamma = (\\gamma_1, \\ldots, \\gamma_p)\\) and \\(\\alpha = (\\alpha_1, \\ldots, \\alpha_p)\\) such that \\(\\beta_i = \\gamma_i \\alpha_i\\), with probabilistic structure\n\\[\n\\begin{array}{rcl}\n\\gamma_i|\\theta & \\sim & \\text{Bernoulli}(\\theta) \\\\\n\\alpha_i | \\sigma_\\beta^2 &\\sim & N(0, \\sigma_\\beta^2)\n\\end{array}\n\\tag{7.3}\\]\nSince \\(\\gamma_i\\) and \\(\\alpha_i\\) are independent, the joint prior density becomes\n\\[\np(\\gamma_i, \\alpha_i | \\theta, \\sigma_\\beta^2) = \\theta^{\\gamma_i}(1-\\theta)^{1-\\gamma_i}\\frac{1}{\\sqrt{2\\pi}\\sigma_\\beta}\\exp\\left\\{-\\frac{\\alpha_i^2}{2\\sigma_\\beta^2}\\right\\}, \\quad 1 \\leq i \\leq p\n\\]\nThe indicator \\(\\gamma_i \\in \\{0, 1\\}\\) can be viewed as a dummy variable to indicate whether \\(\\beta_i\\) is included in the model .\nLet \\(S = \\{i: \\gamma_i = 1\\} \\subseteq \\{1, \\ldots, p\\}\\) be the “active set” of \\(\\gamma\\), and \\(\\|\\gamma\\|_0 = \\sum_{i=1}^p \\gamma_i\\) be its cardinality. The joint prior on the vector \\(\\{\\gamma, \\alpha\\}\\) then factorizes as\n\\[\n\\begin{array}{rcl}\np(\\gamma, \\alpha | \\theta, \\sigma_\\beta^2) &=& \\prod_{i=1}^p p(\\alpha_i, \\gamma_i | \\theta, \\sigma_\\beta^2) \\\\\n&=& \\theta^{\\|\\gamma\\|_0} (1-\\theta)^{p-\\|\\gamma\\|_0} (2\\pi\\sigma_\\beta^2)^{-p/2} \\exp\\left\\{-\\frac{1}{2\\sigma_\\beta^2} \\sum_{i=1}^p \\alpha_i^2\\right\\}\n\\end{array}\n\\]\nLet \\(X_\\gamma := [X_i]_{i \\in S}\\) be the set of “active explanatory variables” and \\(\\alpha_\\gamma := (\\alpha_i)_{i \\in S}\\) be their corresponding coefficients. We can write \\(X\\beta = X_\\gamma \\alpha_\\gamma\\). The likelihood can be expressed in terms of \\(\\gamma\\), \\(\\alpha\\) as\n\\[\np(y | \\gamma, \\alpha, \\theta, \\sigma_e^2) = (2\\pi\\sigma_e^2)^{-n/2} \\exp\\left\\{ -\\frac{1}{2\\sigma_e^2} \\|y - X_\\gamma \\alpha_\\gamma\\|_2^2 \\right\\}\n\\]\nUnder this re-parameterization by \\(\\{\\gamma, \\alpha\\}\\), the posterior is given by\n\\[\n\\begin{array}{rcl}\np(\\gamma, \\alpha | \\theta, \\sigma_\\beta^2, \\sigma_e^2, y) &\\propto& p(\\gamma, \\alpha | \\theta, \\sigma_\\beta^2) p(y | \\gamma, \\alpha, \\theta, \\sigma_e^2) \\\\\n&\\propto& \\exp\\left\\{ -\\frac{1}{2\\sigma_e^2} \\|y - X_\\gamma \\alpha_\\gamma\\|_2^2 - \\frac{1}{2\\sigma_\\beta^2} \\|\\alpha\\|_2^2 - \\log\\left(\\frac{1-\\theta}{\\theta}\\right) \\|\\gamma\\|_0 \\right\\}\n\\end{array}\n\\]\nOur goal then is to find the regularized maximum a posterior (MAP) estimator\n\\[\n\\arg\\max_{\\gamma, \\alpha} p(\\gamma, \\alpha | \\theta, \\sigma_\\beta^2, \\sigma_e^2, y)\n\\]\nBy construction, \\(\\gamma \\in \\{0, 1\\}^p\\) will directly perform variable selection. Spike-and-slab priors, on the other hand, will sample the full posterior and calculate the posterior probability of variable inclusion.\nFinding the MAP estimator is equivalent to minimizing over \\(\\{\\gamma, \\alpha\\}\\) the regularized least squares objective function:\n\\[\n\\min_{\\gamma, \\alpha} \\|y - X_\\gamma \\alpha_\\gamma\\|_2^2 + \\frac{\\sigma_e^2}{\\sigma_\\beta^2} \\|\\alpha\\|_2^2 + 2\\sigma_e^2 \\log\\left(\\frac{1-\\theta}{\\theta}\\right) \\|\\gamma\\|_0\n\\tag{7.4}\\]\nThis objective possesses several interesting properties:\n\nThe first term is essentially the least squares loss function.\nThe second term looks like a ridge regression penalty and has connection with the signal-to-noise ratio (SNR) \\(\\sigma_\\beta^2/\\sigma_e^2\\). Smaller SNR will be more likely to shrink the estimates towards \\(0\\). If \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the prior uncertainty on the size of non-zero coefficients is much larger than the noise level, that is, the SNR is sufficiently large, this term can be ignored. This is a common assumption in spike-and-slab framework in that people usually want \\(\\sigma_\\beta \\to \\infty\\) or to be “sufficiently large” in order to avoid imposing harsh shrinkage to non-zero signals.\nIf we further assume that \\(\\theta &lt; 1/2\\), meaning that the coefficients are known to be sparse a priori, then \\(\\log((1-\\theta)/\\theta) &gt; 0\\), and the third term can be seen as an \\(\\ell_0\\) regularization.\n\nTherefore, our Bayesian objective inference is connected to \\(\\ell_0\\)-regularized least squares, which we summarize in the following proposition.\n\nProposition 1 (Spike-and-slab MAP & \\(\\ell_0\\) regularization)\nFor some \\(\\lambda &gt; 0\\), assuming \\(\\theta &lt; 1/2\\), \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the Bayesian MAP estimate defined by (Equation 7.4) is equivalent to the \\(\\ell_0\\) regularized least squares objective, for some \\(\\lambda &gt; 0\\),\n\\[\n\\min_{\\beta} \\frac{1}{2} \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_0\n\\tag{7.5}\\]\n\n\nProof. Proof. First, assuming that\n\\[\n\\theta &lt; 1/2, \\quad \\sigma_\\beta^2 \\gg \\sigma_e^2, \\quad \\frac{\\sigma_e^2}{\\sigma_\\beta^2} \\|\\alpha\\|_2^2 \\to 0\n\\]\ngives us an objective function of the form\n\\[\n\\min_{\\gamma, \\alpha} \\frac{1}{2} \\|y - X_\\gamma \\alpha_\\gamma\\|_2^2 + \\lambda \\|\\gamma\\|_0, \\quad \\lambda := \\sigma_e^2 \\log\\left(\\frac{1-\\theta}{\\theta}\\right) &gt; 0\n\\tag{7.6}\\]\nEquation 7.6 can be seen as a variable selection version of Equation 7.5. The interesting fact is that Equation 7.5 and Equation 7.6 are equivalent. To show this, we need only to check that the optimal solution to Equation 7.5 corresponds to a feasible solution to Equation 7.6 and vice versa. This is explained as follows.\nOn the one hand, assuming \\(\\hat\\beta\\) is an optimal solution to Equation 7.5, then we can correspondingly define \\(\\hat\\gamma_i := I\\{\\hat\\beta_i \\neq 0\\}\\), \\(\\hat\\alpha_i := \\hat\\beta_i\\), such that \\(\\{\\hat\\gamma, \\hat\\alpha\\}\\) is feasible to (Equation 7.6) and gives the same objective value as \\(\\hat\\beta\\) gives (Equation 7.5).\nOn the other hand, assuming \\(\\{\\hat\\gamma, \\hat\\alpha\\}\\) is optimal to (Equation 7.6), implies that we must have all of the elements in \\(\\hat\\alpha_\\gamma\\) non-zero, otherwise a new \\(\\tilde\\gamma_i := I\\{\\hat\\alpha_i \\neq 0\\}\\) will give a lower objective value of (Equation 7.6). As a result, if we define \\(\\hat\\beta_i := \\hat\\gamma_i \\hat\\alpha_i\\), \\(\\hat\\beta\\) will be feasible to Equation 7.5 and gives the same objective value as \\(\\{\\hat\\gamma, \\hat\\alpha\\}\\) gives Equation 7.6.\nCombining both arguments shows that the two problems Equation 7.5 and Equation 7.6 are equivalent. Hence we can use results from non-convex optimization literature to find Bayes MAP estimators.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#survey",
    "href": "15-select.html#survey",
    "title": "7  Model Selection",
    "section": "7.15 Computing the \\(\\ell_0\\)-regularized regression solution",
    "text": "7.15 Computing the \\(\\ell_0\\)-regularized regression solution\nWe now turn to the problem of computation. \\(\\ell_0\\)-regularized least squares (Equation 7.5) is closely related to the best subset selection in linear regression as follows.\n\\[\n\\begin{array}{rl}\n\\min_{\\beta} & \\frac{1}{2}\\|y - X\\beta\\|_2^2 \\\\\n\\text{s.t.} & \\|\\beta\\|_0 \\leq k\n\\end{array}\n\\tag{7.7}\\]\nThe \\(\\ell_0\\)-regularized least squares (Equation 7.5) can be seen as (Equation 7.7)’s Lagrangian form. However, due to high non-convexity of the \\(\\ell_0\\)-norm, (Equation 7.5) and (Equation 7.7) are connected but not equivalent. In particular, for any given \\(\\lambda \\geq 0\\), there exists an integer \\(k \\geq 0\\), such that (Equation 7.5) and (Equation 7.7) have the same global minimizer \\(\\hat\\beta\\). However, it’s not true the other way around. It’s possible, even common, that for a given \\(k\\), we cannot find a \\(\\lambda \\geq 0\\), such that the solutions to (Equation 7.7) and (Equation 7.5) are the same.\nIndeed, for \\(k \\in \\{1, 2, \\ldots, p\\}\\), let \\(\\hat\\beta_k\\) be respective optimal solutions to (Equation 7.7) and \\(f_k\\) respective optimal objective values, and so \\(f_1 \\geq f_2 \\geq \\cdots \\geq f_p\\). If we want a solution \\(\\hat\\beta_\\lambda\\) to (Equation 7.5) with \\(\\|\\hat\\beta_\\lambda\\|_0 = k\\), we need to find a \\(\\lambda\\) such that\n\\[\n\\max_{i &gt; k} \\{f_k - f_i\\} \\leq \\lambda \\leq \\min_{j &lt; k} \\{f_j - f_k\\}\n\\]\nwith the caveat that such \\(\\lambda\\) need not exist.\nBoth problems involve discrete optimization and have thus been seen as intractable for large-scale data sets. As a result, in the past, \\(\\ell_0\\) norm is usually replaced by its convex relaxation \\(l_1\\) norm to facilitate computation. However, it’s widely known that the solutions of \\(\\ell_0\\) norm problems provide superior variable selection and prediction performance compared with their \\(l_1\\) convex relaxation such as Lasso. It is known that the solution to the \\(\\ell_0\\)-regularized least squares should be better than Lasso in terms of variable selection especially when we have a design matrix \\(X\\) that has high collinearity among its columns.\nBertsimas, King, and Mazumder (2016) introduced a first-order algorithm to provide a stationary solution \\(\\beta^*\\) to a class of generalized \\(\\ell_0\\)-constrained optimization problem, with convex \\(g\\):\n\\[\n\\begin{array}{rl}\n\\min_{\\beta} & g(\\beta) \\\\\n\\text{s.t.} & \\|\\beta\\|_0 \\leq k\n\\end{array}\n\\tag{7.8}\\]\nLet \\(L\\) be the Lipschitz constant for \\(\\nabla g\\) such that \\(\\forall \\beta_1, \\beta_2\\), \\(\\|\\nabla g(\\beta_1) - \\nabla g(\\beta_2)\\| \\leq L \\|\\beta_1 - \\beta_2\\|\\). Their “Algorithm 1” is as follows.\n\nInitialize \\(\\beta^0\\) such that \\(\\|\\beta^0\\|_0 \\leq k\\).\nFor \\(t \\geq 0\\), obtain \\(\\beta^{t+1}\\) as\n\n\\[\n\\beta^{t+1} = H_k\\left(\\beta^t - \\frac{1}{L} \\nabla g(\\beta^t)\\right)\n\\tag{7.9}\\]\nuntil convergence to \\(\\beta^*\\).\nwhere the operator \\(H_k(\\cdot)\\) is to keep the largest \\(k\\) elements of a vector as the same, whilst to set all else to zero. It can also be called the hard thresholding at the \\(k\\)th largest element. In the least squares setting when \\(g(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2\\), \\(\\nabla g\\) and \\(L\\) are easy to compute. Bertsimas, King, and Mazumder (2016) then uses the stationary solution \\(\\beta^*\\) obtained by the aforementioned algorithm (Equation 7.9) as a warm start for their mixed integer optimization (MIO) scheme to produce a “provably optimal solution” to the best subset selection problem (Equation 7.7).\nIt’s worth pointing out that the key iteration step (Equation 7.9) is connected to the proximal gradient descent (PGD) algorithm many have used to solve the \\(\\ell_0\\)-regularized least squares (Equation 7.5), as well as other non-convex regularization problems. PGD methods solve a general class of problems such as\n\\[\n\\min_{\\beta} g(\\beta) + \\lambda \\phi(\\beta)\n\\tag{7.10}\\]\nwhere \\(g\\) is the same as in (Equation 7.8), and \\(\\phi\\), usually non-convex, is a regularization term. In this framework, in order to obtain a stationary solution \\(\\beta^*\\), the key iteration step is\n\\[\n\\beta^{t+1} = \\mathrm{prox}_{\\lambda\\phi}\\left(\\beta^t - \\frac{1}{L} \\nabla g(\\beta^t)\\right)\n\\tag{7.11}\\]\nwhere \\(\\beta^t - \\frac{1}{L} \\nabla g(\\beta^t)\\) can be seen as a gradient descent step for \\(g\\) and \\(\\mathrm{prox}_{\\lambda\\phi}\\) is the proximal operator for \\(\\lambda\\phi\\). In \\(\\ell_0\\)-regularized least squares, \\(\\lambda\\phi(\\cdot) = \\lambda\\|\\cdot\\|_0\\), and its proximal operator \\(\\mathrm{prox}_{\\lambda\\|\\cdot\\|_0}\\) is just the hard thresholding at \\(\\lambda\\). That is, \\(\\mathrm{prox}_{\\lambda\\|\\cdot\\|_0}\\) is to keep the same all elements no less than \\(\\lambda\\), whilst to set all else to zero. As a result, the similarity between (Equation 7.9) and (Equation 7.11) are quite obvious.\n###Single best replacement (SBR) algorithm {#sbr}\nThe single best replacement (SBR) algorithm, provides a solution to the variable selection regularization (Equation 7.6). Since (Equation 7.6) and the \\(\\ell_0\\)-regularized least squares (Equation 7.5) are equivalent, SBR also provides a practical way to give a sufficiently good local optimal solution to the NP-hard \\(\\ell_0\\) regularization.\nTake a look at the objective (Equation 7.6). For any given variable selection indicator \\(\\gamma\\), we have an active set \\(S = \\{i: \\gamma_i = 1\\}\\), based on which the minimizer \\(\\hat\\alpha_\\gamma\\) of (Equation 7.6) has a closed form. \\(\\hat\\alpha_\\gamma\\) will set every coefficient outside \\(S\\) to zero, and regress \\(y\\) on \\(X_\\gamma\\), the variables inside \\(S\\). Therefore, the minimization of the objective function can be determined by \\(\\gamma\\) or \\(S\\) alone. Accordingly, the objective function (Equation 7.6) can be rewritten as follows.\n\\[\n\\min_{S} f_{SBR}(S) = \\frac{1}{2} \\|y - X_S \\beta_S\\|_2^2 + \\lambda |S|\n\\] {#obj-sbr}\nSBR thus tries to minimize \\(f_{SBR}(S)\\) via choosing the optimal \\(\\hat S\\).\nThe algorithm works as follows. Suppose we start with an initial \\(S\\), usually the empty set. At each iteration, SBR aims to find a “single change of \\(S\\)”, that is, a single removal from or adding to \\(S\\) of one element, such that this single change decreases \\(f_{SBR}(S)\\) the most. SBR stops when no such change is available, or in other words, any single change of \\(\\gamma\\) or \\(S\\) will only give the same or larger objective value. Therefore, intuitively SBR stops at a local optimum of \\(f_{SBR}(S)\\).\nSBR is essentially a stepwise greedy variable selection algorithm. At each iteration, both adding and removal are allowed, so this algorithm is one example of the “forward-backward” stepwise procedures. It’s provable that with this feature the algorithm “can escape from some [undesirable] local minimizers” of \\(f_{SBR}(S)\\). Therefore, SBR can solve the \\(\\ell_0\\)-regularized least squares in a sub-optimal way, providing a satisfactory balance between efficiency and accuracy.\nWe now write out the algorithm more formally. For any currently chosen active set \\(S\\), define a single replacement \\(S \\cdot i, i \\in \\{1, \\ldots, p\\}\\) as \\(S\\) adding or removing a single element \\(i\\):\n\\[\nS \\cdot i :=\n\\begin{cases}\nS \\cup \\{i\\}, & i \\notin S \\\\\nS \\setminus \\{i\\}, & i \\in S\n\\end{cases}\n\\]\nThen we compare the objective value at current \\(S\\) with all of its single replacements \\(S \\cdot i\\), and choose the best one. SBR proceeds as follows:\n\nStep 0: Initialize \\(S_0\\). Usually, \\(S_0 = \\emptyset\\). Compute \\(f_{SBR}(S_0)\\). Set \\(k = 1\\).\nStep \\(k\\): For every \\(i \\in \\{1, \\ldots, p\\}\\), compute \\(f_{SBR}(S_{k-1} \\cdot i)\\). Obtain the single best replacement \\(j := \\arg\\min_{i} f_{SBR}(S_{k-1} \\cdot i)\\).\n\nIf \\(f_{SBR}(S_{k-1} \\cdot j) \\geq f_{SBR}(S_{k-1})\\), stop. Report \\(\\hat S = S_{k-1}\\) as the solution.\nOtherwise, set \\(S_k = S_{k-1} \\cdot j\\), \\(k = k+1\\), and repeat step \\(k\\).\n\n\nIt can be shown that SBR always stops within finite steps. With the output \\(\\hat S\\), the locally optimal solution to the \\(\\ell_0\\)-regularized least squares \\(\\hat\\beta\\) is just the coefficients of \\(y\\) regressed on \\(X_{\\hat S}\\) and zero elsewhere. In order to include both forward and backward steps in the variable selection process, the algorithm needs to compute \\(f_{SBR}(S_{k-1} \\cdot i)\\) for every \\(i\\) at every step. Because it involves a one-column update of current design matrix \\(X_{S_{k-1}}\\), this computation can be made very efficient by using the Cholesky decomposition, without explicitly calculating \\(p\\) linear regressions at each step . An R package implementation of the algorithm is available upon request.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#double-descent",
    "href": "15-select.html#double-descent",
    "title": "7  Model Selection",
    "section": "7.16 Double Descent",
    "text": "7.16 Double Descent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDouble Descent Stylized",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#polya-gamma",
    "href": "15-select.html#polya-gamma",
    "title": "7  Model Selection",
    "section": "7.17 Polya-Gamma",
    "text": "7.17 Polya-Gamma\nBayesian inference for logistic regression has long been recognized as a computationally challenging problem due to the analytically inconvenient form of the binomial likelihood function(Polson, Scott, and Windle 2013). While the probit model enjoys simple latent-variable methods for posterior sampling, the logistic model has historically required more complex approaches involving multiple layers of auxiliary variables or approximations(Polson, Scott, and Windle 2013). The breakthrough work of Polson, Scott, and Windle (2013) introduced a revolutionary data-augmentation strategy using a novel class of distributions called Pólya-Gamma distributions, which enables simple and exact Gibbs sampling for Bayesian logistic regression(Polson, Scott, and Windle 2013).\nThis methodology represents a significant advancement in Bayesian computation, providing a direct analog to the Albert and Chib (1993) method for probit regression while maintaining both exactness and simplicity(Polson, Scott, and Windle 2013). The approach has proven particularly valuable for complex hierarchical models where traditional Metropolis-Hastings samplers are difficult to tune and implement effectively(Polson, Scott, and Windle 2013).\n\n\n\n\n\n\nKey Innovation\n\n\n\nThe Pólya-Gamma methodology provides exact Gibbs sampling for Bayesian logistic regression, eliminating the need for complex Metropolis-Hastings tuning while maintaining theoretical guarantees.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#the-pólya-gamma-distribution",
    "href": "15-select.html#the-pólya-gamma-distribution",
    "title": "7  Model Selection",
    "section": "7.18 The Pólya-Gamma Distribution",
    "text": "7.18 The Pólya-Gamma Distribution\nThe Pólya-Gamma distribution, denoted as PG(b,c), is carefully constructed as a subset of infinite convolutions of gamma distributions(Polson, Scott, and Windle 2013). A random variable X follows a Pólya-Gamma distribution with parameters b &gt; 0 and c ∈ ℝ if:\n\\[X \\stackrel{d}{=} \\frac{1}{2\\pi^2} \\sum_{k=1}^{\\infty} \\frac{g_k}{(k-1/2)^2 + c^2/(4\\pi^2)}\\]\nwhere \\(g_k \\sim \\text{Ga}(b,1)\\) are independent gamma random variables, and \\(\\stackrel{d}{=}\\) indicates equality in distribution(Polson, Scott, and Windle 2013).\nThe Pólya-Gamma family exhibits several remarkable properties that make it ideal for data augmentation:\n\nLaplace Transform: For \\(\\omega \\sim \\text{PG}(b,0)\\), the Laplace transform is \\(E\\{\\exp(-\\omega t)\\} = \\cosh^{-b}(\\sqrt{t}/2)\\)(Polson, Scott, and Windle 2013)\nExponential Tilting: The general PG(b,c) distribution arises through exponential tilting of the PG(b,0) density:\n\n\\[p(x|b,c) = \\frac{\\exp(-c^2x/2)p(x|b,0)}{E[\\exp(-c^2\\omega/2)]}\\]\nwhere the expectation is taken with respect to PG(b,0)(Polson, Scott, and Windle 2013)\n\nConvolution Property: The family is closed under convolution for random variates with the same tilting parameter(Polson, Scott, and Windle 2013)\nKnown Moments: All finite moments are available in closed form, with the expectation given by:\n\n\\[E(\\omega) = \\frac{b}{2c}\\tanh(c/2) = \\frac{b}{2c}\\frac{e^c-1}{1+e^c}\\]\n\n\n\n\n\n\nComputational Advantage\n\n\n\nThe known moments and convolution properties make the Pólya-Gamma distribution computationally tractable and theoretically well-behaved.\n\n\n\n7.18.1 The Data-Augmentation Strategy\nThe core of the Pólya-Gamma methodology rests on a fundamental integral identity that represents binomial likelihoods as mixtures of Gaussians(Polson, Scott, and Windle 2013). The key theorem states:\n\nTheorem 1: For b &gt; 0 and a ∈ ℝ, the following integral identity holds:\n\\[\\frac{(e^\\psi)^a}{(1+e^\\psi)^b} = 2^{-b}e^{\\kappa\\psi} \\int_0^{\\infty} e^{-\\omega\\psi^2/2} p(\\omega) d\\omega\\]\nwhere \\(\\kappa = a - b/2\\), and \\(p(\\omega)\\) is the density of \\(\\omega \\sim \\text{PG}(b,0)\\)(Polson, Scott, and Windle 2013).\nMoreover, the conditional distribution \\(p(\\omega|\\psi)\\) is also in the Pólya-Gamma class: \\((\\omega|\\psi) \\sim \\text{PG}(b,\\psi)\\)(Polson, Scott, and Windle 2013).\n\n\n\n7.18.2 Gibbs Sampling Algorithm\nThis integral identity leads directly to a simple two-step Gibbs sampler for Bayesian logistic regression(Polson, Scott, and Windle 2013). For a dataset with observations \\(y_i \\sim \\text{Binom}(n_i, 1/(1+e^{-\\psi_i}))\\) where \\(\\psi_i = x_i^T\\beta\\), and a Gaussian prior \\(\\beta \\sim N(b,B)\\), the algorithm iterates:\n\nSample auxiliary variables: \\((\\omega_i|\\beta) \\sim \\text{PG}(n_i, x_i^T\\beta)\\) for each observation\nSample parameters: \\((\\beta|y,\\omega) \\sim N(m_\\omega, V_\\omega)\\) where:\n\n\\(V_\\omega = (X^T\\Omega X + B^{-1})^{-1}\\)\n\\(m_\\omega = V_\\omega(X^T\\kappa + B^{-1}b)\\)\n\\(\\kappa = (y_1-n_1/2, \\ldots, y_n-n_n/2)\\)\n\\(\\Omega = \\text{diag}(\\omega_1, \\ldots, \\omega_n)\\)\n\n\nThis approach requires only Gaussian draws for the main parameters and Pólya-Gamma draws for a single layer of latent variables, making it significantly simpler than previous methods(Polson, Scott, and Windle 2013).\n\n\n7.18.3 The PG(1,z) Sampler\nThe practical success of the Pólya-Gamma method depends on efficient simulation of Pólya-Gamma random variables(Polson, Scott, and Windle 2013). The authors developed a sophisticated accept-reject sampler based on the alternating-series method of Devroye (1986)(Devroye 1986). For the fundamental PG(1,c) case, the sampler:\n\nUses exponential and inverse-Gaussian draws as proposals\nAchieves acceptance probability uniformly bounded below at 0.99919\nRequires no tuning for optimal performance\nEvaluates acceptance using iterative partial sums\n\n\n\n7.18.4 General PG(b,z) Sampling\nFor integer values of b, PG(b,z) random variables are generated by summing b independent PG(1,z) draws, exploiting the convolution property(Polson, Scott, and Windle 2013). This approach maintains efficiency for moderate values of b, though computational cost scales linearly with the total number of counts in negative binomial applications(Polson, Scott, and Windle 2013).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "15-select.html#implementation-with-bayeslogit-package",
    "href": "15-select.html#implementation-with-bayeslogit-package",
    "title": "7  Model Selection",
    "section": "7.19 Implementation with BayesLogit Package",
    "text": "7.19 Implementation with BayesLogit Package\n\n7.19.1 Package Overview\nThe BayesLogit package provides efficient tools for sampling from the Pólya-Gamma distribution(Windle 2023). The current version (2.1) focuses on core functionality: sampling from the Pólya-Gamma distribution through the rpg() function and its variants(Windle 2023).\n\n\n7.19.2 Core Functions\nThe package offers several sampling methods:\n\nrpg(): Main function that automatically selects the best method\nrpg.devroye(): Devroye-like method for integer h values\nrpg.gamma(): Sum of gammas method (slower but works for all parameters)\nrpg.sp(): Saddlepoint approximation method\n\n\n\n7.19.3 Installation and Basic Usage\n\n# Install from CRAN\ninstall.packages(\"BayesLogit\")\nlibrary(BayesLogit)\n\n# Basic usage examples\n# Sample from PG(1, 0)\nsamples1 &lt;- rpg(1000, h=1, z=0)\n\n# Sample with tilting parameter\nsamples2 &lt;- rpg(1000, h=1, z=2.5)\n\n# Multiple shape parameters\nh_values &lt;- c(1, 2, 3)\nz_values &lt;- c(1, 2, 3)\nsamples3 &lt;- rpg(100, h=h_values, z=z_values)\n\n\n\n7.19.4 Implementing Bayesian Logistic Regression\nHere’s a complete implementation of Bayesian logistic regression using the Pólya-Gamma methodology:\n\n# Bayesian Logistic Regression with Pólya-Gamma Data Augmentation\nbayesian_logit_pg &lt;- function(y, X, n_iter=5000, burn_in=1000) {\n  n &lt;- length(y)\n  p &lt;- ncol(X)\n  \n  # Prior specification (weakly informative)\n  beta_prior_mean &lt;- rep(0, p)\n  beta_prior_prec &lt;- diag(0.01, p)  # Precision matrix\n  \n  # Storage for samples\n  beta_samples &lt;- matrix(0, n_iter, p)\n  omega_samples &lt;- matrix(0, n_iter, n)\n  \n  # Initialize\n  beta &lt;- rep(0, p)\n  \n  for(iter in 1:n_iter) {\n    # Step 1: Sample omega (auxiliary variables)\n    psi &lt;- X %*% beta\n    omega &lt;- rpg(n, h=1, z=psi)\n    \n    # Step 2: Sample beta (regression coefficients)\n    # Posterior precision and mean\n    V_omega &lt;- solve(t(X) %*% diag(omega) %*% X + beta_prior_prec)\n    kappa &lt;- y - 0.5\n    m_omega &lt;- V_omega %*% (t(X) %*% kappa + beta_prior_prec %*% beta_prior_mean)\n    \n    # Sample from multivariate normal\n    beta &lt;- mvrnorm(1, m_omega, V_omega)\n    \n    # Store samples\n    beta_samples[iter, ] &lt;- beta\n    omega_samples[iter, ] &lt;- omega\n  }\n  \n  # Return samples after burn-in\n  list(\n    beta = beta_samples[(burn_in+1):n_iter, ],\n    omega = omega_samples[(burn_in+1):n_iter, ],\n    n_samples = n_iter - burn_in\n  )\n}\n\n# Example usage with simulated data\nset.seed(123)\nn &lt;- 100\nX &lt;- cbind(1, matrix(rnorm(n*2), n, 2))  # Intercept + 2 predictors\nbeta_true &lt;- c(-0.5, 1.2, -0.8)\nlogits &lt;- X %*% beta_true\nprobs &lt;- 1/(1 + exp(-logits))\ny &lt;- rbinom(n, 1, probs)\n\n# Fit model\nresults &lt;- bayesian_logit_pg(y, X, n_iter=3000, burn_in=500)\n\n# Posterior summaries\nposterior_means &lt;- colMeans(results$beta)\nposterior_sds &lt;- apply(results$beta, 2, sd)\n\nComputational Advantages\nExtensive benchmarking studies demonstrate the superior performance of the Pólya-Gamma method across various scenarios(Polson, Scott, and Windle 2013):\n\nSimple logistic models: Competitive with well-tuned Metropolis-Hastings samplers\nHierarchical models: Significantly outperforms alternative methods\nMixed models: Provides substantial efficiency gains over traditional approaches\nSpatial models: Shows dramatic improvements for Gaussian process spatial models\n\nTheoretical Guarantees\nThe Pólya-Gamma Gibbs sampler enjoys strong theoretical properties(Polson, Scott, and Windle 2013):\n\nUniform ergodicity: Proven by Choi and Hobert (2013), guaranteeing convergence and central limit theorems for Monte Carlo averages(Polson, Scott, and Windle 2013)\nNo tuning required: Unlike Metropolis-Hastings methods, the sampler requires no manual tuning\nExact sampling: Produces draws from the correct posterior distribution without approximation\n\n\n\n\n\n\n\nImportant Note\n\n\n\nThe theoretical guarantees hold under standard regularity conditions, and the method requires proper prior specification for optimal performance.\n\n\nBeyond Binary Logistic Regression\nThe Pólya-Gamma methodology extends naturally to various related models(Polson, Scott, and Windle 2013):\n\nNegative binomial regression: Direct application using the same data-augmentation scheme\nMultinomial logistic models: Extended through partial difference of random utility models(Windle, Polson, and Scott 2014)\nMixed effects models: Seamless incorporation of random effects structures\nSpatial models: Efficient inference for spatial count data models\n\n\n\n7.19.5 Modern Applications\nRecent developments have expanded the methodology’s applicability[Windle, Polson, and Scott (2014)](Zhang, Datta, and Banerjee 2018):\n\nGaussian process classification: Scalable variational approaches using Pólya-Gamma augmentation\nDeep learning: Integration with neural network architectures for Bayesian deep learning\nState-space models: Application to dynamic binary time series models\n\nThe Pólya-Gamma methodology represents a fundamental advancement in Bayesian computation for logistic models, combining theoretical elegance with practical efficiency(Polson, Scott, and Windle 2013). Its introduction of the Pólya-Gamma distribution class and the associated data-augmentation strategy has enabled routine application of Bayesian methods to complex hierarchical models that were previously computationally prohibitive(Polson, Scott, and Windle 2013).\nThe BayesLogit package provides researchers and practitioners with efficient, well-tested implementations of these methods(Windle 2023). The combination of exact inference, computational efficiency, and theoretical guarantees makes the Pólya-Gamma approach the method of choice for Bayesian logistic regression in most practical applications(Polson, Scott, and Windle 2013).\nAs computational demands continue to grow and models become increasingly complex, the Pólya-Gamma methodology’s advantages become even more pronounced, establishing it as an essential tool in the modern Bayesian statistician’s toolkit (Tiao (2019)). Ongoing research continues to extend the Pólya-Gamma methodology to new domains, including high-dimensional settings, nonparametric models, and integration with modern machine learning frameworks.\n\n\n\n\nBertsimas, Dimitris, Angela King, and Rahul Mazumder. 2016. “Best Subset Selection via a Modern Optimization Lens.” The Annals of Statistics 44 (2): 813–52.\n\n\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation. Springer Science & Business Media.\n\n\nGeorge, Edward I., and Robert E. and McCulloch. 1993. “Variable Selection via Gibbs Sampling.” Journal of the American Statistical Association 88 (423): 881–89.\n\n\nJeffreys, Harold. 1998. Theory of Probability. Third Edition, Third Edition. Oxford Classic Texts in the Physical Sciences. Oxford, New York: Oxford University Press.\n\n\nMitchell, T. J., and J. J. Beauchamp. 1988. “Bayesian Variable Selection in Linear Regression.” Journal of the American Statistical Association 83 (404): 1023–32.\n\n\nPolson, Nicholas G., and James G. Scott. 2011. “Shrink Globally, Act Locally: Sparse Bayesian Regularization and Prediction.” In Bayesian Statistics 9, edited by José M. Bernardo, M. J. Bayarri, James O. Berger, A. P. Dawid, David Heckerman, Adrian F. M. Smith, and Mike West, 0. Oxford University Press.\n\n\nPolson, Nicholas G., James G. Scott, and Jesse Windle. 2013. “Bayesian Inference for Logistic Models Using Pólya–Gamma Latent Variables.” Journal of the American Statistical Association 108 (504): 1339–49.\n\n\nTiao, Louis. 2019. “Pólya-Gamma Bayesian Logistic Regression.” Blog post.\n\n\nWindle, Jesse. 2023. “BayesLogit: Bayesian Logistic Regression.” R package version 2.1.\n\n\nWindle, Jesse, Nicholas G. Polson, and James G. Scott. 2014. “Sampling Polya-Gamma Random Variates: Alternate and Approximate Techniques.” arXiv.\n\n\nZhang, Yichi, Anirban Datta, and Sudipto Banerjee. 2018. “Scalable Gaussian Process Classification with Pólya-Gamma Data Augmentation.” arXiv Preprint arXiv:1802.06383.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-tree.html",
    "href": "16-tree.html",
    "title": "8  Tree Models",
    "section": "",
    "text": "8.1 Finding Good Bayes Predictors\nWe’ve used decision trees before to describe the decision-making process as a sequence of actions and conditions. In this section, we’ll use decision trees to make predictions. You can think of a prediction as a decision task, where you need to decide which value of \\(y\\) to use for a given \\(x\\). Similar to decision tree predictive tree model is a nested sequence of if-else statements that map any input data point \\(x\\) to a predicted output \\(y\\). Each if-else statement checks a feature of \\(x\\) and sends the data left or right along the tree branch. At the end of the branch, a single value of \\(y\\) is predicted.\nFigure 8.1 shows a decision tree for predicting a chess piece given a four-dimension input vector that describes the types of moves available to the piece. The tree is a sequence of nested if-else statements that check the values of the input vector. The tree has six leaves, one for each of the chess pieces and has a depth of four. The tree is a predictive model that maps a four-dimensional input vector to a single output categorical value with six possible values.\nThe prediction algorithm is simple. Start at the root node and move down the tree until you reach a leaf node. The process of building a tree, given a set of training data, is more complicated and has three main components:\nThe splitting process is the most important part of the tree-building process. At each step the splitting process need to decide on the feature index \\(j\\) to be used for splitting and the location of the split. For binary variable there is only one possible split location, but for continuous variables there are many possible split locations. The goal is to find the split that creates the most homogeneous subsets. In the case of regression trees, the best split is the one that minimizes the sum of squared errors. In the case of classification trees, the best split is the one that minimizes the Gini impurity. The Gini impurity is a measure of how homogeneous the subsets are.\nThe widely used CART algorithm uses the table \\(X \\in \\mathbb{R}^{n\\times p}\\) during the splitting process. It loops over every element \\(x_{ij}\\) and finds the best split \\(x_{ij}\\) that creates the most homogeneous subsets. A split creates two subsets the left subset \\(X_L = \\{x  \\mid x_j &lt;  x_{ij}\\}\\) and the right subset \\(X_R = \\{x \\mid x_j \\ge  x_{ij}\\}\\).\nTo measure how homogeneous the sets are, tn case of regression, we calculate \\(\\hat y_L = \\mathrm{Average}(y_i), i \\in I_L\\) and \\(\\hat y_R = \\mathrm{Average}(y_i), i \\in I_R\\), where \\(I_L\\) and \\(I_R\\) are indenes of observed inputs \\(x_i\\) in the left and right subsets. The best split is the one that minimizes the sum of squared errors. \\[\n\\text{SSE} = \\sum_{i\\in I_L}^n (y_i - \\hat{y}_L)^2 + \\sum_{i\\in I_R}^n (y_i - \\hat{y}_R)^2\n\\]\nLet’s start with a quick demo and look at the data. We’ll use the Hitters data set from the ISLR package. The data set contains information about Major League Baseball players. The goal is to predict the salary of a player based on his performance and experience. The data set contains 322 observations and 20 variables. The three variable we are to analyze are:\nThe question we are interested in is, does experience and performance effect the salary of a baseball player?\nSeems like there is a relation between number of years, number of hits and the salary. Left bottom corner of the input space is mostly occupied by the low salary players. We will use the CART algorithm to find the optimal splits\nWe use tree function that has a similar syntax to the lm. Then we cal the prune.tree to find the best tree with 3 leaves (terminal nodes). Each terminal node corresponds to a region\nThe prediction is rather straightforward. The tree divides the predictor space-that is, the set of possible values for \\(x_1, x_2, \\ldots, x_p\\) - into \\(J\\) distinct and non-overlapping boxes, \\(R_1,R_2,...,R_J\\). For every observation that falls into the region \\(R_j\\), we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_j\\).\n\\[\nf(x) = \\sum_{j=1}^J\\bar{y}_jI(x \\in R_j)\n\\]\n\\[\n\\bar{y}_j = \\text{Average}(y_i \\mid x_i \\in R_j)\n\\]\nTge overall goal of building a tree is to find find regions that lead to minima of the total Residual Sum of Squares (RSS) \\[\n\\mathrm{RSS} = \\sum_{j=1}^J\\sum_{i \\in R_j}(y_i - \\bar{y}_j)^2 \\rightarrow \\mathrm{minimize}\n\\]\nUnfortunately, it is computationally infeasible (NP-hard problem) to consider every possible partition of the feature space into \\(J\\) boxes. We can find a good approximate solution, using top-down approach (the CART algorithm). As mentioned earlier at each iteatoin we decide on: which variable \\(j\\) to split and split point \\(s\\). \\[\nR_1(j, s) = \\{x\\mid x_j &lt; s\\} \\mbox{ and } R_2(j, s) = \\{x\\mid x_j \\ge s\\},\n\\] thus, we seek to minimize (in case of regression tree) \\[\n\\min_{j,s}\\left[ \\sum_{i:x_i\\in  R_1}(y_i - \\bar{y}_1)^2 + \\sum_{i:x_i  \\in R_2}(y_i - \\bar{y}_2)^2\\right]\n\\] As a result, every observed input point belongs to a single region.\nNow let’s discuss how many regions we should have. At one extreme end, we can have \\(n\\) regions, one for each observation. Then the tree model will work similar to the one-nearest neighbor model. At the other end, we can have one big region for the entire input space and then every prediction will be the same (average across observed \\(y\\)’s). Both models can be used but usually the best one is in the middle. The number of regions (branches) controls the complexity of the model. We need to find a good size on the variance-bias scale. A smaller tree with fewer splits (that is, fewer regions \\(R_1,...,R_J\\)) might lead to lower variance and better interpretation at the cost of a little bias.\nHow do we construct a tree with a “manageable” number of branches? This is accomplished through the steps of forward tree construction and backward pruning. The forward step is a greedy algorithm that begins with a single region and divides it into two. This procedure is repeated until a certain stopping criterion is met. A practical method is to continue building the tree until the Residual Sum of Squares (RSS) plateaus. However, this method can be myopic as an initially unproductive split might be followed by a highly beneficial one, leading to a significant decrease in RSS in subsequent iterations. A more effective strategy is to grow an extensive tree \\(T_0\\), and then trim it down to obtain a subtree. The size of the subtree can be determined using cross-validation. However, be aware that the number of subtrees can be exponential!\nInstead of considering all possible sub-trees, we will do cost complexity pruning - also known as weakest link pruning. We consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\). For each value of \\(\\alpha\\) there corresponds a subtree \\(T \\subset T_0\\) such that minimizes \\[\n\\sum_{m=1}^{|T|}\\sum_{i:x_i\\in R_m}(y_i - \\bar{y}_m)^2 + \\alpha |T|\n\\] The parameter \\(\\alpha\\) balances the complexity of the subtree and its adherence to the training data. When we increment \\(\\alpha\\) starting from zero, branches are predictably and sequentially pruned from the tree, making it straightforward to acquire the entire series of subtrees as a function of \\(\\alpha\\). We determine the optimal value \\(\\hat \\alpha\\) through cross-validation. Afterward, we refer back to the complete data set and extract the subtree that corresponds to \\(\\hat \\alpha\\).\nLet’s return back to the Baseball example\nLet’s find the best tree\nSize of 3 seems good!\nBayesian methods tackle the problem of good predictive performance in a number of ways. The goal is to find a good predictive MSE \\(E_{Y,\\hat{Y}}(\\Vert\\hat{Y} - Y \\Vert^2)\\). First, Stein shrinkage (a.k.a regularization with an \\(\\ell_2\\) norm) has long been known to provide good mean squared error properties in estimation, namely \\(E(||\\hat{\\theta} - \\theta||^2)\\) as well. These gains translate into predictive performance (in an iid setting) for \\(E(||\\hat{Y}-Y||^2)\\). One of the main issues is how to tune the amount of regularisation (a.k.a prior hyper-parameters). Stein’s unbiased estimator of risk provides a simple empirical rule to address this problem, as does cross-validation. From a Bayes perspective, the marginal likelihood (and full marginal posterior) provides a natural method for hyper-parameter tuning. The issue is computational tractability and scalability. The posterior for \\((W,b)\\) is extremely high dimensional and multimodal and posterior MAP provides good predictors \\(\\hat{Y}(X)\\).\nBayes conditional averaging can also perform well in high dimensional regression and classification problems. High dimensionality brings with it the curse of dimensionality and it is instructive to understand why certain kernel can perform badly.\nAdaptive Kernel predictors (a.k.a. smart conditional averager) are of the form\n\\[\n\\hat{Y}(X) = \\sum_{r=1}^R K_r ( X_i , X ) \\hat{Y}_r (X)\n\\]\nHere \\(\\hat{Y}_r(X)\\) is a deep predictor with its own trained parameters. For tree models, the kernel \\(K_r( X_i , X)\\) is a cylindrical region \\(R_r\\) (open box set). Figure Figure 8.2 illustrates the implied kernels for trees (cylindrical sets) and random forests. Not too many points will be neighbors in high dimensional input space.\nConstructing the regions is fundamental to reduce the curse of dimensionality. It is useful to imagine a very large dataset, e.g. 100k images and think about how a new image’s input coordinates, \\(X\\), are “neighbors” to data point in the training set. Our predictor will then be a smart conditional average of the observed outputs, \\(Y\\), for our neighbors. When \\(p\\) is large, spheres (\\(L^2\\) balls or Gaussian kernels) are terrible: either no points or all points are “neighbors” of the new input variable. Trees are good as not too many “neighbors”.\nTo illustrate the problem further, Figure Figure 8.3 below shows the 2D image of 1000 uniform samples from a 50-dimensional ball \\(B_{50}\\). The image is calculated as \\(w^T Y\\), where \\(w = (1,1,0,\\ldots,0)\\) and \\(Y \\sim U(B_{50})\\). Samples are centered around the equators and none of the samples fall close to the boundary of the set.\nAs dimensionality of the space grows, the variance of the marginal distribution goes to zero. We can empirically see it from Figure Figure 8.4, which shows histogram of 1D image of uniform sample from balls of different dimensionality, i.e. \\(e_1^T Y\\), where \\(e_1 = (1,0,\\ldots,0)\\).\nSimilar central limit results were known to Maxwell who showed that random variable \\(w^TY\\) is close to standard normal, when \\(Y \\sim U(B_p)\\), \\(p\\) is large, and \\(w\\) is a unit vector (lies on the boundary of the ball). For the history of this fact, see Diaconis and Freedman (1987). More general results in this direction were obtained in Klartag (2007). Further, Milman and Schechtman (2009) presents many analytical and geometrical results for finite dimensional normed spaces, as the dimension grows to infinity.\nDeep learning improves on this by performing a sequence of GLM-like transformations, effectively DL learns a distributed partition of the input space. Specifically, suppose that we have \\(K\\) partitions. Then the DL predictor takes the form of a weighted average or soft-max of the weighted average in case of classification of observations in this partition. Given a new high dimensional input \\(X_{\\mathrm{new}}\\), many deep learners are an average of learners obtained by our hyper-plane decomposition. Generically, we have\n\\[\n\\hat{Y}(X) = \\sum_{k \\in K} w_k(X)\\hat{Y}_k(X)\n\\]\nwhere \\(w_k\\) are the weights learned in region \\(K\\), and \\(w_k(X)\\) is an indicator of the region with appropriate weighting given the training data. Where \\(w_k\\) is a weight which also indicates which partition the new \\(X_{new}\\) lies in.\nThe partitioning of the input space by a deep learner is similar to the one performed by decision trees and partition-based models such as CART, MARS, RandomForests, BART. However, trees are more local in the regions that they use to construct their estimators within a region. Each neuron in deep learning model corresponds to a manifold that divides the input space. In case of ReLU activation function \\(f(x) = \\max(0,x)\\) the manifold is simply a hyperplane and neuron gets activated when the new observation is on the “right” side of this hyperplane, the activation amount is equal to how far from the boundary the given point is. For example in two dimensions, three neurons with ReLU activation functions will divide the space into seven regions, as shown on Figure Figure 8.5.\nThe key difference then between tree-based architecture and neural network based models is the way hyper-planes are combined. Figure Figure 8.6 shows the comparison of space decomposition by hyperplanes as performed by a tree-based and neural network architectures. We compare a neural network with two layers (bottom row) with tree model trained with CART algorithm (top row). The network architecture used is:\n\\[\nY =  \\mathrm{softmax}(w^0Z^2 + b^0)\\\\\nZ^2 =  \\tanh(w^2Z^1 + b^2)\\\\\nZ^1 =  \\tanh(w^1X + b^1)\n\\]\nThe weight matrices for simple data \\(W^1, W^2 \\in \\mathbb{R}^{2 \\times 2}\\), for circle data \\(W^1 \\in \\mathbb{R}^{2 \\times 2}\\) and \\(W^2 \\in \\mathbb{R}^{3 \\times 2}\\), for spiral data we have \\(W^1 \\in \\mathbb{R}^{2 \\times 2}\\) and \\(W^2 \\in \\mathbb{R}^{4 \\times 2}\\). In our notations, we assume that the activation function is applied pointwise at each layer. An advantage of deep architectures is that the number of hyper-planes grow exponentially with the number of layers. The key property of an activation function (link) is \\(f(0) = 0\\) and it has zero value in certain regions. For example, hinge or rectified learner \\(\\max(x,0)\\), box car (differences in Heaviside) functions are very common. As compared to a logistic regression, rather than using \\(\\mathrm{softmax}(1/(1+e^{-x}))\\) in deep learning \\(\\tanh\\) is typically used for training.\nFormally, a Bayesian probabilistic approach (if computationally feasible) knows how to optimally weight predictors via a model averaging approach:\n\\[\n\\hat{Y}(X) = \\sum_{r=1}^R w_k \\hat{Y}_k(X)\n\\]\nwhere \\(\\hat{Y}_k(x) = E(Y \\mid X_k)\\). Such rules can achieve great out-of-sample performance. Amit, Blanchard, and Wilder (2000) discuss the striking success of multiple randomized classifiers. Using a simple set of binary local features, one classification tree can achieve 5% error on the NIST data base with 100,000 training data points. On the other hand 100 trees, trained under one hour, when aggregated yield an error rate under 7%. We believe that this stems from the fact that a sample from a very rich and diverse set of classifiers produces on average weakly dependent classifiers conditional on class. A Bayesian model of weak dependence is exchangeability.\nThe use of pooling (a.k.a. averaging) of multiple predictors is commonplace in machine learning. Ensemble methods (a.k.a. some form clever conditional averaging) is prevalent in high dimensions. One reason for these procedures is that it is relatively easy to find unbiased predictors, the caveat that they have large variances due to the dimensionality. The following result on exchangeability (Kingman, 1975) shows that we can simply use the \\(1/N\\)-rule and average to reduce risk. Specifically, suppose that we have \\(K\\) exchangeable, \\(\\mathbb{E} ( \\hat{Y}_i ) = \\mathbb{E} ( \\hat{Y}_{\\pi(i)} )\\), predictors\n\\[\n\\hat{Y} = ( \\hat{Y}_1 , \\ldots , \\hat{Y}_K )\n\\]\nFind \\(w\\) to attain \\(\\operatorname{argmin}_W E l( Y , w^T \\hat{Y} )\\) where \\(l\\) convex in the second argument;\n\\[\nE l( Y , w^T \\hat{Y} )  = \\frac{1}{K!} \\sum_\\pi E l( Y , w^T \\hat{Y} ) \\geq  E l \\left ( Y , \\frac{1}{K!} \\sum_\\pi w_\\pi^T \\hat{Y} )\\right ) =  E l \\left ( Y , (1/K) \\iota^T \\hat{Y} \\right )\n\\]\nwhere \\(\\iota = ( 1 , \\ldots ,1 )\\). Hence, the randomized multiple predictor with weights \\(w = (1/K)\\iota\\) provides close to optimal Bayes predictive performance. We now turn to algorithmic issues.\nAn alternative approach is to perform Bayesian model selection. Here we calculate the optimal Bayes weight for each predictor in accordance with Bayes Rule.\nWe formalize the gains in Classification Risk with the following discussion.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "16-tree.html#finding-good-bayes-predictors",
    "href": "16-tree.html#finding-good-bayes-predictors",
    "title": "8  Tree Models",
    "section": "",
    "text": "Figure 8.2\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.6",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "16-tree.html#ensemble-averaging-and-1n-rule",
    "href": "16-tree.html#ensemble-averaging-and-1n-rule",
    "title": "8  Tree Models",
    "section": "8.2 Ensemble Averaging and \\(1/N\\) Rule",
    "text": "8.2 Ensemble Averaging and \\(1/N\\) Rule\nIn high dimensions, when I have large number of predictive models that generate uncorrelated predictions, the optimal approach to generate a prediction is to average out predictions from those individual models/ weak predictors. This is called the \\(1/N\\) rule. The variance in the prediction is reduced by a factor of \\(N\\) when we average out \\(N\\) uncorrelated predictions. \\[\n\\mbox{Var} \\left ( \\frac{1}{N} \\sum_{i=1}^N \\hat y_i \\right ) = \\frac{1}{N^2} \\mbox{Var} \\left ( \\hat y_i \\right ) + \\frac{2}{N^2} \\sum_{i \\neq j} \\mbox{Cov} \\left ( \\hat y_i, \\hat y_j \\right )\n\\] In high dimensions it relatively easy to find uncorrelated predictors and those techniques prove to lead to a winning solution in many machine learning competitions. The \\(1/N\\) rule is optimal due to exchangeability of the weak predictors, see Polson, Sokolov, et al. (2017)\n\n\n8.2.1 Classification variance decomposition\nThe famous result is due to Cover who proved that k-nearest neighbors are at most twice the bayes risk.\nAmit, Blanchard, and Wilder (2000) use the population conditional probability distribution of a point \\(X\\) given \\(Y=c\\), denoted by \\(P_{c}\\), and the associated conditional expectation and variance operators will be denoted \\(E_{c}\\) and \\(V a r_{c}\\). Define the vectors of average aggregates conditional on class \\(c\\) as \\[\n\\begin{equation*}\nM_{c}(d)=E_{c}\\left[H_{\\mathbf{Q}}(X, d)\\right]=E\\left[H_{\\mathbf{Q}}(X, d) \\mid Y=c\\right] \\tag{8}\n\\end{equation*}\n\\]\nfor \\(d=1, \\ldots, K\\). The average conditional margin (ACM) for class \\(c\\) is defined as\n\\[\n\\begin{equation*}\n\\theta_{c}=\\min _{d \\neq c}\\left(M_{c}(c)-M_{c}(d)\\right) \\tag{9}\n\\end{equation*}\n\\]\nWe assume that \\(\\theta_{c}&gt;0\\). This assumption is very weak since it involves only the average over the population of class \\(c\\). It is quite natural since one would not expect good classification results when it is violated. Indeed as shown below it is satisfied in all cases.\nGiven that \\(\\theta_{c}&gt;0\\), the error rate for class \\(c\\) depends on the extent to which the aggregate classifier \\(H_{\\mathbf{Q}}(X, d)\\) is concentrated around \\(M_{c}(d)\\) for each \\(d=1, \\ldots, K\\). The simplest measure of concentration is the variance of \\(H_{\\mathbf{Q}}(X, d)\\) with respect to the distribution \\(P_{c}\\). Using Chebyshev’s inequality we write a coarse bound on the misclassification probability with respect to \\(P_{c}\\) as follows.\n\\[\n\\begin{align*}\nP_{c}\\left(C_{\\mathbf{Q}}(X) \\neq c\\right) \\leq & P_{c}\\left(H_{\\mathbf{Q}}(X, c)&lt;M_{c}(c)-\\theta_{c} / 2\\right) \\\\\n& +\\sum_{d \\neq c} P_{c}\\left(H_{\\mathbf{Q}}(X, d)&gt;M_{c}(d)+\\theta_{c} / 2\\right) \\\\\n\\leq & \\sum_{d=1}^{K} P_{c}\\left(\\left|H_{\\mathbf{Q}}(X, d)-M_{c}(d)\\right|&gt;\\theta_{c} / 2\\right) \\\\\n\\leq & \\frac{4}{\\theta_{c}^{2}} \\sum_{d=1}^{K} \\operatorname{Var}_{c}\\left[H_{\\mathbf{Q}}(X, d)\\right] . \\tag{10}\n\\end{align*}\n\\]\nOf course Chebyshev’s inequality is coarse and will not give very sharp results in itself, be we state it here as a landmark pointing to the relative importance of margin and variance, and to the tradeoff between the two quantities.\nWe rewrite each of the variance terms of the last equation as\n\\[\n\\begin{align*}\n\\operatorname{Var}_{c}\\left[E_{\\mathbf{Q}} h(X, d)\\right] & =E_{c}\\left[E_{\\mathbf{Q}} h(X, d)\\right]^{2}-\\left[E_{c} E_{\\mathbf{Q}} h(X, d)\\right]^{2} \\\\\n& =E_{\\mathbf{Q} \\otimes \\mathbf{Q}} E_{c}\\left[h_{1}(X, d) h_{2}(X, d)\\right]-E_{\\mathbf{Q} \\otimes \\mathbf{Q}}\\left[E_{c}\\left[h_{1}(X, d)\\right] E_{c}\\left[h_{2}(X, d)\\right]\\right] \\\\\n& =E_{\\mathbf{Q} \\otimes \\mathbf{Q}} \\operatorname{Cov}_{c}\\left[h_{1}(X, d), h_{2}(X, d)\\right] \\doteq \\gamma_{c, d} \\tag{11}\n\\end{align*}\n\\]\nwhere the notation \\(E_{\\mathbf{Q} \\otimes \\mathbf{Q}}\\) means that \\(h_{1}, h_{2}\\) are two classifiers sampled independently from the distribution \\(\\mathbf{Q}\\). We can therefore interpret this variance term as the conditional covariance of two classifiers independently sampled from \\(\\mathbf{Q}\\). We call this quantity the average conditional covariance (ACC). Even if \\(\\mathbf{Q}\\) is a discrete distribution, such as that provided by a particular run of \\(N\\) classifiers, when it is supported on a moderate number of classifiers, it is dominated by the conditional covariances of which there are order \\(N^{2}\\), and not the conditional variances of which there are order \\(N\\).\n\n\n8.2.2 Conditional and unconditional dependence\nIt should be emphasized that two classifiers, provided that they achieve reasonable classification rate (that is, better than just picking a class at random) will not be unconditionally independent. If we do not know the class label of a point, and vector \\(\\left(h_{1}(X, i)\\right)_{i}\\) is large at class \\(c\\), then we actually change our expectations regarding \\(\\left(h_{2}(X, i)\\right)_{i}\\). On the other hand if we were given in advance the class label \\(Y\\), then knowing \\(h_{1}(X)\\) would hardly affect our guess about \\(h_{2}(X)\\). This is the motivation behind the notion of weak conditional dependence.\nThis is in contrast to the measure of dependence introduced in Dietterich (1998), which involves the unconditional covariance. The \\(\\kappa\\) statistic used there is\n\\[\n\\kappa\\left(h_{1}, h_{2}\\right)=\\frac{\\sum_{d} \\operatorname{Cov}\\left[h_{1}(X, d), h_{2}(X, d)\\right]}{1-\\sum_{d} E h_{1}(X, d) E h_{2}(X, d)},\n\\]\nA simple decomposition of the numerator yields: \\(\\operatorname{Cov}\\left[h_{1}(X, d), h_{2}(X, d)\\right]=E \\operatorname{Cov}\\left[h_{1}(X, d), h_{2}(X, d) \\mid Y\\right]+\\operatorname{Cov}\\left[E\\left[h_{1}(X, d) \\mid Y\\right], E\\left[h_{2}(X, d) \\mid Y\\right]\\right]\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "16-tree.html#classification-trees",
    "href": "16-tree.html#classification-trees",
    "title": "8  Tree Models",
    "section": "8.3 Classification Trees",
    "text": "8.3 Classification Trees\nA classification tree operates much like a regression tree. The prediction is made based on the “majority vote”, which means selecting the class that appears most frequently within the region. The process of developing a classification tree is largely the same as that of a regression tree, involving recursive binary splitting. However, instead of using the Residual Sum of Squares (RSS), we use the classification error rate, which is the proportion of observations in that region that do not belong to the most prevalent class.\nWe start by introducing s0me notations \\[\np_{mk} = \\dfrac{1}{N_m}\\sum_{x_i \\in R_m} I(y_i=k),\n\\] which is proportion of observations of class \\(k\\) in region \\(m\\).\nThe classification then done as follows \\[\np_m = \\max_k p_{mk},~~~ E_m = 1-p_m\n\\] i.e the most frequent observation in region \\(m\\)\nThen classification is done as follows \\[\nP(y=k) = \\sum_{j=1}^J p_j I(x \\in R_j)\n\\]\nAn alternative method to evaluate the quality of a split in a classification tree is through the use of the Gini Index or Cross-Entropy. Let’s consider a scenario where we have an equal number of observations in each class, say 400 in each.\nNow, suppose we create a tree that results in two regions: one with a distribution of (300,100) and the other with (100,300). This means that in the first region, 300 observations belong to one class and 100 to the other, and vice versa in the second region.\nConsider another scenario where we have a different tree that results in two regions with distributions of (200,400) and (200,0).\nIn both cases, the misclassification rate is 0.25, meaning that 25% of the observations are incorrectly classified. However, the second tree is more desirable. Why is that? The second tree has a region with no misclassifications at all (200,0), which means it’s perfectly classifying all observations in that region. This is an ideal situation in classification problems. On the other hand, the first tree, despite having the same overall misclassification rate, doesn’t have any region where all observations are correctly classified.\nThis illustrates that while the misclassification rate is a useful metric, it doesn’t always capture the full picture. Other metrics like the Gini Index or Cross-Entropy can provide a more nuanced view of the quality of a split, taking into account not just the overall error rate, but also the distribution of errors across different regions.\nAnother way to measure the quality of the split is to use the Gini Index and Cross-Entropy Say, I have 400 observations in each class (400,400). I create a tree with two region: (300,100) and (100,300). Say I have another tree: (200,400) and (200,0). In both cases misclassification rate is 0.25. The later tree is preferable. We prefer to have more “pure nodes” and Gini index does a better job.\nThe Gini index: \\[\nG_m = \\sum_{k=1}^K p_{mk}(1-p_{mk})\n\\] It measures a variance across the \\(K\\) classes. It takes on a small value if all of the \\(p_{mk}\\)’s are close to zero or one\nAn alternative to the Gini index is cross-entropy (a.k.a deviance), given by \\[\nD_m = -\\sum_{k=1}^Kp_{mk}\\log p_{mk}\n\\] It is near zero if the \\(p_mk\\)’s are all near zero or near one. Gini index and the cross-entropy led to similar results.\nNow we apply the tree model to the Boston housing dataset.\n\nlibrary(MASS); data(Boston); attach(Boston)\nhead(Boston)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nblack\nlstat\nmedv\n\n\n\n\n0.01\n18\n2.3\n0\n0.54\n6.6\n65\n4.1\n1\n296\n15\n397\n5.0\n24\n\n\n0.03\n0\n7.1\n0\n0.47\n6.4\n79\n5.0\n2\n242\n18\n397\n9.1\n22\n\n\n0.03\n0\n7.1\n0\n0.47\n7.2\n61\n5.0\n2\n242\n18\n393\n4.0\n35\n\n\n0.03\n0\n2.2\n0\n0.46\n7.0\n46\n6.1\n3\n222\n19\n395\n2.9\n33\n\n\n0.07\n0\n2.2\n0\n0.46\n7.2\n54\n6.1\n3\n222\n19\n397\n5.3\n36\n\n\n0.03\n0\n2.2\n0\n0.46\n6.4\n59\n6.1\n3\n222\n19\n394\n5.2\n29\n\n\n\n\n\n\nFirst we build a big tree\n\ntemp = tree(medv~lstat,data=Boston,mindev=.0001)\nlength(unique(temp$where)) # first big tree size\n\n 73\n\n\nThen prune it down to one with 7 leaves\n\nboston.tree=prune.tree(temp,best=7)\nlength(unique(boston.tree$where)) # pruned tree size\n\n 7\n\n\ntext(boston.tree,col=\"blue\",label=c(\"yval\"),cex=.8)\nboston.fit = predict(boston.tree) #get training fitted values\nplot(Boston$lstat,Boston$medv,cex=.5,pch=16) #plot data\noo=order(Boston$lstat)\nlines(Boston$lstat[oo],boston.fit[oo],col='red',lwd=3) #step function fit\ncvals=c(9.725,4.65,3.325,5.495,16.085,19.9) #cutpoints from tree\nfor(i in 1:length(cvals)) abline(v=cvals[i],col='magenta',lty=2) #cutpoints\n\n\n\n\n\n\n\n\n\n\nPick off dis,lstat,medv\n\ndf2=Boston[,c(8,13,14)] \nprint(names(df2))\n\n \"dis\"   \"lstat\" \"medv\" \n\n\nBuild the big tree\n\ntemp = tree(medv~.,df2,mindev=.0001)\nlength(unique(temp$where)) #\n\n 74\n\n\nThen prune it down to one with 7 leaves\n\nboston.tree=prune.tree(temp,best=7)\n\nplot(boston.tree,type=\"u\")# plot tree and partition in x.\ntext(boston.tree,col=\"blue\",label=c(\"yval\"),cex=.8)\npartition.tree(boston.tree)\n\n\n\n\n\n\n\n\n\n\nGet predictions on 2d grid\n\npv=seq(from=.01,to=.99,by=.05)\nx1q = quantile(df2$lstat,probs=pv)\nx2q = quantile(df2$dis,probs=pv)\nxx = expand.grid(x1q,x2q) #matrix with two columns using all combinations of x1q and x2q\ndfpred = data.frame(dis=xx[,2],lstat=xx[,1])\nlmedpred = predict(boston.tree,dfpred)\n\nMake perspective plot\n\npersp(x1q,x2q,matrix(lmedpred,ncol=length(x2q),byrow=T),\n      theta=150,xlab='dis',ylab='lstat',zlab='medv',\n      zlim=c(min(df2$medv),1.1*max(df2$medv)))\n\n\n\n\n\n\n\n\nAdvantages of Decision Trees:\nDecision trees are incredibly intuitive and simple to explain. They can be even more straightforward to understand than linear regression models. Some theorists argue that decision trees mimic human decision-making processes more accurately than other regression and classification methods we’ve discussed in previous chapters. Decision trees can be visually represented, making them easily interpretable, even for those without a deep understanding of the underlying mechanics, particularly when the trees are not overly complex. Decision trees can effortlessly manage qualitative predictors, eliminating the need to create dummy variables.\nDisadvantages of Decision Trees:\nLarge trees can exhibit high variance. This means that a minor change in the data can lead to a significant change in the final estimated tree, making the model unstable. Conversely, small trees, while more stable, may not be powerful predictors as they might oversimplify the problem. It can be challenging to find a balance between bias and variance when using decision trees. A model with too much bias oversimplifies the problem and performs poorly, while a model with too much variance overfits the data and may not generalize well to unseen data.\nThere are several techniques used to address the issue of overfitting in decision trees. We considered the pruning technique which reduces the complexity of the final classifier, and hence improve predictive accuracy by reducing overfitting. Two other methods are random forests and boosting. Random Forests is an ensemble method where multiple decision trees are created and their predictions are averaged (for regression) or majority voting is done (for classification). Boosting is another ensemble technique where trees are built sequentially such that each subsequent tree aims to reduce the bias of the combined classifier.\nIn the bagging approach, we treat the sample as if it were the population and then take iid draws. That is, you sample with replacement so that you can get the same original sample value more than once in a bootstrap sample.\nTo Bootsrap Aggregate (Bag) we:\n\nTake \\(B\\) bootstrap samples from the training data, each of the same size as the training data.\nFit a large tree to each bootstrap sample (we know how to do this fast!). This will give us \\(B\\) trees.\nCombine the results from each of the B trees to get an overall prediction.\n\nWhen the target variable \\(y\\) is numeric, the bagging process is straightforward. The final prediction is simply the average of the predictions from each of the \\(B\\) trees. However, when \\(y\\) is categorical, the process of combining results from different trees is less straightforward. One common approach is to use a voting system. In this system, each tree in the ensemble makes a prediction for a given input \\(x\\). The predicted category that receives the most votes (out of \\(B\\) total votes) is chosen as the final prediction. Another approach is to average the predicted probabilities \\(\\hat p\\) from each tree. This method can provide a more nuanced prediction, especially in cases where the voting results are close.\nDespite the potential benefits of averaging predicted probabilities, most software implementations of bagging for decision trees use the voting method. This is likely due to its simplicity and intuitive appeal. However, the best method to use can depend on the specific characteristics of the problem at hand.\nThe simple idea behind every ensemble modes is that variance of the average is lowe than variance of individual. Say we have \\(B\\) models \\(f_1(x),\\ldots,f_B(x)\\) then we combine those \\[\nf_{avg}(x) = \\dfrac{1}{B}\\sum_{b=1}^Bf_b(x)\n\\] Combining models helps fighting overfilling. On the negative side, it is harder to interpret those ensembles\nLet’s experiment with the number of trees in the model\n\nlibrary(randomForest)\nn = nrow(Boston)\nntreev = c(10,500,5000)\nfmat = matrix(0,n,3)\nfor(i in 1:3) {\n  rffit = randomForest(medv~lstat,data=Boston,ntree=ntreev[i],maxnodes=15)\n  fmat[,i] = predict(rffit)\n  print(mean((fmat[,i] - Boston$medv)^2, na.rm = TRUE))\n}\n\n 30\n 29\n 29\n\n\nLet’s plot the results\noo = order(Boston$lstat)\nfor(i in 1:3) {\n  plot(Boston$lstat,Boston$medv,xlab='lstat',ylab='medv',pch=16)\n  lines(Boston$lstat[oo],fmat[oo,i],col=i+1,lwd=3)\n  title(main=paste('bagging ntrees = ',ntreev[i]))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith 10 trees our fit is too jumbly.\nWith 1,000 and 5,000 trees the fit is not bad and very similar.\nNote that although our method is based multiple trees (average over) so we no longer have a simple step function!!\n\nRandom Forest\nIn the bagging technique, models can become correlated, which prevents the achievement of a \\(1/n\\) reduction in variance. This happens because most, if not all, of the trees will use the most influential predictor in the top split. As a result, bagged trees tend to look very similar to each other.\nRandom Forests, on the other hand, introduce an element of randomness that helps to decorrelate the trees, making the ensemble more robust and improving prediction accuracy. This randomness comes into play when considering a split in a tree. Instead of considering all \\(p\\) predictors for a split, a random sample of \\(m\\) predictors is chosen as split candidates. This subset of predictors is different for each split, which means that different trees are likely to use different predictors in the top split, leading to a more diverse set of trees.\nThe number of predictors considered at each split, \\(m\\), is typically chosen to be the square root of the total number of predictors, \\(p\\). This choice is a rule of thumb that often works well in practice, but it can be tuned based on the specific characteristics of the dataset.\nBy decorrelating the trees, Random Forests can often achieve better performance than bagging, especially when there’s a small number of very strong predictors in the dataset. In such cases, bagging can end up with an ensemble of very similar trees that all rely heavily on these strong predictors, while Random Forests can leverage the other, weaker predictors more effectively.\nOve of the “interpretation” tools that comes with ensemble models is importance rank: total amount that the deviance (loss) is decreased due to splits over a given predictor, averaged over all tree\n\nrf.boston = randomForest(medv~.,data=Boston,mtry=4,importance=TRUE,ntree=50)\nvarImpPlot(rf.boston,pch=21,bg=\"lightblue\",main=\"\")\n\n\n\n\n\n\n\n\n\nrf.boston = randomForest(medv~.,data=Boston,mtry=6,ntree=50, maxnodes=50)\nyhat.rf = predict(rf.boston,newdata=Boston)\noo=order(Boston$lstat)\nplot(Boston$lstat[oo],Boston$medv[oo],pch=21,bg=\"grey\", xlab=\"lstat\", ylab=\"medv\") #plot data\nlines(Boston$lstat[oo],yhat.rf[oo],col='red',lwd=3) #step function fit\n\n\n\n\n\n\n\n\nBoosting\nBoosting, like Random Forests, is a method that combines multiple trees to create a more powerful predictive model. However, the approach it takes is quite distinct.\nHere’s how Boosting works:\n\nInitially, a single decision tree is fitted to the data.\nThis initial tree is intentionally made weak, meaning it doesn’t perfectly fit the data.\nWe then examine the residuals, which represent the portion of the target variable \\(y\\) not explained by the weak tree.\nA new tree is then fitted to these residuals, essentially trying to predict the error of the first tree.\nThis new tree is also “weakened” or “shrunk”. The prediction from this tree is then added to the prediction of the first tree.\nThis process is repeated iteratively. In each iteration, a new tree is fitted to the residuals of the current ensemble of trees, shrunk, and then added to the ensemble.\nThe final model is the sum of all these “shrunk” trees. The key idea behind Boosting is to iteratively improve the model by focusing on the parts of the data that the current model is not explaining well (the residuals). Each new tree is trying to correct the mistakes of the ensemble of previous trees. By adding together many weak models (the shrunk trees), Boosting can often achieve a strong overall model.\n\nPick a loss function \\(L\\) that reflects setting; e.g., for continuous \\(y\\), could take \\(L(y_i , \\theta_i ) = (y_i - \\theta_i )^2\\) Want to solve \\[\\mathrm{minimize}_{\\beta \\in R^M} \\sum_{i=1}^n L \\left(y_i, \\sum_{j=1}^M \\beta_j \\cdot T_j(x_i)\\right)\\]\n\nIndexes all trees of a fixed size (e.g., depth = 5), so \\(M\\) is huge\nSpace is simply too big to optimize\nGradient boosting: basically a version of gradient descent that is forced to work with trees\nFirst think of optimization as \\(\\min_\\theta f (\\theta)\\), over predicted values \\(\\theta\\) (subject to \\(\\theta\\) coming from trees)\n\n\n\n\n\n\n\n\n\n\n\nSet \\(f_1(x)=0\\) (constant predictor) and \\(r_i=y_i\\)\nFor \\(b=1,2,\\ldots,B\\)\n\nFit a tree \\(f_b\\) with \\(d\\) splits to the training set \\((X,r)\\)\nUpdate the model \\[f(x) = f(x) +\\lambda f_b(x)\\]\nUpdate the residuals \\[r_i=r_i - \\lambda f_b(x)\\]\n\nHere are some boosting fits where we vary the number of trees, but fix the depth at 2 (suitable with 1 x) and shrinkage = \\(\\lambda\\) at .2.\n\nlibrary(gbm)\nboost.boston=gbm(medv~.,data=Boston,distribution=\"gaussian\",n.trees=5000,interaction.depth=4)\nyhat.boost=predict(boost.boston,newdata=Boston,n.trees=5000)\nmean((yhat.boost-Boston$medv)^2)\n\n 4e-04\n\n\n\nsummary(boost.boston, plotit=FALSE)\n\n\n\n\n\n\nvar\nrel.inf\n\n\n\n\nlstat\nlstat\n36.32\n\n\nrm\nrm\n30.98\n\n\ndis\ndis\n7.63\n\n\ncrim\ncrim\n5.09\n\n\nnox\nnox\n4.63\n\n\nage\nage\n4.50\n\n\nblack\nblack\n3.45\n\n\nptratio\nptratio\n3.11\n\n\ntax\ntax\n1.74\n\n\nrad\nrad\n1.17\n\n\nindus\nindus\n0.87\n\n\nchas\nchas\n0.39\n\n\nzn\nzn\n0.13\n\n\n\n\n\n\nplot(boost.boston,i=\"rm\")\nplot(boost.boston,i=\"lstat\")\n\n\n\n\n\n\n\n\n\n\nAdvantages of Boosting over Random Forests:\n\nPerformance: Boosting, in many cases, provides better predictive accuracy than Random Forests. By focusing on the residuals or mistakes, Boosting can incrementally improve model performance.\nModel Interpretability: While both methods are not as interpretable as a single decision tree, Boosting models can sometimes be more interpretable than Random Forests, especially when the number of weak learners (trees) is small.\n\nDisadvantages of Boosting compared to Random Forests:\n\nComputation Time and Complexity: Boosting can be more computationally intensive than Random Forests. This is because trees are built sequentially in Boosting, while in Random Forests, they are built independently and can be parallelized.\nOverfitting: Boosting can overfit the training data if the number of trees is too large, or if the trees are too complex. This is less of a problem with Random Forests, which are less prone to overfitting due to the randomness injected into the tree building process.\nOutliers: Boosting can be sensitive to outliers since it tries to correct the mistakes of the predecessors. On the other hand, Random Forests are more robust to outliers.\nNoise: Boosting can overemphasize instances that are hard to classify and can overfit to noise, whereas Random Forests are more robust to noise.\n\nRemember, the choice between Boosting and Random Forests (or any other model) should be guided by the specific requirements of your task, including the nature of your data, the computational resources available, and the trade-off between interpretability and predictive accuracy.\n\n\n\n\nAmit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. “Multiple Randomized Classifiers: MRCL.”\n\n\nDiaconis, Persi, and David Freedman. 1987. “A Dozen de Finetti-style Results in Search of a Theory.” In Annales de l’IHP Probabilités Et Statistiques, 23:397–423.\n\n\nKlartag, Bo’az. 2007. “A Central Limit Theorem for Convex Sets.” Inventiones Mathematicae 168 (1): 91–131.\n\n\nMilman, Vitali D, and Gideon Schechtman. 2009. Asymptotic Theory of Finite Dimensional Normed Spaces: Isoperimetric Inequalities in Riemannian Manifolds. Vol. 1200. Springer.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep Learning: A Bayesian Perspective.” Bayesian Analysis 12 (4): 1275–1304.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "17-forecasting.html",
    "href": "17-forecasting.html",
    "title": "9  Forecasting",
    "section": "",
    "text": "9.1 Structural time series models\nTime series data are everywhere, but time series modeling is a fairly specialized area within statistics and data science. This post describes the bsts software package, which makes it easy to fit some fairly sophisticated time series models with just a few lines of R code.\nTime series data appear in a surprising number of applications, ranging from business, to the physical and social sciences, to health, medicine, and engineering. Forecasting (e.g. next month’s sales) is common in problems involving time series data, but explanatory models (e.g. finding drivers of sales) are also important. Time series data are having something of a moment in the tech blogs right now, with Facebook announcing their “Prophet” system for time series forecasting (Sean J. Taylor and Ben Letham (2017)), and Google posting about its forecasting system in this blog (Eric Tassone and Farzan Rohani (2017)).\nThis post summarizes the bsts R package, a tool for fitting Bayesian structural time series models. These are a widely useful class of time series models, known in various literatures as “structural time series,” “state space models,” “Kalman filter models,” and “dynamic linear models,” among others. Though the models need not be fit using Bayesian methods, they have a Bayesian flavor and the bsts package was built to use Bayesian posterior sampling.\nThe bsts package is open source. You can download it from CRAN with the R command install.packages(\"bsts\"). It shares some features with Facebook and Google systems, but it was written with different goals in mind. The other systems were written to do “forecasting at scale,” a phrase that means something different in time series problems than in other corners of data science. The Google and Facebook systems focus on forecasting daily data into the distant future. The “scale” in question comes from having many time series to forecast, not from any particular time series being extraordinarily long. The bottleneck in both cases is the lack of analyst attention, so the systems aim to automate analysis as much as possible. The Facebook system accomplishes this using regularized regression, while the Google system works by averaging a large ensemble of forecasts. Both systems focus on daily data, and derive much of their efficiency through the careful treatment of holidays.\nThere are aspects of bsts which can be similarly automated, and a specifically configured version of bsts is a powerful member of the Google ensemble. However, bsts can also be configured for specific tasks by an analyst who knows whether the goal is short term or long term forecasting, whether or not the data are likely to contain one or more seasonal effects, and whether the goal is actually to fit an explanatory model, and not primarily to do forecasting at all.\nThe workhorse behind bsts is the structural time series model. These models are briefly described in the section Structural time series models. Then the software is introduced through a series of extended examples that focus on a few of the more advanced features of bsts. Example 1: Nowcasting includes descriptions of the local linear trend and seasonal state models, as well as spike and slab priors for regressions with large numbers of predictors. Example 2: Long term forecasting describes a situation where the local level and local linear trend models would be inappropriate. It offers a semilocal linear trend model as an alternative. Example 3: Recession modeling describes an model where the response variable is non-Gaussian. The goal in Example 3 is not to predict the future, but to control for serial dependence in an explanatory model that seeks to identify relevant predictor variables. A final section concludes with a discussion of other features in the package which we won’t have space (maybe “time” is a better word) to explore with fully fleshed out examples.\nA structural time series model is defined by two equations. The observation equation relates the observed data \\(y_t\\) to a vector of latent variables \\(\\alpha_t\\) known as the “state.” \\[\ny_t = Z_t^T\\alpha_t + \\epsilon_t.\n\\]\nThe transition equation describes how the latent state evolves through time. \\[\n\\alpha_{t+1} = T_t \\alpha_t + R_t \\eta_t.\n\\]\nThe error terms \\(\\epsilon_t\\) and \\(\\eta_t\\) are Gaussian and independent of everything else. The arrays \\(Z_t\\) , \\(T_t\\) and \\(R_t\\) are structural parameters. They may contain parameters in the statistical sense, but often they simply contain strategically placed 0’s and 1’s indicating which bits of \\(\\alpha_t\\) are relevant for a particular computation. An example will hopefully make things clearer.\nThe simplest useful model is the “local level model,” in which the vector \\(\\alpha_t\\) is just a scalar \\(\\mu_t\\). The local level model is a random walk observed in noise. \\[\\begin{align*}\ny_t = &\\mu_t + \\epsilon_t\\\\\n\\mu_{t+1} = &\\mu_t + \\eta_t.\n\\end{align*}\\] Here \\(\\alpha_t=\\mu_t\\) , and \\(Z_t\\) , \\(T_t\\), and \\(R_t\\) all collapse to the scalar value 1. Similar to Bayesian hierarchical models for nested data, the local level model is a compromise between two extremes. The compromise is determined by variances of \\(\\epsilon_t \\sim N(0,\\sigma^2)\\) and \\(\\eta_t \\sim N(0,\\tau^2)\\). If \\(\\tau^2=0\\) then \\(\\mu_t\\) is a constant, so the data are IID Gaussian noise. In that case the best estimator of \\(y_{t+1}\\) is the mean of \\(y_1,\\ldots,y_t\\). Conversely, if \\(\\sigma^2=0\\) then the data follow a random walk, in which case the best estimator of \\(y_{t+1}\\) is \\(y_t\\). Notice that in one case the estimator depends on all past data (weighted equally) while in the other it depends only on the most recent data point, giving past data zero weight. If both variances are positive then the optimal estimator of \\(y_{t+1}\\) winds up being “exponential smoothing,” where past data are forgotten at an exponential rate determined by the ratio of the two variances. Also notice that while the state in this model is Markov (i.e. it only depends on the previous state), the dependence among the observed data extends to the beginning of the series.\nFigure 9.1: Apple Adjusted Closing Price\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.2: Apple Adjusted Closing Price\nIn the example above, one of the plots shows the price of the Apple stock from ‘2021-01-01’, to = 2022-12-31. The other plot is a sequence generated from a random walk model fitted to the Apple price data. Can you spot which one is which?\nStructural time series models are useful because they are flexible and modular. The analyst chooses the structure of \\(\\alpha_t\\) based on things like whether short or long term predictions are more important, whether the data contains seasonal effects, and whether and how regressors are to be included. Many of these models are standard, and can be fit using a variety of tools, such as the StructTS function distributed with base R or one of several R packages for fitting these models (with the dlm package (Petris (2010), Campagnoli, Petrone, and Petris (2009)) deserving special mention). The bsts package handles all the standard cases, but it also includes several useful extensions, described in the next few sections through a series of examples. Each example includes a mathematical description of the model and example bsts code showing how to work with the model using the bsts software. To keep things short, details about prior assumptions are largely avoided.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "17-forecasting.html#structural-time-series-models",
    "href": "17-forecasting.html#structural-time-series-models",
    "title": "9  Forecasting",
    "section": "",
    "text": "Example 9.1 (Nowcasting) S. Scott and Varian (2014) and S. L. Scott and Varian (2015) used structural time series models to show how Google search data can be used to improve short term forecasts (“nowcasts”) of economic time series. Figure below shows the motivating data set from S. Scott and Varian (2014), which is also included with the bsts package. The data consist of the weekly initial claims for unemployment insurance in the US, as reported by the US Federal Reserve. Like many official statistics they are released with delay and subject to revision. At the end of the week, the economic activity determining these numbers has taken place, but the official numbers are not published until several days later. For economic decisions based on these and similar numbers, it would help to have an early forecast of the current week’s number as of the close of the week. Thus the output of this analysis is truly a “nowcast” of data that has already happened rather than a “forecast” of data that will happen in the future.\n\nlibrary(bsts)     # load the bsts package\ndata(iclaims)     # bring the initial.claims data into scope\nplot(initial.claims$iclaimsNSA, lwd=2, ylab=\"Unemployment claims (thousand)\")\n\n\n\n\n\n\n\nFigure 9.3: Weekly initial claims for unemployment in the US.\n\n\n\n\n\nThere are two sources of information about the current value \\(y_t\\) in the initial claims series: past values \\(y_{t-\\tau}\\) describing the time series behavior of the series, and contemporaneous predictors \\(x_t\\) from a data source which is correlated with \\(y_t\\) , but which is available without the delay exhibited by \\(y_t\\) . The time series structure shows an obvious trend (in which the financial and housing crises in 2008 - 2009 are apparent) as well as a strong annual seasonal pattern. The external data source explored by Scott and Varian was search data from Google trends with search queries such as “how to file for unemployment” having obvious relevance.\nScott and Varian modeled the data using a structural time series with three state components:\n\ntrend \\(\\mu_t\\)\nseasonal pattern \\(\\tau_t\\)\n\nregression component \\(\\beta^Tx_t\\).\n\nThe model is \\[\\begin{align*}\ny_t = & \\mu_t + \\tau_t + \\beta^T x_t + \\epsilon_t\\\\\n\\mu_{t+1} = &\\mu_t + \\delta_t + \\eta_{0t}\\\\\n\\delta_{t+1} = &\\delta_t + \\eta_{1t}\\\\\n\\tau_{t+1} = &-\\sum_{s = 1}^{S-1}\\tau_{t} + \\eta_{2t}.\n\\end{align*}\\]\nThe trend component looks similar to the local level model above, but it has an extra term \\(\\delta_t\\) . Notice that \\(\\delta_t\\) is the amount of extra \\(\\mu\\) you can expect as \\(t\\rightarrow t+1\\), so it can be interpreted as the slope of the local linear trend. Slopes normally multiply some \\(x\\) variable, but in this case \\(x=\\Delta t\\), which omitted from the equation because it is always 1. The slope evolves according to a random walk, which makes the trend an integrated random walk with an extra drift term. The local linear trend is a better model than the local level model if you think the time series is trending in a particular direction and you want future forecasts to reflect a continued increase (or decrease) seen in recent observations. Whereas the local level model bases forecasts around the average value of recent observations, the local linear trend model adds in recent upward or downward slopes as well. As with most statistical models, the extra flexibility comes at the price of extra volatility.\nThe best way to understand the seasonal component \\(\\tau_t\\) is in terms of a regression with seasonal dummy variables. Suppose you had quarterly data, so that \\(S=4\\). You might include the annual seasonal cycle using 3 dummy variables, with one left out as a baseline. Alternatively, you could include all four dummy variables but constrain their coefficients to sum to zero. The seasonal state model takes the latter approach, but the constraint is that the \\(S\\) most recent seasonal effects must sum to zero in expectation. This allows the seasonal pattern to slowly evolve. Scott and Varian described the annual cycle in the weekly initial claims data using a seasonal state component with \\(S=52\\). Of course weeks don’t neatly divide years, but given the small number of years for which Google data are available the occasional one-period seasonal discontinuity was deemed unimportant.\nLet’s ignore the regression component for now and fit a bsts model with just the trend and seasonal components.\n\nss &lt;- AddLocalLinearTrend(list(), initial.claims$iclaimsNSA)\nss &lt;- AddSeasonal(ss, initial.claims$iclaimsNSA, nseasons = 52)\nmodel1 &lt;- bsts(initial.claims$iclaimsNSA,state.specification = ss,niter = 1000)\n\nThe first thing to do when fitting a bsts model is to specify the contents of the latent state vector \\(\\alpha_t\\). The bsts package offers a library of state models, which are included by adding them to a state specification (which is just a list with a particular format). The call to AddLocalLinearTrend above adds a local linear trend state component to an empty state specification (the list() in its first argument). The call to AddSeasonal adds a seasonal state component with 52 seasons to the state specification created on the previous line. The state vector \\(\\alpha_t\\) is formed by concatenating the state from each state model. Similarly, the vector \\(Z_t\\) is formed by concatenating the \\(Z\\) vectors from the two state models, while the matrices \\(T_t\\) and \\(R_t\\) are combined in block-diagonal fashion.\nThe state specification is passed as an argument to bsts, along with the data and the desired number of MCMC iterations. The model is fit using an MCMC algorithm, which in this example takes about 20 seconds to produce 1000 MCMC iterations. The returned object is a list (with class attribute bsts). You can see its contents by typing\n\nnames(model1)\n\n \"sigma.obs\"                  \"sigma.trend.level\"         \n \"sigma.trend.slope\"          \"sigma.seasonal.52\"         \n \"final.state\"                \"state.contributions\"       \n \"one.step.prediction.errors\" \"log.likelihood\"            \n \"has.regression\"             \"state.specification\"       \n \"prior\"                      \"timestamp.info\"            \n \"model.options\"              \"family\"                    \n \"niter\"                      \"original.series\"           \n\n\nThe first few elements contain the MCMC draws of the model parameters. Most of the other elements are data structures needed by various S3 methods (plot, print, predict, etc.) that can be used with the returned object. MCMC output is stored in vectors (for scalar parameters) or arrays (for vector or matrix parameters) where the first index in the array corresponds to MCMC iteration number, and the remaining indices correspond to dimension of the deviate being drawn.\nMost users won’t need to look inside the returned bsts object because standard tasks like plotting and prediction are available through familiar S3 methods. For example, there are several plot methods available.\n\npar(mar=c(4,4,2,0))\nplot(model1)\nplot(model1, \"components\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Components\n\n\n\n\n\n\n\nFigure 9.4: Structural time series model for unemployment claims\n\n\n\n\nThe Figure 9.4 (a) above shows the Posterior distribution of model state. Blue circles are actual data points. The Figure 9.4 (b) shows the individual state components. The plot looks fuzzy because it is showing the marginal posterior distribution at each time point.\nThe default plot method plots the posterior distribution of the conditional mean \\(Z_t^T\\alpha_t\\) given the full data \\(y=y_1,\\ldots,y_T\\). Other plot methods can be accessed by passing a string to the plot function. For example, to see the contributions of the individual state components, pass the string “components” as a second argument, as shown above. Figure below shows the output of these two plotting functions. You can get a list of all available plots by passing the string help as the second argument.\nTo predict future values there is a predict method. For example, to predict the next 12 time points you would use the following commands.\n\npar(mar=c(4,4,0,0))\npred1 &lt;- predict(model1, horizon = 12)\nplot(pred1, plot.original = 156)\n\n\n\n\n\n\n\n\nThe output of predict is an object of class bsts.prediction, which has its own plot method. The plot.original = 156 argument says to plot the prediction along with the last 156 time points (3 years) of the original series.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "17-forecasting.html#regression-with-spike-and-slab-priors",
    "href": "17-forecasting.html#regression-with-spike-and-slab-priors",
    "title": "9  Forecasting",
    "section": "9.2 Regression with spike and slab priors",
    "text": "9.2 Regression with spike and slab priors\nNow let’s add a regression component to the model described above, so that we can use Google search data to improve the forecast. The bsts package only includes 10 search terms with the initial claims data set, to keep the package size small, but S. Scott and Varian (2014) considered examples with several hundred predictor variables. When faced with large numbers of potential predictors it is important to have a prior distribution that induces sparsity. A spike and slab prior is a natural way to express a prior belief that most of the regression coefficients are exactly zero.\nA spike and slab prior is a prior on a set of regression coefficients that assigns each coefficient a positive probability of being zero. Upon observing data, Bayes’ theorem updates the inclusion probability of each coefficient. When sampling from the posterior distribution of a regression model under a spike and slab prior, many of the simulated regression coefficients will be exactly zero. This is unlike the “lasso” prior (the Laplace, or double-exponential distribution), which yields MAP estimates at zero but where posterior simulations will be all nonzero. You can read about the mathematical details of spike and slab priors in S. Scott and Varian (2014).\nWhen fitting bsts models that contain a regression component, extra arguments captured by ... are passed to the SpikeSlabPrior function from the BoomSpikeSlab package. This allows the analyst to adjust the default prior settings for the regression component from the bsts function call. To include a regression component in a bsts model, simply pass a model formula as the first argument.\n\n# Fit a `bsts` model with expected model size 1, the default.\nmodel2 &lt;- bsts(iclaimsNSA ~ .,state.specification = ss,niter = 1000,data = initial.claims)\n# Fit a `bsts` model with expected model size 5, to include more coefficients.\nmodel3 &lt;- bsts(iclaimsNSA ~ .,state.specification = ss,niter = 1000,data = initial.claims,expected.model.size = 5)  # Passed to SpikeSlabPrior.\n\nTo examine the output you can use the same plotting functions as before. For example, to see the contribution of each state component you can type\n\npar(mar=c(4,4,3,0))\nplot(model2, \"comp\")\n\n\n\n\n\n\n\n\nIt produces the contribution of each state component to the initial claims data, assuming a regression component with default prior. Compare to the previous model. The regression component is explaining a substantial amount of variation in the initial claims series.\nThere are also plotting functions that you can use to visualize the regression coefficients. The following commands plot posterior inclusion probabilities for predictors in the “initial claims” nowcasting example assuming an expected model size of 1 and 5.\npar(mar=c(4,0,0,0))\nplot(model2, \"coef\")\nplot(model3, \"coef\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Full\n\n\n\n\n\n\n\n\n\n\n\n(b) Sparse\n\n\n\n\n\n\n\nFigure 9.5: Variable Importance\n\n\n\nThe search term “unemployment office” shows up with high probability in both models. Increasing the expected model size from 1 (the default) to 5 allows other variables into the model, though “Idaho unemployment” is the only one that shows up with high probability.\nThose probabilities are calculated from the histogram of the samples of each \\(\\beta\\) calculated by the estimation algorithm (MCMC)\npar(mar=c(4,4,0,0))\n# unemployment.office\nhist(model3$coefficients[,10], breaks = 40, main=\"\",xlab=\"unemployment.office\", col=\"lightblue\")\n# pennsylvania.unemployment\nhist(model3$coefficients[,3], breaks = 40, main = \"\", xlab=\"pennsylvania.unemployment\", col=\"lightblue\")\nhist(model2$coefficients[,3], breaks = 40, main = \"\", xlab=\"pennsylvania.unemployment\", col=\"lightblue\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Sparse\n\n\n\n\n\n\n\n\n\n\n\n(b) Sparse\n\n\n\n\n\n\n\n\n\n\n\n(c) Full\n\n\n\n\n\n\n\nFigure 9.6: Sample from the distribution over two beta parameters",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "17-forecasting.html#model-diagnostics-did-the-google-data-help",
    "href": "17-forecasting.html#model-diagnostics-did-the-google-data-help",
    "title": "9  Forecasting",
    "section": "9.3 Model diagnostics: Did the Google data help?",
    "text": "9.3 Model diagnostics: Did the Google data help?\nAs part of the model fitting process, the algorithm generates the one-step-ahead prediction errors \\(y_t - E(y_t | Y_{t-1}, \\theta)\\), where \\(Y_{t-1}=y_1,\\ldots,y_{t-1}\\), and the vector of model parameters \\(\\theta\\) is fixed at its current value in the MCMC algorithm. The one-step-ahead prediction errors can be obtained from the bsts model by calling bsts.prediction.errors(model1).\nThe one step prediction errors are a useful diagnostic for comparing several bsts models that have been fit to the same data. They are used to implement the function CompareBstsModels, which is called as shown below.\n\nCompareBstsModels(list(\"Model 1\" = model1,\n               \"Model 2\" = model2,\n               \"Model 3\" = model3),\n          colors = c(\"black\", \"red\", \"blue\"))\n\n\n\n\nComparison of Errors for the three models.\n\n\n\n\nThe bottom panel shows the original series. The top panel shows the cumulative total of the mean absolute one step prediction errors for each model. The final time point in the top plot is proportional to the mean absolute prediction error for each model, but plotting the errors as a cumulative total lets you see particular spots where each model encountered trouble, rather than just giving a single number describing each model’s predictive accuracy. This figure shows that the Google data help explain the large spike near 2009, where model 1 accumulates errors at an accelerated rate, but models 2 and 3 continue accumulating errors at about the same rate they had been before. The fact that the lines for models 2 and 3 overlap in this figure means that the additional predictors allowed by the relaxed prior used to fit model 3 do not yield additional predictive accuracy.\n\nExample 9.2 (Long term forecasting) A common question about bsts is “which trend model should I use?” To answer that question it helps to know a bit about the different models that the bsts software package provides, and what each model implies. In the local level model the state evolves according to a random walk: \\[\n\\mu_{t+1}=\\mu_t+\\eta_t.\n\\] If you place your eye at time 0 and ask what happens at time \\(t\\) , you find that \\(\\mu_t \\sim N(\\mu_0,t\\sigma^2\\eta)\\). The variance continues to grow with \\(t\\), all the way to \\(t=\\infty\\). The local linear trend is even more volatile. When forecasting far into the future the flexibility provided by these models becomes a double edged sword, as local flexibility in the near term translates into extreme variance in the long term.\nAn alternative is to replace the random walk with a stationary AR process. For example \\[\n\\mu_{t+1}=\\rho\\mu_t+\\eta_t,\n\\]\nwith \\(\\eta_t \\sim N(0,\\sigma^2\\eta)\\) and \\(|\\rho|&lt;1\\). This model has stationary distribution \\[\n\\mu_{\\infty} \\sim N\\left(0,\\dfrac{\\sigma^2_{\\eta}}{1-\\rho^2}\\right),\n\\] which means that uncertainty grows to a finite asymptote, rather than infinity, in the distant future. bsts offers autoregressive state models through the functions AddAr, when you want to specify a certain number of lags, and AddAutoAr when you want the software to choose the important lags for you.\nA hybrid model modifies the local linear trend model by replacing the random walk on the slope with a stationary AR(1) process, while keeping the random walk for the level of the process. The bsts package refers to this is the “semilocal linear trend” model. \\[\\begin{align*}\n\\mu_{t+1}=&    \\mu_t+\\delta_t+\\eta_{0t}\\\\\n\\delta_{t+1}=& D+\\rho(\\delta_t-D)+\\eta_{1t}\n\\end{align*}\\] The \\(D\\) parameter is the long run slope of the trend component, to which \\(\\delta_t\\) will eventually revert. However \\(\\delta_t\\) can have short term autoregressive deviations from the long term trend, with memory determined by \\(\\rho\\). Values of \\(\\rho\\) close to 1 will lead to long deviations from \\(D\\). To see the impact this can have on long term forecasts, consider the time series of daily closing values for the S&P 500 stock market index over the last 5 years, shown below.\n\nGSPC = read.csv(\"../data/GSPC.csv\")\nGSPC = xts(GSPC, order.by = as.Date(rownames(GSPC), \"%Y-%m-%d\"))\nknitr::kable(head(GSPC))\nplot(GSPC$GSPC.Adjusted, main=\"\")\n\n\n\n\nDaily closing values for the S&P 500 stock market index\n\n\n\n\nConsider two forecasts of the daily values of this series for the next 360 days. The first assumes the local linear trend model. The second assumes the semilocal linear trend.\n\nsp500 = GSPC$GSPC.Adjusted\nss1 &lt;- AddLocalLinearTrend(list(), sp500)\nmodel1 &lt;- bsts(sp500, state.specification = ss1, niter = 1000)\nss2 &lt;- AddSemilocalLinearTrend(list(), sp500)\nmodel2 &lt;- bsts(sp500, state.specification = ss2, niter = 1000)\n\nFigure below shows long term forecasts of the S&P 500 closing values under the (left) local linear trend and (right) semilocal linear trend state models.\nload(\"../data/timeseries/model12-sp500.RData\")\npar(mar=c(4,4,0,1))\npred1 &lt;- predict(model1, horizon = 360)\npred2 &lt;- predict(model2, horizon = 360)\nplot(pred2, plot.original = 360, ylim = range(pred1))\nplot(pred1, plot.original = 360, ylim = range(pred1))\n\n\n\n\n\n\n\n\n\n\n\n(a) Semi-local trend\n\n\n\n\n\n\n\n\n\n\n\n(b) Local trend\n\n\n\n\n\n\n\nFigure 9.7: S&P 500 Prediction\n\n\n\nNot only the forecast expectations from the two models are different, but the forecast errors from the local linear trend model are implausibly wide, including a small but nonzero probability that the S&P 500 index could close near zero in the next 360 days. The error bars from the semilocal linear trend model are far more plausible, and more closely match the uncertainty observed over the life of the series thus far.\n\n\nExample 9.3 (Recession modeling using non-Gaussian data) Although we have largely skipped details about how the bsts software fits models, the Gaussian error assumptions in the observation and transition equations are important for the model fitting process. Part of that process involves running data through the Kalman filter, which assumes Gaussian errors in both the state and transition equations. In many settings where Gaussian errors are obviously inappropriate, such as for binary or small count data, one can introduce latent variables that give the model a conditionally Gaussian representation. Well known “data augmentation” methods exist for probit regression (Albert (1993)) and models with student-T errors (Rubin (2015)). Somewhat more complex methods exist for logistic regression (Frühwirth-Schnatter and Frühwirth (2007), Held and Holmes (2006), Gramacy and Polson (2012)) and Poisson regression (Frühwirth-Schnatter et al. (2008)). Additional methods exist for quantile regression (Benoit and Van den Poel (2012)), support vector machines (Polson and Scott (2011)), and multinomial logit regression (Frühwirth-Schnatter and Frühwirth (2010)). These are not currently provided by the bsts package, but they might be added in the future.\nTo see how non-Gaussian errors can be useful, consider the analysis done by Berge, Sinha, and Smolyansky (2016) who used Bayesian model averaging (BMA) to investigate which of several economic indicators would best predict the presence or absence of a recession. We will focus on their nowcasting example, which models the probability of a recession at the same time point as the predictor variables. Berge, Sinha, and Smolyansky (2016) also analyzed the data with the predictors at several lags.\nThe model used in Berge, Sinha, and Smolyansky (2016) was a probit regression, with Bayesian model averaging used to determine which predictors should be included. The response variable was the the presence or absence of a recession (as determined by NBER).\n\ndat &lt;- read.csv(\"../data/timeseries/rec_data_20160613.csv\")\nrec = ts(dat$nber, start=c(1973, 1), end=c(2016, 5), frequency=12)\nplot(rec, type='l', col='blue', ylab=\"Recession\")\n\n\n\n\nRecession periods identified by NBER\n\n\n\n\nThe BMA done by Berge, Sinha, and Smolyansky (2016) is essentially the same as fitting a logistic regression under a spike-and-slab prior with the prior inclusion probability of each predictor set to 1/2 . That analysis can be run using the BoomSpikeSlab R package (S. L. Scott (2022)), which is similar to bsts, but with only a regression component and no time series.\nThe logistic regression model is highly predictive, but it ignores serial dependence in the data. To capture serial dependence, consider the following dynamic logistic regression model with a local level trend model. \\[\\begin{align*}\n\\mathrm{logit}(p_t)= &  \\mu_t+\\beta^Tx_t\\\\\n\\mu_{t+1}= &            \\mu_t+\\eta_t\n\\end{align*}\\] Here \\(p_t\\) is the probability of a recession at time \\(t\\) ,and \\(x_t\\) is the set of economic indicators used by Berge, Sinha, and Smolyansky (2016) in their analysis. The variables are listed in the table below\n\n\n\n\n\n\n\n\nVariable\nDefinition/notes\nTransformation\n\n\n\n\nFinancial Variables\n\n\n\n\nSlope of yield curve\n10-year Treasury less 3-month yield\n\n\n\nCurvature of yield curve\n2 x 2-year minus 3-month and 10-year\n\n\n\nGZ index\nGilchrist and Zakrajsek (AER, 2012)\n\n\n\nTED spread\n3-month ED less 3-month Treasury yield\n\n\n\nBBB corporate spread\nBBB less 10-year Treasury yield\n\n\n\nS 500, 1-month return\n\n1-month log diff.\n\n\nS 500, 3-month return\n\n3-month log diff.\n\n\nTrade-weighted dollar\n\n3-month log diff.\n\n\nVIX\nCBOE and extended following Bloom\n\n\n\nMacroeconomic Indicators\n\n\n\n\nReal personal consumption expend.\n\n3-month log diff.\n\n\nReal disposable personal income\n\n3-month log diff.\n\n\nIndustrial production\n\n3-month log diff.\n\n\nHousing permits\n\n3-month log diff.\n\n\nNonfarm payroll employment\n\n3-month log diff.\n\n\nInitial claims\n4-week moving average\n3-month log diff.\n\n\nWeekly hours, manufacturing\n\n3-month log diff.\n\n\nPurchasing managers index\n\n3-month log dif\n\n\n\nFirst, we prepare the data by shifting it by \\(h\\), which is the forecast horison.\n\nh=0\n# predict h months ahead\ny.h &lt;- dat$nber[-(1:h)]\nhh &lt;- length(dat$nber) - h\ndat.h &lt;- dat[1:hh,-1]\n# h=0 is a special case\nif(h==0) y.h   &lt;- dat$nber\nif(h==0) dat.h &lt;- dat[,-1]\n\nTo fit this model, we can issue the commands shown below.\n\n# Because 'y' is 0/1 and the state is on the logit scale the default prior\n# assumed by AddLocalLevel won't work here, so we need to explicitly set the\n# priors for the variance of the state innovation errors and the initial value\n# of the state at time 0.  The 'SdPrior' and 'NormalPrior' functions used to\n# define these priors are part of the Boom package.  See R help for\n# documentation.  Note the truncated support for the standard deviation of the\n# random walk increments in the local level model.\n# A more complex model\nss &lt;- AddLocalLevel(list(),y.h,\n                    sigma.prior = SdPrior(sigma.guess = .1,\n                                          sample.size = 1,\n                                          upper.limit = 1),\n                    initial.state.prior = NormalPrior(0, 5))\n# Tell bsts that the observation equation should be a logistic regression by\n# passing the 'family = \"logit\"' argument.\nts.model &lt;- bsts(y.h ~ ., ss, data = dat.h, niter = 20000,family = \"logit\", expected.model.size = 10)\n\nNot let’s plot the results\npar(mar=c(4,4,0,0))\nplot(ts.model,\"coef\")\nplot(ts.model)\nlines(y.h, lwd=3,col=\"blue\")\n\n\n\n\n\n\n\n\n\n\n\nplot(ts.model,\"predictors\")\n\n\n\n\n\n\n\n\nNotice, the distribution of \\(p_t\\), it is moving to very large values during a recession, and to very small values outside of a recession. This effect captures the strong serial dependence in the recession data. Recessions are rare, but once they occur they tend to persist. Assuming independent time points is therefore unrealistic, and it substantially overstates the amount of information available to identify logistic regression coefficients.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "17-forecasting.html#final-remarks-on-structural-models",
    "href": "17-forecasting.html#final-remarks-on-structural-models",
    "title": "9  Forecasting",
    "section": "9.4 Final Remarks on Structural Models",
    "text": "9.4 Final Remarks on Structural Models\nThe preceding examples have shown that the bsts software package can handle several nonstandard, but useful, time series applications. These include the ability to handle large numbers of contemporaneous predictors with spike and slab priors, the presence of trend models suitable for long term forecasting, and the ability to handle non-Gaussian data. We have run out of space, but bsts can do much more.\nFor starters there are other state models you can use. Bsts has elementary support for holidays. It knows about 18 US holidays, and has capacity to add more, including holidays that occur on the same date each year, holidays that occur on a fixed weekday of a fixed month (e.g. 3rd Tuesday in February, or last Monday in November). The model for each holiday is a simple random walk, but look for future versions to have improved holiday support via Bayesian shrinkage.\nBsts offers support for multiple seasonalities. For example, if you have several weeks of hourly data then you will have an hour-of-day effect as well as a day-of-week effect. You can model these using a single seasonal effect with 168 seasons (which would allow for different hourly effects on weekends and weekdays), or you can assume additive seasonal patterns using the season.duration argument to AddSeasonal,\nss &lt;- AddSeasonal(ss, y, nseasons = 24)\nss &lt;- AddSeasonal(ss, y, nseasons = 7, season.duration = 24)\nThe latter specifies that each daily effect should remain constant for 24 hours. For modeling physical phenomena, bsts also offers trigonometric seasonal effects, which are sine and cosine waves with time varying coefficients. You obtain these by calling AddTrig. Time varying effects are available for arbitrary regressions with small numbers of predictor variables through a call to AddDynamicRegression.\nIn addition to the trend models discussed so far, the function AddStudentLocalLinearTrend gives a version of the local linear trend model that assumes student-t errors instead of Gaussian errors. This is a useful state model for short term predictions when the mean of the time series exhibits occasional dramatic jumps. Student-t errors can be introduced into the observation equation by passing the family = \"student\" argument to the bsts function call. Allowing for heavy tailed errors in the observation equation makes the model robust against individual outliers, while heavy tails in the state model provides robustness against sudden persistent shifts in level or slope. This can lead to tighter prediction limits than Gaussian models when modeling data that have been polluted by outliers. The observation equation can also be set to a Poisson model for small count data if desired.\nFinally, the most recent update to bsts supports data with multiple observations at each time stamp. The Gaussian version of the model is \\[\\begin{align*}\ny_{it} = &\\beta^T x_{it} + Z_t^T\\alpha_t + \\epsilon_{it}\\\\\n\\alpha_{t+1} = & T_t \\alpha_t + R_t \\eta_t,\n\\end{align*}\\] which is best understood as a regression model with a time varying intercept.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "17-forecasting.html#algorithms",
    "href": "17-forecasting.html#algorithms",
    "title": "9  Forecasting",
    "section": "9.5 Algorithms",
    "text": "9.5 Algorithms\nThe classic filtering and prediction algorithms for linear and Gaussian systems are described in Kalman (1960) and Kalman and Bucy (1961). Early work on discrete recursions for hidden Markov models are in Baum, Petrie, Soules and Weiss (1970) who use an EM-type algorithm, Viterbi (1967) who provides a modal state filter estimate and recursions developed in Lindgren (1978). While these can be used to evaluate the marginal likelihood for the parameters they are computationally too intensive to solve the filtering and learning, Lindgren (1978). Scott (2002) provides a review of FFBS algorithms for discrete HMMs.\nMarkov chain Monte Carlo (MCMC) algorithms for parameter learning and nonlinear non-Gaussian state space models were developed by Carlin, Polson and Stoffer (1992). For linear and Gaussian systems, Carter and Kohn (1994) provide the filter forward and backwards sample (FFBS) algorithm to draw the block of hidden states. Scott (2008) and Fruhwirth-Schnatter (2006) for a mixture of normals approximation for the multinomial logit. West and Harrison (1997) has conditionally conjugate prior where parameters can be marginalized out of the updating equations.\n\n9.5.1 Kalman Filtering\nThe Normal/ Normal Bayesian learning model provides the basis for shrinkage estimation of multiple means and the basis of the Kalman filter for dynamically tracking a path of an object.\nThe Kalman filter is arguable the most common application of Bayesian inference. The Kalman filter assumes a linear and Gaussian state-space model: \\[\ny_{t}=x_{t}+\\sigma\\varepsilon_{t}^{y}\\text{ and }x_{t}=x_{t-1}+\\sigma\n_{x}\\varepsilon_{t}^{x},\n\\] where \\(\\varepsilon_{t}^{y}\\) and \\(\\varepsilon_{t}^{x}\\) are i.i.d. standard normal and \\(\\sigma\\) and \\(\\sigma_{x}\\) are known. The observation equation posits that the observed data, \\(y_{t}\\), consists of the random-walk latent state, \\(x_{t}\\), that is polluted by noise, \\(\\sigma\\varepsilon_{t}^{y}\\). Further, \\(\\sigma_{x}/\\sigma\\) is the “signal-to-noise” ratio, measures the information content of the signal. As \\(\\sigma\\) increases relatively to \\(\\sigma_{x}\\), the observations become noisier and less informative. The model is initialized via a prior distribution over \\(x_{0}\\), which is for analytical tractability must be normally distributed, \\(x_{0}\\sim\\mathcal{N}\\left( \\mu_{0},\\sigma_{0}^{2}\\right)\\).\nThe posterior distribution solves the filtering problem and is defined recursively via Bayes rule: \\[\np\\left(  x_{t+1} \\mid y^{t+1}\\right)  =\\frac{p\\left(  y_{t+1} \\mid x_{t+1}\\right)\np\\left(  x_{t+1} \\mid y^{t}\\right)  }{p\\left(  y_{t+1} \\mid y^{t}\\right)  }\\propto\np\\left(  y_{t+1} \\mid x_{t+1}\\right)  p\\left(  x_{t+1} \\mid y^{t}\\right)  \\text{.}%\n\\] and the likelihood function, \\(p\\left( y_{t+1} \\mid x_{t+1}\\right)\\). The predictive distribution summarizes all of the information about \\(x_{t+1}\\) based on lagged observations. The likelihood function summarizes the new information in \\(y_{t+1}\\) about \\(x_{t+1}\\).\nThe Kalman filter relies on an inductive argument: assume that \\(p\\left( x_{t} \\mid y^{t}\\right) \\sim\\mathcal{N}\\left( \\mu_{t},\\sigma_{t}^{2}\\right)\\) and then verify that \\(p\\left( x_{t+1} \\mid y^{t+1}\\right) \\sim\\mathcal{N}\\left( \\mu_{t+1},\\sigma_{t+1}^{2}\\right)\\) with analytical expressions for the hyperparameters. To verify, note that since \\(p\\left(x_{t} \\mid y^{t}\\right) \\sim\\mathcal{N}\\left( \\mu_{t},\\sigma_{t}^{2}\\right)\\), \\(x_{t}=\\mu_{t}+\\sigma_{t}\\eta_{t}\\) for some standard normal \\(\\eta_{t}\\). Substituting into the state evolution, the predictive is \\(x_{t+1} \\mid y^{t}\\sim\\mathcal{N}\\left( \\mu_{t},\\sigma_{t}^{2}+\\sigma_{x}^{2}\\right)\\). Since \\(p\\left( y_{t+1} \\mid x_{t+1}\\right) \\sim\\mathcal{N}\\left(x_{t+1},\\sigma^{2}\\right)\\), the posterior is \\[\\begin{align*}\np\\left(  x_{t+1} \\mid y^{t+1}\\right)   &  \\propto p\\left(  y_{t+1} \\mid x_{t+1}\\right)\np\\left(  x_{t+1} \\mid y^{t}\\right)  \\propto\\exp\\left[  -\\frac{1}{2}\\left( \\frac{\\left(  y_{t+1}-x_{t+1}\\right)  ^{2}}{\\sigma^{2}}+\\frac{\\left( x_{t+1}-\\mu_{t}\\right)  ^{2}}{\\sigma_{t}^{2}+\\sigma_{x}^{2}}\\right)  \\right]\n\\\\\n&  \\propto\\exp\\left(  -\\frac{1}{2}\\frac{\\left(  x_{t+1}-\\mu_{t+1}\\right)\n^{2}}{\\sigma_{t+1}^{2}}\\right)\n\\end{align*}\\] where \\(\\mu_{t+1}\\) and \\(\\sigma_{t+1}^{2}\\) are computed by completing the square: \\[\\begin{equation}\n\\frac{\\mu_{t+1}}{\\sigma_{t+1}^{2}}=\\frac{y_{t+1}}{\\sigma^{2}}+\\frac{\\mu_{t}%\n}{\\sigma_{t}^{2}+\\sigma_{x}^{2}}\\text{ and }\\frac{1}{\\sigma_{t+1}^{2}}%\n=\\frac{1}{\\sigma^{2}}+\\frac{1}{\\sigma_{t}^{2}+\\sigma_{x}^{2}}\\text{.}\\nonumber\n\\end{equation}\\] Here, inference on \\(x_{t}\\) is merely running the Kalman filter, that is, sequential computing \\(\\mu_{t}\\) and \\(\\sigma_{t}^{2}\\), which are state sufficient statistics.\nThe Kalman filter provides an excellent example of the mechanics of Bayesian inference: given a prior and likelihood, compute the posterior distribution. In this setting, it is hard to imagine an more intuitive or alternative approach. The same approach applied to learning fixed static parameters. In this case, \\(y_{t}=\\mu+\\sigma\\varepsilon_{t},\\) where \\(\\mu\\sim\\mathcal{N}\\left( \\mu_{0},\\sigma_{0}^{2}\\right)\\) is the initial distribution. Using the same arguments as above, it is easy to show that \\(p\\left(\\mu \\mid y^{t+1}\\right) \\sim\\mathcal{N}\\left( \\mu_{t+1},\\sigma_{t+1}^{2}\\right)\\), where \\[\\begin{align*}\n\\frac{\\mu_{t+1}}{\\sigma_{t+1}^{2}}  &  =\\left(  \\frac{y_{t+1}}{\\sigma^{2}}+\\frac{\\mu_{t}}{\\sigma_{t}^{2}}\\right)  =\\frac{\\left(  t+1\\right) \\overline{y}_{t+1}}{\\sigma^{2}}+\\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\text{,}\\\\\n\\frac{1}{\\sigma_{t+1}^{2}}  &  =\\frac{1}{\\sigma^{2}}+\\frac{1}{\\sigma_{t}^{2}}=\\frac{\\left(  t+1\\right)  }{\\sigma^{2}}+\\frac{1}{\\sigma_{0}^{2}}\\text{,}\n\\end{align*}\\] and \\(\\overline{y}_{t}=t^{-1}\\sum_{t=1}^{t}y_{t}\\).\nNow, given this example, the same statements can be posed as in the state variable learning problem: it is hard to think of a more intuitive or alternative approach for sequential learning. In this case, researchers often have different feelings about assuming a prior distribution over the state variable and a parameter. In the state filtering problem, it is difficult to separate the prior distribution and the likelihood. In fact, one could view the initial distribution over \\(x_{0}\\), the linear evolution for the state variable, and the Gaussian errors as the “prior” distribution.\nNow consider,linear multivariate Gaussian state space model: \\[\\begin{align*}\ny_{t} &  =F_{t}x_{t}+\\varepsilon_{t} \\; \\; \\text{where} \\; \\; \\varepsilon_{t}\\sim\\mathcal{N}\\left( 0,\\Sigma_{t}\\right) \\\\\nx_{t} &  =G_{t}x_{t-1}+\\varepsilon_{t}^{x} \\; \\; \\text{where} \\; \\; \\varepsilon_{t}^x\\sim\\mathcal{N}\\left( 0,\\Sigma_{t}^x\\right)\n\\end{align*}\\] where we allow for heteroscedascity in the error variance-covariance matrices. We complete the model specification with a normal prior on the initial starting condition \\(x_{0}\\sim\\mathcal{N}\\left(  \\mu_{0},\\Sigma_{0}\\right)\\). It is important to recognize that \\(\\varepsilon_{t}\\) and \\(\\varepsilon_{t}^{x}\\) need only be conditionally normal. There are a number of distributions of interest \\[\\begin{align*}\n\\text{Filtering}  &  :p\\left(  x_{t}|y^{t}\\right)  \\text{ }t=1,...,T\\\\\n\\text{Forecasting}  &  :p\\left(  x_{t+1}|y^{t}\\right)  \\text{ } t=1,...,T\\\\\n\\text{Smoothing}  &  :p\\left(  x_{t}|y^{t+1}\\right)  \\text{ }t=1,...,T \\\\\n\\text{Prediction}  &  :p\\left(  y_{t+1}|y^{t}\\right)  \\text{ }t=1,...,T\n\\end{align*}\\] For known parameters with linearity and Gaussianity we have the following Kalman filter recursions for calculation these distributions.\nThe fundamental filtering relationship is based on the fact that the filtering distribution is of the form \\[\np(x_{t}|y^{t})\\sim\\mathcal{N}\\left(  \\mu_{t|t},\\Sigma_{t|t}\\right)\n\\;\\;\\mathrm{and}\\;\\;p(x_{t+1}|y^{t+1})\\sim\\mathcal{N}\\left(  \\mu\n_{t+1|t+1},\\Sigma_{t+1|t+1}\\right)\n\\] where \\((\\mu_{t+1|t+1},\\Sigma_{t+1|t+1})\\) are related to \\((\\mu_{t|t}%\n,\\Sigma_{t|t})\\) via the Kalman filter recursions. In the following, it is sometimes useful to write this as \\[\np(x_{t}|y^{t})\\sim\\mathcal{N}\\left(  \\mu_{t|t},\\Sigma_{t|t}\\right) \\; \\Rightarrow \\; x_{t}=\\mu_{t|t}+\\Sigma_{t|t}^{\\frac{1}{2}}\\widehat{\\varepsilon}_{t}%\n\\] where \\(\\widehat{\\varepsilon}_{t} \\sim \\mathcal{N}(0,1)\\). Before we derive the filtering recursions and characterize the state filtering distribution we first find the forecasting distribution. The predictive or forecast distribution is defined as follows.\nPredictive Distribution, \\(p( x_{t+1} | y^{t} )\\).\nThe key distributions in Bayes rule are the predictive and the conditional state posterior given by \\[\np( x_{t+1}|y^{t} )\\sim\\mathcal{N}\\left(  \\mu_{t+1|t},\\Sigma_{t+1|t}\\right)\n\\] To compute the predictive or forecasting distribution note that:\n\\[\np\\left(  x_{t+1}|y^{t}\\right)  =p\\left(  G_{t+1}x_{t}+\\varepsilon_{t+1}%\n^{x}|y^{t}\\right)  \\sim\\mathcal{N}\\left(  \\mu_{t+1|t},\\Sigma_{t+1|t} \\right)\n\\] where the predictive moments are\n\\[\\begin{align*}\n\\mu_{t+1|t}  &  =G_{t+1}\\mu_{t}\\\\\n\\Sigma_{t+1|t}  &  =G_{t+1}\\Sigma_{t}G_{t+1}^{\\prime}+\\Sigma_{t+1}^{x}.\n\\end{align*}\\] We now state and derive the main Kalman filtering recursions for linear Gaussian models with known parameters.\nFiltering Distribution The classic Kalman filter characterisation of the state filtering distributionp(x_{t+1}|y^{t+1})$ and moment recursions are given by \\[\np(x_{t+1}|y^{t+1})\\sim\\mathcal{N}\\left(  \\mu_{t+1|t+1},\\Sigma_{t+1|t+1}\\right)\n\\] The updated posterior means and variances are defined by \\[\\begin{align*}\n\\mu_{t+1|t+1} &  =\\mu_{t+1|t}+K_{t+1}e_{t+1}\\\\\n\\Sigma_{t+1|t+1} &  =(I-K_{t+1}F_{t+1})\\Sigma_{t+1|t}%\n\\end{align*}\\] where the Kalman gain \\(K_{t+1}\\) matrix and innovations vector \\(e_{t+1}\\) are \\[\\begin{align*}\nK_{t+1}  &  = \\Sigma_{t+1|t} F_{t+1}^{\\prime}\\left(  F_{t+1} \\Sigma_{t+1|t}\nF_{t+1}^{\\prime}+ \\Sigma_{t+1} \\right)  ^{-1}\\\\\ne_{t+1}  &  = y_{t+1} - F_{t+1} \\mu_{t+1|t}%\n\\end{align*}\\]\nTo prove this result we use the predictive distribution and an application of Bayes rule which implies that\n\\[\\begin{align*}\np\\left(  x_{t+1}|y^{t+1}\\right)   & = p\\left(  x_{t+1}|y_{t+1}%\n,y^{t}\\right) \\\\\n&  = \\frac{ p\\left(  y_{t+1}|x_{t+1}\\right)  p\\left(  x_{t+1}|y^{t}\\right)}{  p\\left(  y_{t+1}|y^t\\right) }\n\\text{.}%\n\\end{align*}\\] Under the normality assumption, the likelihood term is \\[\np( y_{t+1} | x_{t+1} ) = ( 2 \\pi )^{-\\frac{p}{2}} | \\Sigma_{t+1} |^{-\\frac{1}{2}}\n  \\exp \\left ( - \\frac{1}{2} ( y_{t+1} - F_{t+1} x_{t+1} )^{\\prime}\\Sigma_{t+1}^{-1} ( y_{t+1} - F_{t+1}\nx_{t+1} ) \\right )\n\\] Combining with the exponent term from the state predicitive distribution, then gives an exponent for the filtering distribution of the form \\[\n( y_{t+1} - F_{t+1} x_{t+1} )^{\\prime}\\Sigma_{t+1}^{-1} ( y_{t+1} - F_{t+1}\nx_{t+1} ) +( x_{t+1} - \\mu_{t+1|t} )^{\\prime}\\Sigma_{t+1|t}^{-1} ( x_{t+1} -\n\\mu_{t+1|t} )\n\\] Now we define the de-meaned state and innovations vectors,\n\\[\n\\tilde{x}_{t+1} = x_{t+1} - \\mu_{t+1|t} \\; \\text{and} \\; e_{t+1} = y_{t+1} - F_{t+1} \\mu_{t+1|t}\n\\] Using the usual completing the square trick we can re-write the exponent as \\[\n( e_{t+1} - F_{t+1} \\tilde{x}_{t+1} )^{\\prime}\\Sigma_{t+1}^{-1} ( e_{t+1} -\nF_{t+1} \\tilde{x}_{t+1} ) + \\tilde{x}_{t+1}^{\\prime}\\Sigma_{t+1|t}^{-1}\n\\tilde{x}_{t+1}\n\\] The sums of squares can be decomposed further as \\[\n\\tilde{x}_{t+1}^{\\prime}\\left(  F_{t+1}^{\\prime}\\Sigma_{t+1}^{\\prime}+\n\\Sigma_{t+1|t}^{-1} \\right)  \\tilde{x}_{t+1} + 2 \\tilde{x}_{t+1}^{\\prime\n}\\left(  F_{t+1}^{\\prime}\\Sigma_{t+1} e_{t+1} \\right)  + e_{t+1}^{\\prime\n}\\Sigma_{t+1}^{-1} e_{t+1}\n\\] The exponent is then a quadratic form implying that the vector \\(\\tilde{x}_{t+1}^{\\prime}\\) is normal distributed with the appropriate mean and variance-covariance matrix. The definitions are given by \\[\n\\Sigma_{t+1|t+1} F_{t+1}^{\\prime}\\Sigma_{t+1} e_{t+1} \\; \\; \\mathrm{and} \\; \\;\n\\Sigma_{t+1|t+1} = \\left(  F_{t+1}^{\\prime}\\Sigma_{t+1}^{-1} F_{t+1} +\n\\Sigma_{t+1|t}^{-1} \\right)  ^{-1}\n\\] respectively. Hence, we obtain the identity \\[\n\\Sigma_{t+1|t+1} = \\left(  F_{t+1}^{\\prime}\\Sigma_{t+1}^{\\prime}F_{t+1} +\n\\Sigma_{t+1|t}^{-1} \\right)  ^{-1} = ( I - K_{t+1} F_{t+1} ) \\Sigma_{t+1|t}\n\\] where \\(K_{t+1} = \\Sigma_{t+1|t} F_{t+1}^{\\prime}\\left(  F_{t+1} \\Sigma_{t+1|t}^{-1} F_{t+1}^{\\prime}+ \\Sigma_{t+1} \\right)^{-1}\\) is the Kalman gain matrix.\nThe mean of the \\(\\tilde{x}_{t+1}^{\\prime} = x_{t+1} - \\mu_{t+1|t}\\) distribution is then \\(K_{t+1}e_{t+1}\\). Un de-meaning the vector, we have \\(x_{t+1} = \\tilde{x}_{t+1} + \\mu_{t+1|t} =K_{t+1}e_{t+1}\\) leads to the following distributional result \\[\np( x_{t+1}|y^{t+1}) \\sim\\mathcal{N}\\left(  \\mu_{t+1|t+1},\\Sigma_{t+1|t+1}%\n\\right)  ,\n\\] The moments for the next filtering distribution are given by the classic recursions \\[\\begin{align*}\n\\mu_{t+1|t+1}  &  =\\mu_{t+1|t}+K_{t+1}\\left(  y_{t+1}-F_{t+1}\\mu_{t+1|t}\n\\right) \\\\\n\\Sigma_{t+1|t+1}  &  =\\left(  I-K_{t+1}F_{t+1}\\right)  \\Sigma_{t+1|t}\\text{.}%\n\\end{align*}\\] There are two other distributions to compute: the data predictive \\(p( y_{t+1} | y^t )\\) and the state smoothing distribution \\(p( x_t | y^{t+1} )\\). These are derived as follows.\nThe data predictive \\(p(y_{t+1}|y^t)\\) is determined from the observation equation and the state predictive distribution as follows \\[\\begin{align*}\ny_{t+1}  & =F_{t+1}x_{t+1}+\\varepsilon_{t+1} \\; \\text{with} \\;   \\varepsilon_{t+1}\\sim\\mathcal{N}\\left( 0,\\Sigma_{t+1}\\right) \\\\\np( x_{t+1} | y^t ) & \\sim \\mathcal{N} \\left ( \\mu_{t+1|t} , \\Sigma_{t+1|t} \\right )  \n\\end{align*}\\] Then substituting we have a predictive distribution for the next observation of the form \\[\np( y_{t+1}|y^{t}) \\sim\\mathcal{N}\\left(  F_{t+1}\\mu_{t+1|t},F_{t+1}%\n\\Sigma_{t+1|t} F_{t+1}+\\Sigma_{t+1} \\right)  \\text{.}%\n\\] The state smoothing distribution \\(p(x_t | y^{t+1} )\\) is determined from the joint distribution, \\(p( x_{t} , x_{t+1} | y^{t} )\\) as follows. First, factorise this joint distribution as \\[\np( x_{t+1} , x_{t} | y^{t} ) = p( x_{t+1} | x_{t} ) p( x_{t} | y^{t} )\n\\] Then calculate the conditional posterior distribution, \\(p( x_{t+1} | x_{t} , y^{t+1} )\\) by Bayes rule as \\[\np(x_{t+1}|x_{t},y^{t+1}) = \\frac{p\\left(  y_{t+1}|x_{t+1}\\right)\np(x_{t+1}|x_{t})p(x_{t}|y^{t})}{p(x_{t+1}|y^{t})}%\n\\] Now, we can view the system as having two observations on \\(x_{t+1}\\), namely \\[\\begin{align*}\nx_{t+1} &  =F_{t+1}x_{t}+\\Sigma_{t+1}^{x}\\epsilon_{t+1}^{x}\\\\\nx_{t} &  =\\mu_{t|t}+\\Sigma_{t|t}^{\\frac{1}{2}}\\hat{\\epsilon}_{t}%\n\\end{align*}\\] where the errors \\(\\epsilon_{t+1}^{x},\\hat{\\epsilon}_{t}\\) are independent.\nThis leads to a joint posterior with an exponent that is proportional to \\[\n(x_{t+1}-G_{t+1}x_{t})^{\\prime}\\left(  \\Sigma\n_{t+1}^{x}\\right)  ^{-1}(x_{t+1}-G_{t+1}x_{t})- (x_{t}-\\mu\n_{t|t})^{\\prime}\\Sigma_{t|t}^{-1}(x_{t}-\\mu_{t|t})\n\\] The first term comes from the state evolution and the second from the current filtering posterior. Completing the square gives \\[\n(x_{t+1}-G_{t+1}x_{t})^{\\prime}\\left(  \\Sigma_{t+1}^{x}\\right)  ^{-1}%\n(x_{t+1}-G_{t+1}x_{t})+(x_{t}-\\mu_{t|t})^{\\prime}\\Sigma_{t|t}^{-1}(x_{t}%\n-\\mu_{t|t})\n\\] \\[\n=(x_{t+1}-\\mu_{t+1|t})^{\\prime}\\Sigma_{t|t}^{-1}(x_{t+1}-\\mu_{t+1|t}%\n)+(x_{t}-\\mu_{t|t+1})^{\\prime}\\Sigma_{t|t+1}^{-1}(x_{t}-\\mu_{t|t+1})\n\\] which leads to the smoothed state moments \\[\\begin{align*}\n\\mu_{t|t+1} &  =\\Sigma_{t|t+1}\\left(  \\Sigma_{t|t}\\mu_{t|t}+F_{t+1}^{\\prime\n}\\left(  \\Sigma_{t+1}^{x}\\right)  ^{-1}x_{t+1}\\right)  \\\\\n\\Sigma_{t|t+1} &  =F_{t+1}^{\\prime}\\left(  \\Sigma_{t+1}^{x}\\right)\n^{-1}F_{t+1}+\\Sigma_{t|t}^{-1}%\n\\end{align*}\\] The Kalman filter recursions then follow by induction.\n\nExample 9.4 (Kalman Filter for Robot Localization) The Kalman filter is a powerful tool for estimating the state of a system, given noisy observations. It is used in a wide range of applications, from tracking the position of a robot to estimating the state of a financial market. The Kalman filter is particularly useful when the state of the system is not directly observable, and must be inferred from noisy measurements.\nOften KF is used for localization problem: given noisy measurements about the position of a robot and the motion model of the robot, the Kalman filter can estimate the true position of the robot. The Kalman filter is a recursive algorithm that estimates the state of a system at each time step, based on the state estimate from the previous time step and a new observation. We will use the language of state-space models in this example and will use the notation \\(x_t\\) to denote the state of the system at time \\(t\\) (parameter we are trying to estimate), and \\(y_t\\) to denote the observation at time \\(t\\) (observed data). The state-space model is given by \\[\n\\begin{aligned}\nx_{t+1} & = A x_t + w,\\quad w \\sim N(0,Q)\\\\\ny_t &=G x_t + \\nu, \\quad \\nu \\sim N(0,R)\\\\\nx_0 & \\sim N(\\hat{x}_0, \\Sigma_0),\n\\end{aligned}\n\\] where \\(A\\) is the state transition matrix, \\(G\\) is the observation matrix, \\(w\\) is the process noise, and \\(\\nu\\) is the observation noise. The process noise and observation noise are assumed to be independent and normally distributed with zero mean and covariance matrices \\(Q\\) and \\(R\\), respectively. The initial state \\(x_0\\) is assumed to be normally distributed with mean \\(\\hat{x}_0\\) and covariance matrix \\(\\Sigma_0\\). The Kalman filter provides a recursive algorithm for estimating the state of the system at each time step, based on the state estimate from the previous time step and a new observation. The state estimate is normal with mean \\(\\hat{x}_t\\) and the covariance matrix \\(\\Sigma_t\\). The Kalman filter equations are given by \\[\n\\begin{aligned}\n\\hat{x}_{t+1} &= A \\hat{x}_t + K_{t} (y_t - G \\hat{x}_t) \\\\\nK_{t} &= A \\Sigma_t G^T (G \\Sigma_t G^T + R)^{-1}\\\\\n\\Sigma_{t+1} &= A \\Sigma_t A^T - K_{t} G \\Sigma_t A^T + Q\n\\end{aligned}\n\\] Kalman filter performs a multivariate normal-normal update using \\(N(A \\hat{x}_t,,A \\Sigma_t A^T)\\) as prior and \\(N(y_t, G \\Sigma_t G^T + R)\\) as likelihood. The posterior distribution is \\(N(\\hat{x}_{t+1}, \\Sigma_{t+1})\\). Matrix \\(K_{t}\\) is called the Kalman gain and provides a weight on the residual between observed and prior \\(y_t - G \\hat{x}_t\\) in the update.\nAssume our robot starts at \\(\\hat x_0 = (0.2,-0.2)\\) (x-y Cartesian coordinates) and initial covariance is \\[\n\\Sigma_0 = \\begin{bmatrix} 0.4 & 0.3 \\\\ 0.3 & 0.45 \\end{bmatrix}.\n\\] The prior distribution of the robot’s position can be visualized in R with a contour plot.\n\nlibrary(mnormt)\nxhat &lt;- c(0.2, -0.2)\nSigma &lt;- matrix(c(0.4, 0.3, \n                  0.3, 0.45), ncol=2)\nx1 &lt;- seq(-2, 4,length=151)\nx2 &lt;- seq(-4, 2,length=151)\nf &lt;- function(x1,x2, mean=xhat, varcov=Sigma) \n  dmnorm(cbind(x1,x2), mean,varcov)\nz &lt;- outer(x1,x2, f)\nmycols &lt;- topo.colors(100,0.5)\nimage(x1,x2,z, col=mycols, main=\"Prior density\",\n      xlab=expression('x'[1]), ylab=expression('x'[2]))\ncontour(x1,x2,z, add=TRUE)\npoints(0.2, -0.2, pch=19)\ntext(0.1, -0.2, labels = expression(hat(x)), adj = 1)\n\n\n\n\n\n\n\n\nNow I get readings from GPS \\(y_0 = (2.3, -1.9)\\) and I know from the manufacturer that the GPS has a covariance matrix of \\(R = 0.5\\Sigma_0\\). We assume the measurement matrix \\(G\\) to be identity matrix, thus \\[\ny_t = Gx_t + \\nu_t = x_t + \\nu, \\quad \\nu \\sim N(0, R).\n\\]\n\nR &lt;- 0.5 * Sigma\nz2 &lt;- outer(x1,x2, f, mean=c(2.3, -1.9), varcov=R)\nimage(x1, x2, z2, col=mycols, main=\"Sensor density\")\ncontour(x1, x2, z2, add=TRUE)\npoints(2.3, -1.9, pch=19)\ntext(2.2, -1.9, labels = \"y\", adj = 1)\ncontour(x1, x2,z, add=TRUE)\npoints(0.2, -0.2, pch=19)\ntext(0.1, -0.2, labels = expression(hat(x)), adj = 1)\n\n\n\n\n\n\n\n\nNow we combine our initial guess about the location \\(x_0\\) with the measure noisy location data \\(y_0\\) to obtain posterior distribution of the location of the robot \\(p(x\\mid \\hat x_0, \\Sigma,R) = N(x\\mid \\hat x_f, \\Sigma_f)\\) \\[\n\\begin{aligned}\n\\hat{x}_f & = (\\Sigma^{-1} + R^{-1})^{-1} (\\Sigma^{-1} \\hat{x} + R^{-1} y) \\\\\n\\Sigma_f & = (\\Sigma^{-1} + R^{-1})^{-1}\n\\end{aligned}\n\\] Using the matrix inversion identity \\[\n\\begin{aligned}\n(A^{-1} + B^{-1})^{-1} & = A - A (A + B)^{-1}A = A (A + B)^{-1} B\n\\end{aligned}\n\\] I can write the above as: \\[\n\\begin{aligned}\n\\hat{x}_f & = (\\Sigma - \\Sigma (\\Sigma + R)^{-1}\\Sigma)(\\Sigma^{-1} \\hat{x} + R^{-1}y)\\\\\n& =\\hat{x} - \\Sigma (\\Sigma + R)^{-1} \\hat{x} + \\Sigma R^{-1} y -\\Sigma (\\Sigma + R)^{-1}\\Sigma R^{-1}y\\\\\n& = \\hat{x} + \\Sigma (\\Sigma + R)^{-1} (y - \\hat{x})\\\\\n& = (1.667, -1.333)\\\\\n\\Sigma_f &= \\Sigma - \\Sigma (\\Sigma + R)^{-1} \\Sigma\\\\\n&=\\left[\\begin{array}{lll}\n0.133 & 0.10\\\\\n0.100 & 0.15\n\\end{array}\n\\right]\n\\end{aligned}\n\\] In the more general case when \\(G\\) is not the identity matrix I have \\[\n\\begin{aligned}\n\\hat{x}_f & = \\hat{x} + \\Sigma G^T (G \\Sigma G^T + R)^{-1} (y - G \\hat{x})\\\\\n\\Sigma_f &= \\Sigma - \\Sigma G^T (G \\Sigma G^T + R)^{-1} G \\Sigma\n\\end{aligned}\n\\]\n\nG &lt;- diag(2)\ny &lt;- c(2.4, -1.9)\nxhatf &lt;- xhat + Sigma %*% t(G) %*% solve(G %*% Sigma %*% t(G) + R) %*% (y - G %*% xhat)\nSigmaf &lt;- Sigma - Sigma %*% t(G) %*% solve(G %*% Sigma %*% t(G) + R) %*% G %*% Sigma\nz3 &lt;- outer(x1, x2, f, mean=c(xhatf), varcov=Sigmaf)\nimage(x1, x2, z3, col=mycols,\n      xlab=expression('x'[1]), ylab=expression('x'[2]),\n      main=\"Filtered density\")\ncontour(x1, x2, z3, add=TRUE)\npoints(xhatf[1], xhatf[2], pch=19)\ntext(xhatf[1]-0.1, xhatf[2],\n     labels = expression(hat(x)[f]), adj = 1)\nlb &lt;- adjustcolor(\"black\", alpha=0.5)\ncontour(x1, x2, z, add=TRUE, col=lb)\npoints(0.2, -0.2, pch=19, col=lb)\ntext(0.1, -0.2, labels = expression(hat(x)), adj = 1, col=lb)\ncontour(x1, x2, z2, add=TRUE, col=lb)\npoints(2.3, -1.9, pch=19, col=lb)\ntext(2.2, -1.9,labels = \"y\", adj = 1, col=lb)\n\n\n\n\n\n\n\n\nNow I assume my robot moves according to the following model \\[\nx_t = A x_{t-1} + w_t, \\quad w_t \\sim N(0, Q)\n\\] with \\[\n\\begin{split}\nA = \\left( \\begin{array}{cc}\n1.2 & 0.0 \\\\\n0.0 & -0.2\n\\end{array} \\right),\n\\qquad\nQ = 0.3 \\Sigma\n\\end{split}\n\\] Then the next location is normally distributed with the parameters \\[\n\\begin{split}\nA= \\left(\\begin{array}{cc}\n1.2 & 0.0 \\\\\n0.0 & -0.2\n\\end{array}\\right),\n\\qquad\nQ = 0.3 \\Sigma\n\\end{split}\n\\] Here \\(K = A \\Sigma G^T (G \\Sigma G^T + R)^{-1}\\) is so-called Kalman gain matrix.\n\nA &lt;- matrix(c(1.2, 0,\n              0, -0.2), ncol=2)\nQ &lt;- 0.3 * Sigma\nK &lt;- A %*% Sigma %*% t(G) %*% solve(G%*% Sigma %*% t(G) + R)\nxhatnew &lt;- A %*% xhat + K %*% (y - G %*% xhat)\nSigmanew &lt;- A %*% Sigma %*% t(A) - K %*% G %*% Sigma %*% t(A) + Q\nz4 &lt;- outer(x1,x2, f, mean=c(xhatnew), varcov=Sigmanew)\nimage(x1, x2, z4, col=mycols,\n      xlab=expression('x'[1]), ylab=expression('x'[2]),\n      main=\"Predictive density\")\ncontour(x1, x2, z4, add=TRUE)\npoints(xhatnew[1], xhatnew[2], pch=19)\ntext(xhatnew[1]-0.1, xhatnew[2],\n     labels = expression(hat(x)[new]), adj = 1)\ncontour(x1, x2, z3, add=TRUE, col=lb)\npoints(xhatf[1], xhatf[2], pch=19, col=lb)\ntext(xhatf[1]-0.1, xhatf[2], col=lb, \n     labels = expression(hat(x)[f]), adj = 1)\ncontour(x1, x2, z, add=TRUE, col=lb)\npoints(0.2, -0.2, pch=19, col=lb)\ntext(0.1, -0.2, labels = expression(hat(x)), adj = 1, col=lb)\ncontour(x1, x2, z2, add=TRUE, col=lb)\npoints(2.3, -1.9, pch=19, col=lb)\ntext(2.2, -1.9,labels = \"y\", adj = 1, col=lb)\n\n\n\n\n\n\n\n\n\nForward filtering and Backwards Sampling\nThe Kalman filtering recursions lead to a fully recursive algorithm for characterizing \\(p( x| y)\\) known as FFBS (Forward filtering and Backwards Sampling). This provides the counterpart to the Baum-Welch algorithm developed earlier for HMMs. The details are as follows. The first step is to factorize the joint posterior distribution of the states via \\[\\begin{align*}\np\\left(  x|y^{T}\\right)   &  =p\\left(  x_{T}|y^{T}\\right)  \\prod_{t=1}%\n^{T-1}p\\left(  x_{t}|x_{t+1},...,x_{T},y^{T}\\right)  \\\\\n&  =p\\left(  x_{T}|y^{T}\\right)  \\prod_{t=1}^{T-1}p\\left(  x_{t}|x_{t+1}%\n,y^{t}\\right) \\label{ffbs-1}\n\\end{align*}\\] where we have used the fact that \\(p\\left(  x_{t}|x_{t+1},...,x_{T},y^{T}\\right)  =p\\left(  x_{t}|x_{t+1},y^{T}\\right)\\) by the Markov property (conditional on \\(x_{t+1}\\), \\(x_{t}\\) is independent of all \\(x_{t+2}\\), etc.).\nThis forms the FFBS algorithm: forward-filtering, backward sampling algorithm for generating a block sample from \\(p\\left(  x|y^{T}\\right)\\). Filter forward using the Kalman recursions and obtain a sample from \\(p( x_T|y^T)\\) and then backwards sample using \\(p\\left(x_{t}|x_{t+1},y^{t}\\right)\\) to generate a block draw of \\(x\\). In what follows, we will often write \\[\np\\left(  x|y^{T}\\right)  \\sim FFBS\n\\] to denote that the FFBS algorithm can be used to generate a block draw.\nBackwards Sampling.\nThe distribution of the final state given the data history \\(p( x_{T} | y^{T} )\\) is given by the Kalman filter \\[\np( x_{T}|y^{T}) \\sim\\mathcal{N}\\left(  \\mu_{T},\\Sigma_{T}\\right)\n\\] where \\(( \\mu_{T} , \\Sigma_{T} )\\) are computed via the Kalman filter recursions. The second distribution comes from the factorization of \\(p( x_{t}, x_{t+1} | y^{t} )\\) in the derivation of the Kalman filtering recursions. Hence, the conditional state filtering distribution given \\(( x_{t+1} , y^t )\\) is \\[\np( x_{t} | x_{t+1} , y^{t} ) \\sim\\mathcal{N}\\left(  \\mu_{t|t+1} ,\n\\Sigma_{t|t+1} \\right)\n\\] where the one-step back smoothed moments are \\[\\begin{align*}\n\\mu_{t|t+1}  &  = \\Sigma_{t|t+1} \\left(  \\Sigma_{t|t} \\mu_{t|t} +\nF_{t+1}^{\\prime}\\left(  \\Sigma^{x}_{t+1} \\right)  ^{-1} x_{t+1} \\right) \\\\\n\\Sigma_{t|t+1}  &  = F_{t+1}^{\\prime}\\left(  \\Sigma^{x}_{t+1} \\right)  ^{-1}\nF_{t+1} + \\Sigma_{t|t}^{-1}%\n\\end{align*}\\] as computed above. Then we can sequentially sample from this distribution.\n\n\n9.5.2 HMM: Hidden Markov Models\nThe algorithms described in this section were originally developed by Baum and Welch Baum et al. (1970) and Viterbi (1967). Baum developed original trading algorithms for Renissance Technology which later became a multi-billion dollar hedge fund. The algorithms are now widely used in many applications, including speech recognition, bioinformatics, and finance. The algorithms are also known as the Baum-Welch and Viterbi algorithms, respectively. The algorithms are used to estimate the parameters of a Hidden Markov Model (HMM) from a sequence of observations. The HMM is a statistical model that describes a system that is assumed to be a Markov process with unobserved (hidden) states. The HMM is widely used in many applications, including speech recognition, bioinformatics, and finance. The algorithms are also known as the Baum-Welch and Viterbi algorithms, respectively. The algorithms are used to estimate the parameters of a Hidden Markov Model (HMM) from a sequence of observations. The HMM is a statistical model that describes a system that is assumed to be a Markov process with unobserved (hidden) states.\nViterbi on the other hand was one of the founders of the what is now known as Qualcomm, a company that is now a multi-billion dollar semiconductor and telecommunications equipment company. Viterbi’s algorithm is used to find the most likely sequence of hidden states in a Hidden Markov Model (HMM) given a sequence of observations. The algorithm is widely used in many applications, including speech recognition, bioinformatics, and finance. The algorithm is also known as the Viterbi algorithm.\nBaum-Welch (1970) and Viterbi (1967) the two famous discrete HMM algorithms.\nConsider a model with a Hidden Chain or regime-switching variable\n\\[\ny_{t} = \\mu\\left( x_{t}\\right) + \\sigma\\left( x_{t}\\right) \\varepsilon _{t}\\text{.}\n\\]\nSuppose that \\(x_{t}\\) is a finite state Markov chain with a time-homogeneous transition matrix P with entries \\(\\left\\{ p_{ij}\\right\\}\\) which are given by\n\\[\np_{ij} = P\\left( x_{t} = i | x_{t-1} = j, \\theta\\right) \\text{.}\n\\]\nWe define the marginal filtering and smoothing distributions\n\\[\np_{i}^{t,t} = P\\left( x_{t} = i | \\theta, y^{t}\\right) \\text{ and } p_{i}^{t,T} = P\\left( x_{t} = i | \\theta, y^{T}\\right)\n\\]\nand the corresponding joint filtering and smoothing matrices:\n\\[\np_{ij}^{t,t} = P\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{t}\\right) \\text{ and } p_{ij}^{t,T} = P\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{T}\\right) \\text{.}\n\\]\nThe key to the algorithm is that we are just going to track the joint matrices, and then peel-off marginals from the rows and columns.\n\nForward-filtering\nTo derive the forward equations\n\\[\n\\begin{aligned}\np_{ij}^{t,t} & = P\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{t}\\right) \\propto p\\left( y_{t}, x_{t-1} = i, x_{t} = j | \\theta, y^{t-1}\\right) \\\\\n& \\propto p\\left( y_{t} | x_{t} = j, \\theta\\right) p\\left( x_{t} = j | x_{t-1} = i, \\theta\\right) p\\left( x_{t-1} = i | \\theta, y^{t-1}\\right) \\\\\n& \\propto p\\left( y_{t} | x_{t} = j, \\theta\\right) p_{ij}\\left( p_{i}^{t-1,t-1}\\right) \\text{.}\n\\end{aligned}\n\\]\nThis shows how to compute today’s filtering distribution given the likelihood. The advantage of this is that it only requires matrix multiplication.\n\n\nBackward-sampling\nThe result of the forward-filtering is the final observation \\(p_{ij}^{T,T}\\). Like in the previous section, we can then filter in reverse to compute \\(p_{ij}^{t,T}\\), which is required for the MCMC algorithm. We have that\n\\[\n\\begin{aligned}\np_{ij}^{t,T} & = p\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{T}\\right) \\\\\n& \\propto p\\left( x_{t-1} = i | x_{t} = j, \\theta, y^{T}\\right) p\\left( x_{t} = j | \\theta, y^{T}\\right) \\\\\n& \\propto p\\left( x_{t-1} = i | x_{t} = j, \\theta, y^{t}\\right) p\\left( x_{t} = j | \\theta, y^{T}\\right) \\\\\n& \\propto \\frac{p\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{t}\\right)}{p\\left( x_{t} = j | \\theta, y^{t}\\right)} p\\left( x_{t} = j | \\theta, y^{T}\\right) \\\\\n& \\propto p_{ij}^{t,t} \\frac{p_{j}^{t,T}}{p_{j}^{t,t}} \\text{.}\n\\end{aligned}\n\\]\nIn deriving this, we have used the fact that\n\\[\np\\left( x_{t-1} = i | x_{t} = j, \\theta, y^{T}\\right) \\propto p\\left( x_{t-1} = i | x_{t} = j, \\theta , y^{t}\\right)\n\\]\nbecause conditional time \\(t\\) information, the past transition is independent of the future. This is the discrete-state version of the FFBS algorithm.\n\n\nSmoothing: Forwards and Backwards\nLet \\(y^T = \\{y_1, \\dots, y_T\\}\\) be a sequence of random variables where the conditional distribution\n\\[\np(y^T | x^T) = p (y_1 | x_1 ) \\prod_{t=2}^T p(y_t | x_t, y_{t-1})\n\\]\nwhere we suppress the dependence of the mixture components on a parameter \\(\\theta\\). The full smoothing distribution can be written\n\\[\np( x | y ) = p( x_T | y^T ) \\prod_{t=1}^{T-1} p( x_t | x_{t+1} , \\theta , y_{t+1} )\n\\]\nSuppose \\(\\{x_t\\}\\) follows a finite state Markov chain with initial distribution \\(\\pi_0\\) and transition probabilities\n\\[\nQ_t(r,s) = Pr(x_t = s | x_{t-1} = r) \\; \\; \\mathrm{and} \\; \\; P_t ( t, r ,s) = Pr(x_{t-1} = r, x_t = s | y_1^t)\n\\]\nwhich we will compute sequentially. Let the current filtering distribution of the state be given by, for \\(t &gt; 0\\),\n\\[\np_t(s) = Pr(x_t = s | y^t) \\; \\; \\mathrm{and} \\; \\; A_t(x_{t-1}, x_t, y_t) = p(x_{t-1}, x_t, y_t | y^{t-1})\n\\]\nBy definition,\n\\[\nA_t(r,s,y_t) = \\frac{p_{t-1}(r) Q_t(r,s)}{p(y_t | y_{t-1})}.\n\\tag{9.1}\\]\nwhere the marginal likelihood is given by\n\\[\np(y_t | y_{1}^{t-1}) = \\sum_{r,s} A_t(r,s,y_t)\n\\]\nThe filtered transition distribution\n\\[\np_{trs} = A_t(r,s,y_t) / p(y_t | y^{t-1})\n\\]\nThe forward-backward recursions are used to efficiently compute the observed data likelihood\n\\[\np(y) = \\sum_{ x } p(y | x)p( x )\n\\]\nwhere \\(x = ( x_1 , \\ldots , x_T )\\). We also need the posterior distribution \\(p( x | y )\\) of the latent Markov chain given observed data. The recursions consist of a forward step that computes the distribution of the \\(t\\)’th transition given all the data up to time \\(t\\), and a backward recursion that updates each distribution to condition on all observed data.\nThe forward recursion operates on the set of transition distributions, represented by a sequence of matrices \\(P_t = ( p_{trs} )\\). Computing\n\\[\np_t(s) = \\sum_{r}p_{trs}\n\\]\nsets up the next step in the recursion. The recursion is initialized by replacing \\(p\\) with \\(p^0\\) in equation Equation 9.1.\nThe observed data log-likelihood can be computed as\n\\[\n\\log p(y) = \\log p(y_1) + \\sum_{t=2}^T \\log p(y_t | y^{t-1})\n\\]\nWith appropriate use of logarithms, \\(p\\) and \\(p(y_t | y^{t-1})\\) need only ever be evaluated on the log scale.\nThe stochastic version of the backward recursion simulates from\n\\[\np(x | y).\n\\]\nBegin with the factorization\n\\[\n\\label{eq:bkwd} p(x | y) = p(x_T | y^T) \\prod_{t=1}^{T-1} p(x_{t} | x_{t+1}^T, y).\n\\]\nThen notice that, given \\(x_{t+1}\\), \\(x_t\\) is conditionally independent of \\(y_{t+1}^T\\) and all later \\(x\\)’s. Thus\n\\[\np(x_t | x_{t+1}, y) = P(x_t = r | x_{t+1} = s, y^{t+1}) \\propto p_{t+1rs}\n\\]\nTherefore, if one samples \\((x_{t-1}, x_t)\\) from the discrete bivariate distribution given by \\(P_t\\) and then repeatedly samples \\(x_t\\) from a multinomial distribution proportional to column \\(x_{t+1}\\) of \\(P_{t+1}\\) then \\(x=(x_1, \\ldots , x_T)\\) is a draw from \\(p(x|y)\\).\n\n\n\n9.5.3 Mixture Kalman filter\nWe can also introduce a \\(\\lambda_t\\) state variable and consider a system\n\\[\n\\begin{aligned}\ny_t & = F_{\\lambda_t} x_t + D_{\\lambda_t} \\epsilon_t  \\\\\nx_t & = G_{\\lambda_t} x_{t-1} + B_{\\lambda_t} v_t\n\\end{aligned}\n\\]\nThe Kalman filter gives moments of the state filtering distribution\n\\[\nx_t | \\lambda^t , y^t \\sim \\mathcal{N} \\left ( \\mu_{t|t} , \\Sigma_{t|t} \\right )\n\\]\nHere we assume that the iid auxiliary state variable shocks \\(\\lambda_t \\sim p( \\lambda_t )\\).\nFirst we marginalize over the state variable \\(x_t\\). Then we can track the sufficient statistics for the hidden \\(z_t\\) variable dynamically in time, namely \\(\\left ( z_{t|t} , S_t \\right )\\), just the Kalman filter moments, in the conditionally Gaussian and discrete cases Lindgren (1978). Then we re-sample \\(( z_{t|t} , S_t  , \\theta )^{(i)}\\) particles.\n\n\n9.5.4 Regime Switching Models\nThe general form of a continuous-time regime switching model is\n\\[\ndy_{t}=\\mu\\left(  \\theta,x_{t},y_{t}\\right)  dt+\\sigma\\left(  \\theta\n,x_{t},y_{t}\\right)  d B_{t}%\n\\]\nwhere \\(x_{t}\\) takes values in a discrete space with transition matrix \\(P_{ij}\\left(  t\\right)\\) with parameters \\(\\theta=\\left(  \\theta_{1},\\ldots,\\theta_{J}\\right)\\). Common specifications assume the drift and diffusion coefficients are parametric functions and the parameters switch over time. In this case, it is common to write the model as\n\\[\ndy_{t}=\\mu\\left(  \\theta_{x_{t}},y_{t}\\right)  dt+\\sigma\\left(  \\theta_{x_{t}%\n},y_{t}\\right)  d B_{t}\\text{.}%\n\\]\nScott (2002) provides a fast MCMC algorithm for state filtering by adapting the FFBS algorithm. Time discretized the model is:\n\\[\ny_{t}=\\mu\\left(  \\theta_{x_{t}},y_{t-1}\\right)  +\\sigma\\left(  \\theta_{x_{t}%\n},y_{t-1}\\right)  \\varepsilon_{t}\\text{.}%\n\\]\nNote that we use the standard notation from discrete-time models where the time index on the Markov state is equal to the current observation. The discrete-time transition probabilities are\n\\[\np_{ij}=P\\left(  x_{t}=i|x_{t-1}=j\\right)\n\\]\nand we assume, apriori, that the transition functions are time and state invariant. The joint likelihood is given by\n\\[\np\\left(  y| x,\\theta\\right)  = \\prod_{t=1}^{T} p\\left(  y_{t}|y_{t-1}%\n,x_{t-1},\\theta\\right)\\]\nwhere \\(p\\left(  y_{t}|y_{t-1},x_{t-1},\\theta\\right)  =N\\left(  \\mu\\left(\n\\theta_{x_{t-1}},y_{t-1}\\right)  ,\\sigma^{2}\\left(  \\theta_{x_{t-1}}%\n,y_{t-1}\\right)  \\right)\\).\nClifford-Hammersley implies that the complete conditionals are given by \\(p\\left(  \\theta| x ,s , y \\right)\\), \\(p\\left(  s | x , \\theta, y \\right)\\), and \\(p\\left(  x | s ,\\theta, y \\right)\\). Conditional on the states and the transition probabilities, updating the parameters is straightforward. Conditional on the states, the transition matrix has a Dirchlet distribution, and updating this is also straightforward. To update the states use FFBS.\nAn important component of regime switching models is the prior distribution. Regime switching models (and most mixture models) are not formally identified. For example, in all regime switching models, there is a labeling problem: there is no unique way to identify the states. A common approach to overcome this identification issue is to order the parameters.\n\nChangepoint Problems\nSmith (1975) introduced the single changepoint problem from a Bayesian perspective. Consider a sequence of random variables \\(y_{1} , \\ldots, y_{T}\\) which has a change-point at time \\(\\tau\\) in the sense that\n\\[\ny_t | \\theta_{k} \\sim    \\left\\{\n\\begin{array}[c]{l}\np( y | \\theta_{1} ) \\; \\; \\; \\; \\; \\; \\mathrm{for} \\; \\; 1 \\leq i \\leq\\tau \\\\\np( y | \\theta_{2} ) \\; \\; \\; \\; \\; \\; \\mathrm{for} \\; \\; \\tau+1 \\leq\ni \\leq T\n\\end{array}  \\right\\}\\]\nThis can be rewritten as a state space model\n\\[\ny_t = \\theta_{ x_t } + \\sigma_{ x_t } \\epsilon_t\n\\]\nwhere \\(x_t\\) has a Markov transition evolutiuon.\nAn idea that appears to be under-exploited is that of “model reparametrisation”. The multiple change-point problem which is computationally expensive if approached directly has a natural model reparametrisation exists however that makes the implementation of MCMC methods straightforward (see Chib (1998)). Specifically, suppose that the data generating process \\(y^{T} = \\{ y_{1} , \\ldots, y_{T} \\}\\) is given by a sequence of conditionals \\(f ( y_{t} | y^{t-1} , \\theta_{k} )\\) for parameters \\(\\theta_{k}\\) that change at un-known change-points \\(\\{ \\tau_{1} , \\ldots,\n\\tau_{k} \\}\\).\nThe model parameterization is based on using a hidden Markov state space model with a vector of latent variables \\(s_{t}\\) where \\(s_{t}\n= k\\) indicates that \\(y_{t}\\) is drawn from \\(p ( y_{t} | y^{t-1} ,\n\\theta_{k} )\\). Let the prior distribution on the \\(s_{t}\\)’s have transition matrix where \\(p_{ij} = P \\left(  s_{t} = j | s_{t-1} = i\n\\right)\\) is the probability of jumping regimes. With this model parameterization the \\(k\\)th change occurs at \\(\\tau_{k}\\) if \\(s_{\n\\tau_{k} } = k\\) and \\(s_{ \\tau_{k} + 1 } = k + 1\\). The reparameterisation automatically enforces the order constraints on the change-points and is it very easy to perform MCMC analysis on the posterior distribution. This provides a more efficient strategy for posterior computation. MCMC analysis of the \\(s_{t}\\)’s is straightforward and the posterior for the \\(\\tau_{k}\\)’s can be obtained by inverting the definition above. Hence\n\\[\np( \\tau = t | y ) = p( x_t =1 | y )\n\\]\nThe alternative is single state updating conditional on \\(\\tau\\) which is slow for finding the multiple-changeponts.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "17-forecasting.html#particle-learning-for-general-mixture-models",
    "href": "17-forecasting.html#particle-learning-for-general-mixture-models",
    "title": "9  Forecasting",
    "section": "9.6 Particle Learning for General Mixture Models",
    "text": "9.6 Particle Learning for General Mixture Models\nParticle learning (PL) offers a powerful and flexible approach for sequential inference in general mixture models. Unlike traditional MCMC methods, which require repeated passes over the entire dataset and can be computationally demanding, particle learning operates in an online fashion. This means it can efficiently update inference as new data arrives, making it particularly well-suited for large or high-dimensional datasets and real-time applications.\nThe particle learning framework is designed to efficiently and sequentially learn from a broad class of mixture models. At its core, the approach models data as arising from a mixture distribution:\n\\[\nf(z) = \\int k(z;\\theta) d G(\\theta)\n\\]\nwhere \\(G\\) is a discrete mixing measure and \\(k(z;\\theta)\\) is a kernel parameterized by \\(\\theta\\). The generality of this formulation allows PL to be applied to a wide variety of models, including finite mixture models, Dirichlet process mixtures, Indian buffet processes, and probit stick-breaking models. This flexibility is a significant advantage, as it enables practitioners to use PL across many settings without needing to redesign the inference algorithm for each new model structure.\nIn addition to its generality, particle learning provides an alternative to MCMC for tasks such as online model fitting, marginal likelihood estimation, and posterior cluster allocation. Its sequential nature makes it particularly attractive for streaming data and scenarios where computational resources are limited.\nA general mixture model can be described by three components: a likelihood, a transition equation for latent allocations, and a prior over parameters. Specifically,\n\nThe likelihood is given by \\(p(y_{t+1} | k_{t+1}, \\theta)\\), representing the probability of the next observation given the current allocation and parameters.\nThe transition equation \\(p(k_{t+1} | k^t, \\theta)\\) governs how the latent allocation variables evolve over time, where \\(k^t = \\{k_1, ···, k_t\\}\\) denotes the history of allocations.\nThe parameter prior \\(p(\\theta)\\) encodes prior beliefs about the mixture component parameters.\n\nThis structure can be expressed in a state-space form:\n\\[\n\\begin{align}\ny_{t+1} &= f(k_{t+1}, \\theta) \\\\\nk_{t+1} &= g(k^t, \\theta)\n\\end{align}\n\\]\nwhere the first equation is the observation model and the second describes the evolution of the latent allocation states.\nThe mixture modeling framework described above is closely related to hidden Markov models (HMMs). In this context, the observed data \\(y_t\\) are assumed to be generated from a mixture, with allocation variables \\(k_t\\) determining which mixture component is responsible for each observation. The parameters \\(\\theta_{k_t}\\) for each component are drawn from the mixing measure \\(G\\). This structure allows for both standard mixture models, where each observation is assigned to a single component, and more general latent feature models, where multivariate allocation variables \\(k_t\\) allow an observation to be associated with multiple components simultaneously.\nA central concept in particle learning is the essential state vector \\(\\mathcal{Z}_t\\), which is tracked over time. This vector is constructed to be sufficient for sequential inference, meaning that it contains all the information needed to compute the posterior predictive distribution for new data, update the state as new observations arrive, and learn about the underlying parameters:\n\nPosterior predictive: \\(p(y_{t+1} | \\mathcal{Z}_t)\\)\nPosterior updating: \\(p(\\mathcal{Z}_{t+1} | \\mathcal{Z}_t, y_{t+1})\\)\nParameter learning: \\(p(\\theta | \\mathcal{Z}_{t+1})\\)\n\n\n9.6.1 The Particle Learning Algorithm\nParticle learning approximates the posterior distribution \\(p(\\mathcal{Z}_t | y^t)\\) with a set of equally weighted particles \\(\\{\\mathcal{Z}_t^{(i)}\\}_{i=1}^N\\). When a new observation \\(y_{t+1}\\) becomes available, the algorithm proceeds in two main steps:\n\nResample: The current set of particles is resampled with weights proportional to the predictive likelihood \\(p(y_{t+1} | \\mathcal{Z}_t^{(i)})\\). This step focuses computational effort on the most plausible states given the new data.\nPropagate: Each resampled particle is then propagated forward by sampling from the transition distribution \\(p(\\mathcal{Z}_{t+1} | \\mathcal{Z}_t^{(i)}, y_{t+1})\\), thus updating the state to incorporate the new observation.\n\nThis two-step process is grounded in Bayes’ theorem, where the resampling step corresponds to updating the posterior with the new data, and the propagation step advances the state according to the model dynamics. After these steps, the set of particles provides an updated approximation to the posterior \\(p(\\mathcal{Z}_{t+1} | y^{t+1})\\).\nOne important distinction between particle learning and standard particle filtering methods is that the essential state vector $_t $does not necessarily need to include the full history of allocation variables \\(k^t\\) to be sufficient for inference. This makes PL both more efficient and more flexible than many existing particle filtering approaches for mixture models. Furthermore, the order of resampling and propagation steps is reversed compared to standard filters, which helps mitigate particle degeneracy and improves performance in mixture modeling contexts.\nParticle learning also provides an efficient mechanism for sampling from the full posterior distribution of the allocation vector \\(p(k^t | y^t)\\). This is achieved using a backwards uncertainty update, which allows for the recovery of smoothed samples of the allocation history. For each particle, and for each time step in reverse order, the allocation variable \\(k_r\\) is sampled with probability proportional to the product of the likelihood and the prior for that allocation, given the state vector. This results in an algorithm with computational complexity linear in the number of particles, making it practical even for large datasets.\nThe particle learning framework is applicable to a wide range of density estimation problems involving mixtures of the form\n\\[\nf(y;G) = \\int k(y ; \\theta) dG(\\theta)\n\\]\nThere are many possible choices for the prior on the mixing measure \\(G\\). Common examples include finite mixture models, which use a finite number of components; Dirichlet process mixtures, which allow for an infinite number of components via a stick-breaking construction; beta two-parameter processes; and kernel stick-breaking processes. Each of these priors offers different modeling flexibility and computational properties, and the choice depends on the specific application and desired level of model complexity.\nIn some cases, it is useful to consider a collapsed state-space model, where the predictive distribution for a new observation is expressed as an expectation over the mixing measure \\(G\\) given the current state vector:\n\\[\n\\mathbb{E}[f(y_{t+1};G) | \\mathcal{Z}_t] = \\int k(y_{t+1};\\theta) d \\mathbb{E}[G(\\theta) | \\mathcal{Z}_t]\n\\]\nIf \\(t\\) observations have been allocated to \\(m_t\\) mixture components, the posterior expectation of \\(G\\) can be written as a weighted sum of the base measure and point masses at the component parameters. The predictive density then combines contributions from both new and existing components, weighted according to their posterior probabilities.\nParticle learning offers a versatile and efficient framework for sequential inference in general mixture models. By representing the posterior with a set of particles and updating these particles as new data arrives, PL enables real-time model fitting, efficient posterior allocation, and flexible density estimation across a wide range of mixture modeling scenarios. Its ability to handle both finite and infinite mixture models, as well as latent feature models, makes it a valuable tool for modern statistical analysis.\n\nExample 9.5 (Particle Learning for Poisson Mixture Models) We will implement Particle Learning (PL) for a finite mixture of Poisson distributions based on the example from (carvalho2010particlea?). This example follows Algorithm 1 for finite mixture models from Section 2.1 of the paper.\nWe generate data from a mixture of two Poisson distributions (\\(\\lambda_1=2\\) with weight 0.7, \\(\\lambda_2\\)=10 with weight 0.3).\n\nset.seed(8) # Ovi\n# Simulate data (500 observations)\nn_obs &lt;- 500\ntrue_z &lt;- sample(1:2, n_obs, replace=TRUE, prob=c(0.7, 0.3))\ny &lt;- ifelse(true_z == 1, rpois(n_obs, 2), rpois(n_obs, 10))\n\n\n\nCode\n# Plot two empirical density plots for each mixture component. Put them in one plot \nplot(density(y[true_z == 1]), xlab = \"y\", col = \"blue\", lwd = 2, xlim = c(0, 15), ylim = c(0, 0.3), main=\"\")\nlines(density(y[true_z == 2]), col = \"red\", lwd = 2)\nlines(density(y),xlab = \"y\", col = \"purple\", lwd = 2, xlim = c(0, 15), ylim = c(0, 0.3))\nlegend(\"topright\", legend=c(\"Component 1 (Lambda=2)\", \"Component 2 (Lambda=10)\", \"Mixture Density\"), col=c(\"blue\", \"red\", \"purple\"), lwd=2)\n\n\n\n\n\n\n\n\n\nThe code below implements the Particle Learning algorithm using the following steps: 1 Particle Initialization: - Each particle tracks sufficient statistics: - s: Sum of observations per component - n: Count of observations per component 2. PL Algorithm: - Resample: Particles are weighted by the posterior predictive probability of the next observation - Propagate: For each particle: - Compute component allocation probabilities - Sample component assignment - Update sufficient statistics - Learn: Store posterior estimates of \\(\\lambda\\) parameters and mixture weights\nThe key features of this implementaiton is the use of posterior predictive uses Poisson-Gamma conjugacy and allocation of probabilities by combining prior weights and likelihood.\n\n\nCode\n# Model parameters\nm &lt;- 2  # Number of components\nalpha &lt;- c(1, 1)  # Gamma prior shape parameters\nbeta &lt;- c(1, 1)   # Gamma prior rate parameters\ngamma &lt;- c(1, 1)  # Dirichlet prior parameters\nn_particles &lt;- 1000  # Number of particles\n\n# Initialize particles\nparticles &lt;- lapply(1:n_particles, function(i) {\n  list(s = c(0, 0),    # Sufficient statistics (sum of y in each component)\n       n = c(0, 0))    # Counts per component\n})\n\n# Store posterior samples\nposterior_lambda &lt;- matrix(0, nrow = n_obs, ncol = m)\nposterior_weights &lt;- matrix(0, nrow = n_obs, ncol = m)\n\n# Particle Learning algorithm\nfor (t in 1:n_obs) {\n  y_t &lt;- y[t]\n  log_weights &lt;- numeric(n_particles)\n  \n  # 1. Compute weights using posterior predictive\n  for (i in 1:n_particles) {\n    total_obs &lt;- sum(particles[[i]]$n)\n    weight_components &lt;- (particles[[i]]$n + gamma) / (total_obs + sum(gamma))\n    \n    # Predictive for each component (Poisson-Gamma)\n    pred_prob &lt;- sapply(1:m, function(j) {\n      shape_post &lt;- alpha[j] + particles[[i]]$s[j]\n      rate_post &lt;- beta[j] + particles[[i]]$n[j]\n      exp(dpois(y_t, shape_post/rate_post, log = TRUE) +\n            dgamma(shape_post/rate_post, shape_post, rate_post, log = TRUE))\n    })\n    \n    log_weights[i] &lt;- log(sum(weight_components * pred_prob))\n  }\n  \n  # Normalize weights\n  max_logw &lt;- max(log_weights)\n  weights &lt;- exp(log_weights - max_logw)\n  weights &lt;- weights / sum(weights)\n  \n  # 2. Resample particles\n  idx &lt;- sample(1:n_particles, size = n_particles, replace = TRUE, prob = weights)\n  particles &lt;- particles[idx]\n  \n  # 3. Propagate state\n  for (i in 1:n_particles) {\n    # Compute allocation probabilities\n    alloc_probs &lt;- sapply(1:m, function(j) {\n      shape_post &lt;- alpha[j] + particles[[i]]$s[j]\n      rate_post &lt;- beta[j] + particles[[i]]$n[j]\n      log_prior &lt;- log(particles[[i]]$n[j] + gamma[j]) - log(sum(particles[[i]]$n) + sum(gamma))\n      log_lik &lt;- dpois(y_t, shape_post/rate_post, log = TRUE)\n      exp(log_prior + log_lik)\n    })\n    alloc_probs &lt;- alloc_probs / sum(alloc_probs)\n    \n    # Sample component allocation\n    k &lt;- sample(1:m, size = 1, prob = alloc_probs)\n    \n    # Update sufficient statistics\n    particles[[i]]$s[k] &lt;- particles[[i]]$s[k] + y_t\n    particles[[i]]$n[k] &lt;- particles[[i]]$n[k] + 1\n  }\n  \n  # 4. Learn parameters (store posterior means)\n  lambda_samples &lt;- t(sapply(particles, function(p) {\n    (alpha + p$s) / (beta + p$n)\n  }))\n  weight_samples &lt;- t(sapply(particles, function(p) {\n    (gamma + p$n) / sum(gamma + p$n)\n  }))\n  \n  posterior_lambda[t,] &lt;- colMeans(lambda_samples)\n  posterior_weights[t,] &lt;- colMeans(weight_samples)\n}\n\n\nNow we are ready to plot the results\n\n\nCode\nlibrary(ggplot2)\n# Convert posterior estimates to data frames for ggplot\nposterior_df &lt;- data.frame(\n  Observation = 1:n_obs,\n  Lambda1 = posterior_lambda[, 1],\n  Lambda2 = posterior_lambda[, 2],\n  Weight1 = posterior_weights[, 1],\n  Weight2 = posterior_weights[, 2]\n)\n# Plot Lambda parameters\nggplot(posterior_df, aes(x = Observation)) +\n  geom_line(aes(y = Lambda1, color = \"Lambda1\"), size = 1) +\n  geom_line(aes(y = Lambda2, color = \"Lambda2\"), size = 1) +\n  geom_hline(yintercept = 2, linetype = \"dashed\", color = \"blue\") +\n  geom_hline(yintercept = 10, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Posterior Estimates: Lambda Parameters\",\n       x = \"Observation\", y = \"Lambda\") +\n  scale_color_manual(values = c(\"black\", \"red\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n# Plot Weights\nggplot(posterior_df, aes(x = Observation)) +\n  geom_line(aes(y = Weight1, color = \"Weight1\"), size = 1) +\n  geom_line(aes(y = Weight2, color = \"Weight2\"), size = 1) +\n  geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"blue\") +\n  geom_hline(yintercept = 0.3, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Posterior Estimates: Mixture Weights\",\n       x = \"Observation\", y = \"Weight\") +\n  scale_color_manual(values = c(\"black\", \"red\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nPosterior Estimates: Lambda Parameters\n\n\n\n\n\n\n\nPosterior Estimates: Mixture Weights\n\n\n\n\n\nThe first plot shows the posterior estimates of \\(\\lambda\\) parameters converging to true values (2 and 10). The second plot shows mixture weights converging to true weights (0.7 and 0.3). Convergence typically occurs after 100-200 observations. Particle degeneracy is mitigated through systematic resampling\nAdvantages of PL for Mixtures:\n\nSequential Updating: Processes observations one-at-a-time\nEfficiency: Only tracks sufficient statistics, not full history\nFlexibility: Easily extends to other mixture types (DP, IBP, etc.)\nReal-time Inference: Posterior updates after each observation\n\nThis implementation demonstrates PL’s ability to handle finite mixtures, but the same framework extends to infinite mixtures (DP mixtures) and other general mixture models described in the paper by modifying the propagation and resampling steps.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "17-forecasting.html#modern-era-forecasting",
    "href": "17-forecasting.html#modern-era-forecasting",
    "title": "9  Forecasting",
    "section": "9.7 Modern Era Forecasting",
    "text": "9.7 Modern Era Forecasting\nA recent post by the Amazon Science group Amazon (2021) describes the evolution of the time series algorithms used for forecasting from 2007 to 2021. Figure below shows the entire evolution of the algorithms.\n\n\n\n\n\nThey went from standard textbook time series forecasting methods to make predictions to the quantile-based transformer models. The main problem of the traditional TS models is that they assume stationary. A stationary time series is one whose properties do not depend on the time at which the series is observed. For exmaple, a white noise series is stationary - it does not matter when you observe it, it should look much the same at any point in time.\n\nyt = rnorm(100)\nplot(yt,type='l')\n\n\n\n\n\n\n\n\nIn other words, all the coefficients of a time series model do not change over time. We know how to deal with trends and seasonality quite well. Thus, those types of non-stationary are not an issue. Below some of the example of time series data. Although most of those are not stationary, we can model them using traditional techniques (Hyndman and Athanasopoulos (2021)).\n\n\n\n\n\n\n\n\nFigure 9.8: Which of these series are stationary? (a) Dow Jones index on 292 consecutive days; (b) Daily change in the Dow Jones index on 292 consecutive days; (c) Annual number of strikes in the US; (d) Monthly sales of new one-family houses sold in the US; (e) Annual price of a dozen eggs in the US (constant dollars); (f) Monthly total of pigs slaughtered in Victoria, Australia; (g) Annual total of lynx trapped in the McKenzie River district of north-west Canada; (h) Monthly Australian beer production; (i) Monthly Australian electricity production.\n\n\n\n\n\nHowever, when you try to forecast for a time series with no prior history or non-recurrent “jumps”, like recessions, traditional models are unlikely to work well.\nAmazon used a sequence of “patches” to hack the model and to make it produce useful results. All of those reacquired manual feature engineering and led to less transparent and fragile models. One solution is to use random forests.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "17-forecasting.html#quantile-regression-forests.",
    "href": "17-forecasting.html#quantile-regression-forests.",
    "title": "9  Forecasting",
    "section": "9.8 Quantile Regression Forests.",
    "text": "9.8 Quantile Regression Forests.\nMost estimators during prediction return \\(E(Y|X)\\), which can be interpreted as the answer to the question, what is the expected value of your output given the input?\nQuantile methods, return \\(y\\) at \\(q\\) for which \\(F(Y=y|X)=q\\) where \\(q\\) is the percentile and \\(y\\) is the quantile. One quick use-case where this is useful is when there are a number of outliers which can influence the conditional mean. It is sometimes important to obtain estimates at different percentiles, (when grading on a curve is done for instance.)\nNote, Bayesian models return the entire distribution of \\(P(Y|X)\\).\nIt is fairly straightforward to extend a standard decision tree to provide predictions at percentiles. When a decision tree is fit, the trick is to store not only the sufficient statistics of the target at the leaf node such as the mean and variance but also all the target values in the leaf node. At prediction, these are used to compute empirical quantile estimates.\nThe same approach can be extended to Random Forests. To estimate \\(F(Y=y|x)=q\\) each target value in training \\(y\\)s is given a weight. Formally, the weight given to \\(y_j\\) while estimating the quantile is \\[\n\\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\mathbb{1}(y_j \\in L(x))}{\\sum_{i=1}^N \\mathbb{1}(y_i \\in L(x))},\n\\] where \\(L(x)\\) denotes the leaf that \\(x\\) falls into.\nInformally, what it means that for a new unknown sample, we first find the leaf that it falls into at each tree. Then for each \\((X, y)\\) in the training data, a weight is given to \\(y\\) at each tree in the following manner.\n\nIf it is in the same leaf as the new sample, then the weight is the fraction of samples in the same leaf.\nIf not, then the weight is zero.\n\nThese weights for each y are summed up across all trees and averaged. Now since we have an array of target values and an array of weights corresponding to these target values, we can use this to measure empirical quantile estimates.nding to these target values, we can use this to measure empirical quantile estimates.\nMotivated by the success of gradient boositg model for predicting Walmart sales (kaggle (2020)), Januschowski et al. (2022) tries to explain why tree-based methods were so widely used for forecasting.\n\n\n\nJanuschowski et al. (2022)\n\n\n\n\n\n\nAlbert, Jim. 1993. “A Statistical Analysis of Hitting Streaks in Baseball: Comment.” Journal of the American Statistical Association 88 (424): 1184–88.\n\n\nAmazon. 2021. “The History of Amazon’s Forecasting Algorithm.” Amazon Science. https://www.amazon.science/latest-news/the-history-of-amazons-forecasting-algorithm.\n\n\nBaum, Leonard E., Ted Petrie, George Soules, and Norman Weiss. 1970. “A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains.” The Annals of Mathematical Statistics 41 (1): 164–71.\n\n\nBenoit, Dries F., and Dirk Van den Poel. 2012. “Binary Quantile Regression: A Bayesian Approach Based on the Asymmetric Laplace Distribution.” Journal of Applied Econometrics 27 (7): 1174–88.\n\n\nBerge, Travis, Nitish Sinha, and Michael Smolyansky. 2016. “Which Market Indicators Best Forecast Recessions?” FEDS Notes, August.\n\n\nCampagnoli, Patrizia, Sonia Petrone, and Giovanni Petris. 2009. Dynamic Linear Models with R. New York, NY: Springer.\n\n\nChib, Siddhartha. 1998. “Estimation and Comparison of Multiple Change-Point Models.” Journal of Econometrics 86 (2): 221–41.\n\n\nEric Tassone, and Farzan Rohani. 2017. “Our Quest for Robust Time Series Forecasting at Scale.”\n\n\nFrühwirth-Schnatter, Sylvia, and Rudolf Frühwirth. 2007. “Auxiliary Mixture Sampling with Applications to Logistic Models.” Computational Statistics & Data Analysis 51 (April): 3509–28.\n\n\n———. 2010. “Data Augmentation and MCMC for Binary and Multinomial Logit Models.” In Statistical Modelling and Regression Structures: Festschrift in Honour of Ludwig Fahrmeir, 111–32.\n\n\nFrühwirth-Schnatter, Sylvia, Rudolf Frühwirth, Leonhard Held, and Håvard Rue. 2008. “Improved Auxiliary Mixture Sampling for Hierarchical Models of Non-Gaussian Data.” Statistics and Computing 19 (4): 479.\n\n\nGramacy, Robert B., and Nicholas G. Polson. 2012. “Simulation-Based Regularized Logistic Regression.” arXiv.\n\n\nHeld, Leonhard, and Chris C. Holmes. 2006. “Bayesian Auxiliary Variable Models for Binary and Multinomial Regression.” Bayesian Analysis 1 (1): 145–68.\n\n\nHyndman, Rob J., and George Athanasopoulos. 2021. Forecasting: Principles and Practice. 3rd ed. edition. Melbourne, Australia: Otexts.\n\n\nJanuschowski, Tim, Yuyang Wang, Kari Torkkola, Timo Erkkilä, Hilaf Hasson, and Jan Gasthaus. 2022. “Forecasting with Trees.” International Journal of Forecasting, Special Issue: M5 competition, 38 (4): 1473–81.\n\n\nkaggle. 2020. “M5 Forecasting - Accuracy.” https://kaggle.com/competitions/m5-forecasting-accuracy.\n\n\nLindgren, Georg. 1978. “Markov Regime Models for Mixed Distributions and Switching Regressions.” Scandinavian Journal of Statistics 5 (2): 81–91.\n\n\nPetris, Giovanni. 2010. “An R Package for Dynamic Linear Models.” Journal of Statistical Software 36 (October): 1–16.\n\n\nPolson, Nicholas, and Steven Scott. 2011. “Data Augmentation for Support Vector Machines.” Bayesian Analysis 6 (March).\n\n\nRubin, Hal S. Stern, John B. Carlin. 2015. Bayesian Data Analysis. 3rd ed. New York: Chapman and Hall/CRC.\n\n\nScott, Steven L. 2022. “BoomSpikeSlab: MCMC for Spike and Slab Regression.”\n\n\nScott, Steven L., and Hal R. Varian. 2015. “Bayesian Variable Selection for Nowcasting Economic Time Series.” In Economic Analysis of the Digital Economy, 119–35. University of Chicago Press.\n\n\nScott, Steven, and Hal Varian. 2014. “Predicting the Present with Bayesian Structural Time Series.” Int. J. Of Mathematical Modelling and Numerical Optimisation 5 (January): 4–23.\n\n\nSean J. Taylor, and Ben Letham. 2017. “Prophet: Forecasting at Scale - Meta Research.” Meta Research. https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/.\n\n\nSmith, A. F. M. 1975. “A Bayesian Approach to Inference about a Change-Point in a Sequence of Random Variables.” Biometrika 62 (2): 407–16.\n\n\nViterbi, A. 1967. “Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.” IEEE Transactions on Information Theory 13 (2): 260–69.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acemoglu, Daron, and Pascual Restrepo. 2018. “Artificial\nIntelligence, Automation and Work.” National Bureau of Economic\nResearch.\n\n\nAlbert, Jim. 1993. “A Statistical Analysis of\nHitting Streaks in Baseball:\nComment.” Journal of the American Statistical\nAssociation 88 (424): 1184–88.\n\n\nAltić, Mirela Slukan. 2013. “Exploring Along the Rome Meridian:\nRoger Boscovich and the First Modern Map of the Papal\nStates.” In History of Cartography:\nInternational Symposium of the ICA, 2012,\n71–89. Springer.\n\n\nAmazon. 2021. “The History of Amazon’s Forecasting\nAlgorithm.” Amazon Science.\nhttps://www.amazon.science/latest-news/the-history-of-amazons-forecasting-algorithm.\n\n\nAmit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. “Multiple\nRandomized Classifiers: MRCL.”\n\n\nAndrews, D. F., and C. L. Mallows. 1974. “Scale\nMixtures of Normal Distributions.”\nJournal of the Royal Statistical Society. Series B\n(Methodological) 36 (1): 99–102.\n\n\nBaum, Leonard E., Ted Petrie, George Soules, and Norman Weiss. 1970.\n“A Maximization Technique Occurring in the\nStatistical Analysis of Probabilistic\nFunctions of Markov Chains.” The Annals\nof Mathematical Statistics 41 (1): 164–71.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo,\nZakaria Haque, Salem Haykal, et al. 2017. “Tfx: A\nTensorflow-Based Production-Scale Machine Learning Platform.” In\nProceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBenoit, Dries F., and Dirk Van den Poel. 2012. “Binary Quantile\nRegression: A Bayesian Approach Based on the Asymmetric\nLaplace Distribution.” Journal of Applied\nEconometrics 27 (7): 1174–88.\n\n\nBerge, Travis, Nitish Sinha, and Michael Smolyansky. 2016. “Which\nMarket Indicators Best Forecast Recessions?”\nFEDS Notes, August.\n\n\nBertsimas, Dimitris, Angela King, and Rahul Mazumder. 2016. “Best\nSubset Selection via a Modern Optimization Lens.” The Annals\nof Statistics 44 (2): 813–52.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard\nFirner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016.\n“End to End Learning for Self-Driving Cars.” arXiv\nPreprint arXiv:1604.07316.\n\n\nBonfiglio, Rita, Annarita Granaglia, Raffaella Giocondo, Manuel Scimeca,\nand Elena Bonanno. 2021. “Molecular Aspects and Prognostic\nSignificance of Microcalcifications in Human Pathology: A\nNarrative Review.” International Journal of Molecular\nSciences 22 (120).\n\n\nBox, George E. P., and George C. Tiao. 1992. Bayesian\nInference in Statistical Analysis. New\nYork: Wiley-Interscience.\n\n\nCampagnoli, Patrizia, Sonia Petrone, and Giovanni Petris. 2009.\nDynamic Linear Models with R. New\nYork, NY: Springer.\n\n\nCandes, Emmanuel J, and Michael B Wakin. 2008. “An\nIntroduction To Compressive Sampling. A\nSensing/Sampling Paradigm That Goes Against the Common Knowledge in Data\nAquisition.” IEEE Signal Processing Magazine 25 (21).\n\n\nCarvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010.\n“The Horseshoe Estimator for Sparse Signals.”\nBiometrika, asq017.\n\n\nChib, Siddhartha. 1998. “Estimation and Comparison of Multiple\nChange-Point Models.” Journal of Econometrics 86 (2):\n221–41.\n\n\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation.\nSpringer Science & Business Media.\n\n\nDiaconis, Persi, and David Freedman. 1987. “A Dozen de Finetti-style Results in Search of a\nTheory.” In Annales de l’IHP\nProbabilités Et Statistiques, 23:397–423.\n\n\nDiaconis, P., and D. Ylvisaker. 1983. “Quantifying Prior\nOpinion.”\n\n\nEfron, Bradley, and Carl Morris. 1977. “Stein’s Paradox in\nStatistics.” Scientific American 236 (5): 119–27.\n\n\nEnikolopov, Ruben, Vasily Korovkin, Maria Petrova, Konstantin Sonin, and\nAlexei Zakharov. 2013. “Field Experiment Estimate of Electoral\nFraud in Russian Parliamentary Elections.”\nProceedings of the National Academy of Sciences 110 (2):\n448–52.\n\n\nEric Tassone, and Farzan Rohani. 2017. “Our Quest for Robust Time\nSeries Forecasting at Scale.”\n\n\nFeynman, Richard. n.d. “Feynman :: Rules of\nChess.”\n\n\nFrank, Ildiko E., and Jerome H. Friedman. 1993. “A\nStatistical View of Some Chemometrics Regression\nTools.” Technometrics 35 (2): 109–35.\n\n\nFrühwirth-Schnatter, Sylvia, and Rudolf Frühwirth. 2007.\n“Auxiliary Mixture Sampling with Applications to Logistic\nModels.” Computational Statistics & Data Analysis 51\n(April): 3509–28.\n\n\n———. 2010. “Data Augmentation and MCMC\nfor Binary and Multinomial Logit\nModels.” In Statistical Modelling and\nRegression Structures: Festschrift in\nHonour of Ludwig Fahrmeir, 111–32.\n\n\nFrühwirth-Schnatter, Sylvia, Rudolf Frühwirth, Leonhard Held, and Håvard\nRue. 2008. “Improved Auxiliary Mixture Sampling for Hierarchical\nModels of Non-Gaussian Data.” Statistics and\nComputing 19 (4): 479.\n\n\nGan, Link, and Alan Fritzler. 2016. “How to Become an\nExecutive.”\n\n\nGeorge, Edward I., and Robert E. and McCulloch. 1993. “Variable\nSelection via Gibbs Sampling.”\nJournal of the American Statistical Association 88 (423):\n881–89.\n\n\nGramacy, Robert B., and Nicholas G. Polson. 2012.\n“Simulation-Based Regularized Logistic\nRegression.” arXiv.\n\n\nHahn, P. Richard, Jared S. Murray, and Carlos M. Carvalho. 2020.\n“Bayesian Regression Tree Models for Causal\nInference: Regularization, Confounding,\nand Heterogeneous Effects (with\nDiscussion).” Bayesian Analysis 15 (3):\n965–1056.\n\n\nHalevy, Alon, Peter Norvig, and Fernando Pereira. 2009. “The\nUnreasonable Effectiveness of Data.” IEEE Intelligent\nSystems 24 (2): 8–12.\n\n\nHeld, Leonhard, and Chris C. Holmes. 2006. “Bayesian Auxiliary\nVariable Models for Binary and Multinomial Regression.”\nBayesian Analysis 1 (1): 145–68.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo:\nUber’s Machine Learning Platform.”\n\n\nHuang, Jian, Joel L. Horowitz, and Shuangge Ma. 2008. “Asymptotic\nProperties of Bridge Estimators in Sparse High-Dimensional Regression\nModels.” The Annals of Statistics 36 (2): 587–613.\n\n\nHyndman, Rob J., and George Athanasopoulos. 2021. Forecasting:\nPrinciples and Practice. 3rd ed. edition.\nMelbourne, Australia: Otexts.\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in\nArtificial Intelligence.”\n\n\nIrwin, Neil. 2016. “How to Become a\nC.E.O.? The Quickest Path\nIs a Winding One.” The New York\nTimes, September.\n\n\nJanuschowski, Tim, Yuyang Wang, Kari Torkkola, Timo Erkkilä, Hilaf\nHasson, and Jan Gasthaus. 2022. “Forecasting with Trees.”\nInternational Journal of Forecasting, Special\nIssue: M5 competition, 38 (4): 1473–81.\n\n\nJeffreys, Harold. 1998. Theory of Probability.\nThird Edition, Third Edition. Oxford Classic Texts in the\nPhysical Sciences. Oxford, New York: Oxford University\nPress.\n\n\nkaggle. 2020. “M5 Forecasting -\nAccuracy.”\nhttps://kaggle.com/competitions/m5-forecasting-accuracy.\n\n\nKlartag, Bo’az. 2007. “A Central Limit Theorem for Convex\nSets.” Inventiones Mathematicae 168 (1): 91–131.\n\n\nLindgren, Georg. 1978. “Markov Regime Models for\nMixed Distributions and Switching\nRegressions.” Scandinavian Journal of Statistics\n5 (2): 81–91.\n\n\nMazumder, Rahul, Friedman, and Trevor and Hastie. 2011. “SparseNet:\nCoordinate Descent With Nonconvex Penalties.”\nJournal of the American Statistical Association 106 (495):\n1125–38.\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg\nMori. 2017. “Learning Person Trajectory Representations for Team\nActivity Analysis.” arXiv Preprint arXiv:1706.00893.\n\n\nMilman, Vitali D, and Gideon Schechtman. 2009. Asymptotic Theory of\nFinite Dimensional Normed Spaces: Isoperimetric\nInequalities in Riemannian Manifolds. Vol. 1200. Springer.\n\n\nMitchell, T. J., and J. J. Beauchamp. 1988. “Bayesian\nVariable Selection in Linear\nRegression.” Journal of the American Statistical\nAssociation 83 (404): 1023–32.\n\n\nNicosia, Luca, Giulia Gnocchi, Ilaria Gorini, Massimo Venturini,\nFederico Fontana, Filippo Pesapane, Ida Abiuso, et al. 2023.\n“History of Mammography: Analysis of Breast Imaging\nDiagnostic Achievements over the Last Century.”\nHealthcare 11 (1596).\n\n\nPetris, Giovanni. 2010. “An R Package for\nDynamic Linear Models.” Journal of Statistical\nSoftware 36 (October): 1–16.\n\n\nPolson, Nicholas G., and James G. Scott. 2011. “Shrink\nGlobally, Act Locally: Sparse Bayesian\nRegularization and Prediction.” In\nBayesian Statistics 9, edited by José M. Bernardo,\nM. J. Bayarri, James O. Berger, A. P. Dawid, David Heckerman, Adrian F.\nM. Smith, and Mike West, 0. Oxford University Press.\n\n\nPolson, Nicholas G., James G. Scott, and Jesse Windle. 2013.\n“Bayesian Inference for Logistic\nModels Using Pólya–Gamma Latent\nVariables.” Journal of the American Statistical\nAssociation 108 (504): 1339–49.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How\nPeople and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep\nLearning: A Bayesian Perspective.”\nBayesian Analysis 12 (4): 1275–1304.\n\n\nPolson, Nicholas, and Steven Scott. 2011. “Data\nAugmentation for Support Vector\nMachines.” Bayesian Analysis 6 (March).\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V\nMcConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018.\n“Prediction of Cardiovascular Risk Factors from Retinal Fundus\nPhotographs via Deep Learning.” Nature Biomedical\nEngineering 2 (3): 158.\n\n\nRubin, Hal S. Stern, John B. Carlin. 2015. Bayesian Data\nAnalysis. 3rd ed. New York: Chapman and\nHall/CRC.\n\n\nSchwertman, Neil C, AJ Gilks, and J Cameron. 1990. “A Simple\nNoncalculus Proof That the Median Minimizes the Sum of the Absolute\nDeviations.” The American Statistician 44 (1): 38–39.\n\n\nScott, Steven L. 2022. “BoomSpikeSlab:\nMCMC for Spike and Slab\nRegression.”\n\n\nScott, Steven L., and Hal R. Varian. 2015. “Bayesian\nVariable Selection for Nowcasting Economic Time\nSeries.” In Economic Analysis of the\nDigital Economy, 119–35. University of Chicago Press.\n\n\nScott, Steven, and Hal Varian. 2014. “Predicting the\nPresent with Bayesian Structural Time\nSeries.” Int. J. Of Mathematical Modelling and\nNumerical Optimisation 5 (January): 4–23.\n\n\nSean J. Taylor, and Ben Letham. 2017. “Prophet: Forecasting at\nScale - Meta Research.” Meta Research.\nhttps://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,\nMatthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering\nChess and Shogi by Self-Play with\na General Reinforcement Learning Algorithm.” arXiv.\n\n\nSmith, A. F. M. 1975. “A Bayesian Approach to\nInference about a Change-Point in a\nSequence of Random Variables.”\nBiometrika 62 (2): 407–16.\n\n\nStein, Charles. 1964. “Inadmissibility of the Usual Estimator for\nthe Variance of a Normal Distribution with Unknown Mean.”\nAnnals of the Institute of Statistical Mathematics 16 (1):\n155–60.\n\n\nStigler, Stephen M. 1981. “Gauss and the Invention of Least\nSquares.” The Annals of Statistics, 465–74.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and\nTD-Gammon.” Communications of the ACM 38\n(3): 58–68.\n\n\nTiao, Louis. 2019. “Pólya-Gamma Bayesian\nLogistic Regression.” Blog post.\n\n\nTikhonov, Andrei N. 1963. “Solution of Incorrectly Formulated\nProblems and the Regularization Method.” Sov Dok 4:\n1035–38.\n\n\nTikhonov, Andrey Nikolayevich et al. 1943. “On the Stability of\nInverse Problems.” In Dokl. Akad. Nauk Sssr, 39:195–98.\n\n\nVarian, Hal R. 2010. “Computer Mediated\nTransactions.” American Economic Review 100 (2):\n1–10.\n\n\nViterbi, A. 1967. “Error Bounds for Convolutional Codes and an\nAsymptotically Optimum Decoding Algorithm.” IEEE Transactions\non Information Theory 13 (2): 260–69.\n\n\nWindle, Jesse. 2023. “BayesLogit:\nBayesian Logistic Regression.” R package version\n2.1.\n\n\nWindle, Jesse, Nicholas G. Polson, and James G. Scott. 2014.\n“Sampling Polya-Gamma Random Variates: Alternate and\nApproximate Techniques.” arXiv.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu,\nYeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of\nStructured Information from Street View Imagery.” arXiv\nPreprint arXiv:1704.03549.\n\n\nZhang, Yichi, Anirban Datta, and Sudipto Banerjee. 2018. “Scalable\nGaussian Process Classification with Pólya-Gamma Data\nAugmentation.” arXiv Preprint arXiv:1802.06383.",
    "crumbs": [
      "References"
    ]
  }
]