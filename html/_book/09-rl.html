<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Reinforcement Learning – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./10-data.html" rel="next">
<link href="./08-gp.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-680e7c22d93ef26f016bec9199f8e6d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="9&nbsp; Reinforcement Learning – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="09-rl_files/figure-html/unnamed-chunk-4-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="9&nbsp; Reinforcement Learning – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="09-rl_files/figure-html/unnamed-chunk-4-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./09-rl.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#multi-armed-bandits" id="toc-multi-armed-bandits" class="nav-link active" data-scroll-target="#multi-armed-bandits"><span class="header-section-number">9.1</span> Multi-Armed Bandits</a>
  <ul class="collapse">
  <li><a href="#when-to-end-experiments" id="toc-when-to-end-experiments" class="nav-link" data-scroll-target="#when-to-end-experiments">When to End Experiments</a></li>
  <li><a href="#contextual-bandits" id="toc-contextual-bandits" class="nav-link" data-scroll-target="#contextual-bandits">Contextual Bandits</a></li>
  <li><a href="#summary-of-mab-experimentation" id="toc-summary-of-mab-experimentation" class="nav-link" data-scroll-target="#summary-of-mab-experimentation">Summary of MAB Experimentation</a></li>
  </ul></li>
  <li><a href="#bellman-principle-of-optimality" id="toc-bellman-principle-of-optimality" class="nav-link" data-scroll-target="#bellman-principle-of-optimality"><span class="header-section-number">9.2</span> Bellman Principle of Optimality</a></li>
  <li><a href="#markov-decision-processes" id="toc-markov-decision-processes" class="nav-link" data-scroll-target="#markov-decision-processes"><span class="header-section-number">9.3</span> Markov Decision Processes</a>
  <ul class="collapse">
  <li><a href="#mathematical-representation" id="toc-mathematical-representation" class="nav-link" data-scroll-target="#mathematical-representation">Mathematical Representation</a></li>
  <li><a href="#mdp-solvers" id="toc-mdp-solvers" class="nav-link" data-scroll-target="#mdp-solvers">MDP Solvers</a></li>
  <li><a href="#model-free-methods" id="toc-model-free-methods" class="nav-link" data-scroll-target="#model-free-methods">Model-Free Methods</a></li>
  <li><a href="#q-learning" id="toc-q-learning" class="nav-link" data-scroll-target="#q-learning">Q-Learning</a></li>
  </ul></li>
  <li><a href="#bayesian-optimization" id="toc-bayesian-optimization" class="nav-link" data-scroll-target="#bayesian-optimization"><span class="header-section-number">9.4</span> Bayesian Optimization</a></li>
  <li><a href="#concluding-remarks" id="toc-concluding-remarks" class="nav-link" data-scroll-target="#concluding-remarks"><span class="header-section-number">9.5</span> Concluding Remarks</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./09-rl.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>“<em>Information is the resolution of uncertainty.</em>” Claude Shannon, 1948</p>
<p>Thus far we have discussed making decisions under uncertainty (<a href="04-dec.html" class="quarto-xref"><span>Chapter 4</span></a> and <a href="05-ab.html" class="quarto-xref"><span>Chapter 5</span></a>) and two modes of data collection: field experiments and observational studies. In a field experiment we control the data‐generation process before the study begins, whereas in an observational study we have no such control and must work with whatever data are produced.</p>
<p>What if we can choose which data to collect <em>while</em> the experiment is running? This leads to sequential (or adaptive) experimental design, in which each new observation is selected on the basis of the data gathered so far, creating a feedback loop between data generation and decision-making. The idea is illustrated in the following diagram:</p>
<!-- https://www.mermaidchart.com/app/projects/ab5d5333-d1a5-42f8-ac1d-8e287a49d7b8/diagrams/30323a8e-087d-4321-b95e-7d89ad5d2f25/share/invite/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJkb2N1bWVudElEIjoiMzAzMjNhOGUtMDg3ZC00MzIxLWI5NWUtN2Q4OWFkNWQyZjI1IiwiYWNjZXNzIjoiRWRpdCIsImlhdCI6MTc1MDIyMDU2OX0.Oy_fw9FvQjwYhw_cxd2t12YWjYK6ur3QQ65ZMgcTYmo -->
<!-- ![](fig/rl-loop.svg) -->
<div class="cell" data-fig-width="4" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-rl-loop" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-loop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-rl-loop">graph LR
    d("Decision on next sample") --"Collect"--&gt;s("System under study")
    s--"Observe"--&gt;d
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-rl-loop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1
</figcaption>
</figure>
</div>
</div>
</div>
<p>There are multiple applications where this general framework can be applied and multiple algorithms that implement this concept. In this section, we will consider the most widely used among them:</p>
<ul>
<li>Multi-Armed Bandits</li>
<li>Q-Learning</li>
<li>Active Learning</li>
<li>Bayesian Optimization</li>
</ul>
<p>One of the first practical demonstrations of reinforcement learning (then called <em>trial-and-error learning</em>) was Claude Shannon’s 1950s mechanical mouse <em>Theseus</em>, which learned to find its way through a maze.</p>
<section id="multi-armed-bandits" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="multi-armed-bandits"><span class="header-section-number">9.1</span> Multi-Armed Bandits</h2>
<p>When the number of alternatives is large and the testing budget limited, a different approach to A/B testing —- <em>the multi-armed bandit</em> (MAB) can be more sample-efficient. Although the mathematical analysis and design of the experiments is slightly more complicated, the idea is simple. Instead of testing all alternatives at the same time, we test them sequentially. We start with a small number of alternatives and collect data. Based on the data we decide which alternative to test next. This way we can test more alternatives with the same number of samples. Thus, MABs allow us to explore and exploit simultaneously. MABs require that the outcome of each experiment is available immediately or with small enough delay to make a decision on the next experiment <span class="citation" data-cites="scott2015multiarmed">Scott (<a href="references.html#ref-scott2015multiarmed" role="doc-biblioref">2015</a>)</span>. On the other hand, MABs are more sample efficient and allow us to find an optimal alternative faster. Thus, in the case when time is of the essence and there is an opportunity cost associated with delaying the decision, MABs are a better choice.</p>
<p>Formally, there are <span class="math inline">\(K\)</span> alternatives (arms) and each arm <span class="math inline">\(a\)</span> is associated with a reward distribution <span class="math inline">\(v_a\)</span>, the value of this arm. The goal is to find the arm with the highest expected reward and accumulate the highest total reward in doing so. The reward distribution is unknown and we can only observe the reward after we select an arm <span class="math inline">\(a\)</span> but we assume that we know the distribution <span class="math inline">\(f_a(y\mid \theta)\)</span>, where <span class="math inline">\(a\)</span> is the arm index, <span class="math inline">\(y\)</span> is the reward, and <span class="math inline">\(\theta\)</span> is the set of unknown parameters to be learned. The value of arm <span class="math inline">\(v_a(\theta)\)</span> is known, given <span class="math inline">\(\theta\)</span>. Here are a few examples:</p>
<ol type="1">
<li>In online advertisements, we have <span class="math inline">\(K\)</span> possible ads to be shown and probability of user clicking is given by vector <span class="math inline">\(\theta = (\theta_1,\ldots,\theta_K)\)</span> of success probabilities for <span class="math inline">\(K\)</span> independent binomial models, with <span class="math inline">\(v_a(\theta) = \theta_a\)</span>. The goal is to find the ad with the highest click-through rate.</li>
<li>In website design, we have two design variables, the color of a button (red or blue) and its pixel size (27 or 40), we introduce two dummy variables <span class="math inline">\(x_c\)</span> for color, <span class="math inline">\(x_s\)</span> for size and the probability of user clicking is given by <span class="math display">\[
\mathrm{logit} P(\text{click} \mid \theta) = \theta_0 + \theta_x x_c + \theta_s x_s + \theta_{cs}x_cx_s,
\]</span> with <span class="math inline">\(v_a(\theta) = P(\text{click} \mid \theta)\)</span>.</li>
</ol>
<p>A variation of the second example would include controlling for background variables, meaning adding covariates that correspond to variables that are not under our control. For example, we might want to control for the time of the day or the user’s location. This would allow us to estimate the effect of the design variables while controlling for the background variables. Another variation is to replace the binary outcome variable with a continuous variable, such as the time spent on the website or the amount of money spent on the website or count variable. In this case we simply use linear regression or another appropriate generalized linear model. Although at any given time, we might have our best guess about the values of parameters <span class="math inline">\(\hat \theta\)</span>, acting in a greedy way and choosing the arm with the highest expected reward <span class="math inline">\(\hat a = \arg\max_a v_a(\hat \theta)\)</span>, we might be missing out on a better alternative. The problem is that we are not sure about our estimates <span class="math inline">\(\hat \theta\)</span> and we need to explore other alternatives. Thus, we need to balance exploration and exploitation. A widely used and oldest approach for managing the explore/exploit trade-off in a multi-armed bandit problem is Thompson sampling. The idea is to use Bayesian inference to estimate the posterior distribution of the parameters <span class="math inline">\(\theta\)</span> and then sample from this distribution to select the next arm to test. The algorithm is as follows.</p>
<ol type="1">
<li>Initial Beliefs. For each arm <span class="math inline">\(k\)</span> (<span class="math inline">\(k = 1,\ldots,K\)</span>), define a prior belief about its reward distribution using a beta distribution with parameters <span class="math inline">\(\alpha_i\)</span> and <span class="math inline">\(\beta_i\)</span>. These parameters represent the prior knowledge about the number of successes (<span class="math inline">\(\alpha_k\)</span>) and failures (<span class="math inline">\(\beta_k\)</span>) experienced with arm <span class="math inline">\(k\)</span>.
<ol type="1">
<li>Sampling Parameters. For each arm <span class="math inline">\(k\)</span>, sample a reward <span class="math display">\[
\hat \theta_k \sim Beta(\alpha_k, \beta_k)
\]</span><br>
from the beta distribution. This simulates drawing a potential reward from the unknown true distribution of arm <span class="math inline">\(i\)</span>. A suboptimal greedy alternative is to select expected value of the parameter <span class="math inline">\(\hat \theta_k = E(\theta_k) =\alpha_k/(\alpha_k + \beta_k)\)</span>.</li>
</ol></li>
<li>Choosing the Best Arm. Select the arm <span class="math inline">\(k\)</span> with the highest sampled reward <span class="math inline">\(\hat \theta_k\)</span>: <span class="math display">\[
a_t = \arg\max_i \hat \theta_k.
\]</span></li>
<li>Updating Beliefs. After observing the actual reward <span class="math inline">\(R_t\)</span> for the chosen arm <span class="math inline">\(a_t\)</span>, update the parameters for that arm: <span class="math display">\[
\alpha_i = \alpha_i + R_t, \quad \beta_i = \beta_i + (1 - R_t)
\]</span> This update incorporates the new information gained from the actual reward into the belief about arm <span class="math inline">\(a_t\)</span>’s distribution.</li>
<li>Repeat. Go back to step 2 and continue sampling, choosing, and updating until you reach a stopping point.</li>
</ol>
<p>As the algorithm progresses, the posterior distributions of the arms are updated based on observed data, and arms with higher estimated success probabilities are favored for selection. Over time, Thompson Sampling adapts its beliefs and preferences, leading to better exploration and exploitation of arms.</p>
<p>It’s important to note that the effectiveness of Thompson Sampling depends on the choice of prior distributions and the updating process. The algorithm’s Bayesian nature allows it to naturally incorporate uncertainty and make informed decisions in the presence of incomplete information. The initial priors <span class="math inline">\((\alpha_i, \beta_i)\)</span> can be set based on any available information about the arms, or as uniform distributions if no prior knowledge exists. This is a basic implementation of Thompson sampling. Variations exist for handling continuous rewards, incorporating side information, and adapting to non-stationary environments.</p>
<section id="when-to-end-experiments" class="level3">
<h3 class="anchored" data-anchor-id="when-to-end-experiments">When to End Experiments</h3>
<p>Step 2 of the TS algorithm can be replaced by calculating probability of an arm <span class="math inline">\(a\)</span> being the best <span class="math inline">\(w_{at}\)</span> and then choose the arm by sampling from the discrete distribution <span class="math inline">\(w_{1t},\ldots,w_{Kt}\)</span>. The probability of an arm <span class="math inline">\(a\)</span> being the best is given by <span class="math display">\[
w_{at} = P(a \text{ is optimal } \mid y^t) = \int P(a \text{ is optimal } \mid \theta) P(\theta \mid y^t) d\theta,
\]</span> where <span class="math inline">\(y^t = (y_1,\ldots,y_t)\)</span> is the history of observations up to time <span class="math inline">\(t\)</span>. We can calculate the probabilities <span class="math inline">\(w_{at}\)</span> using Monte Carlo. We can sample <span class="math inline">\(\theta^{(1)},\ldots,\theta^{(G)}\)</span> from the posterior distribution <span class="math inline">\(p(\theta \mid y^t)\)</span> and calculate the probability as <span class="math display">\[
w_{at}\approx \dfrac{1}{G}\sum_{g=1}^G I(a = \arg\max_i v_i(\theta^{(g)})),
\]</span> where <span class="math inline">\(I(\cdot)\)</span> is the indicator function. This is simply the proportion of times the arm was the best in the <span class="math inline">\(G\)</span> samples.</p>
<p>Although using a single draw from posterior <span class="math inline">\(p(\theta \mid y^t)\)</span> (as in the original algorithm) is equivalent to sampling proportional to <span class="math inline">\(w_{at}\)</span>, the explicitly calculated <span class="math inline">\(w_{at}\)</span> yields a useful statistic that can be used to decide on when to end the experiment.</p>
<p>We will use the regret statistic to decide when to stop. Regret is the difference in values between the truly optimal arm and the arm that is apparently optimal at time <span class="math inline">\(t\)</span>. Although we cannot know the regret (it is unobservable), we can compute samples from its posterior distribution. We simulate the posterior distribution of the regret by sampling <span class="math inline">\(\theta^{(1)},\ldots,\theta^{(G)}\)</span> from the posterior distribution <span class="math inline">\(p(\theta \mid y^t)\)</span> and calculating the regret as <span class="math display">\[
r^{(g)} =  (v_a^*(\theta^{(g)}) - v_{a^*_t}(\theta^{(g)})),
\]</span> Here <span class="math inline">\(a^*\)</span> is the arm deemed best across all Monte Carlo draws and the first term is the value of the best arm within draw <span class="math inline">\(g\)</span>. We choose <span class="math inline">\(a^*_t\)</span> as <span class="math display">\[
a^*_t = \arg\max_a w_{at}.
\]</span></p>
<p>Often, it is convenient to measure the regret on the percent scale and then we use <span class="math display">\[
r^{(g)} \leftarrow r^{(g)}/v_{a^*_t}(\theta^{(g)})
\]</span></p>
<p>We can demonstrate using simulated data. The function below generates samples <span class="math inline">\(\theta^{(g)}\)</span></p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>bandit <span class="ot">=</span> <span class="cf">function</span>(x,n,<span class="at">alpha =</span> <span class="dv">1</span>,<span class="at">beta =</span> <span class="dv">1</span>,<span class="at">ndraws =</span> <span class="dv">5000</span>) {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">17</span>) <span class="co"># Kharlamov</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  K <span class="ot">&lt;-</span> <span class="fu">length</span>(x) <span class="co"># number of bandits</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  prob <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> ndraws,<span class="at">ncol =</span> K)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  no <span class="ot">=</span> n <span class="sc">-</span> x</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (a <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>K) {<span class="co"># posterior draws for each arm</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    prob[, a] <span class="ot">=</span> <span class="fu">rbeta</span>(ndraws,x[a] <span class="sc">+</span> alpha,no[a] <span class="sc">+</span> beta)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  prob</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Say we have three arms with 20, 30, and 40 sessions that have generated 12, 20, and 30 conversions. We assume a uniform prior for each arm <span class="math inline">\(\theta_i \sim Beta(1,1)\)</span> and generate 6 samples from the posterior of <span class="math inline">\(\theta \mid y^t\)</span>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">12</span>,<span class="dv">20</span>,<span class="dv">30</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">20</span>, <span class="dv">30</span>,<span class="dv">40</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>prob <span class="ot">=</span> <span class="fu">bandit</span>(x, n,<span class="at">ndraws=</span><span class="dv">6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\(\theta_1\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\theta_2\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\theta_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1</td>
<td style="text-align: right;">0.60</td>
<td style="text-align: right;">0.63</td>
<td style="text-align: right;">0.58</td>
</tr>
<tr class="even">
<td style="text-align: left;">2</td>
<td style="text-align: right;">0.62</td>
<td style="text-align: right;">0.62</td>
<td style="text-align: right;">0.74</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3</td>
<td style="text-align: right;">0.69</td>
<td style="text-align: right;">0.53</td>
<td style="text-align: right;">0.67</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: right;">0.49</td>
<td style="text-align: right;">0.59</td>
<td style="text-align: right;">0.73</td>
</tr>
<tr class="odd">
<td style="text-align: left;">5</td>
<td style="text-align: right;">0.61</td>
<td style="text-align: right;">0.51</td>
<td style="text-align: right;">0.69</td>
</tr>
<tr class="even">
<td style="text-align: left;">6</td>
<td style="text-align: right;">0.47</td>
<td style="text-align: right;">0.64</td>
<td style="text-align: right;">0.69</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Now, we calculate the posterior probabilities <span class="math inline">\(w_{at} = P(a \text{ is optimal } \mid y^t)\)</span> for each of the three arms</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>wat <span class="ot">=</span> <span class="fu">table</span>(<span class="fu">factor</span>(<span class="fu">max.col</span>(prob),<span class="at">levels =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>))<span class="sc">/</span><span class="dv">6</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">1</th>
<th style="text-align: right;">2</th>
<th style="text-align: right;">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.17</td>
<td style="text-align: right;">0.17</td>
<td style="text-align: right;">0.67</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Thus far, the third arm is the most likely to be optimal, with probability 67%. Now, we calculate the regret for each of the six draws from the posterior of <span class="math inline">\(\theta \mid y^t\)</span>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>regret <span class="ot">=</span> (<span class="fu">apply</span>(prob,<span class="dv">1</span>,max) <span class="sc">-</span> prob[,<span class="dv">3</span>])<span class="sc">/</span>prob[,<span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">1</th>
<th style="text-align: right;">2</th>
<th style="text-align: right;">3</th>
<th style="text-align: right;">4</th>
<th style="text-align: right;">5</th>
<th style="text-align: right;">6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We compute value row by row by subtracting the largest element of that row from the element in column 3 (because arm 3 has the highest chance of being the optimal arm). All rows but 1 and 3 are zero. In the first row, the value is (0.63-0.58)/0.58 because column 2 is 0.05 larger than column 3. If we keep going down each row we get a distribution of values that we could plot in a histogram. We can generate one for a larger number of draws (10000).</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>prob <span class="ot">=</span> <span class="fu">bandit</span>(x, n,<span class="at">ndraws=</span><span class="dv">10000</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>regret <span class="ot">=</span> (<span class="fu">apply</span>(prob,<span class="dv">1</span>,max) <span class="sc">-</span> prob[,<span class="dv">3</span>])<span class="sc">/</span>prob[,<span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>The histogram of the value remaining in an experiment (regret). The vertical line is the 95th percentile, or the potential value remaining.</figcaption>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>wat <span class="ot">=</span> <span class="fu">table</span>(<span class="fu">factor</span>(<span class="fu">max.col</span>(prob),<span class="at">levels =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>))<span class="sc">/</span><span class="dv">10000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">1</th>
<th style="text-align: right;">2</th>
<th style="text-align: right;">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.08</td>
<td style="text-align: right;">0.2</td>
<td style="text-align: right;">0.72</td>
</tr>
</tbody>
</table>
<p>The histogram of the value remaining in an experiment (regret). The vertical line is the 95th percentile, or the potential value remaining.</p>
</div>
</div>
<p>Arm 3 has a 72% probability of being the best arm, so the value of switching away from arm 3 is zero in 72% of the cases. The 95th percentile of the value distribution is the potential value remaining (CvR) in the experiment, which in this case works out to be about 16%.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(regret,<span class="fl">0.95</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 95% 
0.17 </code></pre>
</div>
</div>
<p>You interpret this number as “We’re still unsure about the CvR for arm 3, but whatever it is, one of the other arms might beat it by as much as 16%.”</p>
<p>Google Analytics, for example, “ends the experiment when there’s at least a 95% probability that the value remaining in the experiment is less than 1% of the champion’s conversion rate. That’s a 1% improvement, not a one percentage point improvement. So if the best arm has a conversion rate of 4%, then we end the experiment if the value remaining in the experiment is less than .04 percentage points of CvR”.</p>
</section>
<section id="contextual-bandits" class="level3">
<h3 class="anchored" data-anchor-id="contextual-bandits">Contextual Bandits</h3>
<p>Traditional multi-armed bandit models, like the binomial model, assume independent observations with fixed reward probabilities. This works well when rewards are consistent across different groups and times. However, for situations with diverse user bases or fluctuating activity patterns, such as international audiences or browsing behavior, this assumption can be misleading.</p>
<p>For instance, companies with a global web presence may experience temporal effects as markets in Asia, Europe, and the Americas become active at different times of the day. Additionally, user behavior can change based on the day of the week, with people engaging in different browsing patterns and purchase behaviors. For example, individuals may research expensive purchases during work hours but make actual purchases on weekends.</p>
<p>Consider an experiment with two arms, A and B. Arm A performs slightly better during the weekdays when users browse but don’t make purchases, while Arm B excels during the weekends when users are more likely to make purchases. If there is a substantial amount of traffic, the binomial model might prematurely conclude that Arm A is the superior option before any weekend traffic is observed. This risk exists regardless of whether the experiment is conducted as a bandit or a traditional experiment. However, bandit experiments are more susceptible to this issue because they typically run for shorter durations.</p>
<p>To mitigate the risk of being misled by distinct sub-populations, two methods can be employed. If the specific sub-populations are known in advance or if there is a proxy for them, such as geographically induced temporal patterns, the binomial model can be adapted to a logistic regression model. This modification allows for a more nuanced understanding of the impact of different factors on arm performance, helping to account for variations in sub-population behavior and temporal effects. <span class="math display">\[
\mathrm{logit} P(\text{click}_a \mid \theta, x) = \theta_{0a} + \beta^Tx,
\]</span> where <span class="math inline">\(x\)</span> describes the circumstances or context of the observation. The success probability for selecting arm <span class="math inline">\(a\)</span> under the context <span class="math inline">\(x\)</span> is represented as <span class="math inline">\(P(\text{click}_a \mid \theta, x)\)</span>. Each arm <span class="math inline">\(a\)</span> has its own specific coefficient denoted as <span class="math inline">\(\beta_{0a}\)</span> with one arm’s coefficient set to zero as a reference point. Additionally, there is another set of coefficients represented as <span class="math inline">\(\beta\)</span> that are associated with the contextual data and are learned as part of the model. The value function can then be <span class="math display">\[
v_a(\theta) = \mathrm{logit}^{-1}(\beta_{0a}).
\]</span></p>
<p>If we lack knowledge about the crucial contexts, one option is to make the assumption that contexts are generated randomly from a context distribution. This approach is often exemplified by the use of a hierarchical model like the beta-binomial model. <span class="math display">\[\begin{align*}
\theta_{at} &amp;\sim Beta(\alpha_a,\beta_a)\\
\text{click}_a \mid \theta &amp;\sim Binomial(\theta_{at}),
\end{align*}\]</span> where <span class="math inline">\(\theta = \{\alpha_a,\beta_a ~:~ a = 1,\ldots,K \}\)</span>, with value function <span class="math inline">\(v_a(\theta) = \alpha_a/\beta_a\)</span></p>
</section>
<section id="summary-of-mab-experimentation" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-mab-experimentation">Summary of MAB Experimentation</h3>
<p>The design phase begins with defining your arms by identifying the different options you want to evaluate, such as different website layouts, pricing strategies, or marketing campaigns. Next, choose a bandit algorithm that balances exploration and exploitation in various ways. Popular choices include Epsilon-greedy, Thompson Sampling, and Upper Confidence Bound (UCB). Then set your parameters by configuring the algorithm parameters based on your priorities and expected uncertainty. For example, a higher exploration rate encourages trying new arms earlier. Finally, randomize allocation by assigning users to arms randomly, ensuring unbiased data collection.</p>
<p>During the analysis phase, track rewards by defining and measuring the reward metric for each arm, such as clicks, conversions, or profit. Monitor performance by regularly analyzing the cumulative reward and arm selection probabilities to see which arms are performing well and how the allocation strategy is adapting. Use statistical tools like confidence intervals or Bayesian methods to compare performance between arms and assess the significance of findings. Make adaptive adjustments by modifying the experiment based on ongoing analysis. You might adjust algorithm parameters, stop arms with demonstrably poor performance, or introduce new arms.</p>
<p>Start with a small pool of arms to avoid information overload by testing a manageable number of options initially. Set a clear stopping criterion by deciding when to end the experiment based on a predetermined budget, time limit, or desired level of confidence in the results. Consider ethical considerations by ensuring user privacy and informed consent if the experiment involves personal data or user experience changes. Interpret results in context by remembering that MAB results are specific to the tested conditions and might not generalize perfectly to other contexts.</p>
<p>By following these steps and utilizing available resources, you can design and analyze effective MAB experiments to optimize your decision-making in various scenarios. Remember to adapt your approach based on your specific goals and context to maximize the benefits of this powerful technique.</p>
</section>
</section>
<section id="bellman-principle-of-optimality" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="bellman-principle-of-optimality"><span class="header-section-number">9.2</span> Bellman Principle of Optimality</h2>
<blockquote class="blockquote">
<p>“An optimum policy has the property that whatever the initial state and initial decision are, the remaining decision sequence must be optimum for the state resulting from the first decision.” – Richard Bellman</p>
</blockquote>
<div id="exm-secretary" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.1 (Secretary Problem)</strong></span> The Secretary Problem, also known as the marriage problem or sultan’s dowry problem, is a classic problem in decision theory and probability theory. The scenario involves making a decision on selecting the best option from a sequence of candidates or options. The problem is often framed as hiring a secretary, but it can be applied to various situations such as choosing a house, a spouse, or any other scenario where you sequentially evaluate options and must make a decision.</p>
<p>In this problem, you receive <span class="math inline">\(T\)</span> offers and must either accept or reject the offer “on the spot”. You cannot return to a previous offer once you have moved on to the next one. Offers are in random order and can be ranked against those previously seen. The aim is to maximize the probability of choosing the offer with the greatest rank. There is an optimal <span class="math inline">\(r\)</span> (<span class="math inline">\(1 \le r &lt; T\)</span>) to be determined such that we examine and reject the first <span class="math inline">\(r\)</span> offers. Then of the remaining <span class="math inline">\(T - r\)</span> offers we choose the first one that is best seen to date.</p>
<p>A decision strategy involves setting a threshold such that the first candidate above this threshold is hired, and all candidates below the threshold are rejected. The optimal strategy, known as the 37% rule, suggests that one should reject the first <span class="math inline">\(r=T/e\)</span> candidates and then select the first candidate who is better than all those seen so far.</p>
<p>The reasoning behind the 37% rule is based on the idea of balancing exploration and exploitation. By rejecting the first <span class="math inline">\(T/e\)</span> candidates, you gain a sense of the quality of the candidates but avoid committing too early. After that point, you select the first candidate who is better than the best among the initial <span class="math inline">\(r\)</span> candidates.</p>
<p>It’s important to note that the 37% rule provides a probabilistic guarantee of selecting the best candidate with a probability close to 1/e (approximately 37%) as <span class="math inline">\(T\)</span> becomes large.</p>
<p>To solve the secretary problem, we will use the principle of optimality due to Richard Bellman. The principle states that an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. In other words, the policy is optimal from the first decision onwards.</p>
<p>The solution to the secretary problem can be found via dynamic programming. Given an agent with utility function <span class="math inline">\(u(x,d)\)</span>, with current state <span class="math inline">\(x\)</span>, and decision <span class="math inline">\(d\)</span>. The law of motion of <span class="math inline">\(x_t\)</span> is given by <span class="math inline">\(x_{t+1} = p(x_t,d_t)\)</span>. Bellman principle of optimality states that the optimal policy is given by the following recursion <span class="math display">\[
V(x_t) = \max_{d_t} \left \{ u(x_t,d_t) + \gamma \mathbb{E} \left [ V(x_{t+1}) \right ] \right \}
\]</span> where <span class="math inline">\(\gamma\)</span> is the discount factor. The optimal policy is given by <span class="math display">\[
d_t^* = \arg \max_{d_t} \left \{ u(x_t,d_t) + \gamma \mathbb{E} \left [ V(x_{t+1}) \right ] \right \}.
\]</span></p>
<p>Now, back to the secretary problem. Let <span class="math inline">\(y^t = (y_1,\ldots,y_t)\)</span> denote the history of observations up to time <span class="math inline">\(t\)</span>. State <span class="math inline">\(x_t=1\)</span> if the <span class="math inline">\(t\)</span>th candidate is the best seen so far and <span class="math inline">\(x_t=0\)</span> otherwise. The decision <span class="math inline">\(d_t=1\)</span> if the <span class="math inline">\(t\)</span>th candidate is hired and <span class="math inline">\(d_t=0\)</span> otherwise. The utility function is given by <span class="math inline">\(u(x_t,d_t) = x_t d_t\)</span>. <!-- The law of motion is given by $x_{t+1} = \max \{ x_t, x_{t+1} \}$.  --> The Bellman equation is given by <span class="math display">\[
P(\text{best of T}\mid x_t=1) = \dfrac{P(\text{best of T})}{P(x_t=1)} = \dfrac{1/T}{1/t} = \dfrac{t}{T}.
\]</span> The <span class="math inline">\(t\)</span>th offer is the best seen so far places no restriction on the relative ranks of the first <span class="math inline">\(t-1\)</span> offers. Therefore, <span class="math display">\[
p(x_t=1,y^{t-1}) = p(x_t=1)p(y^{t-1})
\]</span> by the independence assumption. Hence, we have <span class="math display">\[
p(x_t=1 \mid y^{t-1}) = p(x_t=1) = \dfrac{1}{t}.
\]</span></p>
<p>Let <span class="math inline">\(p^*(x_{t-1}=0)\)</span> be the probability under the optimal strategy. Now we have to select the best candidate, given we have seen <span class="math inline">\(t-1\)</span> offers so far and the last one was not the best or worse. The probability satisfies the Bellman equation <span class="math display">\[
p^*(x_{t-1}=0) = \frac{t-1}{t} p^*(x_{t}=0) + \frac{1}{t}\max\left(t/T, p^*(x_{t}=0)\right).
\]</span> This leads to <span class="math display">\[
p^*(x_{t-1}=0) = \frac{t-1}{T} \sum_{\tau=t-1}^{T-1}\dfrac{1}{\tau}.
\]</span></p>
<p>Remember, the strategy is to reject the first <span class="math inline">\(r\)</span> candidates and then select the first. The probability of selecting the best candidate is given by <span class="math display">\[
P(\text{success}) = \dfrac{1}{T}\sum_{a=r+1}^T \dfrac{r}{a} \approx  \dfrac{1}{T}\int_{r}^{T}\dfrac{r}{a} = \dfrac{r}{T} \log \left ( \dfrac{T}{r} \right ).
\]</span> We optimize over <span class="math inline">\(r\)</span> by setting the derivative <span class="math display">\[
\frac{\log \left(\frac{T}{r}\right)}{T}-\frac{1}{T}
\]</span> to zero, to find the optimal <span class="math inline">\(r=T/e\)</span>.</p>
<p>If we plug in <span class="math inline">\(r=T/e\)</span> back to the probability of success, we get <span class="math display">\[
P(\text{success}) \approx \dfrac{1}{e} \log \left ( e \right ) = \dfrac{1}{e}.
\]</span></p>
<p><strong>Monte Carlo Simulations</strong></p>
<p>Simulations are a powerful tool for making decisions when we deal with a complex system, which is difficult or impossible to analyze mathematically. They are used in many fields, including finance, economics, and engineering. They can also be used to test hypotheses about how a system works and to generate data for statistical analysis.</p>
<p>We start by showing how the secretary problem can be analyzed using simulations rather than analytical derivations provided above.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">17</span>) <span class="co"># Kharlamov</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>nmc <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>sz <span class="ot">=</span> <span class="dv">300</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>rules <span class="ot">=</span> <span class="fu">round</span>(n<span class="sc">*</span><span class="fu">seq</span>(<span class="fl">0.002</span>,<span class="fl">0.8</span>,<span class="at">length.out =</span> sz))</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>rules <span class="ot">=</span> <span class="fu">unique</span>(rules[rules<span class="sc">&gt;</span><span class="dv">0</span>])</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>sz <span class="ot">=</span> <span class="fu">length</span>(rules)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>cnt <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,sz)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>quality <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,sz)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>sz)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nmc){</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n,n)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    screen <span class="ot">=</span> x[<span class="dv">1</span><span class="sc">:</span>(rules[i]<span class="sc">-</span><span class="dv">1</span>)]</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    best_screen <span class="ot">=</span> <span class="fu">max</span>(screen)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    xchoice <span class="ot">=</span> x[(rules[i])<span class="sc">:</span>n]</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    better_candidates <span class="ot">=</span> <span class="fu">which</span>(xchoice <span class="sc">&gt;</span> best_screen)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">length</span>(better_candidates)<span class="sc">==</span><span class="dv">0</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>      choice <span class="ot">=</span> x[n]</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>      choice <span class="ot">=</span> xchoice[<span class="fu">min</span>(better_candidates)]</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    cnt[i] <span class="ot">=</span> cnt[i] <span class="sc">+</span> (choice <span class="sc">==</span> <span class="fu">max</span>(x))</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    quality[i] <span class="ot">=</span> quality[i] <span class="sc">+</span> choice</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">cnt=</span>cnt, <span class="at">quality=</span>quality,<span class="at">nmc=</span>nmc, <span class="at">rules=</span>rules)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(d<span class="sc">$</span>rules, d<span class="sc">$</span>cnt<span class="sc">/</span>d<span class="sc">$</span>nmc, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="dv">3</span>, <span class="at">lwd=</span><span class="dv">3</span>, <span class="at">xlab=</span><span class="st">"Number of Candidates Screened"</span>, </span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"Probability of Picking the Best"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(d<span class="sc">$</span>rules, d<span class="sc">$</span>quality<span class="sc">/</span><span class="dv">1000</span>, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="dv">3</span>, <span class="at">lwd=</span><span class="dv">3</span>, <span class="at">xlab=</span><span class="st">"Number of Candidates Screened"</span>, </span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"Average Quality of Candidate"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/unnamed-chunk-8-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="markov-decision-processes" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="markov-decision-processes"><span class="header-section-number">9.3</span> Markov Decision Processes</h2>
<p>Markov decision process (MDP) is a discrete time stochastic control process which provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. Almost all dynamic programming and reinforcement learning problems are formulated using the formalism of MDPs. MDPs are useful in various fields, including robotics, economics, and artificial intelligence. In fact, the multi-armed bandit problem considered before is a special case of MDP with one state. MDPs were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard’s 1960 book, <a href="https://www.rand.org/pubs/papers/P2081.html">Dynamic Programming and Markov Processes</a>.</p>
<p>A Markov Decision Process (MDP) is a mathematical framework used for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful in various fields, including robotics, economics, and artificial intelligence, particularly in reinforcement learning. An MDP is defined by:</p>
<ol type="1">
<li><strong>States (<span class="math inline">\(S\)</span>):</strong> A set of states representing different scenarios or configurations the system can be in. The key assumption here is the Markov property, which states that the future is independent of the past given the present. This means that the decision only depends on the current state, not on the sequence of events that preceded it.</li>
<li><strong>Actions (<span class="math inline">\(A\)</span>):</strong> A set of actions available in each state.</li>
<li><strong>Transition Probability (<span class="math inline">\(P\)</span>):</strong> <span class="math inline">\(P(s', r | s, a)\)</span> is the probability of transitioning to state <span class="math inline">\(s'\)</span>, receiving reward <span class="math inline">\(r\)</span>, given that action <span class="math inline">\(a\)</span> is taken in state <span class="math inline">\(s\)</span>.</li>
<li><strong>Reward (<span class="math inline">\(R\)</span>):</strong> A reward function <span class="math inline">\(R(s, a, s')\)</span> that gives the feedback signal immediately after transitioning from state <span class="math inline">\(s\)</span> to state <span class="math inline">\(s'\)</span>, due to action <span class="math inline">\(a\)</span>.</li>
<li><strong>Discount Factor (<span class="math inline">\(\gamma\)</span>):</strong> A factor between 0 and 1, which reduces the value of future rewards.</li>
</ol>
<section id="mathematical-representation" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-representation">Mathematical Representation</h3>
<p>The states <span class="math inline">\(s_t\)</span> and rewards <span class="math inline">\(R_t\)</span> in MDP are indexed by time <span class="math inline">\(t\)</span>. The state at time <span class="math inline">\(t+1\)</span> is distributed according to the transition probability <span class="math display">\[
P(s_{t+1}\mid s_t,a_t).
\]</span> The reward function is <span class="math inline">\(R_s^a = E[R_{t+1} \mid s, a]\)</span>.</p>
<p>The Markov property of the state is that the transition probability depends only on the current state and action and not on the history of states and actions. <span class="math display">\[
P(s_{t+1}\mid s_t,a_t) = P(s_{t+1}\mid s_t,a_t,s_{t-1},a_{t-1},\ldots,s_0,a_0).
\]</span> In other words, the future only depends on the present and not on the past history. The state is a sufficient statistic for the future.</p>
<p>In the case when the number of states is finite, we can represent the transition probability as a matrix <span class="math inline">\(P_{ss'}^a = P(s_{t+1} = s' \mid s_t = s, a_t = a)\)</span>, where <span class="math inline">\(s,s' \in S\)</span> and <span class="math inline">\(a \in A\)</span>. For a given action <span class="math inline">\(a\)</span>, the transition probability matrix <span class="math inline">\(P^a\)</span> is a square matrix of size <span class="math inline">\(|S| \times |S|\)</span>, where each row sums to 1 <span class="math display">\[
P^a = \begin{bmatrix}
P_{11}^a &amp; P_{12}^a &amp; \cdots &amp; P_{1|S|}^a \\
P_{21}^a &amp; P_{22}^a &amp; \cdots &amp; P_{2|S|}^a \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
P_{|S|1}^a &amp; P_{|S|2}^a &amp; \cdots &amp; P_{|S||S|}^a
\end{bmatrix}
\]</span> The reward function is also a matrix <span class="math inline">\(R_s^a = E[R_{t+1} \mid s_t = s, a_t = a]\)</span>.</p>
<p><strong>Markov Reward Process</strong></p>
<p>We can consider a simpler example of Markov Process. This is a special case of MDP when there is no action and the transition probability is simply a matrix <span class="math inline">\(P_{ss'} = P(s_{t+1} = s' \mid s_t = s)\)</span>, where <span class="math inline">\(s,s' \in S\)</span>. For a given action <span class="math inline">\(a\)</span>, the transition probability matrix <span class="math inline">\(P^a\)</span> is a square matrix of size <span class="math inline">\(|S| \times |S|\)</span>, where each row sums to 1.</p>
<div id="exm-mdp-classes" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.2 (Student Example)</strong></span> The graph below represents possible states (nodes) and transitions (links). Each node has reward assigned to it which corresponds to the reward function <span class="math inline">\(R(s)\)</span>. The transition probabilities are shown on the links. The graph is a Markov Chain, a special case of MDP with no actions.</p>
<!-- https://www.mermaidchart.com/app/projects/ab5d5333-d1a5-42f8-ac1d-8e287a49d7b8/diagrams/30323a8e-087d-4321-b95e-7d89ad5d2f25/share/invite/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJkb2N1bWVudElEIjoiMzAzMjNhOGUtMDg3ZC00MzIxLWI5NWUtN2Q4OWFkNWQyZjI1IiwiYWNjZXNzIjoiRWRpdCIsImlhdCI6MTc1MDIyMDY5MX0.gSZTr6y2V-aYpzz0blSIrbPuPS-hyW-ckPS6P30x3oM
 -->
<!-- ![](fig/rl-student.svg)
 -->
<div class="cell" data-fig-width="6.5" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-rl-student" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-student-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-rl-student">graph LR
    fb("Facebook; R=1") --0.9--&gt; fb
    fb--0.1--&gt;c1("Class 1; R=-2")
    c1--0.5--&gt;fb
    c1--0.5--&gt;c2("Class 2; R=-2")
    c2--0.8--&gt;c3("Class 3; R=-2")
    c3--0.6--&gt;p("Pass; R=10")
    p--1.0--&gt;s("Sleep; R=0")
    c3--0.4--&gt;pub("Pub; R=3")
    pub--0.2--&gt;f("Fail; R=-20")
    f--1.0--&gt;s
    pub--0.3--&gt;c3
    pub--0.2--&gt;c1
    pub--0.3--&gt;c2
    c2--0.2--&gt;s
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-rl-student-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.2
</figcaption>
</figure>
</div>
</div>
</div>
<p>If we are to pick an initial state and sample a trajectory (path on the graph above) by picking a random action at each state, we will get a random walk on the graph. The reward for each state is shown in the graph. The discounted value of the trajectory is then <span class="math display">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^\infty \gamma^k R_{t+k+1},
\]</span> where <span class="math inline">\(\gamma\)</span> is the discount factor. The discount factor is a number between 0 and 1 that determines the present value of future rewards. A discount factor of 0 makes the agent myopic and only concerned about immediate rewards. A discount factor of 1 makes the agent strive for a long-term high reward. The discount factor is usually denoted by <span class="math inline">\(\gamma\)</span> and is a parameter of the MDP. The discount of less than 1 is used to avoid infinite returns in cyclic Markov chains and allows us to discount less certain future rewards. The value of <span class="math inline">\(\gamma\)</span> is usually close to 1, for example 0.9 or 0.99. The value of <span class="math inline">\(\gamma\)</span> can be interpreted as the probability of the agent surviving from one time step to the next.</p>
<p>We can calculate sample returns <span class="math inline">\(G_t\)</span> for this Markov Chain. We first read in the reward matrix</p>
<div class="cell" data-layout-align="center" data-tbl-cap="Rewards" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reward function</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>R <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/student-reward.tab"</span>, <span class="at">header =</span> T, <span class="at">sep =</span> <span class="st">"</span><span class="sc">\t</span><span class="st">"</span>, <span class="at">row.names=</span><span class="dv">1</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">t</span>(R) <span class="sc">%&gt;%</span> knitr<span class="sc">::</span><span class="fu">kable</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Rewards</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Facebook</th>
<th style="text-align: right;">Class 1</th>
<th style="text-align: right;">Class 2</th>
<th style="text-align: right;">Class 3</th>
<th style="text-align: right;">Pub</th>
<th style="text-align: right;">Pass</th>
<th style="text-align: right;">Fail</th>
<th style="text-align: right;">Sleep</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Reward</td>
<td style="text-align: right;">-1</td>
<td style="text-align: right;">-2</td>
<td style="text-align: right;">-2</td>
<td style="text-align: right;">-2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">-20</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>getR <span class="ot">=</span> <span class="cf">function</span>(s) R[s,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and the transition probability matrix and the reward matrix</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read transition probability matrix</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/student-mdp.tab"</span>, <span class="at">header =</span> T, <span class="at">sep =</span> <span class="st">"</span><span class="sc">\t</span><span class="st">"</span>, <span class="at">row.names=</span><span class="dv">1</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>kbp <span class="ot">=</span> knitr<span class="sc">::</span><span class="fu">kable</span>(p)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="fu">gsub</span>(<span class="dv">0</span>, <span class="st">' '</span>, kbp) <span class="co"># replace 0 with blank</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Facebook</th>
<th style="text-align: right;">Class.1</th>
<th style="text-align: right;">Class.2</th>
<th style="text-align: right;">Class.3</th>
<th style="text-align: right;">Pub</th>
<th style="text-align: right;">Pass</th>
<th style="text-align: right;">Fail</th>
<th style="text-align: right;">Sleep</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Facebook</td>
<td style="text-align: right;">.9</td>
<td style="text-align: right;">.1</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Class 1</td>
<td style="text-align: right;">.5</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.5</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Class 2</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.8</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">Class 3</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.4</td>
<td style="text-align: right;">.6</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Pub</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.2</td>
<td style="text-align: right;">.3</td>
<td style="text-align: right;">.3</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.2</td>
<td style="text-align: right;">.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Pass</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">1.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Fail</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">1.</td>
<td style="text-align: right;">.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sleep</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">1.</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Now we check that all of the rows sum to 1</p>
<div class="cell" data-layout-align="center" data-tbl-cap="Transition probability matrix row sums" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">as.matrix</span>(p)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rowSums</span>(p) <span class="sc">%&gt;%</span> <span class="fu">t</span>() <span class="sc">%&gt;%</span> knitr<span class="sc">::</span><span class="fu">kable</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Transition probability matrix row sums</caption>
<thead>
<tr class="header">
<th style="text-align: right;">Facebook</th>
<th style="text-align: right;">Class 1</th>
<th style="text-align: right;">Class 2</th>
<th style="text-align: right;">Class 3</th>
<th style="text-align: right;">Pub</th>
<th style="text-align: right;">Pass</th>
<th style="text-align: right;">Fail</th>
<th style="text-align: right;">Sleep</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Given the transition probability matrix, we can sample possible trajectories. First, we define a <code>tr(s,m)</code> convenience function that generates a trajectory of length <code>m</code> starting from state <code>s</code></p>
<div class="cell" data-layout-align="center" data-tbl-cap="Sample trajectories" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">17</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample column s' using probabilities in row s </span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>jump <span class="ot">=</span> <span class="cf">function</span>(s) <span class="fu">sample</span>(<span class="fu">rownames</span>(p), <span class="dv">1</span>, <span class="at">prob =</span> p[s,])</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to generate a trajectory</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>tr <span class="ot">=</span> <span class="cf">function</span>(s,m) {</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  traj <span class="ot">=</span> <span class="fu">c</span>(s)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>m) {</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    traj <span class="ot">=</span> <span class="fu">c</span>(traj, <span class="fu">jump</span>(traj[i]))</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(traj)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we generate 6 trajectories of length 5 starting from state “Pub”</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>traj <span class="ot">=</span> <span class="fu">t</span>(<span class="fu">replicate</span>(<span class="dv">6</span>,<span class="fu">tr</span>(<span class="st">"Pub"</span>,<span class="dv">5</span>)))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(traj)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<tbody>
<tr class="odd">
<td style="text-align: left;">Pub</td>
<td style="text-align: left;">Class 3</td>
<td style="text-align: left;">Pub</td>
<td style="text-align: left;">Class 2</td>
<td style="text-align: left;">Class 3</td>
<td style="text-align: left;">Pass</td>
</tr>
<tr class="even">
<td style="text-align: left;">Pub</td>
<td style="text-align: left;">Class 2</td>
<td style="text-align: left;">Class 3</td>
<td style="text-align: left;">Pass</td>
<td style="text-align: left;">Sleep</td>
<td style="text-align: left;">Sleep</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Pub</td>
<td style="text-align: left;">Class 2</td>
<td style="text-align: left;">Class 3</td>
<td style="text-align: left;">Pub</td>
<td style="text-align: left;">Fail</td>
<td style="text-align: left;">Fail</td>
</tr>
<tr class="even">
<td style="text-align: left;">Pub</td>
<td style="text-align: left;">Fail</td>
<td style="text-align: left;">Fail</td>
<td style="text-align: left;">Fail</td>
<td style="text-align: left;">Fail</td>
<td style="text-align: left;">Fail</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Pub</td>
<td style="text-align: left;">Fail</td>
<td style="text-align: left;">Fail</td>
<td style="text-align: left;">Fail</td>
<td style="text-align: left;">Fail</td>
<td style="text-align: left;">Fail</td>
</tr>
<tr class="even">
<td style="text-align: left;">Pub</td>
<td style="text-align: left;">Class 3</td>
<td style="text-align: left;">Pass</td>
<td style="text-align: left;">Sleep</td>
<td style="text-align: left;">Sleep</td>
<td style="text-align: left;">Sleep</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Now we can calculate the discounted value <span class="math inline">\(G_t\)</span> of each of the trajectories</p>
<div class="cell" data-layout-align="center" data-tbl-cap="Discounted value of each trajectory" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>trajR <span class="ot">=</span> <span class="fu">apply</span>(traj,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, getR)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>disc <span class="ot">=</span> <span class="fl">0.5</span><span class="sc">^</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">5</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>trajR <span class="sc">%*%</span> disc <span class="sc">%&gt;%</span> <span class="fu">t</span>() <span class="sc">%&gt;%</span> knitr<span class="sc">::</span><span class="fu">kable</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Discounted value of each trajectory</caption>
<tbody>
<tr class="odd">
<td style="text-align: right;">2.7</td>
<td style="text-align: right;">2.8</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">-16</td>
<td style="text-align: right;">-16</td>
<td style="text-align: right;">4.5</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We can calculate the discounted value for 1000 trajectories</p>
<div class="cell" data-layout-align="center" data-tbl-cap="Discounted value of 1000 trajectories" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Value function of a trajectory</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>value <span class="ot">=</span> <span class="cf">function</span>(s,m, <span class="at">gamma=</span><span class="fl">0.5</span>) {</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  traj <span class="ot">=</span> <span class="fu">tr</span>(s,m)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  disc <span class="ot">=</span> gamma<span class="sc">^</span>(<span class="dv">0</span><span class="sc">:</span>m)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">sum</span>(<span class="fu">sapply</span>(traj,getR) <span class="sc">*</span> disc))</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>vpub <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">1000</span>,<span class="fu">value</span>(<span class="st">"Pub"</span>,<span class="dv">6</span>))</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(vpub)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(vpub)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> -1.2</code></pre>
</div>
</div>
<p>We can see that the distribution of discounted rewards is bimodal and depends on whether you get to “Fail” state or not.</p>
</div>
<p>The value of a state is the expected discounted reward starting from that state <span class="math display">\[
V(s) = E[G_t \mid s_t = s].
\]</span> It evaluates the long-term value of state <span class="math inline">\(s\)</span> (the goodness of a state). It can be drastically different from the reward value associated with the state. In our student example, the reward for the “Pub” state is 3, but the value is -1.2.</p>
<p>The value of a state can be calculated recursively using the Bellman equation <span class="math display">\[\begin{align*}
V(s) &amp;= E[G_t \mid s_t = s] \\
&amp;= E[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \mid s_t = s] \\
&amp;= E[R_{t+1} + \gamma G_{t+1} \mid s_t = s] \\
&amp;= E[R_{t+1} + \gamma V(s_{t+1}) \mid s_t = s] \\
&amp;= \sum_{s'} P(s' \mid s) \left[R(s) + \gamma V(s')\right].
\end{align*}\]</span></p>
<div id="exm-mdp-chess" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.3 (Game of Chess as an MDP)</strong></span> We can consider a simple example of a game of chess.</p>
<p>In chess, a state <span class="math inline">\(s\)</span> represents the configuration of the chessboard at any given time. This includes the positions of all the pieces (pawns, knights, bishops, rooks, queen, and king) for both players (white and black). Each possible arrangement of these pieces on the chessboard is a unique state. The game starts in a standard initial state (the standard chess setup) and progresses through a series of states as moves are made. If the game is played to completion, it ends in a terminal state (checkmate, stalemate, or draw). Also if we reach a state that has been seen before multiple times in a row, we can stop the game and declare a draw. Thus, we need to remember the states we have seen before and essentially expand the state space to include the number of times we have seen the state. In a timed game, the game can also end when a player runs out of time.</p>
<p>Actions <span class="math inline">\(a\)</span> in chess are the legal moves that can be made by the player whose turn it is to move. This includes moving pieces according to their allowed movements, capturing opponent pieces, and special moves like castling or en passant. The set of actions available changes with each state, depending on the position of the pieces on the board.</p>
<p>In chess, the transition probability is deterministic for the most part, meaning that the outcome of a specific action (move) is certain and leads to a predictable next state. For example, moving a knight from one position to another (assuming it’s a legal move) will always result in the same new configuration of the chessboard. However, in the context of playing against an opponent, there is uncertainty in predicting the opponent’s response, which can be seen as introducing a probabilistic element in the larger view of the game.</p>
<p>Defining a reward function <span class="math inline">\(R\)</span> in chess can be complex. In the simplest form, the reward could be associated with the game’s outcome: a win, loss, or draw. Wins could have positive rewards, losses negative, and draws could be neutral or have a small positive/negative value. Alternatively, more sophisticated reward functions can be designed to encourage certain strategies or positions, like controlling the center of the board, protecting the king, or capturing opponent pieces.</p>
<p>Chess is a game of perfect information, meaning all information about the game state is always available to both players. While the number of states in chess is finite, it is extremely large, making exhaustive state analysis (like traditional MDP methods) computationally impractical.</p>
<p>In practice, solving chess as an MDP, especially using traditional methods like value iteration or policy iteration, is not feasible due to the enormous state space. Modern approaches involve heuristic methods, machine learning, and deep learning techniques. For instance, advanced chess engines and AI systems like AlphaZero use deep neural networks and reinforcement learning to evaluate board positions and determine optimal moves, but they do not solve the MDP in the classical sense.</p>
</div>
<p>The goal in an MDP is to find a policy <span class="math inline">\(a = \pi(s)\)</span> (a function from states to actions) that maximizes the sum of discounted rewards: <span class="math display">\[
V^\pi(s) = E_{\pi}[G_t \mid S_t = s],
\]</span> where <span class="math display">\[
G_t = \sum_{t=0}^{\infty} \gamma^t R(s_t, \pi(s_t), s_{t+1})
\]</span></p>
<p>Function <span class="math inline">\(V^\pi(s)\)</span> is the value of state s under policy <span class="math inline">\(\pi\)</span>. Similarly we can define the action-value function <span class="math inline">\(Q^\pi(s,a)\)</span> as the value of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> under policy <span class="math inline">\(\pi\)</span>: <span class="math display">\[
Q^\pi(s,a) = E_{\pi}[G_t \mid S_t = s, A_t = a].
\]</span></p>
<p>Bellman Equations for MDP simply state that the value of a state is the sum of the immediate reward and the discounted value of the next state <span class="math display">\[
V^\pi(s) = E_{\pi}[R_{t+1} + \gamma V^{\pi}(S_{t+1})\mid S_t = s] = \sum_{a\in A}\pi(a\mid s)\left(R_s^a + \gamma \sum_{s'\in S}P^a_{ss'}V^pi(s') \right).
\]</span> The action-value function satisfies the following Bellman equation <span class="math display">\[
Q^\pi(s,a) = E_{\pi}[R_{t+1} + \gamma Q^{\pi}(S_{t+1}, A_{t+1})\mid S_t = s, A_t = a].
\]</span> The value function can be defined as an expectation over the action-value function <span class="math display">\[
V^\pi(s) = E_{\pi}[Q^\pi(s,a)\mid S_t = s] = \sum_{a\in A}\pi(a\mid s)Q^\pi(s,a).
\]</span> In matrix form, we have <span class="math display">\[
Q^\pi(s,a) = R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a V^\pi(s') = R_s^s + \gamma \sum_{s'\in S}P_{ss'}^a\sum_{a'\in A}\pi(a'\mid s')Q^\pi(s',a').
\]</span> Now we can define the Bellman equation in the matrix form <span class="math display">\[
V^\pi = R^\pi + \gamma P^\pi V^\pi.
\]</span> The direct solution is then <span class="math display">\[
V^\pi = (I - \gamma P^\pi)^{-1}R^\pi.
\]</span> The optimal value function <span class="math inline">\(V^*(s)\)</span> is the value function for the optimal policy <span class="math inline">\(\pi^*(s)\)</span> <span class="math display">\[
V^*(s) = \max_\pi V^\pi(s).
\]</span> The optimal action-value function <span class="math inline">\(Q^*(s,a)\)</span> is the action-value function for the optimal policy <span class="math inline">\(\pi^*(s)\)</span> <span class="math display">\[
Q^*(s,a) = \max_\pi Q^\pi(s,a).
\]</span> The optimal policy <span class="math inline">\(\pi^*(s)\)</span> is the policy that maximizes the value function <span class="math display">\[
\pi^*(s) = \arg\max_a Q^*(s,a).
\]</span> The optimal value function satisfies the Bellman optimality equation <span class="math display">\[
V^*(s) = \max_a Q^*(s,a).
\]</span> and vice-versa <span class="math display">\[
Q^*(s,a) = R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a V^*(s').
\]</span></p>
<p>The Bellman optimality equation is non-linear and is typically solved iteratively using Value Iteration, Policy Iteration, or Q-learning. Q-learning is an off-policy algorithm that learns the optimal policy by directly estimating the optimal action-value function <span class="math inline">\(Q^*(s,a)\)</span>. The algorithm iteratively updates the action-value function using the Bellman optimality backup. The off-policy means that the algorithm learns the optimal policy while following a different policy. The algorithm can learn the optimal policy while following a random policy, for example.</p>
</section>
<section id="mdp-solvers" class="level3">
<h3 class="anchored" data-anchor-id="mdp-solvers">MDP Solvers</h3>
<p>The underlying approach behind all MDP solvers is to iteratively apply the Bellman equations until convergence. The main difference between the solvers is how they update the value function. All of them use dynamic programming approach to find optimal policy. Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems. It is applicable to problems exhibiting the properties of overlapping subproblems and optimal substructure. If a problem can be solved by combining optimal solutions to non-overlapping subproblems, the strategy is called divide and conquer instead. This is why dynamic programming is applicable to solving MDPs.</p>
<p>First, we consider how to find the values of states under a given policy <span class="math inline">\(\pi\)</span>. We can iteratively apply Bellman expectation backup. We update the values using the following update rule <span class="math display">\[
V_{k+1}(s) = \sum_{a} \pi(a \mid s) \sum_{s'} P(s' \mid s, a)[R(s, a, s') + \gamma V_k(s')].
\]</span> We will introduce the short-cut notation <span class="math display">\[  
P_{ss'}^a = P(s' \mid s, a), \quad R_s^a = \sum_{s'} P(s' \mid s, a)R(s, a, s').
\]</span> Then in matrix form the update rule becomes <span class="math display">\[
V_{k+1} = R^{\pi} + \gamma P^{\pi} V_k.
\]</span></p>
<p>The policy iteration algorithm involves two main steps: policy evaluation and policy improvement, which are iteratively applied until convergence. We start with an arbitrary value function, often initialized to zero for all states. <span class="math display">\[\begin{align*}
V_0(s) &amp;= 0 \\
V_{k+1} = &amp; R^{\pi} + \gamma P^{\pi} V_k\\
\pi_{k+1} &amp;= \arg\max_a R^a + \gamma P^a V_{k+1} = \arg\max_a Q^\pi(s,a)
\end{align*}\]</span> The last step is to simply choose the action that maximizes the expected return in each state. Although it can be slow in practice, the convergence is guaranteed because the value function is a contraction mapping. We typically stop the iterations when the maximum change in the value function is below a threshold.</p>
<p>It can be used for calculating the optimal policy. The Bellman optimality equation is a fundamental part of finding the best policy in MDPs. <span class="math display">\[
V^*(s) = \max_a \sum_{s', r} P(s', r | s, a)[r + \gamma V^*(s')]
\]</span> The optimal policy is then <span class="math display">\[
\pi^*(s) = \arg\max_a \sum_{s', r} P(s', r | s, a)[r + \gamma V^*(s')]
\]</span> The optimal policy is the one that maximizes the value function. The optimal value function is the value function for the optimal policy. The optimal value function satisfies the Bellman optimality equation. The optimal policy can be found by maximizing the right hand side of the Bellman optimality equation.</p>
<p>Given an optimal policy, we can subdivide it into two parts: the optimal first action <span class="math inline">\(A^*\)</span> and the optimal policy from the next state <span class="math inline">\(S'\)</span>. The optimal value <span class="math inline">\(V^*\)</span> can be found using one-step lookahead <span class="math display">\[
V^*(s) = \max_a R_s^a +  \gamma \sum_{s'\in S} P_{ss'}^a V^*(s')
\]</span> This allows us to define another approach to solving MDPs, called value iteration. The value iteration algorithm starts with an arbitrary value function and iteratively applies the Bellman optimality backup. The algorithm updates the value function using the following update rule <span class="math display">\[
V_{k+1}(s) = \max_a R_s^a +  \gamma \sum_{s'\in S} P_{ss'}^a V_k(s').
\]</span> In matrix form, the update rule becomes <span class="math display">\[
V_{k+1} = \max_a R^a + \gamma P^a V_k.
\]</span> The algorithm stops when the maximum change in the value function is below a threshold. The optimal policy can be found by maximizing the right hand side of the Bellman optimality equation <span class="math display">\[
\pi^*(s) = \arg\max_a R_s^a +  \gamma \sum_{s'\in S} P_{ss'}^a V^*(s').
\]</span></p>
<p>In practice, exactly solving the Bellman Expectation Equation in the policy evaluation step can be computationally expensive for large state spaces. Approximate methods may be used. Policy Iteration is particularly effective when the optimal policy needs to be very precise, as in high-stakes decision-making environments.</p>
<div id="exm-mdp-maze" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.4 (MDP for a Maze)</strong></span> We use a <a href="https://github.com/sally-gao/mazemdp"><code>mazemdp</code></a> archive by Sally Gao, Duncan Rule, Yi Hao to demonstrate the value and policy iterations. Both are applied to the problem of finding an optimal policy for a maze. The maze is represented as a grid, with each cell either being a wall or empty. Agent (decision maker) does not know the maze structure and needs to find the optimal path from the start to the goal state. The agent starts in the bottom right corner and needs to reach the top left corner (marked as red). The agent can move up, down, left, or right, but not diagonally (actions). Moving into a wall keeps the agent in the same cell. Reaching the goal state gives a reward of +1, and all other transitions give a reward of 0. The goal is to find the optimal policy that maximizes the expected return (sum of discounted rewards) for the agent. In other words, the agent needs to find the shortest path to the exit.</p>
<p>Figures below show the snapshot from policy (top row) and value (bottom row) iterations.</p>
<div id="fig-elephants" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-elephants-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/maze0.jpg" class="img-fluid figure-img"></p>
<figcaption><span class="math inline">\(t=0\)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/maze12.jpg" class="img-fluid figure-img"></p>
<figcaption><span class="math inline">\(t=12\)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/maze24.jpg" class="img-fluid figure-img"></p>
<figcaption><span class="math inline">\(t=24\)</span></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-elephants-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.3: Policy Iteration Solver
</figcaption>
</figure>
</div>
<div id="fig-elephants" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-elephants-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/mazev0.jpg" class="img-fluid figure-img"></p>
<figcaption><span class="math inline">\(t=0\)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/mazev15.jpg" class="img-fluid figure-img"></p>
<figcaption><span class="math inline">\(t=15\)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/mazev30.jpg" class="img-fluid figure-img"></p>
<figcaption><span class="math inline">\(t=30\)</span></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-elephants-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.4: Value Iteration Solver
</figcaption>
</figure>
</div>
<p>The policy iterations converged after 24 iterations.</p>
</div>
<div id="exm-mdp-blackjack" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.5 (MDP for Forest Management)</strong></span> We can consider one of the classic examples of a Markov Decision Process (MDP). Imagine you need to calculate an optimal policy to manage a forest to prevent possible fires. The goal is to decide between two possible actions to either ‘Wait’ or ‘Cut’. They correspond to balancing between ecological preservation and economic gain, considering the random event of a fire. We can break down the elements of this model.</p>
<ol type="1">
<li><p><strong>States:</strong> Represent the age of the forest. The states are denoted as <span class="math inline">\(\{0, 1,\ldots, S-1\}\)</span>, where 0 is the youngest state (just after a fire or cutting) and <span class="math inline">\(S-1\)</span> is the oldest state of the forest.</p></li>
<li><p><strong>Actions:</strong> There are two actions available:</p>
<ul>
<li>‘Wait’ (Action 0): Do nothing and let the forest grow for another year.</li>
<li>‘Cut’ (Action 1): Harvest the forest, which brings immediate economic benefit but resets its state to the youngest.</li>
</ul></li>
<li><p><strong>Probabilities:</strong> There’s a probability ‘p’ each year that a fire occurs, regardless of the action taken. If a fire occurs, the forest returns to state 0.</p></li>
<li><p><strong>Transition Matrix (P):</strong> This matrix defines the probabilities of moving from one state to another, given a specific action.</p></li>
</ol>
<p>We will use <code>mdp_example_forest</code> function from the <code>MDPtoolbox</code> package to generate the transition probability matrix and reward matrix for the Forest example.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MDPtoolbox)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the transition and reward matrices for the Forest example</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>res <span class="ot">=</span> <span class="fu">mdp_example_forest</span>(<span class="at">S=</span><span class="dv">4</span>,<span class="at">r1=</span><span class="dv">10</span>,<span class="at">r2=</span><span class="dv">1</span>,<span class="at">p=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This function generates a transition probability <span class="math inline">\(P\)</span> of size <span class="math inline">\((|A| \times |S| \times |S|\)</span>, there are three states by default <span class="math inline">\(S = \{0,1,2\}\)</span> and two actions.</p>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>res<span class="sc">$</span>P[,,<span class="dv">1</span>]</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>res<span class="sc">$</span>P[,,<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-layout-align="center" data-layout-ncol="2" data-null_prefix="true">
<div class="quarto-layout-row">
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>     [,1] [,2] [,3] [,4]
[1,] 0.01 0.99 0.00 0.00
[2,] 0.01 0.00 0.99 0.00
[3,] 0.01 0.00 0.00 0.99
[4,] 0.01 0.00 0.00 0.99</code></pre>
</div>
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<pre><code>     [,1] [,2] [,3] [,4]
[1,]    1    0    0    0
[2,]    1    0    0    0
[3,]    1    0    0    0
[4,]    1    0    0    0</code></pre>
</div>
</div>
</div>
<p>As well as the reward matrix <span class="math inline">\(R\)</span> of size <span class="math inline">\(|S| \times |A|\)</span>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>res<span class="sc">$</span>R</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     R1 R2
[1,]  0  0
[2,]  0  1
[3,]  0  1
[4,] 10  1</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mdp_value_iteration</span>(res<span class="sc">$</span>P, res<span class="sc">$</span>R, <span class="at">discount =</span> <span class="fl">0.9</span>, <span class="at">epsilon =</span> <span class="fl">1e-6</span>, <span class="at">max_iter =</span> <span class="dv">10000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> "MDP Toolbox WARNING: max_iter is bounded by  5000"
 "MDP Toolbox: iterations stopped, epsilon-optimal policy found"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 13 21 30 40

$policy
[1] 1 1 1 1

$iter
[1] 5

$time
Time difference of 0.00096 secs

$epsilon
[1] 1e-06

$discount
[1] 0.9</code></pre>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mdp_policy_iteration</span>(res<span class="sc">$</span>P, res<span class="sc">$</span>R, <span class="at">discount=</span><span class="fl">0.9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  4.7  5.2  5.2 92.1

$policy
[1] 1 2 2 1

$iter
[1] 1

$time
Time difference of 0.011 secs</code></pre>
</div>
</div>
</div>
<p>A more general form of a value function is the action-value function <span class="math inline">\(Q^\pi(s,a)\)</span>, which represents the expected return when starting from state <span class="math inline">\(s\)</span>, taking action <span class="math inline">\(a\)</span>, and following policy <span class="math inline">\(\pi\)</span> thereafter. <span class="math display">\[
Q^\pi(s,a) = E_{\pi}\left[G_t \mid s_t = s, a_t = a\right].
\]</span> We can derive both the value and optimal policy functions from the action-value function: <span class="math display">\[
V^\pi(s) = \sum_{a \in A} \pi(a|s) Q^\pi(s,a)
\]</span> <span class="math display">\[
\pi^*(s) = \arg\max_a Q^*(s,a)
\]</span></p>
</section>
<section id="model-free-methods" class="level3">
<h3 class="anchored" data-anchor-id="model-free-methods">Model-Free Methods</h3>
<p>Both policy and value iterations we’ve considered thus far assume that transition probabilities between states given actions are known. However, this is often not the case in many real-world problems. Model-free methods learn through trial and error, by interacting with the environment and observing the rewards. The first method we consider is Monte Carlo methods. Monte Carlo methods for Markov Decision Processes (MDPs) are a class of algorithms used for finding optimal policies when the model of the environment (i.e., the transition probabilities and rewards) is unknown or too complex to model explicitly. These methods rely on learning from experience, specifically from complete episodes of interaction with the environment. Here’s a detailed look at how Monte Carlo methods work in the context of MDPs:</p>
<ol type="1">
<li><strong>Generate Episodes:</strong> An episode is a sequence of states, actions, and rewards, from the start state to a terminal state. <span class="math display">\[
S_0, A_0, R_1, S_1, A_1, R_2, \ldots, S_{T-1}, A_{T-1}, R_T \sim \pi.
\]</span> In Monte Carlo methods, these episodes are generated through actual or simulated interaction with the environment, based on a certain policy.</li>
<li><strong>Estimate Value Functions:</strong> Unlike dynamic programming methods, which update value estimates based on other estimated values, Monte Carlo methods update estimates based on actual returns received over complete episodes. This involves averaging the returns received after visits to each state. We use empirical mean to estimate the expected value.</li>
<li><strong>Policy Improvement:</strong> After a sufficient number of episodes have been generated and value functions estimated, the policy is improved based on these value function estimates.</li>
</ol>
<p>Monte Carlo methods require complete episodes to update value estimates. This makes them particularly suitable for episodic tasks, where interactions naturally break down into separate episodes with clear starting and ending points. MC methods require sufficient exploration of the state space. This can be achieved through various strategies, like <span class="math inline">\(\epsilon\)</span>-greedy policies, where there’s a small chance of taking a random action instead of the current best-known action. In this case, the policy is given by <span class="math display">\[
\pi(a \mid s) = \begin{cases}
1 - \epsilon + \frac{\epsilon}{|A|} &amp; \text{if } a = \arg\max_{a'} Q(s,a') \\
\frac{\epsilon}{|A|} &amp; \text{otherwise}
\end{cases}
\]</span> where <span class="math inline">\(\epsilon\)</span> is the probability of taking a random action and <span class="math inline">\(|A|\)</span> is the number of actions. The <span class="math inline">\(\epsilon\)</span>-greedy policy is an example of an exploration-exploitation strategy, where the agent explores the environment by taking random actions (exploration) while also exploiting the current knowledge of the environment by taking the best-known action (exploitation). The value of <span class="math inline">\(\epsilon\)</span> is typically decayed over time, so that the agent explores more in the beginning and exploits more later on.</p>
<p>Monte Carlo methods are model-free, meaning they do not require a model of the environment (transition probabilities and rewards). They are also effective in dealing with high variance in returns, which can be an issue in some environments. However, they can be inefficient due to high variance and the need for many episodes to achieve accurate value estimates. They also require careful handling of the exploration-exploitation trade-off. The two main approaches for Monte Carlo methods are first-visit and every-visit methods.</p>
<ol type="1">
<li>First-Visit MC: In this approach, the return for a state is averaged over all first visits to that state in each episode.</li>
<li>Every-Visit Monte Carlo: Here, the return is averaged over every visit to the state, not just the first visit in each episode.</li>
</ol>
<p>Monte Carlo Policy Iteration involves alternating between policy evaluation (estimating the value function of the current policy using Monte Carlo methods) and policy improvement (improving the policy based on the current value function estimate). This process is repeated until the policy converges to the optimal policy.</p>
<p>To find the optimal policy, a balance between exploration and exploitation must be maintained. This is achieved through strategies like <span class="math inline">\(\epsilon\)</span>-greedy exploration. In Monte Carlo Control, the policy is often improved in a greedy manner based on the current value function estimate.</p>
<p>Recall that an arithmetic average can be updated recursively <span class="math display">\[
\bar{x}_n = \frac{1}{n}\sum_{i=1}^n x_i = \frac{1}{n}\left(x_n + \sum_{i=1}^{n-1} x_i\right) = \frac{1}{n}\left(x_n + (n-1)\bar{x}_{n-1}\right) = \bar{x}_{n-1} + \frac{1}{n}(x_n - \bar{x}_{n-1}).
\]</span> This is called a running average. We can use this recursion to update the value function <span class="math inline">\(V(s)\)</span> incrementally, each time we visit state <span class="math inline">\(s\)</span> at time <span class="math inline">\(t\)</span>. <span class="math display">\[
V(s_t) = V(s_t) + \frac{1}{N(s_t)}(G_t - V(s_t)),
\]</span> where <span class="math inline">\(N(s_t)\)</span> is the number of times we visited state <span class="math inline">\(s_t\)</span> before time <span class="math inline">\(t\)</span> and <span class="math inline">\(G_t\)</span> is the return at time <span class="math inline">\(t\)</span>. This is called first-visit Monte Carlo method. Alternatively, we can use every-visit Monte Carlo method, where we update the value function each time we visit state <span class="math inline">\(s\)</span>.</p>
<p>Alternatively, we can use a learning rate <span class="math inline">\(\alpha\)</span> <span class="math display">\[
V_{n+1} = V_n + \alpha(G_n - V_n).
\]</span> This is called constant step size update. The learning rate is a hyperparameter that needs to be tuned. The constant step size update is more convenient because it does not require keeping track of the number of visits to each state. The constant step size update is also more robust to non-stationary problems.</p>
<p><strong>Temporal Difference Learning (TD Learning)</strong> Similar to MC, TD methods learn directly from raw experience without a model of the environment. However, unlike MC methods, TD methods update value estimates based on other learned estimates, without waiting for the end of an episode. This is called bootstrapping. TD methods combine the sampling efficiency of Monte Carlo methods with the low variance of dynamic programming methods. They are also model-free and can learn directly from raw experience. However, they are more complex than MC methods and require careful tuning of the learning rate.</p>
<p>A simple TD method is TD(0), which updates value estimates based on the current reward and the estimated value of the next state. The update rule is <span class="math display">\[
V(S_t) = V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1}) - V(S_t)),
\]</span> where <span class="math inline">\(\alpha\)</span> is the learning rate. The TD(0) method is also called one-step TD because it only looks one step ahead. The <span class="math inline">\(R_{t+1} + \gamma V(S_{t+1})\)</span> term is called the TD target and is a biased estimate of <span class="math inline">\(V(S_t)\)</span>. The difference <span class="math inline">\(R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\)</span> is called the TD error. The TD target is an estimate of the return <span class="math inline">\(G_t\)</span> and the TD error is the difference between the TD target and the current estimate <span class="math inline">\(V(S_t)\)</span>. Although TD algorithms have lower variance than MC methods, they have higher bias. In practice TD methods are more efficient than MC methods.</p>
</section>
<section id="q-learning" class="level3">
<h3 class="anchored" data-anchor-id="q-learning">Q-Learning</h3>
<p>Q-learning is an off-policy algorithm that learns the optimal policy by directly estimating the optimal action-value function <span class="math inline">\(Q^*(s,a)\)</span>. The algorithm iteratively updates the action-value function using the Bellman optimality backup. The off-policy means that the algorithm learns the optimal policy while following a different policy. The algorithm can learn the optimal policy while following a random policy, for example. The algorithm can be summarized as follows: <span class="math display">\[
Q(S_t,A_t) = Q(S_t,A_t) + \alpha(R_{t+1} + \gamma \max_a Q(S_{t+1},a) - Q(S_t,A_t)),
\]</span> where <span class="math inline">\(\alpha\)</span> is the learning rate. The algorithm can be summarized as follows:</p>
<ol type="1">
<li>Initialize <span class="math inline">\(Q(s,a)\)</span> arbitrarily</li>
<li>Repeat for each episode:
<ol type="1">
<li>Initialize <span class="math inline">\(S\)</span></li>
<li>Repeat for each step of the episode:
<ol type="1">
<li>Choose <span class="math inline">\(A\)</span> from <span class="math inline">\(S\)</span> using policy derived from <span class="math inline">\(Q\)</span> (e.g., <span class="math inline">\(\epsilon\)</span>-greedy)</li>
<li>Take action <span class="math inline">\(A\)</span>, observe <span class="math inline">\(R\)</span>, <span class="math inline">\(S'\)</span></li>
<li><span class="math inline">\(Q(S,A) = Q(S,A) + \alpha(R + \gamma \max_a Q(S',a) - Q(S,A))\)</span></li>
<li><span class="math inline">\(S = S'\)</span></li>
</ol></li>
<li>Until <span class="math inline">\(S\)</span> is terminal</li>
</ol></li>
</ol>
<p>Then we can simplify the update rule to <span class="math display">\[
Q(S_t,A_t) = (1-\alpha)Q(S_t,A_t) + \alpha(R_{t+1} + \gamma \max_a Q(S_{t+1},a)).
\]</span></p>
<div id="exm-deal" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.6 (Q-Learning and Deal or No Deal)</strong></span> Deal or No Deal is a popular TV show where a contestant is presented with a number of sealed boxes, each containing a prize. The contestant selects a box and then proceeds to open the remaining boxes one by one. After a certain number of boxes have been opened, the banker makes an offer to buy the contestant’s box. The contestant can either accept the offer and sell the box or reject the offer and continue opening boxes. The game continues until the contestant either accepts an offer or opens all the boxes. The goal is to maximize the expected value of the prize in the contestant’s box. The rule of thumb is to continue as long as there are two large prizes left. Continuation value is large. For example, with three prizes and two large ones, risk averse people will naively choose deal, when if they incorporated the continuation value they would choose no deal.</p>
<p>Let <span class="math inline">\(s\)</span> denote the current state of the system and <span class="math inline">\(a\)</span> an action. The <span class="math inline">\(Q\)</span>-value, <span class="math inline">\(Q_t(s,a)\)</span>, is the value of using action <span class="math inline">\(a\)</span> today and then proceeding optimally in the future. We use <span class="math inline">\(a=1\)</span> to mean no deal and <span class="math inline">\(a=0\)</span> means deal. The Bellman equation for <span class="math inline">\(Q\)</span>-values becomes <span class="math display">\[
Q_{t} ( s , a) = u( s , a  ) + \sum_{ s^\star } P( s^\star | s ,a ) \max_{ a } Q_{t+1} ( s^\star , a )
\]</span> where <span class="math inline">\(u(s,a)\)</span> is the immediate utility of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>. The value function and optimal action are given by <span class="math display">\[
V(s) = \max_a Q ( s , a ) \; \; \text{and} \; \;  a^\star = \text{argmax}  Q ( s , a )
\]</span></p>
<p><em>Transition Matrix</em>: Consider the problem where you have three prizes left. Now <span class="math inline">\(s\)</span> is the current state of three prizes. <span class="math display">\[
s^\star = \{ \text{all \; sets \; of \; two \; prizes} \} \; \; \text{and} \; \; P( s^\star | s, a =1) = \frac{1}{3}
\]</span> where the transition matrix is uniform to the next state. There’s no continuation for <span class="math inline">\(P( s^\star | s, a =0)\)</span>.</p>
<p><em>Utility</em>: The utility of the next state depends on the contestant’s value for money and the bidding function of the banker <span class="math display">\[
u( B ( s^\star ) ) = \frac{ B ( s^\star )^{1-\gamma} -1 }{1 - \gamma }
\]</span> in power utility case.</p>
<p>Expected value implies <span class="math inline">\(B( s ) = \bar{s}\)</span> where <span class="math inline">\(s\)</span> are the remaining prizes.</p>
<p>The website uses the following criteria: with three prizes left: <span class="math display">\[
B( s) = 0.305 \times \text{big} + 0.5 \times \text{small}
\]</span> and with two prizes left <span class="math display">\[
B( s) = 0.355 \times \text{big} + 0.5 \times \text{small}   
\]</span></p>
<p>Three prizes left: <span class="math inline">\(s = \{ 750 , 500 , 25 \}\)</span>.</p>
<p>Assume the contestant is risk averse with log-utility <span class="math inline">\(U(x) = \ln x\)</span>. Banker offers the expected value we get <span class="math display">\[
u( B( s = \{ 750 , 500 , 25 \}) ) = \ln ( 1275/3  ) = 6.052
\]</span> and so <span class="math inline">\(Q_t ( s , a= 0 ) = 6.052\)</span>.</p>
<p>In the continuation problem, <span class="math inline">\(s^\star = \{ s_1^\star , s_2^\star , s_3^\star  \}\)</span> where <span class="math inline">\(s_1^\star = \{750,500 \}\)</span> and <span class="math inline">\(s_2^\star = \{ 750,25 \}\)</span> and <span class="math inline">\(s_3^\star = \{ 500,25 \}\)</span>.</p>
<p>We’ll have offers <span class="math inline">\(625 , 387.5 , 137.5\)</span> under the expected value. As the banker offers expected value the optimal action at time <span class="math inline">\(t+1\)</span> is to take the deal <span class="math inline">\(a=0\)</span> with Q-values given by <span class="math display">\[\begin{align*}
Q_{t} ( s , a=1) &amp; = \sum_{ s^\star } P( s^\star | s ,a =1) \max_{ a } Q_{t+1} ( s^\star , a ) \\
&amp; = \frac{1}{3} \left (  \ln (625) + \ln (387.5) + \ln (262.5) \right ) = 5.989
\end{align*}\]</span> as immediate utility <span class="math inline">\(u( s,a ) = 0\)</span>. Hence as <span class="math display">\[
Q_{t} ( s , a=1)=5.989 &lt;  6.052 = Q_{t} ( s , a=0)
\]</span> the optimal action is <span class="math inline">\(a^\star = 0\)</span>, deal. Continuation value is not large enough to overcome the generous (expected value) offered by the banker.</p>
<p><em>Sensitivity analysis</em>: we perform it by assuming different Banker’s bidding function. If we use the function from the website (2 prizes): <span class="math display">\[
B( s) = 0.355 \times \text{big} + 0.5 \times \text{small},
\]</span> Hence <span class="math display">\[\begin{align*}
B( s_1^\star = \{750,500 \}) &amp;  = 516.25 \\
B( s_2^\star = \{ 750,25 \}) &amp; =  278.75 \\
B( s_3^\star = \{ 500,25 \}) &amp;  = 190  
\end{align*}\]</span></p>
<p>The optimal action with two prizes left for the contestant is <span class="math display">\[\begin{align*}
Q_{t+1} ( s_1^\star , a=1) &amp; = \frac{1}{2} \left (  \ln (750) + \ln (500) \right ) = 6.415 \\
&amp; &gt; 6.246 = Q_{t+1} ( s_1^\star , a=0) = \ln \left ( 516.25 \right ) \\
Q_{t+1} ( s_1^\star , a=1) &amp; = \frac{1}{2} \left (  \ln (750) + \ln (25) \right ) = 4.9194 \\
&amp; &lt; 5.63 = Q_{t+1} ( s_1^\star , a=0)  = \ln \left ( 278.75 \right ) \\
Q_{t+1} ( s_1^\star , a=1) &amp; = \frac{1}{2} \left (  \ln (500) + \ln (25) \right ) = 4.716 \\
&amp; &lt; 5.247 = Q_{t+1} ( s_1^\star , a=0)  = \left ( 516.25 \right ) \\
\end{align*}\]</span> Hence future optimal policy will be no deal under <span class="math inline">\(s_1^\star\)</span>, and deal under <span class="math inline">\(s_2^\star , s_3^\star\)</span>.</p>
<p>Therefore solving for <span class="math inline">\(Q\)</span>-values at the previous step gives <span class="math display">\[\begin{align*}
Q_{t} ( s , a=1) &amp; = \sum_{ s^\star } P( s^\star | s ,a =1) \max_{ a } Q_{t+1} ( s^\star , a ) \\
&amp; = \frac{1}{3} \left (  6.415+ 5.63 + 5.247 \right ) = 5.764
\end{align*}\]</span> with a monetary equivalent as <span class="math inline">\(\exp(5.764  ) = 318.62\)</span>.</p>
<p>With three prizes we have <span class="math display">\[\begin{align*}
Q_{t} ( s , a=0) &amp; = u( B( s = \{ 750 , 500 , 25 \}) ) \\
&amp; = \ln \left ( 0.305 \times 750 + 0.5 \times 25 \right ) \\
&amp; = \ln ( 241.25 ) = 5.48.
\end{align*}\]</span> The contestant is offered $ 241.</p>
<p>Now we have <span class="math inline">\(Q_{t} ( s , a=1)= 5.7079  &gt; 5.48 = Q_{t} ( s , a=0)\)</span> and the optimal action is <span class="math inline">\(a^\star = 1\)</span>, no deal. The continuation value is large. The premium is $ 241 compared to $319, a 33% premium.</p>
</div>
</section>
</section>
<section id="bayesian-optimization" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="bayesian-optimization"><span class="header-section-number">9.4</span> Bayesian Optimization</h2>
<p>Bayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. It is particularly useful when the objective function is expensive to evaluate. Bayesian optimization uses a surrogate model to approximate the objective function and an acquisition function to decide where to sample next. The surrogate model is typically a Gaussian process (GP) model, which is a probabilistic model that defines a distribution over functions. The acquisition function is a heuristic that trades off exploration and exploitation to decide where to sample next. Bayesian optimization is a global optimization method, meaning it does not require derivatives and can find the global optimum of the objective function. It is also sample-efficient, meaning it can find the optimum with fewer samples than other methods. However, it can be slow in practice and is not suitable for high-dimensional problems.</p>
<p>Given a function <span class="math inline">\(f(x)\)</span> that is not known analytically (it can represent, for example, output of a complex computer program), the goal is to optimize <span class="math display">\[
x^* = \arg\min_x f(x).
\]</span></p>
<p>The Bayesian approach to this problem is the following:</p>
<ol type="1">
<li>Define a prior distribution over <span class="math inline">\(f(x)\)</span></li>
<li>Calculate <span class="math inline">\(f\)</span> at a few points <span class="math inline">\(x_1, \ldots, x_n\)</span></li>
<li>Repeat until convergence:
<ol type="1">
<li>Update the prior to get the posterior distribution over <span class="math inline">\(f(x)\)</span></li>
<li>Choose the next point <span class="math inline">\(x^+\)</span> to evaluate <span class="math inline">\(f(x)\)</span></li>
<li>Calculate <span class="math inline">\(f(x^+)\)</span></li>
</ol></li>
<li>Pick <span class="math inline">\(x^*\)</span> that corresponds to the smallest value of <span class="math inline">\(f(x)\)</span> among evaluated points</li>
</ol>
<p>The prior distribution is typically a Gaussian process (GP) model, which is a probabilistic model that defines a distribution over functions. The GP model is defined by a mean function <span class="math inline">\(m(x)\)</span> and a covariance function <span class="math inline">\(k(x,x')\)</span>. The mean function is typically set to zero. The covariance function is typically a squared exponential function <span class="math display">\[
k(x,x') = \sigma_f^2 \exp\left(-\frac{(x-x')^2}{2l^2}\right),
\]</span> where <span class="math inline">\(\sigma_f^2\)</span> is the signal variance and <span class="math inline">\(l\)</span> is the length scale. The covariance function defines the similarity between two points <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span>. The covariance function is also called a kernel function. The kernel function is a measure of similarity between inputs <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span>.</p>
<p>Now we need to decide where to sample next. We can use the acquisition function to decide where to sample next. The acquisition function is a heuristic that trades off exploration and exploitation to decide where to sample next. The expected improvement (EI) function is a popular acquisition function. Suppose <span class="math display">\[
f^* = \min y
\]</span> is the minimum value of <span class="math inline">\(f(x)\)</span> among evaluated points. At a given point <span class="math inline">\(x\)</span> and function value <span class="math inline">\(y = f(x)\)</span>, the expected improvement function is defined as <span class="math display">\[
a(x) = \mathbb{E}\left[\max(0, f^* - y)\right],
\]</span> The function that we calculate expectation of <span class="math display">\[
u(x) = \max(0, f^* - y)
\]</span> is the utility function. Thus, the acquisition function is the expected value of the utility function.</p>
<p>The acquisition function is high when <span class="math inline">\(y\)</span> is likely to be lower than <span class="math inline">\(f^*\)</span>, and low when <span class="math inline">\(y\)</span> is likely to be higher than <span class="math inline">\(f^*\)</span>. Given the GP prior, we can calculate the acquisition function analytically. The posterior distribution of Normal <span class="math inline">\(y \sim N(\mu,\sigma^2)\)</span>, then the acquisition function is <span class="math display">\[\begin{align*}
a(x) &amp;= \mathbb{E}\left[\max(0, f^* - y)\right] \\
&amp;= \int_{-\infty}^{\infty} \max(0, f^* - y) \phi(y,\mu,\sigma^2) dy \\
&amp;= \int_{-\infty}^{f^*} (f^* - y) \phi(y,\mu,\sigma^2) dy
\end{align*}\]</span> where <span class="math inline">\(\phi(y,\mu,\sigma^2)\)</span> is the probability density function of the normal distribution. A useful identity is <span class="math display">\[
\int y \phi(y,\mu,\sigma^2) dy =\frac{1}{2} \mu ~  \text{erf}\left(\frac{y-\mu }{\sqrt{2} \sigma }\right)-\frac{\sigma
   e^{-\frac{(y-\mu )^2}{2 \sigma ^2}}}{\sqrt{2 \pi }},
\]</span> where <span class="math inline">\(\Phi(y,\mu,\sigma^2)\)</span> is the cumulative distribution function of the normal distribution. Thus, <span class="math display">\[
\int_{-\infty}^{f^*} y \phi(y,\mu,\sigma^2) dy = \frac{1}{2} \mu (1+\text{erf}\left(\frac{f^*-\mu }{\sqrt{2} \sigma
   }\right))-\frac{\sigma  e^{-\frac{(f^*-\mu )^2}{2 \sigma ^2}}}{\sqrt{2 \pi}} = \mu \Phi(f^*,\mu,\sigma^2) + \sigma^2 \phi(f^*,\mu,\sigma^2).
\]</span></p>
<p>we can write the acquisition function as <span class="math display">\[
a(x) = \dfrac{1}{2}\left(\sigma^2 \phi(f^*,\mu,\sigma^2) + (f^*-\mu)\Phi(f^*,\mu,\sigma^2)\right)
\]</span></p>
<p>We can implement it</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>acq <span class="ot">&lt;-</span> <span class="cf">function</span>(xx,p, fstar) {</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> <span class="fu">matrix</span>(xx, <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">=</span> fstar <span class="sc">-</span> p<span class="sc">$</span>mean</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>  s <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(p<span class="sc">$</span>Sigma))</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(s<span class="sc">*</span><span class="fu">dnorm</span>(d) <span class="sc">+</span> d<span class="sc">*</span><span class="fu">pnorm</span>(d))</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="exm-bayesopt" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.7 (Taxi Fleet Optimisation)</strong></span> We will use the taxi fleet simulator from <a href="https://github.com/amzn/emukit-playground">Emukit project</a>. For a given demand (the frequency of trip requests) and the number of taxis in the fleet, it simulates the taxi fleet operations and calculates the profit. The simulator is a black-box function, meaning it does not have an analytical form and can only be evaluated at specific points. The goal is to find the optimal number of taxis in the fleet that maximizes the profit. We will use Bayesian optimization to solve this problem.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/emukit-taxi.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Taxi Simulator Visualization</figcaption>
</figure>
</div>
<p>We start with initial set of three designs <span class="math inline">\(x = (10,30,90)\)</span>, where <span class="math inline">\(x\)</span> is the number of the taxis in the fleet and observe the corresponding profits profit=(3.1,3.6,6.6). When <span class="math inline">\(x=10\)</span>, the demand for taxis exceeds the supply and passengers need to wait for their rides, leading to missed profit opportunities. At another extreme when we have 90 taxis, the profit is slightly better. However, there are many empty taxis, which is not profitable. The optimal number of taxis must be somewhere in the middle. Finally, we try 30 taxis and observe that the profit is higher than both of our previous attempts. However, should we increase or decrease the number of taxis from here? We can use Bayesian optimization to answer this question. First we define a convenience function to plot the GP emulator.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>plotgp <span class="ot">=</span> <span class="cf">function</span>(x,y,XX,p) {</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  q1 <span class="ot">=</span> <span class="fu">qnorm</span>(<span class="fl">0.05</span>, <span class="at">mean =</span> p<span class="sc">$</span>mean, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(p<span class="sc">$</span>Sigma)))</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  q2 <span class="ot">=</span> <span class="fu">qnorm</span>(<span class="fl">0.95</span>, <span class="at">mean =</span> p<span class="sc">$</span>mean, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(p<span class="sc">$</span>Sigma)))</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>  q3 <span class="ot">=</span> <span class="fu">qnorm</span>(<span class="fl">0.5</span>, <span class="at">mean =</span> p<span class="sc">$</span>mean, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(p<span class="sc">$</span>Sigma)))</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>y)) <span class="sc">+</span> </span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>XX,<span class="at">y=</span>q3), <span class="at">col=</span><span class="dv">2</span>, <span class="at">lwd=</span><span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">x=</span>XX, <span class="at">ymin=</span>q1, <span class="at">ymax=</span>q2), <span class="at">fill=</span><span class="st">"blue"</span>, <span class="at">alpha=</span><span class="fl">0.2</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we fit the GP emulator using our initial set of observed taxi-profit pairs.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(laGP)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">10</span>,<span class="dv">90</span>,<span class="dv">30</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>xx <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="dv">100</span>, <span class="at">length=</span><span class="dv">500</span>)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>XX <span class="ot">&lt;-</span> <span class="fu">matrix</span>(xx, <span class="at">ncol =</span> <span class="fu">ncol</span>(x))</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>profit <span class="ot">=</span> <span class="sc">-</span><span class="fu">c</span>(<span class="fl">3.1</span>,<span class="fl">3.6</span>,<span class="fl">6.6</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>gp <span class="ot">&lt;-</span> <span class="fu">newGP</span>(x, profit, <span class="dv">1000</span>, <span class="fl">1e-6</span>, <span class="at">dK =</span> <span class="cn">TRUE</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">predGP</span>(gp, XX)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plotgp</span>(x,profit,XX,p)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/inittaxi-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Instead of maximizing the profit, we minimize the negative profit. We see that there is potentially a better value at around 50 taxis. We can use the acquisition function to decide where to sample next. We define two functions: <code>nextsample</code> that uses the acquisition function to decide where to sample next and <code>updgp</code> that updates the GP emulator with the new sample. Then we call those two functions twice. First time, EI suggests 44 and second time it suggests 42. We update the GP emulator with the new samples and plot the updated emulator. We see that the GP emulator is updated to reflect the new samples.</p>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>nextsample <span class="ot">=</span> <span class="cf">function</span>(){</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>  ei <span class="ot">=</span> <span class="fu">acq</span>(xx,p,<span class="fu">min</span>(profit))</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(xx,ei, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="dv">2</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>  xnext <span class="ot">=</span> <span class="fu">as.integer</span>(xx[<span class="fu">which.max</span>(ei)])</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(xnext)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>updgp <span class="ot">=</span> <span class="cf">function</span>(xnext,f){</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>  profit <span class="ot">&lt;&lt;-</span> <span class="fu">c</span>(profit, f)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;&lt;-</span> <span class="fu">c</span>(x, xnext)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">updateGP</span>(gp, <span class="fu">matrix</span>(xnext,<span class="at">ncol=</span><span class="dv">1</span>), f)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;&lt;-</span> <span class="fu">predGP</span>(gp, XX)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plotgp</span>(x,profit,XX,p)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="fu">nextsample</span>(); <span class="co">#44</span></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="fu">updgp</span>(<span class="dv">44</span>, <span class="sc">-</span><span class="fl">8.4</span>);</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="fu">nextsample</span>(); <span class="co"># 57</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="fu">updgp</span>(<span class="dv">57</span>, <span class="sc">-</span><span class="fl">7.1</span>);</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a><span class="fu">nextsample</span>(); <span class="co"># 45</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="fu">updgp</span>(<span class="dv">45</span>, <span class="sc">-</span><span class="fl">8.5</span>);</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="fu">nextsample</span>(); <span class="co"># 100</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a><span class="fu">updgp</span>(<span class="dv">100</span>, <span class="sc">-</span><span class="fl">3.3</span>);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-layout-align="center" data-layout="[[6,1,6], [6,1,6]]" data-null_prefix="true">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 46.2%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/updtaxi-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 7.7%;justify-content: center;">
<pre><code> 44</code></pre>
</div>
<div class="quarto-layout-cell" style="flex-basis: 46.2%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/updtaxi-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 46.2%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/updtaxi-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 7.7%;justify-content: center;">
<pre><code> 57</code></pre>
</div>
<div class="quarto-layout-cell" style="flex-basis: 46.2%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/updtaxi-4.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 46.2%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/updtaxi-5.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 7.7%;justify-content: center;">
<pre><code> 45</code></pre>
</div>
<div class="quarto-layout-cell" style="flex-basis: 46.2%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/updtaxi-6.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 46.2%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/updtaxi-7.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 7.7%;justify-content: center;">
<pre><code> 100</code></pre>
</div>
<div class="quarto-layout-cell" style="flex-basis: 46.2%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/updtaxi-8.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p>If we run <code>nextsample</code> one more time, we get 47, close to our current best of 45. Further, the model is confident at this location. It means that we can stop the algorithm and declare victory.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">nextsample</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="09-rl_files/figure-html/stopbayes-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 47</code></pre>
</div>
</div>
</div>
</section>
<section id="concluding-remarks" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="concluding-remarks"><span class="header-section-number">9.5</span> Concluding Remarks</h2>
<p>In this chapter, we explored the foundational concepts of Markov Decision Processes (MDPs) and their central role in reinforcement learning. We saw how MDPs provide a flexible mathematical framework for modeling sequential decision-making under uncertainty, with applications ranging from robotics and operations research to online recommendation systems and resource allocation in business.</p>
<p>Through both analytical derivations and Monte Carlo simulations, we examined classic problems such as the secretary problem and taxi fleet optimization, illustrating how simulation and Bayesian optimization can be used to make effective decisions in complex, uncertain environments. The use of Gaussian Process (GP) emulators and acquisition functions like Expected Improvement (EI) demonstrates the power of combining probabilistic modeling with principled exploration strategies—a hallmark of modern reinforcement learning.</p>
<p>As you continue your study of reinforcement learning, remember that the real world rarely presents us with simple, fully known models. The techniques introduced here—modeling uncertainty, simulating outcomes, and iteratively improving decisions—are essential tools for tackling the challenges of real-world AI and data-driven decision making. Whether you are optimizing ad placements, managing supply chains, or designing intelligent agents, the principles of MDPs and Bayesian optimization will serve as a strong foundation for your work.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-scott2015multiarmed" class="csl-entry" role="listitem">
Scott, Steven L. 2015. <span>“Multi-Armed Bandit Experiments in the Online Service Economy.”</span> <em>Applied Stochastic Models in Business and Industry</em> 31 (1): 37–45.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./08-gp.html" class="pagination-link" aria-label="Gaussian Processes">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./10-data.html" class="pagination-link" aria-label="Unreasonable Effectiveness of Data">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>