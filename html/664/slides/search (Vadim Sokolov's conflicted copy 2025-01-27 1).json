[
  {
    "objectID": "05-sp.html#definition",
    "href": "05-sp.html#definition",
    "title": "Bayes AI",
    "section": "Definition",
    "text": "Definition\n\nAn instance (realization) of a process is a function \\(Y:~ U \\rightarrow S\\)\nFrom domain of index set \\(U\\)\nTo process values \\(S\\), called state-space.\n\nThe process then is the distribution over the space of functions from \\(U\\) to \\(S\\)."
  },
  {
    "objectID": "05-sp.html#brownian-motion",
    "href": "05-sp.html#brownian-motion",
    "title": "Bayes AI",
    "section": "Brownian Motion",
    "text": "Brownian Motion\n\nBrownian Motion, named after botanist Robert Brown\nIs a fundamental concept in the theory of stochastic processes.\nIt describes the random motion of particles suspended in a fluid (liquid or gas), as they are bombarded by the fast-moving molecules in the fluid.\n\nA one-dimensional Brownian Motion (also known as Wiener process) is a continuous time stochastic process \\(B(t)_{t\\ge 0}\\) with the following properties\n\n\\(B(0) = 0\\) almost surely\n\\(B(t)\\) has stationary independent increments: \\(B(t) - B(s) \\sim N(0, t-s)\\) for \\(0 \\le s &lt; t\\)\n\\(B(t)\\) is continuous function of \\(t\\)\nFor each time \\(t &gt; 0\\), the random variable \\(B(t)\\) is normally distributed with mean 0 and variance \\(t\\), i.e., \\(B(t) \\sim N(0, t)\\)."
  },
  {
    "objectID": "05-sp.html#formal-definition",
    "href": "05-sp.html#formal-definition",
    "title": "Bayes AI",
    "section": "Formal Definition",
    "text": "Formal Definition\nFormally brownian motion is a stochastic process \\(B(t)\\) is a family of real random variables indexed by the set of nonnegative real numbers \\(t\\).\n\n\nCode\n# Brownian Motion\nset.seed(92)\nt = seq(0, 1, 0.001)\nplot(t, cumsum(rnorm(1001, 0, sqrt(0.001))), type=\"l\", xlab=\"t\", ylab=\"B(t)\", lwd=2, ylim=c(-1.2, 2))\nlines(t, cumsum(rnorm(1001, 0, sqrt(0.001))), lwd=2, col=2)\nlines(t, cumsum(rnorm(1001, 0, sqrt(0.001))),lwd=2, col=3)\n\n\n\n\nFigure 1: Brownian Motion\nThus, for any times \\(0 \\leq t_1 &lt; t_2 &lt; ... &lt; t_n\\), the random variables \\(B(t_2) - B(t_1)\\), \\(B(t_3) - B(t_2)\\), …, \\(B(t_n) - B(t_{n-1})\\) are independent and the function \\(t \\mapsto B(t)\\) is continuous almost surely."
  },
  {
    "objectID": "05-sp.html#mertons-jump-diffusion-model",
    "href": "05-sp.html#mertons-jump-diffusion-model",
    "title": "Bayes AI",
    "section": "Merton’s Jump Diffusion Model",
    "text": "Merton’s Jump Diffusion Model\n\nIntroduced jumps to the model.\nThe additive jump term addresses the issues, asymmetry, and heavy tails in the distribution.\nMerton’s Jump Stochastic volatility model has a discrete-time version for log-returns, \\(y_t\\), with jump times, \\(J_t\\), jump sizes, \\(Z_t\\), and spot stochastic volatility, \\(V_t\\), given by the dynamics \\[\\begin{align*}\n  y_{t} & \\equiv \\log \\left( S_{t}/S_{t-1}\\right) =\\mu + V_t \\varepsilon_{t}+J_{t}Z_{t} \\\\V_{t+1} & = \\alpha_v + \\beta_v V_t + \\sigma_v \\sqrt{V_t} \\varepsilon_{t}^v\n\\end{align*}\\] where \\(\\mathbb{P} \\left ( J_t =1 \\right ) = \\lambda\\), \\(S_t\\) denote a stock or asset price and log-returns \\(y^t\\) is the log-return. The errors \\((\\varepsilon_{t},\\varepsilon_{t}^v)\\) are possibly correlated bivariate normals.\n\nThe investor must obtain optimal filters for \\((V_t,J_t,Z_t)\\), and learn the posterior densities of the parameters \\((\\mu, \\alpha_v, \\beta_v, \\sigma_v^2 , \\lambda )\\). These estimates will be conditional on the information available at each time."
  },
  {
    "objectID": "05-sp.html#gaussian-processes",
    "href": "05-sp.html#gaussian-processes",
    "title": "Bayes AI",
    "section": "Gaussian Processes",
    "text": "Gaussian Processes\n\nA Gaussian Process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution.\nUsed for modeling and predicting, e.g. regression and classification tasks in machine learning.\n\\(n\\) points from Gaussian Process is completely specified by its \\(n\\)-dimensional mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\).\nIndex of the GP is a real number \\(x\\) and values are also real numbers."
  },
  {
    "objectID": "05-sp.html#need-to-define-mean-and-covariance-functions",
    "href": "05-sp.html#need-to-define-mean-and-covariance-functions",
    "title": "Bayes AI",
    "section": "Need to define mean and covariance functions",
    "text": "Need to define mean and covariance functions\n\nTypically: \\(U = \\mathbb{R^d}\\), \\(S = \\mathbb{R}\\)\nMean \\(m(x):~ U \\rightarrow S\\) and covariance is defined by function \\(k(x, x'): :~ U\\times U \\rightarrow S\\), \\(x,x' \\in U\\)\nThe mean function defines the average value of the function at point \\(x\\)\nCovariance function, also known as the kernel, defines the extent to which the values of the function at two points \\(x\\) and \\(x'\\) are correlated.\nTypical notation is \\[\nf(x) \\sim \\mathcal{GP}(m(x), k(x, x')).\n\\]\n\n\\(k(x, x') = \\mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\\) describes the amount of dependence between the values of the function at two different points in the input space."
  },
  {
    "objectID": "05-sp.html#need-to-define-mean-and-covariance-functions-1",
    "href": "05-sp.html#need-to-define-mean-and-covariance-functions-1",
    "title": "Bayes AI",
    "section": "Need to define mean and covariance functions",
    "text": "Need to define mean and covariance functions\n\nTypically the mean function is less important than the covariance function.\nMost of the time data scientists will use a zero mean function, \\(m(x)=0\\), and focus on the covariance function.\nThe kernel function is often chosen to be a function of the distance between the two points \\(d = \\|x-x'\\|_2\\).\nTypically kernel function peaks at \\(d=0\\) and decays as \\(d\\) increases."
  },
  {
    "objectID": "05-sp.html#squared-exponential",
    "href": "05-sp.html#squared-exponential",
    "title": "Bayes AI",
    "section": "Squared exponential",
    "text": "Squared exponential\n\nThe most commonly used kernel function is the squared exponential kernel \\[\nk(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\|x-x'\\|_2^2}{2l^2}\\right)\n\\]\n\\(\\sigma^2\\) is the variance, controls the vertical variation (amplitude)\n\\(l\\) is the length scale parameter, controls horizontal variation (number of “bumps” or smoothness)\n\n\\(k(x,x) = \\sigma^2\\) and \\(k(x,x') \\rightarrow 0\\) as \\(\\|x-x'\\|_2 \\rightarrow \\infty\\)."
  },
  {
    "objectID": "05-sp.html#gaussian-process-demo",
    "href": "05-sp.html#gaussian-process-demo",
    "title": "Bayes AI",
    "section": "Gaussian Process Demo",
    "text": "Gaussian Process Demo\nGenerating a sequence 100 inputs (process indexes)\n\nx = seq(0,10, length.out = 100)\n\nand then define the mean function and the covariance function\n\nmean = rep(0, length(x))\nsqexpcov = function(x, x1, l=1, sigma=1) {\n  exp(-0.5 * (x - x1)^2 / l^2) * sigma^2\n}\n\nThe covariance matrix is then defined as\n\n\nCode\ncov_mat = outer(x, x, sqexpcov)"
  },
  {
    "objectID": "05-sp.html#gaussian-process-demo-1",
    "href": "05-sp.html#gaussian-process-demo-1",
    "title": "Bayes AI",
    "section": "Gaussian Process Demo",
    "text": "Gaussian Process Demo\nGenerate a sample from the GP using the mvrnorm function from the MASS package and plot a sample\n\n\nCode\nlibrary(MASS)\nset.seed(17)\nY = mvrnorm(1, mean, cov_mat)\nplot(x, Y, type=\"l\", xlab=\"x\", ylab=\"y\", ylim=c(-1.5,2), lwd=2)\n\n\n\n\nFigure 2: Sample from a Gaussian Process\nA collection of 100 points of function \\(f(x)\\) sampled from a Gaussian Process with zero mean and squared exponential kernel for the set of 100 indexes \\(x =(0,0.1,0.2,\\ldots,10)\\)."
  },
  {
    "objectID": "05-sp.html#gaussian-process-demo-2",
    "href": "05-sp.html#gaussian-process-demo-2",
    "title": "Bayes AI",
    "section": "Gaussian Process Demo",
    "text": "Gaussian Process Demo\nLet’s generate a few more samples from the same GP and plot them together\n\n\nCode\nYs = mvrnorm(3, mean, cov_mat)\nmatplot(x, t(Ys), type=\"l\", ylab=\"Y\", lwd=5)\n\n\n\n\nFigure 3: Samples from a Gaussian Process"
  },
  {
    "objectID": "05-sp.html#making-predictions-with-gaussian-processes",
    "href": "05-sp.html#making-predictions-with-gaussian-processes",
    "title": "Bayes AI",
    "section": "Making Predictions with Gaussian Processes",
    "text": "Making Predictions with Gaussian Processes\n\nAssuming input indexes \\(X = (x_1,\\ldots,x_n)\\) and outputs \\(Y = (y_1,\\ldots,y_n)\\) are a realization of a Gaussian Process\nCan we predict at new inputs \\(x_* \\in \\mathbb{R}^q\\). The joint distribution of the observed data \\(Y\\) and the new data \\(y_*\\) is given by \\[\n\\begin{bmatrix} Y \\\\ y_* \\end{bmatrix} \\sim \\mathcal{N} \\left ( \\begin{bmatrix} \\mu \\\\ \\mu_* \\end{bmatrix}, \\begin{bmatrix} K & K_* \\\\ K_*^T & K_{**} \\end{bmatrix} \\right )\n\\] where \\(K = k(X, X)\\in \\mathbb{R}^{n\\times n}\\), \\(K_* = k(X, x_*)\\in \\mathbb{R}^{n\\times q}\\), \\(K_{**} = k(x_*, x_*) \\in \\mathbb{R}^{q\\times q}\\), \\(\\mu = \\mathbb{E}[Y]\\), and \\(\\mu_* = \\mathbb{E}[y_*]\\). The conditional distribution of \\(y_*\\) given \\(y\\) is then given by \\[\ny_* \\mid Y \\sim \\mathcal{N}(\\mu_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}}).\n% y_* \\mid Y \\sim \\mathcal{N}(\\mu_* + K_* K^{-1} (y - \\mu), K_{**} - K_*^T K^{-1} K_*).\n\\]"
  },
  {
    "objectID": "05-sp.html#making-predictions-with-gaussian-processes-1",
    "href": "05-sp.html#making-predictions-with-gaussian-processes-1",
    "title": "Bayes AI",
    "section": "Making Predictions with Gaussian Processes",
    "text": "Making Predictions with Gaussian Processes\nThe mean of the conditional distribution is given by \\[\n\\mu_{\\mathrm{post}} = \\mu_* + K_*^TK^{-1} (Y - \\mu)\n\\qquad(1)\\] and the covariance is given by \\[\n\\Sigma_{\\mathrm{post}} = K_{**} - K_*^T K^{-1} K_*.\n\\qquad(2)\\]\nEquation 1 and Equation 2 are convenient properties of a multivariate normal distribution."
  },
  {
    "objectID": "05-sp.html#gaussian-process-for-sin-function",
    "href": "05-sp.html#gaussian-process-for-sin-function",
    "title": "Bayes AI",
    "section": "Gaussian Process for \\(\\sin\\) function",
    "text": "Gaussian Process for \\(\\sin\\) function\n\nn = 8; eps=1e-6\nX = matrix(seq(0, 2*pi, length=n), ncol=1)\nY = sin(X)\nK = outer(X[,1],X[,1], sqexpcov) + diag(eps, n)\n\n\nThe additive term diag(eps, n) \\(=\\epsilon I\\) adds a diagonal matrix with the small \\(\\epsilon\\) on the diagonal.\n\nWhy?"
  },
  {
    "objectID": "05-sp.html#gaussian-process-for-sin-function-1",
    "href": "05-sp.html#gaussian-process-for-sin-function-1",
    "title": "Bayes AI",
    "section": "Gaussian Process for \\(\\sin\\) function",
    "text": "Gaussian Process for \\(\\sin\\) function\nNow we generate a new set of inputs \\(x_*\\) and calculate the covariance matrices \\(K_*\\) and \\(K_{**}\\).\n\nq = 100\nXX = matrix(seq(-0.5, 2*pi + 0.5, length=q), ncol=1)\nKX = outer(X[,1], XX[,1],sqexpcov)\nKXX = outer(XX[,1],XX[,1], sqexpcov) + diag(eps, q)\n\nNotice, we did not add \\(\\epsilon I\\) to \\(K_*\\) = KX matrix, but to add it to \\(K_{**}\\) = KXX to guarantee that the resulting posterior covariance matrix is non-singular (invert able).\n\nSi = solve(K)\nmup = t(KX) %*% Si %*% Y # we assume mu is 0\nSigmap = KXX - t(KX) %*% Si %*% KX"
  },
  {
    "objectID": "05-sp.html#gaussian-process-for-sin-function-2",
    "href": "05-sp.html#gaussian-process-for-sin-function-2",
    "title": "Bayes AI",
    "section": "Gaussian Process for \\(\\sin\\) function",
    "text": "Gaussian Process for \\(\\sin\\) function\nNow, we can generate a sample from the posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\nCode\nplot_gp = function(mup, Sigmap, X, Y, XX, YY){\n  q1 = mup + qnorm(0.05, 0, sqrt(diag(Sigmap)))\n  q2 = mup + qnorm(0.95, 0, sqrt(diag(Sigmap)))\n  matplot(XX, t(YY), type=\"l\", col=\"gray\", lty=1, xlab=\"x\", ylab=\"y\")\n  points(X, Y, pch=20, cex=2)\n  lines(XX, sin(XX), col=\"blue\")\n  lines(XX, mup, lwd=2)\n  lines(XX, q1, lwd=2, lty=2, col=2)\n  lines(XX, q2, lwd=2, lty=2, col=2)\n}\nYY = mvrnorm(100, mup, Sigmap)\nplot_gp(mup, Sigmap, X, Y, XX, YY)"
  },
  {
    "objectID": "05-sp.html#mle-for-gaussian-process",
    "href": "05-sp.html#mle-for-gaussian-process",
    "title": "Bayes AI",
    "section": "MLE for Gaussian Process",
    "text": "MLE for Gaussian Process\n\nUse the observed data to estimate those to parameters.\nIn the context of GP models, they are call hyper-parameters.\n\nLikelihood: \\[\np(Y \\mid X, \\sigma, l) = \\frac{1}{(2\\pi)^{n/2} |K|^{1/2}} \\exp \\left ( -\\frac{1}{2} Y^T K^{-1} Y \\right )\n\\] where \\(K = K(X,X)\\) is the covariance matrix. We assume mean is zero, to simplify the formulas. The log-likelihood is given by \\[\n\\log p(Y \\mid X, \\sigma, l) = -\\frac{1}{2} \\log |K| - \\frac{1}{2} Y^T K^{-1} Y - \\frac{n}{2} \\log 2\\pi.\n\\]"
  },
  {
    "objectID": "05-sp.html#mle-for-gaussian-process-1",
    "href": "05-sp.html#mle-for-gaussian-process-1",
    "title": "Bayes AI",
    "section": "MLE for Gaussian Process",
    "text": "MLE for Gaussian Process\nLet’s implement a function that calculates the log-likelihood of the data given the hyper-parameters \\(\\sigma\\) and \\(l\\) and use optim function to find the maximum of the log-likelihood function.\n\nloglik = function(par, X, Y) {\n  sigma = par[1]\n  l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  return(-(-0.5 * log(det(K)) - 0.5 * t(Y) %*% Si %*% Y - (n/2)* log(2*pi)))\n}\npar = optim(c(1,1), loglik, X=X, Y=Y)$par\nprint(par)\n\n[1] 1.502246 2.398773"
  },
  {
    "objectID": "05-sp.html#mle-for-gaussian-process-2",
    "href": "05-sp.html#mle-for-gaussian-process-2",
    "title": "Bayes AI",
    "section": "MLE for Gaussian Process",
    "text": "MLE for Gaussian Process\nMake predictions about the output values at new inputs \\(x_*\\).\n\n\nCode\nl = par[2]; sigma = par[1]\npredplot = function(X, Y, XX, YY, l, sigma) {\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  KX = outer(X[,1], XX[,1],sqexpcov,l,sigma)\n  KXX = outer(XX[,1],XX[,1], sqexpcov,l,sigma) + diag(eps, q)\n  Si = solve(K)\n  mup = t(KX) %*% Si %*% Y # we assume mu is 0\n  Sigmap = KXX - t(KX) %*% Si %*% KX\n  YY = mvrnorm(100, mup, Sigmap)\n  plot_gp(mup, Sigmap, X, Y, XX, YY)\n}\npredplot(X, Y, XX, YY, l, sigma)\n\n\n\nWe can see that our uncertainty is much “tighter”"
  },
  {
    "objectID": "05-sp.html#log-likelihood-derivative",
    "href": "05-sp.html#log-likelihood-derivative",
    "title": "Bayes AI",
    "section": "Log-likelihood Derivative",
    "text": "Log-likelihood Derivative\nSome matrix calculus: \\[\n\\frac{\\partial  Y^T K^{-1} Y}{\\partial \\theta} =  Y^T \\frac{\\partial K^{-1}}{\\partial \\theta} Y.\n\\] The derivative of the inverse matrix \\[\n\\frac{\\partial K^{-1}}{\\partial \\theta} = -K^{-1} \\frac{\\partial K}{\\partial \\theta} K^{-1}.\n\\] and the log of the determinant of a matrix \\[\n\\frac{\\partial \\log |K|}{\\partial \\theta} = \\mathrm{tr} \\left ( K^{-1} \\frac{\\partial K}{\\partial \\theta} \\right ),\n\\]"
  },
  {
    "objectID": "05-sp.html#log-likelihood-derivative-1",
    "href": "05-sp.html#log-likelihood-derivative-1",
    "title": "Bayes AI",
    "section": "Log-likelihood Derivative",
    "text": "Log-likelihood Derivative\nCalculate the derivative of the log-likelihood function with respect to \\(\\theta\\) \\[\n\\frac{\\partial \\log p(Y \\mid X,\\theta)}{\\partial \\theta} = -\\frac{1}{2}\\frac{\\partial \\log |K|}{\\partial \\theta}  + \\frac{1}{2} Y^T \\frac{\\partial K^{-1}}{\\partial \\theta}  Y.\n\\] Putting it all together, we get \\[\n\\frac{\\partial \\log p(Y \\mid X,\\theta)}{\\partial \\theta} = -\\frac{1}{2} \\mathrm{tr} \\left ( K^{-1} \\frac{\\partial K}{\\partial \\theta} \\right ) + \\frac{1}{2} Y^T K^{-1} \\frac{\\partial K}{\\partial \\theta} K^{-1} Y.\n\\]"
  },
  {
    "objectID": "05-sp.html#log-likelihood-derivative-for-squared-exponential-kernel",
    "href": "05-sp.html#log-likelihood-derivative-for-squared-exponential-kernel",
    "title": "Bayes AI",
    "section": "Log-likelihood Derivative for squared exponential kernel",
    "text": "Log-likelihood Derivative for squared exponential kernel\n\\[\nK_{ij} = k(x_i, x_j) = \\sigma^2 \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right ).\n\\] The derivative of the covariance matrix with respect to \\(\\sigma\\) is given by \\[\n\\frac{\\partial K_{ij}}{\\partial \\sigma} = 2\\sigma \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right );~\\frac{\\partial K}{\\partial \\sigma} = \\dfrac{2}{\\sigma}K.\n\\] The derivative of the covariance matrix with respect to \\(l\\) is given by \\[\n\\frac{\\partial K_{ij}}{\\partial l} = \\sigma^2 \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right ) \\frac{(x_i - x_j)^2}{l^3};~ \\frac{\\partial K}{\\partial l}  = \\frac{(x_i - x_j)^2}{l^3} K.\n\\]"
  },
  {
    "objectID": "05-sp.html#log-likelihood-derivative-2",
    "href": "05-sp.html#log-likelihood-derivative-2",
    "title": "Bayes AI",
    "section": "Log-likelihood Derivative",
    "text": "Log-likelihood Derivative\nNow we can implement a function that calculates the derivative of the log-likelihood function with respect to \\(\\sigma\\) and \\(l\\).\n\n# Derivative of the log-likelihood function with respect to sigma\ndloglik_sigma = function(par, X, Y) {\n  sigma = par[1]; l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  dK = 2*K/sigma\n  tr = sum(diag(Si %*% dK))\n  return(-(-0.5 * tr + 0.5 * t(Y) %*% Si %*% dK %*% Si %*% Y))\n}\n# Derivative of the log-likelihood function with respect to l\ndloglik_l = function(par, X, Y) {\n  sigma = par[1]; l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov ,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  dK =   outer(X[,1],X[,1], function(x, x1) (x - x1)^2)/l^3 * K\n  tr = sum(diag(Si %*% dK))\n  return(-(-0.5 * tr + 0.5 * t(Y) %*% Si %*% dK %*% Si %*% Y))\n}\n# Gradient function that returns a vector of derivatives\ngnlg = function(par,X,Y) {\n  return(c(dloglik_sigma(par, X, Y), dloglik_l(par, X, Y)))\n}"
  },
  {
    "objectID": "05-sp.html#log-likelihood-derivative-3",
    "href": "05-sp.html#log-likelihood-derivative-3",
    "title": "Bayes AI",
    "section": "Log-likelihood Derivative",
    "text": "Log-likelihood Derivative\nNow we can use the optim function to find the maximum of the log-likelihood function and provide the derivative function we just implemented.\n\npar1 = optim(c(1,1), fn=loglik, gr=gnlg ,X=X, Y=Y,method=\"BFGS\")$par\nl = par1[2]; sigma = par1[1]\nprint(par1)\n\n[1] 1.48443 2.39151\n\n\nThe result is the same compared to when we called optim without the derivative function. Even execution time is the same for our small problem. However, at larger scale, the derivative-based optimization algorithm will be much faster."
  },
  {
    "objectID": "05-sp.html#use-r-package-for-gaussian-processes",
    "href": "05-sp.html#use-r-package-for-gaussian-processes",
    "title": "Bayes AI",
    "section": "Use R Package for Gaussian Processes",
    "text": "Use R Package for Gaussian Processes\n\n\nCode\nlibrary(laGP)\ngp = newGP(X, Y, 1, 0, dK = TRUE)\nres = mleGP(gp, tmax=20)\nl.laGP = sqrt(res$d/2)\nprint(l.laGP)\n\n\n[1] 2.36025\n\n\n\nCode\npredplot(X, Y, XX, YY, l, sigma)\npredplot(X, Y, XX, YY, l.laGP, 1)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) MLE Fit\n\n\n\n\n\n\n\n\n\n\n\n(b) laGP Fit\n\n\n\n\n\n\n\nFigure 4: Posterior distribution over \\(y_*\\), given \\(Y\\)"
  },
  {
    "objectID": "05-sp.html#prediction",
    "href": "05-sp.html#prediction",
    "title": "Bayes AI",
    "section": "Prediction",
    "text": "Prediction\n\nCode\nXX1 = matrix(seq(-4*pi, 6*pi + 0.5, length=q), ncol=1)\npredplot(X, Y, XX1, YY, l, sigma)\npredplot(X, Y, XX1, YY, l.laGP, 1)\n\n\n\n\n\n\n\nMLE Fit\n\n\n\n\n\n\n\nlaGP Fit\n\n\n\n\n\n\nExtrapolation: Posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\n\nWe can see that outside of the range of the observed data, the model with \\(\\sigma=1\\) is more “confident” in its predictions."
  },
  {
    "objectID": "05-sp.html#gaussian-process-for-motorcycle-accident-data",
    "href": "05-sp.html#gaussian-process-for-motorcycle-accident-data",
    "title": "Bayes AI",
    "section": "Gaussian Process for Motorcycle Accident Data",
    "text": "Gaussian Process for Motorcycle Accident Data\nWe first estimate the length scale parameter \\(l\\) using the laGP package.\n\n\nCode\nlibrary(MASS)\nlibrary(laGP)\nX = mcycle$times\nY = mcycle$accel\ngp = newGP(matrix(X), Y, 2, 1e-6, dK = TRUE);\nmleGP(gp, tmax=10);\n\n\nNow we plot the data and the fit using the estimated length scale parameter \\(l\\).\n\n\nCode\nlibrary(ggplot2)\nXX = matrix(seq(2.4, 55, length = 499), ncol=1)\np = predGP(gp, XX)\nN = 499\nq1 = qnorm(0.05, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nq2 = qnorm(0.95, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nq3 = qnorm(0.5, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nggplot() + geom_point(aes(x=X,y=Y)) + geom_line(aes(x=XX,y=q3)) + geom_ribbon(aes(x=XX,ymin=q1, ymax=q2), alpha=0.2)\n\n\n\nMotorcycle Accident Data. Black line is the mean of the posterior distribution over \\(y_*\\), given \\(Y\\). Blue lines are the 95% confidence interval."
  },
  {
    "objectID": "05-sp.html#gaussian-process-for-motorcycle-accident-data-1",
    "href": "05-sp.html#gaussian-process-for-motorcycle-accident-data-1",
    "title": "Bayes AI",
    "section": "Gaussian Process for Motorcycle Accident Data",
    "text": "Gaussian Process for Motorcycle Accident Data\n\n\nCode\nhist(X)\n\n\n\nHistogram of time valuesThe \\(\\sqrt{n}\\) decay in variance of the posterior distribution is a property of the squared exponential kernel."
  },
  {
    "objectID": "05-sp.html#definition-1",
    "href": "05-sp.html#definition-1",
    "title": "Bayes AI",
    "section": "Definition",
    "text": "Definition\n\nBayesian optimization is a sequential design strategy for global optimization of black-box functions.\nIt is particularly well-suited for optimization of high-cost functions, situations where the number of function evaluations is limited, and noisy evaluations.\nThe goal is to find the global minimum of an unknown function \\(f(x)\\), where \\(x \\in \\mathbb{R}^d\\).\nThe function is assumed to be a black-box, i.e., we can evaluate it at any point \\(x\\) but we do not have access to its analytical form."
  },
  {
    "objectID": "05-sp.html#basic-idea",
    "href": "05-sp.html#basic-idea",
    "title": "Bayes AI",
    "section": "Basic Idea",
    "text": "Basic Idea\n\n\nCode\ngraph LR\n    NS[Decision on Next Samples]--Collect--&gt;S[System Under Study]\n    S--Observe--&gt;NS\n\n\n\n\n\ngraph LR\n    NS[Decision on Next Samples]--Collect--&gt;S[System Under Study]\n    S--Observe--&gt;NS\n\n\n\n\n\n\n\nMulti-Armed Bandits\nQ-Learning\nActive Learning\nReinforcement Learning\nBayesian Optimization"
  },
  {
    "objectID": "05-sp.html#bayesian-optimization",
    "href": "05-sp.html#bayesian-optimization",
    "title": "Bayes AI",
    "section": "Bayesian Optimization",
    "text": "Bayesian Optimization\nGiven a function \\(f(x)\\), that is not known analytically, it can represent, for example, output of a complex computer program. The goal is to optimize \\[\nx^* = \\arg\\min_x f(x).\n\\]\nThe Bayesian approach to this problem is the following:\n\nDefine a prior distribution over \\(f(x)\\)\nCalculate \\(f\\) at a few points \\(x_1, \\ldots, x_n\\)\nRepeat until convergence:\n\nUpdate the prior to get the posterior distribution over \\(f(x)\\)\nChoose the next point \\(x^+\\) to evaluate \\(f(x)\\)\nCalculate \\(f(x^+)\\)\n\nPick \\(x^*\\) that corresponds to the smallest value of \\(f(x)\\) among evaluated points"
  },
  {
    "objectID": "05-sp.html#where-to-sample-next",
    "href": "05-sp.html#where-to-sample-next",
    "title": "Bayes AI",
    "section": "Where to sample next?",
    "text": "Where to sample next?\nThe expected improvement (EI) acquisition function \\[\nf^* = \\min y\n\\] Our current best. At a given point \\(x\\) and function value \\(y = f(x)\\), the expected improvement function is defined as \\[\na(x) = \\mathbb{E}\\left[\\max(0, f^* - y)\\right],\n\\] The function that we calculate expectation of \\[\nu(x) = \\max(0, f^* - y)\n\\] is the utility function. Thus, the acquisition function is the expected value of the utility function."
  },
  {
    "objectID": "05-sp.html#acquisition-function",
    "href": "05-sp.html#acquisition-function",
    "title": "Bayes AI",
    "section": "Acquisition function",
    "text": "Acquisition function\n\nGiven GP approximation for \\(f\\), we can calculate the acquisition function analytically.\nThe posterior distribution of Normal \\(f(x)\\mid x \\sim N(\\mu,\\sigma^2)\\), then the acquisition function is \\[\\begin{align*}\na(x) &= \\mathbb{E}\\left[\\max(0, f^* - y)\\right] \\\\\n&= \\int_{-\\infty}^{\\infty} \\max(0, f^* - y) \\phi(y,\\mu,\\sigma^2) dy \\\\\n&= \\int_{-\\infty}^{f^*} (f^* - y) \\phi(y,\\mu,\\sigma^2) dy\n\\end{align*}\\] where \\(\\phi(y,\\mu,\\sigma^2)\\) is the probability density function of the normal distribution. A useful identity is"
  },
  {
    "objectID": "05-sp.html#acquisition-function-1",
    "href": "05-sp.html#acquisition-function-1",
    "title": "Bayes AI",
    "section": "Acquisition function",
    "text": "Acquisition function\n\\[\n\\int y \\phi(y,\\mu,\\sigma^2) dy =\\frac{1}{2} \\mu  \\text{erf}\\left(\\frac{y-\\mu }{\\sqrt{2} \\sigma }\\right)-\\frac{\\sigma\n   e^{-\\frac{(y-\\mu )^2}{2 \\sigma ^2}}}{\\sqrt{2 \\pi }},\n\\] where \\(\\Phi(y,\\mu,\\sigma^2)\\) is the cumulative distribution function of the normal distribution. Thus, \\[\n\\int_{-\\infty}^{f^*} y \\phi(y,\\mu,\\sigma^2) dy = \\frac{1}{2} \\mu (1+\\text{erf}\\left(\\frac{f^*-\\mu }{\\sqrt{2} \\sigma\n   }\\right))-\\frac{\\sigma  e^{-\\frac{(f^*-\\mu )^2}{2 \\sigma ^2}}}{\\sqrt{2 \\pi}} = \\mu \\Phi(f^*,\\mu,\\sigma^2) + \\sigma^2 \\phi(f^*,\\mu,\\sigma^2).\n\\]\nwe can write the acquisition function as \\[\na(x) = \\dfrac{1}{2}\\left(\\sigma^2 \\phi(f^*,\\mu,\\sigma^2) + (f^*-\\mu)\\Phi(f^*,\\mu,\\sigma^2)\\right)\n\\]"
  },
  {
    "objectID": "05-sp.html#acquisition-function-2",
    "href": "05-sp.html#acquisition-function-2",
    "title": "Bayes AI",
    "section": "Acquisition function",
    "text": "Acquisition function\nLet’s implement it\n\nacq &lt;- function(xx,p, fstar) {\n  x = matrix(xx, ncol=1)\n  d = fstar - p$mean\n  s = sqrt(diag(p$Sigma))\n  return(s*dnorm(d) + d*pnorm(d))\n}"
  },
  {
    "objectID": "05-sp.html#taxi-fleet-optimisation",
    "href": "05-sp.html#taxi-fleet-optimisation",
    "title": "Bayes AI",
    "section": "Taxi Fleet Optimisation",
    "text": "Taxi Fleet Optimisation\n\nBlack box: taxi fleet simulator from Emukit project.\nInput: fleet size\nSimulates the taxi fleet operations and calculates the profit.\n\n\nTaxi Simulator Visualization"
  },
  {
    "objectID": "05-sp.html#taxi-fleet-optimisation-1",
    "href": "05-sp.html#taxi-fleet-optimisation-1",
    "title": "Bayes AI",
    "section": "Taxi Fleet Optimisation",
    "text": "Taxi Fleet Optimisation\n\nWe start with initial set of three designs \\(x = (10,30,90)\\), where \\(x\\) is the number of the taxis in\nObserve profit=(3.1,3.6,6.6)\n\nDefine a helper function to plot the GP emulator\n\nplotgp = function(x,y,XX,p) {\n  q1 = qnorm(0.05, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  q2 = qnorm(0.95, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  q3 = qnorm(0.5, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  ggplot() + geom_point(aes(x=x,y=y)) + geom_line(aes(x=XX,y=q3)) + geom_ribbon(aes(x=XX,ymin=q1, ymax=q2), alpha=0.2)\n}"
  },
  {
    "objectID": "05-sp.html#fit-gp",
    "href": "05-sp.html#fit-gp",
    "title": "Bayes AI",
    "section": "Fit GP",
    "text": "Fit GP\n\nlibrary(laGP)\nlibrary(mvtnorm)\nx = matrix(c(10,90,30), ncol=1)\nxx = seq(1,100, length=500)\nXX &lt;- matrix(xx, ncol = ncol(x))\nprofit = -c(3.1,3.6,6.6)\ngp &lt;- newGP(x, profit, 1000, 1e-6, dK = TRUE)\np &lt;- predGP(gp, XX)"
  },
  {
    "objectID": "05-sp.html#plot-the-fit",
    "href": "05-sp.html#plot-the-fit",
    "title": "Bayes AI",
    "section": "Plot the Fit",
    "text": "Plot the Fit\n\n\nCode\nplotgp(x,profit,XX,p)\n\n\n\nthere is potentially a better value at around 50 taxis"
  },
  {
    "objectID": "05-sp.html#run-optimisation",
    "href": "05-sp.html#run-optimisation",
    "title": "Bayes AI",
    "section": "Run Optimisation",
    "text": "Run Optimisation\n\nnextsample = function(){\n  ei = acq(xx,p,min(profit))\n  plot(xx,ei, type='l')\n  xnext = as.integer(xx[which.max(ei)])\n  return(xnext)\n}\nupdgp = function(xnext,f){\n  profit &lt;&lt;- c(profit, f)\n  x &lt;&lt;- c(x, xnext)\n  updateGP(gp, matrix(xnext,ncol=1), f)\n  p &lt;&lt;- predGP(gp, XX)\n  plotgp(x,profit,XX,p)\n}"
  },
  {
    "objectID": "05-sp.html#run-optimisation-1",
    "href": "05-sp.html#run-optimisation-1",
    "title": "Bayes AI",
    "section": "Run Optimisation",
    "text": "Run Optimisation\n\nCode\nnextsample(); #44\nupdgp(44, -8.4);\n\n\n\n\n\n\n\n[1] 44"
  },
  {
    "objectID": "05-sp.html#run-optimisation-2",
    "href": "05-sp.html#run-optimisation-2",
    "title": "Bayes AI",
    "section": "Run Optimisation",
    "text": "Run Optimisation\n\nCode\nnextsample(); # 57\nupdgp(57, -7.1);\n\n\n\n\n\n\n\n[1] 57"
  },
  {
    "objectID": "05-sp.html#run-optimisation-3",
    "href": "05-sp.html#run-optimisation-3",
    "title": "Bayes AI",
    "section": "Run Optimisation",
    "text": "Run Optimisation\n\nCode\nnextsample(); # 45\nupdgp(45, -8.5);\n\n\n\n\n\n\n\n[1] 45"
  },
  {
    "objectID": "05-sp.html#run-optimisation-4",
    "href": "05-sp.html#run-optimisation-4",
    "title": "Bayes AI",
    "section": "Run Optimisation",
    "text": "Run Optimisation\n\nCode\nnextsample(); # 100\nupdgp(100, -3.3);\n\n\n\n\n\n\n\n[1] 100"
  },
  {
    "objectID": "05-sp.html#run-optimisation-stop",
    "href": "05-sp.html#run-optimisation-stop",
    "title": "Bayes AI",
    "section": "Run Optimisation: Stop",
    "text": "Run Optimisation: Stop\nIf we run nextsample one more time, we get 47, close to our current best of 45. Further, the model is confident at this location. It means that we can stop the algorithm and “declare a victory”\n\n\nCode\nnextsample()\n\n\n\n[1] 47"
  },
  {
    "objectID": "06-mcmc.html#mcmc-simulation",
    "href": "06-mcmc.html#mcmc-simulation",
    "title": "Bayes AI",
    "section": "MCMC Simulation",
    "text": "MCMC Simulation\nSuppose that \\(X \\sim F_X ( x )\\) and let \\(Y = g (X)\\).\nHow do we find \\(F_Y ( y )\\) and \\(f_Y ( y )\\) ?\n\nvon Neumann\n\nGiven a uniform \\(U\\), how do we find \\(X= g(U)\\)?\n\nIn the bivariate case \\((X,Y) \\rightarrow (U,V)\\).\n\nWe need to find \\(f_{(U,V)} ( u , v )\\) from \\(f_{X,Y}(x,y)\\)\n\nApplications: Simulation, MCMC and PF."
  },
  {
    "objectID": "06-mcmc.html#transformations",
    "href": "06-mcmc.html#transformations",
    "title": "Bayes AI",
    "section": "Transformations",
    "text": "Transformations\nThe cdf identity gives \\[\nF_Y ( y) = \\mathbb{P} ( Y \\leq y ) = \\mathbb{P} ( g( X) \\leq y )\n\\]\n\nHence if the function \\(g ( \\cdot )\\) is monotone we can invert to get\n\n\\[\nF_Y ( y ) = \\int_{ g( x) \\leq y } f_X ( x ) dx\n\\]\n\nIf \\(g\\) is increasing \\(F_Y ( y ) = P( X \\leq g^{-1} ( y ) ) = F_X ( g^{-1} ( y ) )\\)\n\nIf \\(g\\) is decreasing \\(F_Y ( y ) = P( X \\geq g^{-1} ( y ) ) = 1 - F_X ( g^{-1} ( y ) )\\)"
  },
  {
    "objectID": "06-mcmc.html#transformation-identity",
    "href": "06-mcmc.html#transformation-identity",
    "title": "Bayes AI",
    "section": "Transformation Identity",
    "text": "Transformation Identity\n\nTheorem 1: Let \\(X\\) have pdf \\(f_X ( x)\\) and let \\(Y=g(X)\\). Then if \\(g\\) is a monotone function we have\n\n\\[\nf_Y ( y) = f_X ( g^{-1} ( y ) ) \\left  | \\frac{ d}{dy}  g^{-1} ( y ) \\right |\n\\] There’s also a multivariate version of this that we’ll see later.\n\nSuppose \\(X\\) is a continuous rv, what’s the pdf for \\(Y = X^2\\)?\nLet \\(X \\sim N ( 0 ,1 )\\), what’s the pdf for \\(Y = X^2\\)?"
  },
  {
    "objectID": "06-mcmc.html#probability-integral-transform",
    "href": "06-mcmc.html#probability-integral-transform",
    "title": "Bayes AI",
    "section": "Probability Integral Transform",
    "text": "Probability Integral Transform\ntheorem Suppose that \\(U \\sim U[0,1]\\), then for any continuous distribution function \\(F\\), the random variable \\(X= F^{-1} (U)\\) has distribution function \\(F\\).\n\nRemember that for \\(u \\in [0,1]\\), \\(\\mathbb{P} \\left ( U \\leq u \\right ) = u\\), so we have\n\n\\[\n\\mathbb{P} \\left (X \\leq x \\right )= \\mathbb{P} \\left ( F^{-1} (U) \\leq x \\right )= \\mathbb{P} \\left ( U \\leq F(x) \\right )=F(x)\n\\] Hence, \\(X = F_X^{-1}(U)\\)."
  },
  {
    "objectID": "06-mcmc.html#normal",
    "href": "06-mcmc.html#normal",
    "title": "Bayes AI",
    "section": "Normal",
    "text": "Normal\nSometimes thare are short-cut formulas to generate random draws\nNormal \\(N(0,I_2)\\): \\(x_1,x_2\\) uniform on \\([0,1]\\) then \\[\n\\begin{aligned}\ny_1 = & \\sqrt{-2\\log x_1}\\cos(2\\pi x_2)\\\\\ny_2 = & \\sqrt{-2\\log x_1}\\sin(2\\pi x_2)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#simulation-and-transformations",
    "href": "06-mcmc.html#simulation-and-transformations",
    "title": "Bayes AI",
    "section": "Simulation and Transformations",
    "text": "Simulation and Transformations\nAn important application is how to transform multiple random variables?\n\nSuppose that we have random variables:\n\n\\[\n( X , Y ) \\sim  f_{ X , Y} ( x , y )\n\\] A transformation of interest given by: \\[\nU = g ( X , Y )\n\\; \\; {\\rm and} \\; \\;\nV = h ( X , Y )\n\\]\n\nThe problem is how to compute \\(f_{ U , V } ( u , v )\\) ? Jacobian\n\n\\[\nJ = \\frac{ \\partial ( x , y ) }{ \\partial ( u , v ) }  = \\left |  \\begin{array}{cc}\n\\frac{ \\partial x }{ \\partial u} & \\frac{ \\partial x }{ \\partial v} \\\\\n\\frac{ \\partial y }{ \\partial u} & \\frac{ \\partial y }{ \\partial v}\n\\end{array} \\right |\n\\]"
  },
  {
    "objectID": "06-mcmc.html#bivariate-change-of-variable",
    "href": "06-mcmc.html#bivariate-change-of-variable",
    "title": "Bayes AI",
    "section": "Bivariate Change of Variable",
    "text": "Bivariate Change of Variable\n\nTheorem: (change of variable)\n\n\\[\nf_{ U , V } ( u , v ) = f_{ X , Y} ( h_1 ( u , v )  , h_2 ( u , v ) )\n\\left |  \\frac{ \\partial ( x , y ) }{ \\partial ( u , v ) } \\right |\n\\] The last term is the Jacobian.\nThis can be calculated in two ways.\n\\[\n\\left |  \\frac{ \\partial ( x , y ) }{ \\partial ( u , v ) } \\right | =\n1 / \\left |  \\frac{ \\partial ( u , v ) }{ \\partial ( x , y ) } \\right |\n\\]\n\nSo we don’t always need the inverse transformation \\(( x , y ) = ( g^{-1} ( u , v )  , h^{-1} ( u , v ) )\\)"
  },
  {
    "objectID": "06-mcmc.html#inequalities-and-identities",
    "href": "06-mcmc.html#inequalities-and-identities",
    "title": "Bayes AI",
    "section": "Inequalities and Identities",
    "text": "Inequalities and Identities\n\nMarkov\n\n\\[\n\\mathbb{P} \\left ( g( X ) \\geq c \\right ) \\leq \\frac{ \\mathbb{E} ( g(X) ) }{c }\n\\; \\; {\\rm  where} \\; \\;   g( X) \\geq 0\n\\]\n\nChebyshev\n\n\\[\n\\mathbb{P} \\left ( | X - \\mu | \\geq c \\right ) \\leq \\frac{ Var(X) }{c^2 }\n\\]\n\nJensen\n\n\\[\n\\mathbb{E} \\left ( \\phi ( X ) \\right ) \\leq \\phi \\left ( \\mathbb{E}( X ) \\right )\n\\]\n\nCauchy-Schwarz \\[\ncorr (X,Y) \\leq 1\n\\]\n\nChebyshev follows from Markov. Mike Steele and Cauchy-Schwarz."
  },
  {
    "objectID": "06-mcmc.html#markov-inequality",
    "href": "06-mcmc.html#markov-inequality",
    "title": "Bayes AI",
    "section": "Markov Inequality",
    "text": "Markov Inequality\nLet \\(f\\) be non-decreasing \\[\n\\begin{aligned}\nP ( Z &gt; t ) &= P ( f(Z) \\geq f(t) ) \\\\\n& = E \\left (  \\mathbb{I}  ( f( Z) \\geq f(t )  ) \\right ) \\\\\n& \\leq E \\left (  \\mathbb{I}  ( f( Z) \\geq f(t ) )  \\frac{f(Z)}{f(t) }  \\right ) \\\\\n& =  E\\left  (  \\frac{f(Z)}{f(t) }  \\right )\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#concentration-inequalities",
    "href": "06-mcmc.html#concentration-inequalities",
    "title": "Bayes AI",
    "section": "Concentration Inequalities",
    "text": "Concentration Inequalities\nLaw of Large Numbers \\[\n\\lim_{ n \\rightarrow \\infty } \\mathbb{P} \\left ( | Z - E(Z) | &gt; n \\epsilon  \\right ) = 0 \\; \\; \\forall \\epsilon &gt; 0\n\\]\nCentral Limt Theorem (CLT) \\[\n\\lim_{ n \\rightarrow \\infty } \\mathbb{P} \\left ( n^{- 1/2} ( | Z - E(Z) |  ) &gt;  \\epsilon  \\right ) = \\Phi ( x )\n\\]\nPosterior Concentration"
  },
  {
    "objectID": "06-mcmc.html#hoeffding-and-bernstein",
    "href": "06-mcmc.html#hoeffding-and-bernstein",
    "title": "Bayes AI",
    "section": "Hoeffding and Bernstein",
    "text": "Hoeffding and Bernstein\nLet \\(Z= \\sum_{i=1}^n X_i\\).\nHoeffding \\[\nP ( Z &gt; E(Z) +  t ) \\leq \\exp \\left  ( - \\frac{ t^2}{2n} \\right )\n\\]\nBernstein \\[\nP ( Z &gt; E(Z) +  t ) \\leq \\exp  \\left ( - \\frac{ t^2}{ 2 ( Var(Z) + t/3 ) } \\right )\n\\] Large Deviations (Varadhan)"
  },
  {
    "objectID": "06-mcmc.html#special-distributions",
    "href": "06-mcmc.html#special-distributions",
    "title": "Bayes AI",
    "section": "Special Distributions",
    "text": "Special Distributions\nSee Common Distributions\n\nBernoulli and Binomial\nHypergeometric\nPoisson\nNegative Binomial\nNormal Distribution\nGamma Distribution\nBeta Distribution\nMultinomial Distribution\nBivariate Normal Distribution\nWishart Distribution\n\n\\(\\ldots\\)"
  },
  {
    "objectID": "06-mcmc.html#example-markov-dependence",
    "href": "06-mcmc.html#example-markov-dependence",
    "title": "Bayes AI",
    "section": "Example: Markov Dependence",
    "text": "Example: Markov Dependence\n\nWe can always factor a joint distribution as\n\n\\[\np( X_n , X_{n-1} , \\ldots , X_1 )  = p( X_n | X_{n-1} , \\ldots , X_1 ) \\ldots p( X_2 | X_1 ) p( X_1 )\n\\]\n\nA process has the Markov Property if\n\n\\[\np( X_n | X_{n-1} , \\ldots , X_1 ) = p( X_n | X_{n-1} )\n\\]\n\nOnly the current history matter when determining the probabilities."
  },
  {
    "objectID": "06-mcmc.html#a-real-world-probability-model-hidden-markov-models",
    "href": "06-mcmc.html#a-real-world-probability-model-hidden-markov-models",
    "title": "Bayes AI",
    "section": "A real world probability model: Hidden Markov Models",
    "text": "A real world probability model: Hidden Markov Models\nAre stock returns a random walk?\nHidden Markov Models (Baum-Welch, Viterbi)\n\nDaily returns on the SP500 stock market index.\n\nBuild a hidden Markov model to predict the ups and downs.\n\nSuppose that stock market returns on the next four days are \\(X_1 , \\ldots , X_4\\).\nLet’s empirical determine conditionals and marginals"
  },
  {
    "objectID": "06-mcmc.html#sp500-data",
    "href": "06-mcmc.html#sp500-data",
    "title": "Bayes AI",
    "section": "SP500 Data",
    "text": "SP500 Data\nMarginal and Bivariate Distributions\n\nEmpirically, what do we get? Daily returns from \\(1948-2007\\).\n\n\n\n\n\\(x\\)\nDown\nUp\n\n\n\n\n\\(P( X_i ) = x\\)\n0.474\n0.526\n\n\n\n\nFinding \\(p( X_2 | X_1 )\\) is twice as much computational effort: counting \\(UU,UD,DU,DD\\) transitions.\n\n\n\n\n\\(X_i\\)\nDown\nUp\n\n\n\n\n\\(X_{i-1} = Down\\)\n0.519\n0.481\n\n\n\\(X_{i-1} = Up\\)\n0.433\n0.567"
  },
  {
    "objectID": "06-mcmc.html#conditioned-on-two-days",
    "href": "06-mcmc.html#conditioned-on-two-days",
    "title": "Bayes AI",
    "section": "Conditioned on two days",
    "text": "Conditioned on two days\n\nLet’s do \\(p( X_3 | X_2 , X_1 )\\)\n\n\n\n\n\\(X_{i-2}\\)\n\\(X_{i-1}\\)\nDown\nUp\n\n\n\n\nDown\nDown\n0.501\n0.499\n\n\nDown\nUp\n0.412\n0.588\n\n\nUp\nDown\n0.539\n0.461\n\n\nUp\nUp\n0.449\n0.551\n\n\n\n\nWe could do the distribution \\(p( X_2 , X_3 | X_1 )\\). This is a joint, marginal and conditional distribution all at the same time.\n\nJoint because more than one variable \\(( X_2 , X_3 )\\), marginal because it ignores \\(X_4\\) and conditional because its given \\(X_1\\)."
  },
  {
    "objectID": "06-mcmc.html#joint-probabilities",
    "href": "06-mcmc.html#joint-probabilities",
    "title": "Bayes AI",
    "section": "Joint Probabilities",
    "text": "Joint Probabilities\n\nUnder Markov dependence \\[\n\\begin{aligned}\nP( UUD ) & = p( X_1 = U) p( X_2 = U | X_1 = U) p( X_3 | X_2 = U , X_1 = U ) \\\\\n& = ( 0.526 ) ( 0.567 ) ( 0.433)\n\\end{aligned}\n\\]\nUnder independence we would have got \\[\n\\begin{aligned}\nP(UUD) & = P( X_1 = U) p( X_2 = U) p( X_3 = D ) \\\\\n& = (.526)(.526)(.474) \\\\\n& = 0.131\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#markov-chain-monte-carlo",
    "href": "06-mcmc.html#markov-chain-monte-carlo",
    "title": "Bayes AI",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\n\nThink od shuffling problem\nThere are 52! possible permutations of this deck.\nNaive approach: generate a vector with all possible permutations and draw an element of this vector with probability 1/52!.\n\n\n\\(52! \\approx 10^{68}\\)\n\nslightly less then number of particles in the observed universe (\\(10^{80}\\))."
  },
  {
    "objectID": "06-mcmc.html#bayes-rule",
    "href": "06-mcmc.html#bayes-rule",
    "title": "Bayes AI",
    "section": "Bayes rule",
    "text": "Bayes rule\n\\[\np(\\theta \\mid X,Y) = \\dfrac{p(Y \\mid \\theta,X)p(\\theta)}{\\int p(Y \\mid \\theta,X)p(\\theta)d\\theta}.\n\\]\n\nMCMC algorithms generate a Markov chain \\[\n\\left\\{ \\theta ^{\\left( g\\right)}\\right\\} _{g=1}^{G}\n\\] whose stationary distribution is \\(p\\left( \\theta \\mid X,Y\\right)\\). Thus, the key to Bayesian inference is simulation rather than optimization.\n\n\\[\n\\begin{split}\n\\widehat{E}\\left( f\\left( \\theta\\right)  \\mid X,Y\\right)&=G^{-1}\\sum_{g=1}^{G}f\\left( \\theta ^{\\left( g\\right) }\\right)\\\\\n& \\approx \\int f\\left( \\theta\\right) p\\left( \\theta \\mid X,Y\\right)d\\theta=E\\left( f\\left( \\theta\\right)  \\mid X,Y\\right).\n\\end{split}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#mcmc-for-discrete-random-variables",
    "href": "06-mcmc.html#mcmc-for-discrete-random-variables",
    "title": "Bayes AI",
    "section": "MCMC for Discrete Random Variables",
    "text": "MCMC for Discrete Random Variables\n\\[\nP(X_{k+1} = i \\mid X_k = j) = p_{ij}.\n\\]\n\nTransition matrix \\(P = \\{p_{ij}\\}_{i,j=1}^n\\) is column stochastic, i.e. \\(p_{ij} \\ge 0\\) and \\(\\sum_j p_{ij} = 1\\).\nLet \\(\\pi_k \\in R^n\\) be the distribution of the random variable \\(X_k\\) at time \\(k\\), \\(\\pi_{ki} = P(X_k=i)\\)\n\\(\\pi_{k+1} = P\\pi_k\\).\nThe limiting distribution is \\(\\pi = P\\pi\\), i.e. \\(\\pi\\) is an eigenvector of \\(P\\) with eigenvalue 1.\nThe limiting distribution is unique and it is the first eigenvector of \\(P\\). The limiting distribution is also called a stationary distribution."
  },
  {
    "objectID": "06-mcmc.html#mcmc-for-discrete-random-variables-1",
    "href": "06-mcmc.html#mcmc-for-discrete-random-variables-1",
    "title": "Bayes AI",
    "section": "MCMC for Discrete Random Variables",
    "text": "MCMC for Discrete Random Variables\n\nPower method \\(\\pi^{k+1} = P^k\\pi^0\\) to \\(\\pi\\) (Google’s PageRank).\nPerron-Frobenius theorem: if \\(p_{ij}&gt;0\\) then \\(\\pi\\) always exists and it is unique and \\[\n\\Vert \\pi^k - \\pi\\Vert \\le c |\\lambda_2|^k,\n\\]\n\\(\\lambda_2\\) is the second largest eigenvalue of \\(P\\) (the first eigenvalue is always 1) - \\(\\pi\\) as the average time of visiting vertex \\(i\\) under random walk.\n\nThe mixing time of the Markov chain is given by \\[\nT=\\dfrac{1}{\\log(1/\\lambda_2)}\n\\] It is roughly, number of steps over which deviation from equilibrium distribution decreases by factor \\(e\\)."
  },
  {
    "objectID": "06-mcmc.html#three-state-example",
    "href": "06-mcmc.html#three-state-example",
    "title": "Bayes AI",
    "section": "Three-State Example",
    "text": "Three-State Example\n\n\nCode\ngraph LR\n    1 --0.5--&gt; 1\n    1 --0.25--&gt; 2\n    1 --0.25--&gt; 3\n    2 --0.2--&gt; 1\n    2 --0.1--&gt; 2\n    2 --0.7--&gt; 3\n    3 --0.25--&gt; 2\n    3 --0.25--&gt; 1\n    3 --0.5--&gt; 3\n\n\n\n\n\ngraph LR\n    1 --0.5--&gt; 1\n    1 --0.25--&gt; 2\n    1 --0.25--&gt; 3\n    2 --0.2--&gt; 1\n    2 --0.1--&gt; 2\n    2 --0.7--&gt; 3\n    3 --0.25--&gt; 2\n    3 --0.25--&gt; 1\n    3 --0.5--&gt; 3\n\n\n\n\n\n\n\n P &lt;- rbind(c(0.50, 0.2, 0.25),\n            c(0.25, 0.1, 0.25),\n            c(0.25, 0.7, 0.50))\n\nCheck that it is column-stochastic\n\n\nCode\n colSums(P)\n\n\n[1] 1 1 1"
  },
  {
    "objectID": "06-mcmc.html#power-iterations",
    "href": "06-mcmc.html#power-iterations",
    "title": "Bayes AI",
    "section": "Power Iterations",
    "text": "Power Iterations\n\n# Power iterations\niterate.P &lt;- function(x, P, n) {\n    res &lt;- matrix(NA, n+1, length(x))\n    res[1,] &lt;- x\n    for (i in 1:n)\n        res[i+1,] &lt;- x &lt;- P %*% x\n    res\n}\n\nLet’s start with a random vector \\(x = (1, 0, 0)\\) and iterate \\(x \\leftarrow P x\\) for \\(n=10\\) steps.\n\nn  = 10\ny = iterate.P(c(1, 0, 0), P, n)"
  },
  {
    "objectID": "06-mcmc.html#power-iterations-1",
    "href": "06-mcmc.html#power-iterations-1",
    "title": "Bayes AI",
    "section": "Power Iterations",
    "text": "Power Iterations\nWe can compare the results to the eigenvector calculated using built-in (QR decomposition-based) method\n\n\nCode\nv &lt;- eigen(P, FALSE)$vectors[,1]\nv &lt;- v/sum(v) # normalize eigenvector\nknitr::kable(cbind(v, y[n+1,]), col.names=c(\"Eigenvector\", \"Power iterations\"), digits=3)\n\n\n\n\n\nEigenvector\nPower iterations\n\n\n\n\n0.32\n0.32\n\n\n0.22\n0.22\n\n\n0.46\n0.46\n\n\n\nPower iterations convergence to eigenvector. Each color corresponds to a component of the eigenvector (stationary distribution vector)"
  },
  {
    "objectID": "06-mcmc.html#power-iterations-2",
    "href": "06-mcmc.html#power-iterations-2",
    "title": "Bayes AI",
    "section": "Power Iterations",
    "text": "Power Iterations\n\n\nCode\nmatplot(0:n, y, type=\"l\", lty=1, xlab=\"Step\", ylab=\"y\", las=1)\nabline(h=v, lty=2, col=1:3)"
  },
  {
    "objectID": "06-mcmc.html#power-iterations-3",
    "href": "06-mcmc.html#power-iterations-3",
    "title": "Bayes AI",
    "section": "Power Iterations",
    "text": "Power Iterations\nWe can also find the stationary distribution by doing a simple random walk on the graph.\n\nrun &lt;- function(i, P, n) {\n  res &lt;- integer(n)\n  for (t in seq_len(n))\n  res[[t]] &lt;- i &lt;- sample(ncol(P), 1, pr=P[,i])\n  res\n}\nsamples &lt;- run(1, P, 300)"
  },
  {
    "objectID": "06-mcmc.html#power-iterations-4",
    "href": "06-mcmc.html#power-iterations-4",
    "title": "Bayes AI",
    "section": "Power Iterations",
    "text": "Power Iterations\nNow, we plot the fraction of time that we were in each state over time:\n\n\nCode\ncummean &lt;- function(x) cumsum(x) / seq_along(x)\nplot(cummean(samples == 1), type=\"l\", ylim=c(0, 1), xlab=\"Step\", ylab=\"y\", las=1)\nlines(cummean(samples == 2), col=2)\nlines(cummean(samples == 3), col=3)\nabline(h=v, lty=2, col=1:3)"
  },
  {
    "objectID": "06-mcmc.html#english-alphabet",
    "href": "06-mcmc.html#english-alphabet",
    "title": "Bayes AI",
    "section": "English Alphabet",
    "text": "English Alphabet\n\n\nCode\nlibrary(jsonlite)\n# http://norvig.com/mayzner.html\nbg = read_json(\"data/bigrams.json\", simplifyVector=T)\nnbg = nrow(bg)\nbgm = matrix(0,nrow=26,ncol=26)\nrownames(bgm) = letters; colnames(bgm) = letters\nfor (i in 1:nbg) { # from j to i \n  idx = match(unlist(strsplit(bg[i,1], split=\"\")), letters)\n  bgm[idx[2],idx[1]] = as.numeric(bg[i,2])\n}\n# View(bgm)\nbgm = bgm %*% diag(1/colSums(bgm))\n\nimage(bgm, axes=FALSE)\naxis(3, at=seq(0,1, length=26), labels=letters)\naxis(2, at=seq(1,0, length=26), labels=letters)\n\n\n\nCode\nev = eigen(bgm)\nv = ev$vectors[,1]\nv &lt;- as.numeric(v/sum(v))"
  },
  {
    "objectID": "06-mcmc.html#english-alphabet-1",
    "href": "06-mcmc.html#english-alphabet-1",
    "title": "Bayes AI",
    "section": "English Alphabet",
    "text": "English Alphabet\n\nCode\nbarplot(v, names.arg = letters, ylim=c(0,0.13))\nlf = read.csv(\"data/letterfreq.txt\")\nlf$freq = lf$freq/sum(lf$freq)\nlf = lf[order(lf$letter),]\nbarplot(lf$freq, names.arg = letters, ylim=c(0,0.13))\n\nbarplot(bgm[,5], names.arg = letters)\n\n\n\n\n\n\n\nFrom Bigrams\n\n\n\n\n\n\n\nFrom Google\n\n\n\n\n\n\n\nTransitions from letter e\n\n\n\n\n\n\nIndividual Letter Frequencies"
  },
  {
    "objectID": "06-mcmc.html#how-to-construct-transition-probabilities",
    "href": "06-mcmc.html#how-to-construct-transition-probabilities",
    "title": "Bayes AI",
    "section": "How to Construct Transition Probabilities?",
    "text": "How to Construct Transition Probabilities?\n\nThe reverse problem, is how to construct the transition probabilities so that the limiting distribution is \\(\\pi\\).\nFor example, it is easy to see that any row-stochastic matrix which is symmetric has a uniform limiting distribution, i.e. \\(\\pi = e\\), \\(e_i = 1/n\\), where \\(n\\) is the number of vertices.\n\\(e\\) is an eigenvector of \\(P^T\\), since \\(P^T\\) is row-stochastic, thus sum of each row equals to 1, i.e. \\(P^T e = e\\).\n\nGeneral case balance condition: \\[\n\\pi_i p_{ij} = \\pi_j p_{ji}, ~ i,j=1,\\ldots,n,\n\\] from which follows that \\[\n\\sum_i p_{ij}\\pi_i = \\pi_j \\sum_i p_{ji} = \\pi_j,~j=1,\\ldots,n.\n\\]"
  },
  {
    "objectID": "06-mcmc.html#how-to-construct-transition-probabilities-1",
    "href": "06-mcmc.html#how-to-construct-transition-probabilities-1",
    "title": "Bayes AI",
    "section": "How to Construct Transition Probabilities?",
    "text": "How to Construct Transition Probabilities?\nThus we need to find a Markov chain in which the transition probabilities satisfy the detailed balance condition (symmetry). Another way to write the condition is \\[\n\\dfrac{p_{ij}}{p_{ji}} = \\dfrac{\\pi_j}{\\pi_i}.\n\\] Say we have some “teaser” transition probabilities \\(p_{ij}^0\\) and we assume those are symmetric. We want to find new probabilities \\(p_{ij} = p_{ij}^0b_{ij}, ~ i\\ne j\\) and \\(p_{ii} = 1 - \\sum_{j:~j\\ne i} p_{ij}\\). We need to choose \\(b_{ij}\\) is such a way so that \\[\n\\dfrac{p_{ij}}{p_{ji}} = \\dfrac{p_{ij}^0b_{ij}}{p_{ji}^0b_{ji}} = \\dfrac{b_{ij}}{b_{ji}} = \\dfrac{\\pi_j}{\\pi_i}\n\\] We can choose \\[\nb_{ij} = F\\left(\\dfrac{\\pi_j}{\\pi_i}\\right)\n\\]\nFor the detailed balance condition to be satisfied, we need to choose \\(F: R_+ \\rightarrow [0,1]\\) such that \\[\n\\dfrac{F(z)}{F(1/z)} = z.\n\\]"
  },
  {
    "objectID": "06-mcmc.html#how-to-construct-transition-probabilities-2",
    "href": "06-mcmc.html#how-to-construct-transition-probabilities-2",
    "title": "Bayes AI",
    "section": "How to Construct Transition Probabilities?",
    "text": "How to Construct Transition Probabilities?\nAn example of such a function is \\(F(z) = \\min(z,1)\\). This leads to the Metropolis algorithm\n\nWhen at state \\(i\\), draw from \\(p^{0}_{ij}\\) to generate next state \\(j\\)\nCalculate \\(a_{ij} = \\min(1,\\pi_i/\\pi_j)\\) (acceptance probability)\nMove to \\(j\\) with probability \\(a_{ij}\\), stay at \\(i\\) otherwise\n\nWhen move happens, we say step is accepted, otherwise it is rejected. In a more general case (Metropolis-Hastings), when \\(p^0\\) is not symmetric, we calculate \\[\na_{ij} = \\min\\left(1,\\dfrac{\\pi_i p^0_{ji}}{\\pi_j p^0_{ji}}\\right).\n\\]"
  },
  {
    "objectID": "06-mcmc.html#continious-case",
    "href": "06-mcmc.html#continious-case",
    "title": "Bayes AI",
    "section": "Continious Case",
    "text": "Continious Case\nIn the continuous case we use variables \\(S\\) and \\(T\\) instead induces \\(i\\) and \\(j\\). The detailed balance condition becomes \\[\n\\pi(S)p(T \\mid S) = \\pi(T)p(S \\mid T).\n\\] Then \\[\n\\int p(T \\mid S)\\pi(S)dS = \\int p(S \\mid T)\\pi(T)dS = \\pi(T)\\int p(S\\mid T)dS = \\pi(T).\n\\]\nThe following conditions \\[\n\\forall S, \\forall T:\\pi(T)\\ne 0:p(T \\mid S) &gt;0\n\\] are sufficient to guarantee uniqueness of \\(\\pi(T)\\)."
  },
  {
    "objectID": "06-mcmc.html#metropolis-hastings-algorithm",
    "href": "06-mcmc.html#metropolis-hastings-algorithm",
    "title": "Bayes AI",
    "section": "Metropolis-Hastings Algorithm",
    "text": "Metropolis-Hastings Algorithm\nSpecifically, the MH algorithm repeats the following two steps \\(G\\) times: given \\(\\theta ^{\\left( g\\right) }\\) \\[\n\\begin{aligned}   \n&\\text{Step 1. Draw }\\theta^{\\prime} \\text{ from a proposal distribution,} p(\\theta^{\\prime}|\\theta ^{(g)}) \\\\  \n&\\text{Step 2. Accept } \\theta^{\\prime} \\text{ with probability } \\alpha \\left(\\theta ^{(g)},\\theta^{\\prime}\\right) \\text{,}\n\\end{aligned}\n\\] where \\[\n\\alpha \\left( \\theta ^{(g)},\\theta^{\\prime}\\right) =\\min \\left(     \\frac{\\pi(\\theta^{\\prime})}{\\pi (\\theta ^{(g)})}\\frac{q(\\theta^{(g)}|\\theta^{\\prime})}{q(\\theta^{\\prime}|\\theta ^{(g)})},1\\right) \\text{.}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#metropolis-hastings-algorithm-1",
    "href": "06-mcmc.html#metropolis-hastings-algorithm-1",
    "title": "Bayes AI",
    "section": "Metropolis-Hastings Algorithm",
    "text": "Metropolis-Hastings Algorithm\nImplementation of the accept-reject step:\n\nDraw a uniform random variable, \\(U\\sim U \\left[ 0,1\\right]\\), and set \\(\\theta ^{\\left( g+1\\right) }=\\theta^{\\prime}\\)\nif \\(U&lt;\\alpha \\left( \\theta ^{(g)},\\theta^{\\prime}\\right)\\), leaving \\(\\theta ^{\\left( g\\right) }\\) unchanged (\\(\\theta^{\\left( g+1\\right) }=\\theta^{(g)}\\)).\nIt is important to note that the denominator in the acceptance probability cannot be zero, provided the algorithm is started from a \\(\\pi\\) - positive point since \\(q\\) is always positive. The MH algorithmonly requires that \\(\\pi\\) can be evaluated up to proportionality.\nThe output of the algorithm, \\(\\left\\{ \\theta ^{\\left( g\\right) }\\right\\}_{g=1}^{\\infty }\\), is clearly a Markov chain. The key theoretical property is that the Markov chain, under mild regularity, has \\(\\pi \\left( \\theta\\right)\\) as its limiting distribution. We discuss two important specialcases that depend on the choice of \\(q\\)."
  },
  {
    "objectID": "06-mcmc.html#independence-mh",
    "href": "06-mcmc.html#independence-mh",
    "title": "Bayes AI",
    "section": "Independence MH",
    "text": "Independence MH\nOne special case draws a candidate independently of the previous state, \\(q(\\theta^{\\prime}|\\theta ^{(g)})=q(\\theta^{\\prime})\\). In this independence MH algorithm, the acceptance criterion simplifies to \\[\n\\alpha \\left( \\theta ^{(g)},\\theta^{\\prime}\\right) =\\min \\left( \\frac{\\pi        (\\theta^{\\prime})}{\\pi (\\theta ^{(g)})}\\frac{q(\\theta ^{(g)})}{q(\\theta       ^{\\prime})},1\\right).\n\\] Even though \\(\\theta\\) is drawn independently of the previous state, the sequence generated is not being independent, since \\(\\alpha\\) depends on previous draws. The criterion implies a new draw is always accepted if target density ratio \\(\\pi (\\theta^{\\prime})/\\pi (\\theta ^{(g)})\\), increases more than the proposal ratio, \\(q(\\theta ^{(g)})/q(\\theta^{\\prime})\\). When this is not satisfied, a balanced coin is flipped to decide whether or not toaccept the proposal."
  },
  {
    "objectID": "06-mcmc.html#random-walk-metropolis",
    "href": "06-mcmc.html#random-walk-metropolis",
    "title": "Bayes AI",
    "section": "Random-walk Metropolis",
    "text": "Random-walk Metropolis\nRandom-walk (RW) Metropolis is the polar opposite of the independence MH algorithm. It draws a candidate from the following RW model, \\[\n\\theta^{\\prime}=\\theta ^{\\left( g\\right) }+\\sigma \\varepsilon _{g+1},\n\\] where \\(\\varepsilon _{t}\\) is an independent, mean zero, and symmetric error term, typically taken to be a normal or \\(t-\\)distribution, and \\(\\sigma\\) is a scaling factor. The algorithm must be tuned via the choice of \\(\\sigma\\), the scaling factor. Symmetry implies that \\[\nq\\left( \\theta^{\\prime}|\\theta ^{\\left( g\\right) }\\right) =q\\left( \\theta    ^{\\left( g\\right) }|\\theta^{\\prime}\\right),\n\\] and \\[\n\\alpha \\left( \\theta ^{(g)},\\theta^{\\prime}\\right) =\\min \\left( \\pi    (\\theta^{\\prime})/\\pi (\\theta ^{(g)}),1\\right).\n\\]"
  },
  {
    "objectID": "06-mcmc.html#implementation",
    "href": "06-mcmc.html#implementation",
    "title": "Bayes AI",
    "section": "Implementation",
    "text": "Implementation\n\nset.seed(7)\nmh = function(target, proposal, n, x0) {\n  x = x0; p = length(x0)\n  samples = matrix(NA, nrow=n, ncol=p)\n  accept = rep(0, n)\n  for (i in 1:n) {\n    x_new = proposal(x)\n    a = min(1,target(x_new) / target(x))\n    # print(c(x, x_new, a))\n    if (runif(1) &lt; a) {accept[i]=1; x = x_new}\n    samples[i,] = x\n    # print(accept[i])\n  }\n  list(samples = samples, accept = accept)\n}"
  },
  {
    "objectID": "06-mcmc.html#apply",
    "href": "06-mcmc.html#apply",
    "title": "Bayes AI",
    "section": "Apply",
    "text": "Apply\nWe apply MCMC to the weighted sum of two normal distributions. This sort of distribution is fairly straightforward to sample from, but let’s draw samples with MCMC.\n\n\nCode\np &lt;- 0.4\nmu &lt;- c(-1, 2)\nsd &lt;- c(.5, 2)\ntarget &lt;- function(x)\n    p     * dnorm(x, mu[1], sd[1]) +\n    (1-p) * dnorm(x, mu[2], sd[2])\ncurve(target(x), col=\"red\", -4, 8, n=301, las=1)"
  },
  {
    "objectID": "06-mcmc.html#apply-1",
    "href": "06-mcmc.html#apply-1",
    "title": "Bayes AI",
    "section": "Apply",
    "text": "Apply\n\n\nCode\nset.seed(7)\nproposal = function(x) x + rnorm(1, 0, 0.25)\nr = mh(target, proposal, 2000, x0=0)\nhist(r$samples[r$accept==1,], freq=FALSE, breaks=35, col=\"lightblue\", main=\"Metropolis-Hastings\", xlab=\"x\")\ncurve(target(x), add=TRUE, col=\"red\", lwd=2)"
  },
  {
    "objectID": "06-mcmc.html#bivariate",
    "href": "06-mcmc.html#bivariate",
    "title": "Bayes AI",
    "section": "Bivariate",
    "text": "Bivariate\n\n\nCode\nset.seed(92)\nlibrary(ggplot2)\n# Metropolis-Hastings for Bivariate Normal distribution\n# Target distribution\nmu = c(0, 0)\nsigma = matrix(c(1, 0.5, 0.5, 1), 2, 2)\ntarget = function(x) exp(-0.5*t(x-mu) %*% solve(sigma) %*% (x-mu))\nlibrary(mixtools)\nellipse(mu, sigma, npoints = 1000, newplot = TRUE)\nproposal = function(x) x + rnorm(2, 0, .5)\nn = 5000 # Number of samples\nx = c(0, 0)\nsamples = matrix(0, n, 2)\nfor (i in 1:n) {\n  x_new = proposal(x)\n  a = target(x_new) / target(x)\n  if (i&lt;50) {\n  if (runif(1) &gt; a) {arrows(x[1], x[2], x_new[1], x_new[2], col= 'red')} else  {arrows(x[1], x[2], x_new[1], x_new[2], col= 'green'); x = x_new}\n  }\n  if (runif(1) &lt; a) x = x_new\n\n  samples[i,] = x\n}\n\n\n\nPaths of random samples generated by Metropolis algorithm. Red are the rejected steps and green are accepted ones"
  },
  {
    "objectID": "06-mcmc.html#bivariate-1",
    "href": "06-mcmc.html#bivariate-1",
    "title": "Bayes AI",
    "section": "Bivariate",
    "text": "Bivariate\n\n\nCode\nggplot(mapping = aes(x=samples[,1], y=samples[,2])) + geom_point()+ geom_density_2d(size=1)"
  },
  {
    "objectID": "06-mcmc.html#gibbs-samples",
    "href": "06-mcmc.html#gibbs-samples",
    "title": "Bayes AI",
    "section": "Gibbs Samples",
    "text": "Gibbs Samples\nThe first step is the Clifford-Hammersley (CH) theorem, which states that a high-dimensional joint distribution, \\(p\\left( \\theta\\mid X,Y\\right)\\), is completely characterized by a larger number of lower dimensional conditional distributions. Given this characterization, MCMC methods iteratively sample from these conditional distributions using standard sampling methods.\nTo develop the foundations of MCMC in the simplest setting, we consider sampling from a bivariate posterior distribution \\(p\\left( \\theta_1,\\theta_{2} \\mid X,Y \\right)\\), and suppress the dependence on the data for parsimony. For intuition, it is useful to think of \\(\\theta_1\\) as traditional static parameters and \\(\\theta_2\\) as latent variables.\nThe Clifford-Hammersley theorem (CH) proves that the joint distribution,\\(p\\left( \\theta_1,\\theta_2\\right)\\), is completely determined by the conditional distributions, \\(p\\left( \\theta_{1} \\mid \\theta_{2}\\right)\\) and \\(p\\left( \\theta_{2} \\mid \\theta_{1}\\right)\\), under a positivity condition. The positivity condition requires that \\(p\\left( \\theta_{1},\\theta_2\\right)\\), \\(p\\left( \\theta_{1}\\right)\\) and \\(p\\left( \\theta_{2}\\right)\\) have positive mass for all points. These results are useful in practice because in most cases,\\(p\\left( \\theta_1,\\theta_2\\right)\\) is only known up to proportionality and cannot be directly sampled. CH implies that the same information can be extracted from the lower-dimensional conditional distributions, breaking “curse of dimensionality” by transforming a higher dimensional problem of sampling from \\(p\\left(\\theta_{1},\\theta_{2}\\right)\\), into easier problems of sampling from \\(p\\left( \\theta_{1} \\mid \\theta_{2}\\right)\\) and \\(p\\left( \\theta_{2} \\mid \\theta_{1}\\right)\\).\nThe CH theorem is based on the Besag formula: for any pairs \\((\\theta_{1},\\theta_{2})\\) and \\((\\theta_1^{\\prime},\\theta_2^{\\prime})\\),\n\\[\n\\frac{p(\\theta_1,\\theta_2)}{p(\\theta_1^{\\prime},\\theta_2^{\\prime})}=\\frac{p(\\theta_1 \\mid \\theta_2^{\\prime})p(\\theta_2 \\mid \\theta_1)}{p\\left( \\theta_1^{\\prime} \\mid \\theta_2^{\\prime}\\right) p\\left( \\theta_2^{\\prime} \\mid \\theta_2\\right) }.\n\\] The proof uses the fact that \\(p\\left( \\theta_1,\\theta_2\\right)=p\\left( \\theta_2 \\mid \\theta_1\\right) p\\left( \\theta_1\\right)\\) (applied to both \\((\\theta_1,\\theta_2)\\) and \\((\\theta_1^{\\prime},\\theta_2^{\\prime})\\)) and the Bayes rule: \\[\np\\left( \\theta_1\\right) =\\frac{p\\left( \\theta_1 \\mid \\theta_2^{\\prime            }\\right) p\\left( \\theta_2^{\\prime}\\right) }{p\\left( \\theta_2^{\\prime} \\mid \\theta_1\\right) }\\text{.}\n\\]\nThe general version of CH follows by analogy. Partitioning a vector as \\(\\theta =\\left( \\theta_{1},\\theta_{2},\\theta_{3},\\ldots ,\\theta_{K}\\right)\\), then the general CH theorem states that \\[\np\\left( \\theta _{i} \\mid \\theta _{-i}\\right) := p\\left( \\theta    _{i} \\mid \\theta_1,\\theta_1,\\ldots,\\theta_{i-1},\\theta _{i+1},...,\\theta    _{K}\\right) ,\n\\] for \\(i=1,...,K\\), completely characterize \\(p\\left( \\theta_1,\\ldots,\\theta_{K}\\right)\\).\nAn important case arises frequently in models with latent variables. Here, the posterior is defined over vectors of static fixed parameters, \\(\\theta\\), and latent variables, \\(x\\). In this case, CH implies that\\(p\\left( \\theta,x \\mid y\\right)\\) is completely characterized by \\(p\\left( \\theta  \\mid x,y\\right)\\) and \\(p\\left( x \\mid \\theta ,y\\right)\\). The distribution \\(p\\left( \\theta \\mid x,y\\right)\\) is the posterior distribution of the parameters, conditional on the observed data and the latent variables. Similarly, \\(p\\left( x \\mid \\theta,y\\right)\\) is the smoothing distribution of the latent variables.\nThe Gibbs sampler simulates multi-dimensional posterior distributions by iteratively sampling from the lower-dimensional conditional posteriors. Unlike the previous MH algorithms, the Gibbs sampler updates the chain one component at a time, instead of updating the entire vector. This requires either that the conditional posteriors distributions is discrete, or a recognizable distribution (e.g. Normal) for which standard sampling algorithms apply, or that resampling methods, such as accept-reject, can be used.\nIn the case of \\(p\\left( \\theta_1,\\theta_2\\right)\\), given current draws,\\(\\left( \\theta_1^{\\left( g\\right) },\\theta_2^{\\left( g\\right)}\\right)\\), the Gibbs sampler consists of \\[\n\\begin{aligned}\n\\text{1. Draw }\\theta_1^{\\left( g+1\\right) } &\\sim &p\\left( \\theta   _1|\\theta_2^{\\left( g\\right) }\\right) \\\\    \n\\text{2. Draw }\\theta_2^{\\left( g+1\\right) } &\\sim &p\\left( \\theta_2|\\theta_1^{\\left( g+1\\right) }\\right) ,\n\\end{aligned}\n\\] repeating \\(G\\) times. The draws generated by the Gibbs sampler form a Markov chain, as the distribution of \\(\\theta ^{\\left( g+1\\right) }\\) conditional on \\(\\theta ^{\\left( g\\right) }\\) is independent of past draws. Higher-dimensional cases follow by analogy. The Gibbs transition kernel from state \\(\\theta\\) to state \\(\\theta ^{\\prime}\\) is \\(p\\left( \\theta ,\\theta^{\\prime}\\right) =p\\left( \\theta_{1}^{\\prime}|\\theta_2\\right) p\\left( \\theta_2^{\\prime}|\\theta_{1}^{\\prime}\\right),\\) and by definition, \\(\\int p\\left( x,y\\right) dy=1\\). The densities \\(p\\left( \\theta_1|\\theta_2\\right)\\) and \\(p\\left(\\theta_2|\\theta_1\\right)\\) will typically have either discrete or continuous support, and in nearly all cases the chain can reach any point or set in the state space in one step. To establish convergence, we first identify the limiting distribution. A stationary probability distribution, \\(\\pi\\), satisfies the integral equation \\[\n\\pi \\left( \\theta^{\\prime}\\right) =\\int p\\left( \\theta ,\\theta^{\\prime}\\right)  d\\theta.\n\\] If the chain converges, then \\(\\pi\\) is also called the limiting distribution. It is easy to verify that the stationary distribution of the Markov chain generated by the Gibbs sampler is the posterior distribution, \\(\\pi \\left(\\theta \\right) =p\\left( \\theta_1,\\theta_2\\right)\\): \\[\n\\begin{aligned}    \n\\int p\\left( \\theta ,\\theta^{\\prime}\\right) d\\theta &=&p\\left( \\theta_2^{\\prime}|\\theta_1^{\\prime    }\\right) \\int_{\\theta_2}\\int_{\\theta_1}p\\left( \\theta_1^{\\prime}|\\theta_2\\right) p\\left( \\theta_1,\\theta_2\\right) d\\theta_{1}d\\theta_2 \\\\\n&=&p\\left( \\theta_2^{\\prime}|\\theta_1^{\\prime}\\right) \\int_{\\theta_2}p\\left( \\theta_1^{\\prime}|\\theta_2\\right)  p\\left( \\theta_{2}\\right)  d\\theta_2 \\\\&=&p\\left( \\theta_2^{\\prime}|\\theta_1^{\\prime}\\right) p\\left( \\theta_{1}^{\\prime}\\right) =p\\left( \\theta_1^{\\prime},\\theta_2^{\\prime}\\right) =\\pi \\left( \\theta^{\\prime}\\right) \\text{.}\n\\end{aligned}\n\\] To establish convergence to the limiting distribution, the chain must satisfy certain regularity conditions on how it traverses the state space. Starting from an initial \\(\\pi\\)-positive point, the Markov chain in Gibbs samplers can typically reach any set in the state space in one step, implying that states communicate and the chain is irreducible.This does not imply that a chain starting from a given point, will return to that point or visit nearby states frequently. Well-behaved chains are not only irreducible, but stable, in the sense that they make many return visits to states. Chains that visit states or sets frequently are recurrent. Under very mild conditions, the Gibbs sampler generates an irreducible and recurrent chain. In most cases, a measure theoretical condition called Harris recurrence is also satisfied, which implies that the chains converge for any starting values. In this case, the ergodic theorem holds: for a sufficiently integrable function \\(l\\) and for all starting points \\(\\theta ,\\) \\[\n\\underset{G\\rightarrow \\infty }{\\lim }\\frac{1}{G}\\sum_{g=1}^{G}f\\left(\\theta ^{\\left( g\\right) }\\right) =\\int f\\left( \\theta \\right) \\pi \\left(\\theta \\right) d\\theta =E\\left[ f\\left( \\theta \\right) \\right]\n\\] almost surely. Notice the two subtle modes of convergence: there is the convergence of the Markov chain to its stationary distribution, andMonte Carlo convergence, which is the convergence of the partial sums tot he integral. In practice, a chain is typically run for an initial length, often called the burn-in, to remove any dependence on the initial conditions. Once the chain has converged, then a secondary sample of size \\(G\\) is created for Monte Carlo inference"
  },
  {
    "objectID": "06-mcmc.html#gibbs-samples-clifford-hammersley",
    "href": "06-mcmc.html#gibbs-samples-clifford-hammersley",
    "title": "Bayes AI",
    "section": "Gibbs Samples: Clifford-Hammersley",
    "text": "Gibbs Samples: Clifford-Hammersley\n\nClifford-Hammersley (CH) theorem: high-dimensional joint distribution, \\(p\\left( \\theta\\mid X,Y\\right)\\), is completely characterized by a larger number of lower dimensional conditional distributions. Given this characterization, MCMC methods iteratively sample from these conditional distributions using standard sampling methods.\nExample: bivariate posterior distribution \\(p\\left( \\theta_1,\\theta_{2} \\mid X,Y \\right)\\), e.g. \\(\\theta_1\\) as traditional static parameters and \\(\\theta_2\\) as latent variables.\n\nCH: \\(p\\left( \\theta_1,\\theta_2\\right)\\) is determined by \\(p\\left( \\theta_{1} \\mid \\theta_{2}\\right)\\) and \\(p\\left( \\theta_{2} \\mid \\theta_{1}\\right)\\), given \\(p\\left( \\theta_{1},\\theta_2\\right)\\), \\(p\\left( \\theta_{1}\\right)\\) and \\(p\\left( \\theta_{2}\\right)\\) have positive mass for all points"
  },
  {
    "objectID": "06-mcmc.html#section",
    "href": "06-mcmc.html#section",
    "title": "Bayes AI",
    "section": "",
    "text": "The CH theorem is based on the Besag formula: for any pairs \\((\\theta_{1},\\theta_{2})\\) and \\((\\theta_1^{\\prime},\\theta_2^{\\prime})\\),\n\\[\n\\frac{p(\\theta_1,\\theta_2)}{p(\\theta_1^{\\prime},\\theta_2^{\\prime})}=\\frac{p(\\theta_1 \\mid \\theta_2^{\\prime})p(\\theta_2 \\mid \\theta_1)}{p\\left( \\theta_1^{\\prime} \\mid \\theta_2^{\\prime}\\right) p\\left( \\theta_2^{\\prime} \\mid \\theta_2\\right) }.\n\\] The proof uses the fact that \\(p\\left( \\theta_1,\\theta_2\\right)=p\\left( \\theta_2 \\mid \\theta_1\\right) p\\left( \\theta_1\\right)\\) (applied to both \\((\\theta_1,\\theta_2)\\) and \\((\\theta_1^{\\prime},\\theta_2^{\\prime})\\)) and the Bayes rule: \\[\np\\left( \\theta_1\\right) =\\frac{p\\left( \\theta_1 \\mid \\theta_2^{\\prime            }\\right) p\\left( \\theta_2^{\\prime}\\right) }{p\\left( \\theta_2^{\\prime} \\mid \\theta_1\\right) }\\text{.}\n\\]\nThe general version of CH follows by analogy. Partitioning a vector as \\(\\theta =\\left( \\theta_{1},\\theta_{2},\\theta_{3},\\ldots ,\\theta_{K}\\right)\\), then the general CH theorem states that \\[\np\\left( \\theta _{i} \\mid \\theta _{-i}\\right) := p\\left( \\theta    _{i} \\mid \\theta_1,\\theta_1,\\ldots,\\theta_{i-1},\\theta _{i+1},...,\\theta    _{K}\\right) ,\n\\] for \\(i=1,...,K\\), completely characterize \\(p\\left( \\theta_1,\\ldots,\\theta_{K}\\right)\\).\nAn important case arises frequently in models with latent variables. Here, the posterior is defined over vectors of static fixed parameters, \\(\\theta\\), and latent variables, \\(x\\). In this case, CH implies that\\(p\\left( \\theta,x \\mid y\\right)\\) is completely characterized by \\(p\\left( \\theta  \\mid x,y\\right)\\) and \\(p\\left( x \\mid \\theta ,y\\right)\\). The distribution \\(p\\left( \\theta \\mid x,y\\right)\\) is the posterior distribution of the parameters, conditional on the observed data and the latent variables. Similarly, \\(p\\left( x \\mid \\theta,y\\right)\\) is the smoothing distribution of the latent variables.\nThe Gibbs sampler simulates multi-dimensional posterior distributions by iteratively sampling from the lower-dimensional conditional posteriors. Unlike the previous MH algorithms, the Gibbs sampler updates the chain one component at a time, instead of updating the entire vector. This requires either that the conditional posteriors distributions is discrete, or a recognizable distribution (e.g. Normal) for which standard sampling algorithms apply, or that resampling methods, such as accept-reject, can be used.\nIn the case of \\(p\\left( \\theta_1,\\theta_2\\right)\\), given current draws,\\(\\left( \\theta_1^{\\left( g\\right) },\\theta_2^{\\left( g\\right)}\\right)\\), the Gibbs sampler consists of \\[\n\\begin{aligned}\n\\text{1. Draw }\\theta_1^{\\left( g+1\\right) } &\\sim &p\\left( \\theta   _1|\\theta_2^{\\left( g\\right) }\\right) \\\\    \n\\text{2. Draw }\\theta_2^{\\left( g+1\\right) } &\\sim &p\\left( \\theta_2|\\theta_1^{\\left( g+1\\right) }\\right) ,\n\\end{aligned}\n\\] repeating \\(G\\) times. The draws generated by the Gibbs sampler form a Markov chain, as the distribution of \\(\\theta ^{\\left( g+1\\right) }\\) conditional on \\(\\theta ^{\\left( g\\right) }\\) is independent of past draws. Higher-dimensional cases follow by analogy. The Gibbs transition kernel from state \\(\\theta\\) to state \\(\\theta ^{\\prime}\\) is \\(p\\left( \\theta ,\\theta^{\\prime}\\right) =p\\left( \\theta_{1}^{\\prime}|\\theta_2\\right) p\\left( \\theta_2^{\\prime}|\\theta_{1}^{\\prime}\\right),\\) and by definition, \\(\\int p\\left( x,y\\right) dy=1\\). The densities \\(p\\left( \\theta_1|\\theta_2\\right)\\) and \\(p\\left(\\theta_2|\\theta_1\\right)\\) will typically have either discrete or continuous support, and in nearly all cases the chain can reach any point or set in the state space in one step. To establish convergence, we first identify the limiting distribution. A stationary probability distribution, \\(\\pi\\), satisfies the integral equation \\[\n\\pi \\left( \\theta^{\\prime}\\right) =\\int p\\left( \\theta ,\\theta^{\\prime}\\right)  d\\theta.\n\\] If the chain converges, then \\(\\pi\\) is also called the limiting distribution. It is easy to verify that the stationary distribution of the Markov chain generated by the Gibbs sampler is the posterior distribution, \\(\\pi \\left(\\theta \\right) =p\\left( \\theta_1,\\theta_2\\right)\\): \\[\n\\begin{aligned}    \n\\int p\\left( \\theta ,\\theta^{\\prime}\\right) d\\theta &=&p\\left( \\theta_2^{\\prime}|\\theta_1^{\\prime    }\\right) \\int_{\\theta_2}\\int_{\\theta_1}p\\left( \\theta_1^{\\prime}|\\theta_2\\right) p\\left( \\theta_1,\\theta_2\\right) d\\theta_{1}d\\theta_2 \\\\\n&=&p\\left( \\theta_2^{\\prime}|\\theta_1^{\\prime}\\right) \\int_{\\theta_2}p\\left( \\theta_1^{\\prime}|\\theta_2\\right)  p\\left( \\theta_{2}\\right)  d\\theta_2 \\\\&=&p\\left( \\theta_2^{\\prime}|\\theta_1^{\\prime}\\right) p\\left( \\theta_{1}^{\\prime}\\right) =p\\left( \\theta_1^{\\prime},\\theta_2^{\\prime}\\right) =\\pi \\left( \\theta^{\\prime}\\right) \\text{.}\n\\end{aligned}\n\\] To establish convergence to the limiting distribution, the chain must satisfy certain regularity conditions on how it traverses the state space. Starting from an initial \\(\\pi\\)-positive point, the Markov chain in Gibbs samplers can typically reach any set in the state space in one step, implying that states communicate and the chain is irreducible.This does not imply that a chain starting from a given point, will return to that point or visit nearby states frequently. Well-behaved chains are not only irreducible, but stable, in the sense that they make many return visits to states. Chains that visit states or sets frequently are recurrent. Under very mild conditions, the Gibbs sampler generates an irreducible and recurrent chain. In most cases, a measure theoretical condition called Harris recurrence is also satisfied, which implies that the chains converge for any starting values. In this case, the ergodic theorem holds: for a sufficiently integrable function \\(l\\) and for all starting points \\(\\theta ,\\) \\[\n\\underset{G\\rightarrow \\infty }{\\lim }\\frac{1}{G}\\sum_{g=1}^{G}f\\left(\\theta ^{\\left( g\\right) }\\right) =\\int f\\left( \\theta \\right) \\pi \\left(\\theta \\right) d\\theta =E\\left[ f\\left( \\theta \\right) \\right]\n\\] almost surely. Notice the two subtle modes of convergence: there is the convergence of the Markov chain to its stationary distribution, andMonte Carlo convergence, which is the convergence of the partial sums tot he integral. In practice, a chain is typically run for an initial length, often called the burn-in, to remove any dependence on the initial conditions. Once the chain has converged, then a secondary sample of size \\(G\\) is created for Monte Carlo inference"
  },
  {
    "objectID": "06-mcmc.html#besag-formula",
    "href": "06-mcmc.html#besag-formula",
    "title": "Bayes AI",
    "section": "Besag formula",
    "text": "Besag formula\nFor any pairs \\((\\theta_{1},\\theta_{2})\\) and \\((\\theta_1^{\\prime},\\theta_2^{\\prime})\\), \\[\n\\frac{p(\\theta_1,\\theta_2)}{p(\\theta_1^{\\prime},\\theta_2^{\\prime})}=\\frac{p(\\theta_1 \\mid \\theta_2^{\\prime})p(\\theta_2 \\mid \\theta_1)}{p\\left( \\theta_1^{\\prime} \\mid \\theta_2^{\\prime}\\right) p\\left( \\theta_2^{\\prime} \\mid \\theta_2\\right) }.\n\\] The proof uses the fact that \\(p\\left( \\theta_1,\\theta_2\\right)=p\\left( \\theta_2 \\mid \\theta_1\\right) p\\left( \\theta_1\\right)\\) (applied to both \\((\\theta_1,\\theta_2)\\) and \\((\\theta_1^{\\prime},\\theta_2^{\\prime})\\)) and the Bayes rule: \\[\np\\left( \\theta_1\\right) =\\frac{p\\left( \\theta_1 \\mid \\theta_2^{\\prime            }\\right) p\\left( \\theta_2^{\\prime}\\right) }{p\\left( \\theta_2^{\\prime} \\mid \\theta_1\\right) }\\text{.}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#clifford-hammersley-the-general-version",
    "href": "06-mcmc.html#clifford-hammersley-the-general-version",
    "title": "Bayes AI",
    "section": "Clifford-Hammersley: The general version",
    "text": "Clifford-Hammersley: The general version\n\nPartitioning a vector as \\(\\theta =\\left( \\theta_{1},\\theta_{2},\\theta_{3},\\ldots ,\\theta_{K}\\right)\\), then the general CH theorem states that \\[\np\\left( \\theta _{i} \\mid \\theta _{-i}\\right) := p\\left( \\theta    _{i} \\mid \\theta_1,\\theta_1,\\ldots,\\theta_{i-1},\\theta _{i+1},...,\\theta    _{K}\\right) ,\n\\] for \\(i=1,...,K\\), completely characterize \\(p\\left( \\theta_1,\\ldots,\\theta_{K}\\right)\\)."
  },
  {
    "objectID": "06-mcmc.html#latent-variable-models",
    "href": "06-mcmc.html#latent-variable-models",
    "title": "Bayes AI",
    "section": "Latent Variable Models",
    "text": "Latent Variable Models\n\nFixed parameters \\(\\theta\\), and latent variables, \\(x\\). In this case, then CH \\[\np\\left( \\theta,x \\mid y\\right)\n\\] is completely characterized by \\(p\\left( \\theta  \\mid x,y\\right)\\) and \\(p\\left( x \\mid \\theta ,y\\right)\\).\nThe distribution \\(p\\left( \\theta \\mid x,y\\right)\\) is the posterior distribution of the parameters, conditional on the observed data and the latent variables. Similarly, \\(p\\left( x \\mid \\theta,y\\right)\\) is the smoothing distribution of the latent variables."
  },
  {
    "objectID": "06-mcmc.html#gibbs-sampler",
    "href": "06-mcmc.html#gibbs-sampler",
    "title": "Bayes AI",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\n\nThe Gibbs sampler simulates multi-dimensional posterior distributions by iteratively sampling from the lower-dimensional conditional posteriors.\nUnlike the previous MH algorithms, the Gibbs sampler updates the chain one component at a time, instead of updating the entire vector.\nThis requires either that the conditional posteriors distributions is discrete, or a recognizable distribution (e.g. Normal) for which standard sampling algorithms apply, or that resampling methods, such as accept-reject, can be used."
  },
  {
    "objectID": "06-mcmc.html#gibbs-sampler-1",
    "href": "06-mcmc.html#gibbs-sampler-1",
    "title": "Bayes AI",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\nIn the case of \\(p\\left( \\theta_1,\\theta_2\\right)\\), given current draws,\\(\\left( \\theta_1^{\\left( g\\right) },\\theta_2^{\\left( g\\right)}\\right)\\), the Gibbs sampler consists of \\[\n\\begin{aligned}\n\\text{1. Draw }\\theta_1^{\\left( g+1\\right) } &\\sim &p\\left( \\theta   _1|\\theta_2^{\\left( g\\right) }\\right) \\\\    \n\\text{2. Draw }\\theta_2^{\\left( g+1\\right) } &\\sim &p\\left( \\theta_2|\\theta_1^{\\left( g+1\\right) }\\right) ,\n\\end{aligned}\n\\] repeating \\(G\\) times."
  },
  {
    "objectID": "06-mcmc.html#gibbs-sampler-2",
    "href": "06-mcmc.html#gibbs-sampler-2",
    "title": "Bayes AI",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\n\nTransition kernel from state \\(\\theta\\) to state \\(\\theta ^{\\prime}\\) is \\[\np\\left( \\theta ,\\theta^{\\prime}\\right) =p\\left( \\theta_{1}^{\\prime}|\\theta_2\\right) p\\left( \\theta_2^{\\prime}|\\theta_{1}^{\\prime}\\right)\n\\]\nThe limiting (statiobnary) distribution \\(\\pi\\), satisfies \\[\n\\pi \\left( \\theta^{\\prime}\\right) =\\int p\\left( \\theta ,\\theta^{\\prime}\\right)  d\\theta.\n\\]"
  },
  {
    "objectID": "06-mcmc.html#gibbs-sampler-3",
    "href": "06-mcmc.html#gibbs-sampler-3",
    "title": "Bayes AI",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\nIt is easy to verify that the stationary distribution of the Markov chain generated by the Gibbs sampler is the posterior distribution, \\(\\pi \\left(\\theta \\right) =p\\left( \\theta_1,\\theta_2\\right)\\): \\[\n\\begin{aligned}    \n\\int p\\left( \\theta ,\\theta^{\\prime}\\right) d\\theta &=&p\\left( \\theta_2^{\\prime}|\\theta_1^{\\prime    }\\right) \\int_{\\theta_2}\\int_{\\theta_1}p\\left( \\theta_1^{\\prime}|\\theta_2\\right) p\\left( \\theta_1,\\theta_2\\right) d\\theta_{1}d\\theta_2 \\\\\n&=&p\\left( \\theta_2^{\\prime}|\\theta_1^{\\prime}\\right) \\int_{\\theta_2}p\\left( \\theta_1^{\\prime}|\\theta_2\\right)  p\\left( \\theta_{2}\\right)  d\\theta_2 \\\\&=&p\\left( \\theta_2^{\\prime}|\\theta_1^{\\prime}\\right) p\\left( \\theta_{1}^{\\prime}\\right) =p\\left( \\theta_1^{\\prime},\\theta_2^{\\prime}\\right) =\\pi \\left( \\theta^{\\prime}\\right) \\text{.}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#gibbs-sampler-4",
    "href": "06-mcmc.html#gibbs-sampler-4",
    "title": "Bayes AI",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\nThe ergodic theorem holds: for a sufficiently integrable function \\(l\\) and for all starting points \\(\\theta ,\\) \\[\n\\underset{G\\rightarrow \\infty }{\\lim }\\frac{1}{G}\\sum_{g=1}^{G}f\\left(\\theta ^{\\left( g\\right) }\\right) =\\int f\\left( \\theta \\right) \\pi \\left(\\theta \\right) d\\theta =E\\left[ f\\left( \\theta \\right) \\right]\n\\] - Run for an initial length, often called the burn-in - Then a secondary sample of size \\(G\\) is created for Monte Carlo inference"
  },
  {
    "objectID": "06-mcmc.html#hybrid-chains",
    "href": "06-mcmc.html#hybrid-chains",
    "title": "Bayes AI",
    "section": "Hybrid chains",
    "text": "Hybrid chains\n\nGiven a partition of the vector \\(\\theta\\) via CH, a hybrid MCMC algorithm updates the chain one subset at a time, either by direct draws (Gibbs steps) or via MH. - Thus, a hybrid algorithm combines the features of the MH algorithm and the Gibbs sampler, providing significant flexibility in designing MCMC algorithms for different models.\n\\(p\\left( \\theta_2|\\theta_1\\right)\\) is recognizable and can be directly sampled.\n\\(p\\left( \\theta_1|\\theta_2\\right)\\) can only be evaluated and not directly sampled.\nMH accept/reject based on \\[\n\\alpha \\left( \\theta_1^{(g)},\\theta_1^{\\prime}\\right) =\\min \\left(     \\frac{p\\left( \\theta_1^{\\prime}|\\theta_2^{\\left( g\\right) }\\right) }{p\\left( \\theta_1^{\\left ( g \\right ) }|\\theta_2^{\\left( g\\right) }\\right) }\\frac{q\\left( \\theta_1^{\\left( g\\right)        }|\\theta_1^{\\prime},\\theta_2^{\\left( g\\right) }\\right) }{q\\left( \\theta_1^{\\prime}|\\theta_1^{\\left( g\\right) },\\theta    _2^{\\left( g\\right) }\\right) },1\\right) .\n\\]"
  },
  {
    "objectID": "06-mcmc.html#hybrid-chains-1",
    "href": "06-mcmc.html#hybrid-chains-1",
    "title": "Bayes AI",
    "section": "Hybrid chains",
    "text": "Hybrid chains\nThe general hybrid algorithm is as follows. Given \\(\\theta_1^{\\left(g\\right) }\\) and \\(\\theta_2^{\\left( g\\right) }\\), for \\(g=1,\\ldots, G\\), \\[\n\\begin{aligned}    \n1.\\text{ Draw }\\theta_1^{\\left( g+1\\right) } &\\sim &MH\\left[ q\\left(    \\theta_1|\\theta_1^{\\left( g\\right) },\\theta_2^{\\left( g\\right)    }\\right) \\right] \\\\    \n2.\\text{ Draw }\\theta_2^{\\left( g+1\\right) } &\\sim &p\\left( \\theta_2|\\theta_1^{\\left( g+1\\right) }\\right) .\n\\end{aligned}\n\\] In higher dimensional cases, a hybrid algorithm consists of any combination of Gibbs and Metropolis steps. Hybrid algorithms significantly increase the applicability of MCMC methods, as the only requirement is that the model generates posterior conditionals that can either be sampled or evaluated."
  },
  {
    "objectID": "06-mcmc.html#hamiltonian-monte-carlo",
    "href": "06-mcmc.html#hamiltonian-monte-carlo",
    "title": "Bayes AI",
    "section": "Hamiltonian Monte-Carlo",
    "text": "Hamiltonian Monte-Carlo\n\nAdd a momentum to the Metropolis Sampling.\nGradient of the un-normalized posterior to better navigate the surface defined by the posterior.\nMomentum variable \\(r\\) for each model variable \\(\\theta\\). \\[\np(\\theta,r) \\propto \\exp\\left(L(\\theta) - 0.5 r^Tr\\right),\n\\]\n\\(L\\) is the logarithm of the joint density of the variables of interest (negative potential energy)\n\\(0.5 r^Tr\\) is the kinetic energy of the particle\n\\(p(\\theta,r)\\) is the total negative energy"
  },
  {
    "objectID": "06-mcmc.html#hamiltonian-monte-carlo-1",
    "href": "06-mcmc.html#hamiltonian-monte-carlo-1",
    "title": "Bayes AI",
    "section": "Hamiltonian Monte-Carlo",
    "text": "Hamiltonian Monte-Carlo\nWe can simulate the evolution over time of the Hamiltonian dynamics of this system via the “leapfrog” integrator, which proceeds according to the updates \\[\n\\begin{aligned}\nr^{+/2} & = r + (\\epsilon/2) \\nabla_{\\theta}L(\\theta) \\\\\n\\theta^{+} & = \\theta + (\\epsilon/2) r^{+/2}\\\\\nr^{+} & = r^{+/2} + (\\epsilon/2)\\nabla_{\\theta}L(\\theta^{+}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#hierarchical-bayesian-model",
    "href": "06-mcmc.html#hierarchical-bayesian-model",
    "title": "Bayes AI",
    "section": "Hierarchical Bayesian Model",
    "text": "Hierarchical Bayesian Model\nWe consider the following model: \\[\\begin{align*}\n    \\tau &\\sim \\mathrm{Gamma}(0.5, 0.5) \\\\\n    \\lambda_d &\\sim \\mathrm{Gamma}(0.5, 0.5) \\\\\n    \\beta_d &\\sim \\mathcal{N}(0, 20) \\\\\n    y_n &\\sim \\mathrm{Bernoulli}(\\sigma((\\tau \\lambda \\odot \\beta)^T x_n))),\n\\end{align*}\\]\n\n\\(\\tau\\) is a scalar global coefficient scale\n\\(\\lambda\\) is a vector of local scales\n\\(\\beta\\) is the vector of unscaled coefficients,\n\n\n\nCode\nimport tensorflow_probability.substrates.jax as tfp\nimport numpy as np\nimport jax.numpy as jnp\ntfd = tfp.distributions\nimport jax\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "06-mcmc.html#apply-to-iris-data",
    "href": "06-mcmc.html#apply-to-iris-data",
    "title": "Bayes AI",
    "section": "Apply to Iris Data",
    "text": "Apply to Iris Data\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np  \niris = pd.read_csv(\"data/iris.csv\")\nprint(iris.shape)\n\n\n(150, 5)\n\n\nCode\nprint(iris.head())\n\n\n   sepal.length  sepal.width  petal.length  petal.width variety\n0           5.1          3.5           1.4          0.2  Setosa\n1           4.9          3.0           1.4          0.2  Setosa\n2           4.7          3.2           1.3          0.2  Setosa\n3           4.6          3.1           1.5          0.2  Setosa\n4           5.0          3.6           1.4          0.2  Setosa\n\n\nCode\ny = iris['variety']==\"Setosa\"\nx = iris[\"sepal.length\"].values\nplt.scatter(x,y)\nx = np.c_[np.ones(150),x]\nprint(x.shape)\n\n\n(150, 2)"
  },
  {
    "objectID": "06-mcmc.html#log-density-function",
    "href": "06-mcmc.html#log-density-function",
    "title": "Bayes AI",
    "section": "Log Density Function",
    "text": "Log Density Function\nFor Hamiltonian MC, we only need to evaluate the joint log-density pointwise\n\n\nCode\ndef joint_log_prob(x, y, tau, lamb, beta):\n    lp = tfd.Gamma(0.5, 0.5).log_prob(tau)\n    lp += tfd.Gamma(0.5, 0.5).log_prob(lamb).sum() \n    lp += tfd.Normal(0., 20).log_prob(beta).sum() \n    logits = x @ (tau * lamb * beta)\n    lp += tfd.Bernoulli(logits).log_prob(y).sum() \n    return lp   \n\ntau = np.random.rand(1)[0]\nlamb = np.random.rand(2)\nbeta = np.random.rand(2)\njoint_log_prob(x, y, tau, lamb, beta)\n\n\nArray(-124.43358, dtype=float32)"
  },
  {
    "objectID": "06-mcmc.html#change-of-variables",
    "href": "06-mcmc.html#change-of-variables",
    "title": "Bayes AI",
    "section": "Change of Variables",
    "text": "Change of Variables\n\\[\nz\\triangleq T^{-1}(\\theta),\\qquad \\pi(z) = \\pi(\\theta) \\left| \\frac{\\partial T}{\\partial z} (z) \\right|,\n\\]\nTaking the logarithm of both sides, we get \\[\n\\log \\pi(z) = \\log \\pi(\\theta) + \\log \\left| \\frac{\\partial T }{\\partial z}(z) \\right|\n\\] Use \\(T(z)=e^z\\), and \\(\\log|\\frac{\\partial T}{\\partial z}(z)| = z\\)"
  },
  {
    "objectID": "06-mcmc.html#change-of-variables-in-code",
    "href": "06-mcmc.html#change-of-variables-in-code",
    "title": "Bayes AI",
    "section": "Change of Variables in Code",
    "text": "Change of Variables in Code\n\ndef unconstrained_joint_log_prob(x, y, theta):\n    ndims = x.shape[-1]\n    unc_tau, unc_lamb, beta = jnp.split(theta, [1, 1 + ndims])\n    unc_tau = unc_tau.reshape([]) \n    # Make unc_tau a scalar \n    tau = jnp.exp(unc_tau)\n    ldj = unc_tau\n    lamb = jnp.exp(unc_lamb)\n    ldj += unc_lamb.sum()\n    return joint_log_prob(x, y, tau, lamb, beta) + ldj"
  },
  {
    "objectID": "06-mcmc.html#lets-check-out-function",
    "href": "06-mcmc.html#lets-check-out-function",
    "title": "Bayes AI",
    "section": "Let’s check out function",
    "text": "Let’s check out function\n\nfrom functools import partial\ntarget_log_prob = partial(unconstrained_joint_log_prob, x, y)\ntheta = np.r_[tau,lamb,beta]\ntarget_log_prob(theta)\n\nArray(-709.7654, dtype=float32)"
  },
  {
    "objectID": "06-mcmc.html#automatic-differentiation",
    "href": "06-mcmc.html#automatic-differentiation",
    "title": "Bayes AI",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\n\n\nCode\ntarget_log_prob_and_grad = jax.value_and_grad(target_log_prob)\ntlp, tlp_grad = target_log_prob_and_grad(theta)\nprint(tlp)\n\n\n-709.7654\n\n\nCode\ntlp_grad\n\n\nArray([ -695.4309  ,   -40.476734,  -655.5988  ,  -141.32121 ,\n       -1626.3228  ], dtype=float32)"
  },
  {
    "objectID": "06-mcmc.html#hamiltonian-monte-carlo-2",
    "href": "06-mcmc.html#hamiltonian-monte-carlo-2",
    "title": "Bayes AI",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\n\n\nCode\ndef leapfrog_step(target_log_prob_and_grad, step_size, i, leapfrog_state):\n    z, m, tlp, tlp_grad = leapfrog_state\n    m += 0.5 * step_size * tlp_grad\n    z += step_size * m\n    tlp, tlp_grad = target_log_prob_and_grad(z)\n    m += 0.5 * step_size * tlp_grad\n    return z, m, tlp, tlp_grad\n\ndef hmc_step(target_log_prob_and_grad, num_leapfrog_steps, step_size, z, seed):\n    m_seed, mh_seed = jax.random.split(seed)\n    tlp, tlp_grad = target_log_prob_and_grad(z)\n    m = jax.random.normal(m_seed, z.shape)\n    energy = 0.5 * jnp.square(m).sum() - tlp\n    new_z, new_m, new_tlp, _ = jax.lax.fori_loop(\n        0,\n        num_leapfrog_steps,\n        partial(leapfrog_step, target_log_prob_and_grad, step_size),\n        (z, m, tlp, tlp_grad)) \n    new_energy = 0.5 * jnp.square(new_m).sum() - new_tlp\n    log_accept_ratio = energy - new_energy\n    is_accepted = jnp.log(jax.random.uniform(mh_seed, [])) &lt; log_accept_ratio\n    # select the proposed state if accepted\n    z = jnp.where(is_accepted, new_z, z)\n    hmc_output = {\"z\": z,\n                  \"is_accepted\": is_accepted,\n                  \"log_accept_ratio\": log_accept_ratio}\n    # hmc_output[\"z\"] has shape [num_dimensions]\n    return z, hmc_output\n\ndef hmc(target_log_prob_and_grad, num_leapfrog_steps, step_size, num_steps, z,\n        seed):\n    # create a seed for each step\n    seeds = jax.random.split(seed, num_steps)\n    # this will repeatedly run hmc_step and accumulate the outputs\n    _, hmc_output = jax.lax.scan(\n        partial(hmc_step, target_log_prob_and_grad, num_leapfrog_steps, step_size),\n        z, seeds)\n    # hmc_output[\"z\"] now has shape [num_steps, num_dimensions]\n    return hmc_output\n\ndef scan(f, state, xs):\n  output = []\n  for x in xs:\n    state, y = f(state, x)\n    output.append(y)\n  return state, jnp.stack(output)"
  },
  {
    "objectID": "06-mcmc.html#hmc",
    "href": "06-mcmc.html#hmc",
    "title": "Bayes AI",
    "section": "HMC",
    "text": "HMC\n\nnum_leapfrog_steps=30\nstep_size = 0.008\nfrom jax import random\nseed = random.PRNGKey(92)\nnum_samples=10000\nhmc_output = hmc(target_log_prob_and_grad, num_leapfrog_steps, step_size,\n    num_samples, theta, seed)"
  },
  {
    "objectID": "06-mcmc.html#inspect-the-results",
    "href": "06-mcmc.html#inspect-the-results",
    "title": "Bayes AI",
    "section": "Inspect the results",
    "text": "Inspect the results\n\n\nCode\nndims = x.shape[-1]\nthetap = hmc_output['z']\ntaup, lambp, betap = jnp.split(thetap, [1, 1 + ndims],axis=1)\nskip = 2000\nslope = betap[skip:,1]\nintercept = betap[skip:,0]\nplt.hist(slope,bins=50);\nplt.hist(intercept,bins=50);"
  },
  {
    "objectID": "06-mcmc.html#inspect-the-results-1",
    "href": "06-mcmc.html#inspect-the-results-1",
    "title": "Bayes AI",
    "section": "Inspect the results",
    "text": "Inspect the results\n\n\nCode\nprint(np.quantile(slope,[0.05,0.95,0.5]))\n\n\n[-14.41534266  -1.4132225   -5.69631028]\n\n\nCode\nprint(np.quantile(intercept,[0.05,0.95,0.5]))\n\n\n[17.03002386 37.44243698 26.76435947]\n\n\nCode\nprint(np.mean(intercept),np.mean(slope))\n\n\n26.635332 -6.905437\n\n\n\n\nCode\ny = iris$Species==\"setosa\"\nglm(y~iris$Sepal.Length, family=binomial)\n\n\n\nCall:  glm(formula = y ~ iris$Sepal.Length, family = binomial)\n\nCoefficients:\n      (Intercept)  iris$Sepal.Length  \n            27.83              -5.18  \n\nDegrees of Freedom: 149 Total (i.e. Null);  148 Residual\nNull Deviance:      191 \nResidual Deviance: 72   AIC: 76"
  },
  {
    "objectID": "06-mcmc.html#gibbs-sample-for-normal-gamma",
    "href": "06-mcmc.html#gibbs-sample-for-normal-gamma",
    "title": "Bayes AI",
    "section": "Gibbs sample for Normal-Gamma",
    "text": "Gibbs sample for Normal-Gamma\n\n\nCode\n# summary statistics of sample\nn &lt;- 30\nybar &lt;- 15\ns2 &lt;- 3\n\n# sample from the joint posterior (mu, tau | data)\nmu &lt;- tau &lt;- rep(NA, 11000)\nT &lt;- 1000    # burnin\ntau[1] &lt;- 1  # initialisation\nfor(i in 2:11000)\n{   \n    mu[i] &lt;- rnorm(n = 1, mean = ybar, sd = sqrt(1 / (n * tau[i - 1])))    \n    tau[i] &lt;- rgamma(n = 1, shape = n / 2, scale = 2 / ((n - 1) * s2 + n * (mu[i] - ybar)^2))\n}\nmu &lt;- mu[-(1:T)]   # remove burnin\ntau &lt;- tau[-(1:T)] # remove burnin\nhist(mu)\n\n\n\n\n\n\n\n\n\nCode\nhist(tau)"
  },
  {
    "objectID": "06-mcmc.html#hierarchical-model",
    "href": "06-mcmc.html#hierarchical-model",
    "title": "Bayes AI",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nWe will use the Normal-Normal model with kown variance and unknowm mean \\[\n\\begin{aligned}\n\\mu \\sim & \\mathrm{Normal}(0,1) \\\\\nx\\mid \\mu \\sim & \\mathrm{Normal}(\\mu,1)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#problems-of-metropolis-hastings-algorithm",
    "href": "06-mcmc.html#problems-of-metropolis-hastings-algorithm",
    "title": "Bayes AI",
    "section": "Problems of Metropolis-Hastings algorithm",
    "text": "Problems of Metropolis-Hastings algorithm\nMH is very simple and quite general. At the same time\n\ntoo large step size \\(\\sigma\\) leads to a large fraction of rejected samples, while too small \\(\\sigma\\) makes very small steps, thus it takes long time to ‘explore the distribution’ (check this in demonstration!)\nin high-dimensional spaces (very important use-case), MH explores the space very inefficiently because of it’s random-walk behavior. Guessing good direction in 1000 dimensions is incomparably harder than doing this for 2 dimensions!\nMH can’t travel long distances (significantly larger than \\(\\sigma\\)) between isolated local minimums\n\nSampling high-dimensional distributions with MH becomes very inefficient in practice. A more efficient scheme is called Hamiltonian Monte Carlo (HMC)."
  },
  {
    "objectID": "06-mcmc.html#lets-sample",
    "href": "06-mcmc.html#lets-sample",
    "title": "Bayes AI",
    "section": "Let’s sample",
    "text": "Let’s sample\n\n\nCode\nn =20\ndata= sort(rnorm(n,0))\n# hist(data)\nsamples = 15000\nproposal_width = 0.5\nmu_current = 0\nsigma = 1\nmu0 = 0\nsigma0 = 1\nposterior = c(mu_current)\nfor (i in 1:samples) {\n  mu_proposal = rnorm(1,mu_current, proposal_width)\n  likelihood_current =  prod(dnorm(data,mu_current,  sigma))\n  likelihood_proposal = prod(dnorm(data,mu_proposal, sigma))\n  prior_current =  dnorm(mu_current,mu0, sigma0)\n  prior_proposal = dnorm(mu_proposal,mu0, sigma0)\n\n  p_current = likelihood_current * prior_current\n  p_proposal = likelihood_proposal * prior_proposal\n  # Accept proposal?\n  p_accept = p_proposal / p_current\n  accept = (runif(1) &lt; p_accept)\n  if (accept) {\n      mu_current = mu_proposal\n      posterior = c(posterior,mu_current)\n  }\n}\nposterior = posterior[-(1:500)]\nplot(posterior, pch=16)\n\n\n\n\n\n\n\n\n\nCode\nmu_post = (mu0 / sigma0^2 + sum(data) / sigma^2) / (1. / sigma0^2 + n / sigma^2)\nsigma_post = sqrt((1. / sigma0^2 + n / sigma^2)^(-1))\nsprintf(\"Analytical mean: %.2f. MCMC Mean: %.2f\", mu_post, mean(posterior))\n\n\n[1] \"Analytical mean: 0.03. MCMC Mean: 0.03\"\n\n\nCode\nsprintf(\"Analytical sd: %.2f. MCMC sd: %.2f\", sigma_post, sd(posterior))\n\n\n[1] \"Analytical sd: 0.22. MCMC sd: 0.23\"\n\n\nCode\nx = seq(-1,1,by=0.01)\ndpost = dnorm(x,mu_post,sigma_post)\nhist(posterior, freq = F, breaks=30, ylim=c(0,2), main=\"Posterior Mean\")\nlines(density(posterior), ylim=c(0,2), main=\"Posterior Mean\")\nlines(x,dpost, type='l', lwd=3, col='red')"
  },
  {
    "objectID": "06-mcmc.html#lets-compare-to-hmc",
    "href": "06-mcmc.html#lets-compare-to-hmc",
    "title": "Bayes AI",
    "section": "Let’s compare to HMC",
    "text": "Let’s compare to HMC\n\n\nCode\nlibrary(brms)\nd = data.frame(y=data, sigma0=rep(1,20))\nfit &lt;- brm(data = d, \n            family = gaussian,\n            y | se(sigma0) ~ 1,\n    prior = c(prior(normal(0, 1), class = Intercept)),\n    iter = 1000, refresh = 0, chains = 4)\nposterior = as_draws_array(fit, variable = \"b_Intercept\")\nhist(posterior, freq = F, breaks=30, ylim=c(0,2), main=\"Posterior Mean\")\nlines(density(posterior), ylim=c(0,2), main=\"Posterior Mean\")\nlines(x,dpost, type='l', lwd=3, col='red')"
  },
  {
    "objectID": "01-intro.html#random-facts",
    "href": "01-intro.html#random-facts",
    "title": "Bayes AI",
    "section": "Random facts",
    "text": "Random facts\nOn this Day (January 27):\n\n1888: The National Geographic Society is founded in Washington, D.C.\n1945: The Red Army liberates the Auschwitz-Birkenau concentration camp\n1967: The United States, United Kingdom, and Soviet Union sign the Outer Space Treaty in Washington, D.C.\n1973: The Paris Peace Accords officially end the Vietnam War.\n2010: Apple Inc. unveils the iPad."
  },
  {
    "objectID": "01-intro.html#the-first-thoughts-about-artificial-intelligence",
    "href": "01-intro.html#the-first-thoughts-about-artificial-intelligence",
    "title": "Bayes AI",
    "section": "The first thoughts about artificial intelligence",
    "text": "The first thoughts about artificial intelligence\n\n\n\n\n\n\n\nHephaestus created for himself Android robots, such as a giant human-like robot of Talos.\nPygmalion revived Galatea.\nJehovah and Allah - pieces of clay\nParticularly pious and learned rabbis rabbis could create golems.\nAlbert the Great made an artificial speaking head (which very upset Thomas Aquinas)."
  },
  {
    "objectID": "01-intro.html#mechanical-machines",
    "href": "01-intro.html#mechanical-machines",
    "title": "Bayes AI",
    "section": "Mechanical machines",
    "text": "Mechanical machines\nRobots and Automatic Machines Were Generally Very Inventive: Al-Jazari (XII Century)\n\nHesdin Castle (Robert II of Artois), Leonardo’s robot…"
  },
  {
    "objectID": "01-intro.html#mechanical-machines-1",
    "href": "01-intro.html#mechanical-machines-1",
    "title": "Bayes AI",
    "section": "Mechanical machines",
    "text": "Mechanical machines\nJaquet-Droz automata (XVIII century):"
  },
  {
    "objectID": "01-intro.html#mechanical-machines-2",
    "href": "01-intro.html#mechanical-machines-2",
    "title": "Bayes AI",
    "section": "Mechanical machines",
    "text": "Mechanical machines\n\nBut this is in mechanics, in mathematics/logic AI it was quite rudimentary for a long time\n\n\nLogic machine of Ramon Llull (XIII-XIV centuries)\nStarting with Dr. Frankenstein, further AI in the literature appears constantly …"
  },
  {
    "objectID": "01-intro.html#turing-test",
    "href": "01-intro.html#turing-test",
    "title": "Bayes AI",
    "section": "Turing Test",
    "text": "Turing Test\n\nAI as a science begins with a Turing test (1950).\nThe ides of the Turing test is to check if a machine can imitate a human in a conversation.\nThe original formulation was more nuanced."
  },
  {
    "objectID": "01-intro.html#shennons-theseus",
    "href": "01-intro.html#shennons-theseus",
    "title": "Bayes AI",
    "section": "Shennon’s Theseus",
    "text": "Shennon’s Theseus\n\nYouTube Video\nEarly 1950s, Claude Shannon (The father of Information Theory) demonstrates Theseus\nA life-sized magnetic mouse controlled by relay circuits, learns its way around a maze."
  },
  {
    "objectID": "01-intro.html#stanford-cart",
    "href": "01-intro.html#stanford-cart",
    "title": "Bayes AI",
    "section": "Stanford Cart",
    "text": "Stanford Cart\n\n\nYouTube Video\nTakes 2.6-second for signal to travel from earth to the moon\nLatest iterations is automated with 3D vision capabilities\nPause after each meter of movement and take 10-15 minutes to reassess its surroundings and reevaluate its decided path.\nIn 1979, this cautious version of the cart successfully made its way 20 meters through a chair-strewn room in five hours without human intervention."
  },
  {
    "objectID": "01-intro.html#turing-test-1",
    "href": "01-intro.html#turing-test-1",
    "title": "Bayes AI",
    "section": "Turing Test",
    "text": "Turing Test\nIt takes a lot to create an AI system:\n\nProcessing of a natural language\nSensors and actuators\nRepresentation of knowledge\nInference from the existing knowledge\nTraining on experience (Machine Learning)."
  },
  {
    "objectID": "01-intro.html#dartmouth-workshop",
    "href": "01-intro.html#dartmouth-workshop",
    "title": "Bayes AI",
    "section": "Dartmouth workshop",
    "text": "Dartmouth workshop\n\nAI as a science appeared in 1956 at the Dartmouth workshop.\nIt was organized by John McCarthy, Marvin Minsky, Claude Shennon and Nathaniel Rochester.\nIt was probably the most ambitious grant proposal in the history of computer science."
  },
  {
    "objectID": "01-intro.html#dartmouth-workshop-1",
    "href": "01-intro.html#dartmouth-workshop-1",
    "title": "Bayes AI",
    "section": "Dartmouth workshop",
    "text": "Dartmouth workshop\n We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer."
  },
  {
    "objectID": "01-intro.html#great-hopes",
    "href": "01-intro.html#great-hopes",
    "title": "Bayes AI",
    "section": "1956-1960: Great hopes",
    "text": "1956-1960: Great hopes\n\nOptimistic time. It seemed a that we were almost there…\nAllen Newell, Herbert A. Simon, and Cliff Shaw: Logic Theorist.\nAutomated reasoning.\nIt was able to prof most of the Principia Mathematica, in some places even more elegant than Russell and Whitehead."
  },
  {
    "objectID": "01-intro.html#big-hopes",
    "href": "01-intro.html#big-hopes",
    "title": "Bayes AI",
    "section": "1956-1960: Big Hopes",
    "text": "1956-1960: Big Hopes\n\nGeneral Problem Solver - a program that tried to think as a person\nA lot of programs that have been able to do some limited things (MicroWorlds):\n\nAnalogy (IQ tests with multiple choice questions)\nStudent (algebraic verbal tasks)\nBlocks World (rearranged 3D blocks)."
  },
  {
    "objectID": "01-intro.html#sknowledge-based-systems",
    "href": "01-intro.html#sknowledge-based-systems",
    "title": "Bayes AI",
    "section": "1970s:Knowledge Based Systems",
    "text": "1970s:Knowledge Based Systems\n\nThe bottom line: to accumulate a fairly large set of rules and knowledge about the subject area, then draw conclusions.\nFirst success: MYCIN - Diagnosis of blood infections:\n\nabout 450 rules\nThe results are like an experienced doctor and significantly better than beginner doctors."
  },
  {
    "objectID": "01-intro.html#commercial-applications-industry-ai",
    "href": "01-intro.html#commercial-applications-industry-ai",
    "title": "Bayes AI",
    "section": "1980-2010: Commercial applications Industry AI",
    "text": "1980-2010: Commercial applications Industry AI\n\nThe first AI department was at Dec (Digital Equipment Corporation). It is argued that by 1986 he saved the Dec about  $ 10 million per year.\nThe boom ended by the end of the 80s, when many companies could not live up to high expectations."
  },
  {
    "objectID": "01-intro.html#data-mining-machine-learning",
    "href": "01-intro.html#data-mining-machine-learning",
    "title": "Bayes AI",
    "section": "1990-2010: DATA MINING, MACHINE LEARNING",
    "text": "1990-2010: DATA MINING, MACHINE LEARNING\n\nIn recent decades, the main emphasis has shifted to machine training and search for patterns in the data.\nEspecially - with the development of the Internet.\nNot too many people remember the original AI ideas, but Machine Learning is now everywhere.\nBut Robotics flourishes and uses Machine Learning at every step."
  },
  {
    "objectID": "01-intro.html#rule-based-system-vs-bayes",
    "href": "01-intro.html#rule-based-system-vs-bayes",
    "title": "Bayes AI",
    "section": "Rule-Based System vs Bayes",
    "text": "Rule-Based System vs Bayes\n\nSince 1956, the field of artificial intelligence (AI) has undergone significant transformations\ntraditional AI was mostly focused on rule-based systems and boolean logic programming, with limited learning capabilities. It lead to them being brittle in changing environments.\nOn the other hand, emerging AI is focused on modeling uncertainties, pattern matching, and deep learning.\nAll of those are data-driven approaches.\nThese approaches are more adaptable and can handle complex and unstructured data. They are also more data-dependent and lack interpretability."
  },
  {
    "objectID": "01-intro.html#rule-based-system-vs-bayes-1",
    "href": "01-intro.html#rule-based-system-vs-bayes-1",
    "title": "Bayes AI",
    "section": "Rule-Based System vs Bayes",
    "text": "Rule-Based System vs Bayes\n\n\n\n\n\n\n\n\n\nOld AI\n\n\n If rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n\n \n\n\n\n\n\nNew AI\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\n\n\nBayesian approach is a powerful statistical framework based on the work of Thomas Bayes and later Laplace.\nIt provides a probabilistic approach to reasoning and learning\nAllowing us to update our beliefs about the world as we gather new data.\nThis makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information."
  },
  {
    "objectID": "01-intro.html#definition",
    "href": "01-intro.html#definition",
    "title": "Bayes AI",
    "section": "DEFINITION",
    "text": "DEFINITION\n\nHow to determine “learning”?\n\n\n\n\nDefinition:\n\n\nThe computer program learns as the data is accumulating relative to a certain problem class \\(T\\) and the target function of \\(P\\) if the quality of solving these problems (relative to \\(P\\)) improves with gaining new experience.\n\n\n\n\nThe definition is very (too?) General.\nWhat specific examples can be given?"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml",
    "href": "01-intro.html#tasks-and-concepts-of-ml",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML",
    "text": "Tasks and concepts of ML"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-supervised-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-supervised-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: Supervised Learning",
    "text": "Tasks and concepts of ML: Supervised Learning\n\ntraining sample – a set of examples, each of which consists of input features (attributes) and the correct “answers” - the response variable\nLearn a rule that maps input features to the response variable\nThen this rule is applied to new examples (deployment)\nThe main thing is to train a model that explains not only examples from the training set, but also new examples (generalizes)\nOtherwise - overfitting"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: unsupervised learning",
    "text": "Tasks and concepts of ML: unsupervised learning\nThere are no correct answers, only data, e.g. clustering:\n\nWe need to divide the data into pre -unknown classes to some extent similar:\n\nhighlight the family of genes from the sequences of nucleotides\ncluster users and personalize the application for them\ncluster the mass spectrometric image to parts with different composition"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning-1",
    "href": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning-1",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: unsupervised learning",
    "text": "Tasks and concepts of ML: unsupervised learning\n\nDimensionality reduction: data have a high dimension, it is necessary to reduce it, select the most informative features so that all of the above algorithms can work\nMatrix Competition: There is a sparse matrix, we must predict what is in the missing positions.\nAnomaly detection: find anomalies in the data, e.g. fraud detection.\nOften the outputs answers are given for a small part of the data, then we call it semi -supervised Learning."
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-reinforcement-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-reinforcement-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: reinforcement learning",
    "text": "Tasks and concepts of ML: reinforcement learning\n\nMulti-armed bandits: there is a certain set of actions, each of which leads to random results, you need to get as much rewards possible\nExploration vs.Exploitation: how and when to proceed from the study of the new to use what has already studied\nCredit Assignment: You get rewarded at the very end (won the game), and we must somehow distribute this reward on all the moves that led to victory."
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-active-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-active-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: active learning",
    "text": "Tasks and concepts of ML: active learning\n\nActive Learning - how to choose the following (relatively expensive) test\nBoosting - how to combine several weak classifiers so that it turns out good\nModel Selection - where to draw a line between models with many parameters and with a few.\nRanking: response list is ordered (internet search)"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai",
    "href": "01-intro.html#tasks-and-concepts-of-ai",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI",
    "text": "Tasks and concepts of AI"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-reasoning",
    "href": "01-intro.html#tasks-and-concepts-of-ai-reasoning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Reasoning",
    "text": "Tasks and concepts of AI: Reasoning\n\nBayesian networks: given conditional probabilities, calculate the probability of the event\no1 by OpenAI: a family of AI models that are designed to perform complex reasoning tasks, such as math, coding, and science. o1 models placed among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME)\nGemini 2.0: model for the agentic era"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-representation",
    "href": "01-intro.html#tasks-and-concepts-of-ai-representation",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Representation",
    "text": "Tasks and concepts of AI: Representation\n\nKnowledge Graphs: a graph database that uses semantic relationships to represent knowledge\nEmbeddings: a way to represent data in a lower-dimensional space\nTransformers: a deep learning model that uses self-attention to process sequential data"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-generation",
    "href": "01-intro.html#tasks-and-concepts-of-ai-generation",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\nIn shadows of data, uncertainty reigns,\nBayesian whispers, where knowledge remains.\nWith prior beliefs, we start our quest,\nUpdating with evidence, we strive for the best.\nA dance of the models, predictions unfold,\nInferences drawn, from the new and the old.\nThrough probabilities, we find our way,\nIn the world of AI, it’s the Bayesian sway.\nSo gather your data, let prior thoughts flow,\nIn the realm of the unknown, let your insights grow.\nFor in this approach, with each little clue,\nWe weave understanding, both rich and true.\nMusic"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-generation-1",
    "href": "01-intro.html#tasks-and-concepts-of-ai-generation-1",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\n\nfrom openai import OpenAI\nclient = OpenAI(api_key=\"your-api-key\")\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"a hockey player trying to understand the Bayes rule\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1,\n)\n\nprint(response.data[0].url)"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-generation-2",
    "href": "01-intro.html#tasks-and-concepts-of-ai-generation-2",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\nA humorous and illustrative scene of a hockey player sitting on a bench in full gear, holding a hockey stick in one hand and a whiteboard marker in th"
  },
  {
    "objectID": "01-intro.html#chess-and-ai",
    "href": "01-intro.html#chess-and-ai",
    "title": "Bayes AI",
    "section": "Chess and AI",
    "text": "Chess and AI\nOld AI: Deep Blue (1997) vs. Garry Kasparov\n\nKasparov vs IBM’s DeepBlue in 1997"
  },
  {
    "objectID": "01-intro.html#alphago-zero",
    "href": "01-intro.html#alphago-zero",
    "title": "Bayes AI",
    "section": "AlphaGo Zero",
    "text": "AlphaGo Zero\n\nRemove all human knowledge from training process - only uses self play,\nTakes raw board as input and neural network predicts the next move.\nUses Monte Carlo tree search to evaluate the position.\nThe algorithm was able to beat AlphaGo 100-0. The algorithm was then used to play chess and shogi and was able to beat the best human players in those games as well.\n\n\nAlpha GO vs Lee Sedol: Move 37 by AlphaGo in Game Two"
  },
  {
    "objectID": "01-intro.html#probability-in-machine-learning",
    "href": "01-intro.html#probability-in-machine-learning",
    "title": "Bayes AI",
    "section": "Probability in machine learning",
    "text": "Probability in machine learning\n\nIn all methods and approaches, it is useful not only generate an answer, but also evaluate how confident in this answer, how well the model describes the data, how these values ​​will change in further experiments, etc.\nTherefore, the central role in machine learning is played by the theory of probability - and we will also actively use it."
  },
  {
    "objectID": "01-intro.html#references",
    "href": "01-intro.html#references",
    "title": "Bayes AI",
    "section": "References",
    "text": "References\n\nChristopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2007.\nKevin Murphy, Machine Learning: A Probabilistic Perspective, MIT Press, 2013.\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd ed., Springer, 2009."
  },
  {
    "objectID": "01-intro.html#probability",
    "href": "01-intro.html#probability",
    "title": "Bayes AI",
    "section": "Probability",
    "text": "Probability\nSubjective Probability (de Finetti, Ramsey, Savage, von Neumann, ... )\nPrinciple of Coherence:\nA set of subjective probability beliefs must avoid sure loss\n\nIf an event \\(A\\) is certain to occur, it has probability 1\nEither an event \\(A\\) occurs or it does not. \\[\nP(A) = 1 - P(\\mbox{not }A)\n\\]\nIf two events are mutually exclusive (both cannot occur simultaneously) then \\[\nP(A \\mbox{ or } B) = P(A) + P(B)\n\\]\nJoint probability, when events are independent \\[\nP(A \\mbox{ and } B) = P( A) P(B)\n\\]"
  },
  {
    "objectID": "01-intro.html#conditional-joint-and-marginal-distributions",
    "href": "01-intro.html#conditional-joint-and-marginal-distributions",
    "title": "Bayes AI",
    "section": "Conditional, Joint and Marginal Distributions",
    "text": "Conditional, Joint and Marginal Distributions\nUse probability to describe outcomes involving more than one variable at a time. Need to be able to measure what we think will happen to one variable relative to another\nIn general the notation is ...\n\n\\(P(X=x, Y=y )\\) is the joint probability that \\(X =x\\) and \\(Y=y\\)\n\\(P(X=x  \\mid  Y=y )\\) is the conditional probability that \\(X\\) equals \\(x\\) given \\(Y=y\\)\n\\(P(X=x)\\) is the marginal probability of \\(X=x\\)"
  },
  {
    "objectID": "01-intro.html#conditional-joint-and-marginal-distributions-1",
    "href": "01-intro.html#conditional-joint-and-marginal-distributions-1",
    "title": "Bayes AI",
    "section": "Conditional, Joint and Marginal Distributions",
    "text": "Conditional, Joint and Marginal Distributions\nRelationship between the joint and conditional ... \\[\n\\begin{aligned}\nP(x,y) & = P(x) P(y \\mid x) \\\\\n& =  P(y) P(x \\mid y)\n\\end{aligned}\n\\]\nRelationship between the joint and marginal ... \\[\n\\begin{aligned}\nP(x) & = \\sum_y P(x,y) \\\\\nP(y) & =  \\sum_x P(x,y)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-rule",
    "href": "01-intro.html#bayes-rule",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nThe computation of \\(P(x \\mid y)\\) from \\(P(x)\\) and \\(P(y \\mid x)\\) is called Bayes theorem ... \\[\nP(x \\mid y) = \\frac{P(y,x)}{P(y)} = \\frac{P(y,x)}{\\sum_x P(y,x)} = \\frac{P(y \\mid x)P(x)}{\\sum_x P(y \\mid x)P(x)}\n\\]\nThis shows now the conditional distribution is related to the joint and marginal distributions.\nYou’ll be given all the quantities on the r.h.s."
  },
  {
    "objectID": "01-intro.html#bayes-rule-1",
    "href": "01-intro.html#bayes-rule-1",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nKey fact: \\(P(x \\mid y)\\) is generally different from \\(P(y \\mid x)\\)!\nExample: Most people would agree \\[\n\\begin{aligned}\nPr  & \\left ( Practice \\; hard  \\mid  Play \\; in \\; NBA \\right ) \\approx  1\\\\\nPr  & \\left ( Play \\; in \\; NBA  \\mid  Practice \\; hard  \\right ) \\approx  0\n\\end{aligned}\n\\]\nThe main reason for the difference is that \\(P( Play \\; in \\; NBA ) \\approx 0\\)."
  },
  {
    "objectID": "01-intro.html#independence",
    "href": "01-intro.html#independence",
    "title": "Bayes AI",
    "section": "Independence",
    "text": "Independence\nTwo random variable \\(X\\) and \\(Y\\) are independent if \\[\nP(Y = y  \\mid X = x) = P (Y = y)\n\\] for all possible \\(x\\) and \\(y\\) values. Knowing \\(X=x\\) tells you nothing about \\(Y\\)!\nExample: Tossing a coin twice. What’s the probability of getting \\(H\\) in the second toss given we saw a \\(T\\) in the first one?"
  },
  {
    "objectID": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models",
    "href": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models",
    "title": "Bayes AI",
    "section": "Bookies vs Betters: The Battle of Probabilistic Models",
    "text": "Bookies vs Betters: The Battle of Probabilistic Models\n\nimageSource: The Secret Betting Strategy That Beats Online Bookmakers"
  },
  {
    "objectID": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models-1",
    "href": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models-1",
    "title": "Bayes AI",
    "section": "Bookies vs Betters: The Battle of Probabilistic Models",
    "text": "Bookies vs Betters: The Battle of Probabilistic Models\n\nBookies set odds that reflect their best guess on probabilities of a win, draw, or loss. Plus their own margin\nBookies have risk aversion bias. When many people bet for an underdog (more popular team)\nBookies hedge their bets by offering more favorable odds to the opposed team\nSimple algorithm: calculate average odds across many bookies and find outliers with large deviation from the mean"
  },
  {
    "objectID": "01-intro.html#odds-oddschecker",
    "href": "01-intro.html#odds-oddschecker",
    "title": "Bayes AI",
    "section": "Odds: Oddschecker",
    "text": "Odds: Oddschecker\nWe can express probabilities in terms of Odds via \\[\nO(A) = \\frac{ 1- P(A) }{ P(A) }\n\\; \\; {\\rm or} \\; \\; P(A) = \\frac{ 1 }{ 1 + O(A) }\n\\]\n\nFor example if \\(O(A) = 1\\) then for ever $1 bet you will payout $1. An event with probability \\(\\frac{1}{2}\\).\nIf \\(O(A) = 2\\) or \\(2:1\\), then for a $1 bet you’ll payback $3.\n\nIn terms of probability \\(P = \\frac{1}{3}\\)."
  },
  {
    "objectID": "01-intro.html#envelope-paradox",
    "href": "01-intro.html#envelope-paradox",
    "title": "Bayes AI",
    "section": "Envelope Paradox",
    "text": "Envelope Paradox\nThe following problem is known as the “exchange paradox”.\n\nA swami puts \\(m\\) dollars in one envelope and \\(2 m\\) in another. He hands on envelope to you and one to your opponent.\nThe amounts are placed randomly and so there is a probability of \\(\\frac{1}{2}\\) that you get either envelope.\nYou open your envelope and find \\(x\\) dollars. Let \\(y\\) be the amount in your opponent’s envelope."
  },
  {
    "objectID": "01-intro.html#envelope-paradox-1",
    "href": "01-intro.html#envelope-paradox-1",
    "title": "Bayes AI",
    "section": "Envelope Paradox",
    "text": "Envelope Paradox\nYou know that \\(y = \\frac{1}{2} x\\) or \\(y = 2 x\\). You are thinking about whether you should switch your opened envelope for the unopened envelope of your friend. It is tempting to do an expected value calculation as follows \\[\nE( y) = \\frac{1}{2} \\cdot  \\frac{1}{2} x + \\frac{1}{2} \\cdot 2 x = \\frac{5}{4} x &gt; x\n\\] Therefore, it looks as if you should switch no matter what value of \\(x\\) you see. A consequence of this, following the logic of backwards induction, that even if you didn’t open your envelope that you would want to switch!"
  },
  {
    "objectID": "01-intro.html#bayes-rule-2",
    "href": "01-intro.html#bayes-rule-2",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\nWhere’s the flaw in this argument? Use Bayes rule to update the probabilities of which envelope your opponent has! Assume \\(p(m)\\) of dollars to be placed in the envelope by the swami.\nSuch an assumption then allows us to calculate an odds ratio \\[\n\\frac{ p \\left ( y = \\frac{1}{2} x | x \\right ) }{ p \\left ( y = 2 x | x \\right ) }\n\\] concerning the likelihood of which envelope your opponent has.\nThen, the expected value is given by\n\n\\[\nE(y) =  p \\left ( y = \\frac{1}{2} x \\; \\vert \\;  x \\right ) \\cdot  \\frac{1}{2} x +\np \\left ( y = 2 x | x \\right ) \\cdot 2 x\n\\] and the condition \\(E( y) &gt; x\\) becomes a decision rule."
  },
  {
    "objectID": "01-intro.html#prisoners-dilemma",
    "href": "01-intro.html#prisoners-dilemma",
    "title": "Bayes AI",
    "section": "Prisoner’s Dilemma",
    "text": "Prisoner’s Dilemma\nThree prisoners \\(A , B , C\\).\nEach believe are equally likely to be set free.\nPrisoner \\(A\\) goes to the warden \\(W\\) and asks if s/he is getting axed.\n\nThe Warden can’t tell \\(A\\) anything about him.\nHe provides the new information: \\(WB\\) = “\\(B\\) is to be executed”"
  },
  {
    "objectID": "01-intro.html#prisoners-dilemma-1",
    "href": "01-intro.html#prisoners-dilemma-1",
    "title": "Bayes AI",
    "section": "Prisoner’s Dilemma",
    "text": "Prisoner’s Dilemma\nUniform Prior Probabilities: \\[\n\\begin{array}{c|ccc}\nPrior & A  & B  & C  \\\\\\hline\nP ( {\\rm Pardon} ) & 0.33 & 0.33 & 0.33\n\\end{array}\n\\]\nPosterior: Compute \\(P ( A | WB )\\)?\n What happens if \\(C\\) overhears the conversation?\n Compute \\(P ( C | WB )\\)?"
  },
  {
    "objectID": "01-intro.html#game-show-problem",
    "href": "01-intro.html#game-show-problem",
    "title": "Bayes AI",
    "section": "Game Show Problem",
    "text": "Game Show Problem\nNamed after the host of the long-running TV show, Let’s make a Deal.\n\nA contestant is given the choice of 3 doors.\n\nThere is a prize (a car, say) behind one of the doors and something worthless behind the other two doors: two goats.\n\nThe optimal strategy is counter-intuitive"
  },
  {
    "objectID": "01-intro.html#puzzle",
    "href": "01-intro.html#puzzle",
    "title": "Bayes AI",
    "section": "Puzzle",
    "text": "Puzzle\nThe game is as follows:\n\nYou pick a door.\nMonty then opens one of the other two doors, revealing a goat.\nYou have the choice of switching doors.\n\n Is it advantageous to switch?\n Assume you pick door \\(A\\) at random. Then \\(P(A) = ( 1 /3 )\\).\nYou need to figure out \\(P( A | MB )\\) after Monte reveals \\(B\\) is a goat."
  },
  {
    "objectID": "01-intro.html#bayes-rule-3",
    "href": "01-intro.html#bayes-rule-3",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nIn its simplest form.\n\nTwo events \\(A\\) and \\(B\\). Bayes rule \\[\nP ( A | B ) = \\frac{P (  A \\cap  B )}{ P ( B )}\n= \\frac{P ( B | A ) P ( A )}{ P ( B )}\n\\]\nLaw of Total Probability \\[\nP ( B ) = P ( B | A ) P ( A ) + P ( B | \\bar{A} ) P ( \\bar{A} )\n\\] Hence we can calculate the denominator of Bayes rule."
  },
  {
    "objectID": "01-intro.html#bayes-theroem",
    "href": "01-intro.html#bayes-theroem",
    "title": "Bayes AI",
    "section": "Bayes Theroem",
    "text": "Bayes Theroem\nMany problems in decision making can be solved using Bayes rule.\n\nAI: Rule-based decision making.\nIt’s counterintuitive! But gives the “right” answer.\n\nBayes Rule: \\[\n\\mbox{P}(A|B) = \\frac{\\mbox{P}(A \\cap B)}{\\mbox{P}(B)} = \\frac{  \\mbox{P}(B|A) \\mbox{P}(A)}{ \\mbox{P}(B)}\n\\] Law of Total Probability: \\[\n\\mbox{P}(B) =  \\mbox{P}(B|A) \\mbox{P}(A ) +  \\mbox{P}(B| \\bar{A} ) \\mbox{P}(\\bar{A} )\n\\]"
  },
  {
    "objectID": "01-intro.html#apple-watch",
    "href": "01-intro.html#apple-watch",
    "title": "Bayes AI",
    "section": "Apple Watch",
    "text": "Apple Watch\nThe Apple Watch Series 4 can perform a single-lead ECG and detect atrial fibrillation. The software can correctly identify 98% of cases of atrial fibrillation (true positives) and 99% of cases of non-atrial fibrillation (true negatives).\nHowever, what is the probability of a person having atrial fibrillation when atrial fibrillation is identified by the Apple Watch Series 4?\nBayes’ Theorem: \\[\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n\\]"
  },
  {
    "objectID": "01-intro.html#apple-watch-1",
    "href": "01-intro.html#apple-watch-1",
    "title": "Bayes AI",
    "section": "Apple Watch",
    "text": "Apple Watch\n\n\n\nPredicted\natrial fibrillation\nno atrial fibrillation\n\n\n\n\natrial fibrillation\n1960\n980\n\n\nno atrial fibrillation\n40\n97020\n\n\n\n\\[\n0.6667\n=\n\\frac{0.98\\cdot 0.02}{\n0.0294}\n\\]\nThe conditional probability of having atrial fibrillation when the Apple Watch Series 4 detects atrial fibrillation is about 67%."
  },
  {
    "objectID": "01-intro.html#abraham-wald",
    "href": "01-intro.html#abraham-wald",
    "title": "Bayes AI",
    "section": "Abraham Wald",
    "text": "Abraham Wald\nHow Abraham Wald improved aircraft survivability. Raw Reports from the Field\n\n\n\nType of damage suffered\nReturned (316 total)\nShot down (60 total)\n\n\n\n\nEngine\n29\n?\n\n\nCockpit\n36\n?\n\n\nFuselage\n105\n?\n\n\nNone\n146\n0\n\n\n\nThis fact would allow Wald to estimate: \\[\nP(\\text{damage on fuselage} \\mid \\text{returns safely}) = 105/316 \\approx 32\\%\n\\] You need the inverse probability : \\[\nP(\\text{returns safely} \\mid \\text{damage on fuselage})\n\\] Completely different!"
  },
  {
    "objectID": "01-intro.html#abraham-wald-1",
    "href": "01-intro.html#abraham-wald-1",
    "title": "Bayes AI",
    "section": "Abraham Wald",
    "text": "Abraham Wald\nImputation: fill-in missing data.\n\n\n\nType of damage suffered\nReturned (316 total)\nShot down (60 total)\n\n\n\n\nEngine\n29\n31\n\n\nCockpit\n36\n21\n\n\nFuselage\n105\n8\n\n\nNone\n146\n0\n\n\n\nThen Wald got: \\[\n\\begin{aligned}\nP(\\text{returns safely} \\mid \\text{damage on fuselage}) & =\\frac{105}{105+8}\\approx 93\\%\\\\\nP(\\text{returns safely} \\mid \\text{damage on engine}) & =\\frac{29}{29+31}\\approx 48\\%\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "01-intro.html#personalization-conditional-probability",
    "href": "01-intro.html#personalization-conditional-probability",
    "title": "Bayes AI",
    "section": "“Personalization\" \\(=\\)”Conditional Probability\"",
    "text": "“Personalization\" \\(=\\)”Conditional Probability\"\n\nConditional probability is how AI systems express judgments in a way that reflects their partial knowledge.\nPersonalization runs on conditional probabilities, all of which must be estimated from massive data sets in which you are the conditioning event.\n\n Many Business Applications!! Suggestions vs Search…."
  },
  {
    "objectID": "01-intro.html#probability-as-evidence",
    "href": "01-intro.html#probability-as-evidence",
    "title": "Bayes AI",
    "section": "Probability as Evidence",
    "text": "Probability as Evidence\nevidence: known facts about criminal (e.g. blood type, DNA, ...)\nsuspect: matches a trait with evidence at scene of crime\nLet \\(G\\) denote the event that the suspect is the criminal.\nBayes computes the conditional probability of guilt\n\\[\nP ( G | {\\rm evidence} )\n\\] Evidence \\(E\\): suspect and criminal possess a common trait"
  },
  {
    "objectID": "01-intro.html#probability-as-evidence-1",
    "href": "01-intro.html#probability-as-evidence-1",
    "title": "Bayes AI",
    "section": "Probability as Evidence",
    "text": "Probability as Evidence\nBayes Theorem yields \\[\nP ( G | {\\rm evidence} )\n= \\frac{ P ( {\\rm evidence} | G ) P ( G ) }{ P ( {\\rm evidence} )}\n\\]\nIn terms of relative odds \\[\n\\frac{ P ( I | {\\rm evidence} ) }{ P ( G | {\\rm evidence} ) }\n= \\frac{ P ( {\\rm evidence} | I ) }{ P ( {\\rm evidence} | G ) }\n\\frac{ P ( I ) }{ P ( G ) }\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-factors",
    "href": "01-intro.html#bayes-factors",
    "title": "Bayes AI",
    "section": "Bayes Factors",
    "text": "Bayes Factors\nThere are two terms:\n\nPrior Odds of Guilt \\(O ( G ) = P ( I ) / P ( G )\\) ?\n\nHow many people on the island?\nSensitivity “what if” analysis?\n\nThe Bayes factor \\[\n\\frac{ P ( {\\rm evidence} | I ) }{ P ( {\\rm evidence} | G ) }\n\\] is common to all observers and updates everyone’s initials odds"
  },
  {
    "objectID": "01-intro.html#prosecutors-fallacy",
    "href": "01-intro.html#prosecutors-fallacy",
    "title": "Bayes AI",
    "section": "Prosecutor’s Fallacy",
    "text": "Prosecutor’s Fallacy\nThe most common fallacy is confusing \\[\nP ( {\\rm evidence} | G ) \\; \\; {\\rm with} \\; \\;\nP ( G | {\\rm evidence} )\n\\]\nBayes rule yields \\[\nP ( G | {\\rm evidence} ) = \\frac{ P ( {\\rm evidence} | G ) p( G )}{ P ( {\\rm evidence}  )}\n\\] Your assessment of \\(P( G )\\) will matter."
  },
  {
    "objectID": "01-intro.html#island-problem",
    "href": "01-intro.html#island-problem",
    "title": "Bayes AI",
    "section": "Island Problem",
    "text": "Island Problem\nSuppose there’s a criminal on a island of \\(N+1\\) people.\n\nLet \\(I\\) denote innocence and \\(G\\) guilt.\nEvidence \\(E\\): the suspect matches a trait with the criminal.\nThe probabilities are \\[\np(E|I)=p\\;\\;\\mathrm{and}\\;\\;p(E|G)=1\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-factor",
    "href": "01-intro.html#bayes-factor",
    "title": "Bayes AI",
    "section": "Bayes factor",
    "text": "Bayes factor\nBayes factors are likelihood ratios\n\nThe Bayes factor is given by \\[\n\\frac{p(E|I)}{p(E|G)}=p\n\\]\nIf we start with a uniform prior distribution we have\n\n\\[\np(I)=\\frac{1}{N+1}\\;\\;\\mathrm{and}\\;\\;odds(I)=N\n\\]\n\nPriors will matter!"
  },
  {
    "objectID": "01-intro.html#island-problem-1",
    "href": "01-intro.html#island-problem-1",
    "title": "Bayes AI",
    "section": "Island Problem",
    "text": "Island Problem\nPosterior Probability related to Odds \\[\np(I|y)=\\frac\n{1}{1+odds(I|y)}%\n\\]\n\nProsecutors’ fallacy\n\nThe posterior probability \\(p(I|y)\\neq p(y|I)=p\\).\n\nSuppose that \\(N=10^{3}\\) and \\(p=10^{-3}\\). Then\n\n\\[\np( I|y) = \\frac{1}{1 + 10^3 \\cdot 10^{-3}} = \\frac{1}{2}\n\\]\nThe odds on innocence are \\(odds(I|y)=1\\).\nThere’s a \\(50/50\\) chance that the criminal has been found."
  },
  {
    "objectID": "01-intro.html#sally-clark-case-independence-or-bayes",
    "href": "01-intro.html#sally-clark-case-independence-or-bayes",
    "title": "Bayes AI",
    "section": "Sally Clark Case: Independence or Bayes?",
    "text": "Sally Clark Case: Independence or Bayes?\nSally Clark was accused and convicted of killing her two children\nThey could have both died of SIDS.\n\nThe chance of a family which are non-smokers and over 25 having a SIDS death is around 1 in \\(8,500\\).\nThe chance of a family which has already had a SIDS death having a second is around 1 in 100.\nThe chance of a mother killing her two children is around 1 in \\(1,000,000\\)."
  },
  {
    "objectID": "01-intro.html#bayes-or-independence",
    "href": "01-intro.html#bayes-or-independence",
    "title": "Bayes AI",
    "section": "Bayes or Independence",
    "text": "Bayes or Independence\n\nUnder Bayes \\[\n\\begin{aligned}\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)   &  = P \\left(\n\\mathrm{first} \\; \\mathrm{SIDS} \\right)  P \\left(  \\mathrm{Second} \\; \\;\n\\mathrm{SIDS} | \\mathrm{first} \\; \\mathrm{SIDS} \\right) \\\\\n&  = \\frac{1}{8500} \\cdot \\frac{1}{100} = \\frac{1}{850,000}\n\\end{aligned}\n\\] The \\(\\frac{1}{100}\\) comes from taking into account genetics.\nIndependence, as the court did, gets you\n\n\\[\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)  = (1/8500) (1/8500) = (1/73,000,000)\n\\]\n\nBy Bayes rule\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{P( E \\cap I)}{P( E \\cap G)}\n\\] \\(P( E \\cap I) = P(E|I )P(I)\\) needs discussion of \\(p(I)\\)."
  },
  {
    "objectID": "01-intro.html#comparison",
    "href": "01-intro.html#comparison",
    "title": "Bayes AI",
    "section": "Comparison",
    "text": "Comparison\n\nHence putting these two together gives the odds of guilt as\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{1/850,000}{1/1,000,000} = 1.15\n\\] In terms of posterior probabilities\n\\[\np( G|E) = \\frac{1}{1 + O(G|E)} = 0.465\n\\]\n\nIf you use independence\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{1}{73} \\; {\\rm and} \\; p( G|E) \\approx 0.99\n\\] The suspect looks guilty."
  },
  {
    "objectID": "01-intro.html#oj-simpson",
    "href": "01-intro.html#oj-simpson",
    "title": "Bayes AI",
    "section": "OJ Simpson",
    "text": "OJ Simpson\nThe O.J. Simpson trial was possibly the trail of the century\nThe murder of his wife Nicole Brown Simpson, and a friend, Ron Goldman, in June 1994 and the trial dominated the TV networks\n\nDNA evidence and probability: \\(p( E| G)\\)\nBayes Theorem: \\(p( G | E )\\)\nProsecutor’s Fallacy: \\(p( G|E ) \\neq p(E|G)\\)\n\nOdds ratio with gives \\[\n\\frac{ p( I|E) }{ p ( G | E ) } = \\frac{ p( E|I )}{ p( E|G) } \\frac{ p(I) }{p(G ) }\n\\] Prior odds conditioned on background information."
  },
  {
    "objectID": "01-intro.html#oj-simpson-bayes-theorem",
    "href": "01-intro.html#oj-simpson-bayes-theorem",
    "title": "Bayes AI",
    "section": "OJ Simpson: Bayes Theorem",
    "text": "OJ Simpson: Bayes Theorem\nSuppose that you are a juror in a murder case of a husband who is accused of killing his wife.\nThe husband is known is have battered her in the past.\nConsider the three events:\n\n\\(G\\) “husband murders wife in a given year”\n\\(M\\) “wife is murdered in a given year”\n\\(B\\) “husband is known to batter his wife”"
  },
  {
    "objectID": "01-intro.html#oj-simpson-bayes-theorem-1",
    "href": "01-intro.html#oj-simpson-bayes-theorem-1",
    "title": "Bayes AI",
    "section": "OJ Simpson: Bayes Theorem",
    "text": "OJ Simpson: Bayes Theorem\n\nOnly \\(1/10\\)th of one percent of husbands who batter their wife actually murder them.\n\nConditional on eventually murdering their wife, there a one in ten chance it happens in a given year.\nIn 1994, 5000 women were murdered, 1500 by their husband\nGiven a population of 100 million women at the time \\[\np( M | I ) = \\frac{ 3500 }{ 10^8 } \\approx \\frac{1}{30,000} .\n\\] We’ll also need \\(p( M | I , B )  = p( M | I )\\)"
  },
  {
    "objectID": "01-intro.html#oj-simpson-prosecutors-fallacy",
    "href": "01-intro.html#oj-simpson-prosecutors-fallacy",
    "title": "Bayes AI",
    "section": "OJ Simpson: Prosecutor’s Fallacy",
    "text": "OJ Simpson: Prosecutor’s Fallacy\n\nLet \\(G =\\) Guilt and \\(E=\\) Evidence\nProsecutor’s Fallacy: \\(P(G|E) \\neq P(E|G)\\).\nDNA evidence gives \\(P( E | I )\\) – the \\(p\\)-value.\n\nWhat’s the “match probability” for a rare event?\nBayes theorem in Odds \\[\n\\frac{p(G|M,B)}{p(I|M,B)} = \\frac{p(M|G,B)}{p(M|I,B)} \\frac{p(G|B)}{p(I|B)}\n\\]"
  },
  {
    "objectID": "01-intro.html#oj-simpson-bayes-theorem-2",
    "href": "01-intro.html#oj-simpson-bayes-theorem-2",
    "title": "Bayes AI",
    "section": "OJ Simpson: Bayes Theorem",
    "text": "OJ Simpson: Bayes Theorem\nBy assumption,\n\n\\(p(M|G,B)=1\\)\n\\(p(M|I,B)= \\frac{1}{30,000}\\)\n\\(p( G|B) = \\frac{1}{1000}\\) and so\n\n\\[\n\\frac{p(G|B)}{p(I|B)} = \\frac{1}{999}\n\\]\nTherefore, \\[\n\\frac{p(G|M,B)}{p(I|M,B)} \\approx 30 \\; {\\rm and} \\; p(G|M,B) = \\frac{30}{31} \\approx 97\\%\n\\] More than a 50/50 chance that your spouse murdered you!"
  },
  {
    "objectID": "01-intro.html#fallacy-p-g-b-neq-p-g-b-m",
    "href": "01-intro.html#fallacy-p-g-b-neq-p-g-b-m",
    "title": "Bayes AI",
    "section": "Fallacy \\(p ( G | B ) \\neq p( G | B , M )\\)",
    "text": "Fallacy \\(p ( G | B ) \\neq p( G | B , M )\\)\nThe defense stated to the press: in any given year\n“Fewer than 1 in 2000 of batterers go on to murder their wives”.\n\nNow estimate \\(p( M | \\bar{G} , B ) = p( M| \\bar{G} ) = \\frac{1}{20,000}\\).\nThe Bayes factor is then\n\n\\[\n\\frac{ p( G | M , B ) }{ p( \\bar{G} | M , B ) } = \\frac{ 1/999 }{1 /20,000} = 20\n\\] which implies posterior probabilities\n\\[\np( \\bar{G} | M , B ) = \\frac{1}{1+20} \\; {\\rm and} \\; p( G | M , B ) = \\frac{20}{21}\n\\] Hence its over 95% chance that O.J. is guilty based on this information!\nDefense intended this information to exonerate O.J."
  },
  {
    "objectID": "01-intro.html#base-rate-fallacies",
    "href": "01-intro.html#base-rate-fallacies",
    "title": "Bayes AI",
    "section": "Base Rate Fallacies",
    "text": "Base Rate Fallacies\n“Witness” 80 % certain saw a “checker” \\(C\\) taxi in the accident.\n\nWhat’s your \\(P ( C | E )\\) ?\nNeed \\(P ( C )\\). Say \\(P( C ) = 0.2\\) and \\(P( E  | C) = 0.8\\).\nThen your posterior is\n\n\\[\nP ( C | E ) = \\frac{0.8 \\cdot 0.2}{ 0.8 \\cdot 0.2 + 0.2 \\cdot 0.8 } = 0.5\n\\]\nTherefore \\(O ( C ) = 1\\) a 50/50 bet."
  },
  {
    "objectID": "01-intro.html#updating-fallacies",
    "href": "01-intro.html#updating-fallacies",
    "title": "Bayes AI",
    "section": "Updating Fallacies",
    "text": "Updating Fallacies\nMost people don’t update quickly enough in light of new data\nWards Edwards 1960s\nWhen you have a small sample size, Bayes rule still updates probabilities\n\nTwo players: either 70 % A or 30 % A\nObserve \\(A\\) beats \\(B\\) 3 times out of 4.\nWhat’s \\(P ( A = 70 \\% \\; {\\rm player} )\\) ?"
  },
  {
    "objectID": "01-intro.html#conditional-independence",
    "href": "01-intro.html#conditional-independence",
    "title": "Bayes AI",
    "section": "Conditional Independence",
    "text": "Conditional Independence\n\nConsider three variables a,b,c\nConditional distribution of a given b and c, is p(a|b,c)\nIf p(a|b,c) does not depend on value of b, we can write p(a|b,c) = p(a|c)\nWe say that a is conditionally independent of b given c\n\nWe can use the telescoping property of conditional probabilities to write the joint probability distribution as a product of conditional probabilities. This is the essence of the chain rule of probability. It is given by \\[\np(x_1, x_2, \\ldots, x_n) = p(x_1)p(x_2 \\mid x_1)p(x_3 \\mid x_1, x_2) \\ldots p(x_n \\mid x_1, x_2, \\ldots, x_{n-1}).\n\\]\nright hand side can be simplified if some of the variables are conditionally independent"
  },
  {
    "objectID": "01-intro.html#graphical-representation",
    "href": "01-intro.html#graphical-representation",
    "title": "Bayes AI",
    "section": "Graphical Representation",
    "text": "Graphical Representation\nWhen two nodes are connected they are not independent. Consider the following three cases:\n\n\n\n\n\n\n\n\n\nLine Structure\n\n\n\n\n\\[\np(b\\mid c,a) = p(b\\mid c),~ p(a,b,c) = p(a)p(c\\mid a)p(b\\mid c)\n\\]\n\n\n\n\n\n\n\nLambda Structure\n\n\n\n\n\\[\np(a\\mid b,c) = p(a\\mid c), ~ p(a,b,c) = p(a\\mid c)p(b\\mid c)p(c)\n\\]\n\n\n\n\n\n\n\nV-structure\n\n\n\n\n\\[\np(a\\mid b) = p(a),~ p(a,b,c) = p(c\\mid a,b)p(a)p(b)\n\\]"
  },
  {
    "objectID": "01-intro.html#derived-assumptions",
    "href": "01-intro.html#derived-assumptions",
    "title": "Bayes AI",
    "section": "Derived Assumptions",
    "text": "Derived Assumptions\n\n\n\n\n\n\n\n\n\nLine Structure\n\n\n\n\n\\(a\\) and \\(b\\) connected through \\(c\\). Thus, \\(a\\) can influence \\(b\\). However, once \\(c\\) is known, \\(a\\) and \\(b\\) are independent.\n\n\n\n\n\n\n\nLambda Structure\n\n\n\n\n\\(a\\) can influence \\(b\\) through \\(c\\), but once \\(c\\) is known, \\(a\\) and \\(b\\) are independent.\n\n\n\n\n\n\n\nV-structure\n\n\n\n\n\\(a\\) and \\(b\\) are independent, but once \\(c\\) is known, \\(a\\) and \\(b\\) are not independent. You can formally derive these independencies from the graph by comparing \\(p(a,b\\mid c)\\) and \\(p(a\\mid c)p(b\\mid c)\\)."
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics",
    "href": "01-intro.html#bayes-home-diagnostics",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\n\nAlarm system sends me a text notification when some motion inside my house is detected. - - Prior: during an earthquake alarm is triggered in 10% of the cases.\nI get text message and assess \\(p(b\\mid a)\\) is high and I start driving back home\nWhile driving I hear on the radio about a small earthquake in our area, need to calculate \\(p(b \\mid a,r)\\)\n\n\\(b\\) = burglary, \\(e\\) = earthquake, \\(a\\) = alarm, and \\(r\\) = radio message about small earthquake.\nThe joint distribution is then given by \\[\n  p(b,e,a,r) = p(r \\mid a,b,e)p(a \\mid b,e)p(b\\mid e)p(e).\n\\] Since we know the causal relations, we can simplify this expression \\[\np(b,e,a,r) = p(r \\mid e)p(a \\mid b,e)p(b)p(e).\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-1",
    "href": "01-intro.html#bayes-home-diagnostics-1",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\nThe joint distribution is defined by\n\n\n\n\\(p(a=1 \\mid b,e)\\)\nb\ne\n\n\n\n\n0\n0\n0\n\n\n0.1\n0\n1\n\n\n1\n1\n0\n\n\n1\n1\n1"
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-2",
    "href": "01-intro.html#bayes-home-diagnostics-2",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\nGraphically, we can represent the relations between the variables known as a Directed Acyclic Graph (DAG), which is known as Bayesian network.\n\n\nCode\ngraph TB\n    b((b)) --&gt; a((a))\n    e((e)) --&gt; a\n    e --&gt; r((r))\n\n\n\n\n\n\ngraph TB\n    b((b)) --&gt; a((a))\n    e((e)) --&gt; a\n    e --&gt; r((r))\n\n\n\n\nFigure 1: Bayesian network for alarm ."
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-3",
    "href": "01-intro.html#bayes-home-diagnostics-3",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\n\nCalculate \\(p(a=0 \\mid b,e)\\), from \\[\np(a=1 \\mid b,e) + p(a=0 \\mid b,e) = 1.\n\\] Also know \\(p(r=1 \\mid e=1) = 0.5\\) and \\(p(r=1 \\mid e=0) = 0\\), \\(p(b) = 2\\cdot10^{-4}\\) and \\(p(e) = 10^{-2}\\) (historical data)\n\nGraph allowed us to have a more compact representation of the joint probability distribution. The original naive representations requires specifying \\(2^4\\) parameters."
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-4",
    "href": "01-intro.html#bayes-home-diagnostics-4",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\nTo answer our original question, calculate \\[\np(b \\mid a) = \\dfrac{p(a \\mid b)p(b)}{p(a)},~~p(b) = p(a=1 \\mid b=1)p(b=1) + p(a=1 \\mid b=0)p(b=0).\n\\] We have everything but \\(p(a \\mid b)\\). This is obtained by marginalizing \\(p(a=1 \\mid b,e)\\), to yield \\[\np(a \\mid b) = p(a \\mid b,e=1)p(e=1) + p(a \\mid b,e=0)p(e=0).\n\\] We can calculate \\[\np(a=1 \\mid b=1) = 1, ~p(a=1 \\mid b=0) = 0.1*10^{-2} + 0 = 10^{-3}.\n\\] This leads to \\(p(b \\mid a) = 2\\cdot10^{-4}/(2\\cdot10^{-4} + 10^{-3}(1-2\\cdot10^{-4})) = 1/6\\)."
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-5",
    "href": "01-intro.html#bayes-home-diagnostics-5",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\n\nResult is somewhat counterintuitive.\nWe get such a low probability of burglary because its prior is very low compared to prior probability of an earthquake.\nWhat will happen to posterior if we live in an area with higher crime rates, say \\(p(b) = 10^{-3}\\). \\[\np(a \\mid b) = \\dfrac{p(b)}{p(b) + 10^{-3}(1-p(b))}\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-6",
    "href": "01-intro.html#bayes-home-diagnostics-6",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\n\n\nCode\nprior &lt;- seq(0, .1, length.out = 200)\npost &lt;- prior / (prior + 0.001 * (1 - prior))\nplot(prior, post, type = \"l\", lwd = 3, col = \"red\")\n\n\n\n\nFigure 2: Relationship between the prior and posterior"
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-7",
    "href": "01-intro.html#bayes-home-diagnostics-7",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\nNow, suppose that you hear on the radio about a small earthquake while driving. Then, using Bayesian conditioning, \\[\np(b=1 \\mid a=1,r=1) =  \\dfrac{p(a,r  \\mid  b)p(b)}{p(a,r)}\n\\] and \\[\np(a,r  \\mid  b)p(b) = \\dfrac{\\sum_e p(b=1,e,a=1,r=1)}{\\sum_b\\sum_ep(b,e,a=1,r=1)}\n\\] \\[\n=\\dfrac{\\sum_ep(r=1 \\mid e)p(a=1 \\mid b=1,e)p(b=1)p(e)}{\\sum_b\\sum_ep(r=1 \\mid e)p(a=1 \\mid b,e)p(b)p(e)}\n\\] which is \\(\\approx 2\\%\\) in our case. This effect is called explaining away, namely when new information explains some previously known fact."
  },
  {
    "objectID": "01-intro.html#a-random-image",
    "href": "01-intro.html#a-random-image",
    "title": "Bayes AI",
    "section": "A Random Image",
    "text": "A Random Image"
  },
  {
    "objectID": "01-intro.html#bayes-theorem",
    "href": "01-intro.html#bayes-theorem",
    "title": "Bayes AI",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nMany problems in decision making can be solved using Bayes rule.\n\nAI: Rule-based decision making.\nIt’s counterintuitive! But gives the “right” answer.\n\nBayes Rule: \\[\n\\mbox{P}(A|B) = \\frac{\\mbox{P}(A \\cap B)}{\\mbox{P}(B)} = \\frac{  \\mbox{P}(B|A) \\mbox{P}(A)}{ \\mbox{P}(B)}\n\\] Law of Total Probability: \\[\n\\mbox{P}(B) =  \\mbox{P}(B|A) \\mbox{P}(A ) +  \\mbox{P}(B| \\bar{A} ) \\mbox{P}(\\bar{A} )\n\\]"
  }
]