<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-29e2c20b02301cfff04dc8050bf30c7e.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.37">

  <meta name="author" content="Vadim Sokolov   George Mason University   Spring 2025">
  <title>Bayes AI</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-bbe7401fe57d4b791b917637bb662036.css">
  <link rel="stylesheet" href="style.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="site_libs/quarto-diagram/mermaid.min.js"></script>
  <script src="site_libs/quarto-diagram/mermaid-init.js"></script>
  <link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="center">
  <h1 class="title" style="font-size:40px;">Bayes AI</h1>
  <p class="subtitle" style="color:blue;font-size:50px;">Unit 5: Bayesian Regression: Linear and Bayesian Trees</p>
<img style="height: 350px;" src="fig/page/08-regression.jpg">
  <p class="author">Vadim Sokolov <br> George Mason University <br> Spring 2025</p>

<p style="font-size:10px;"> 
<a href="https://vsokolov.org/courses/664.html">Course Page</a>, <a href="https://vsokolov.org/html/664/slides/">Slides</a>
</p>


</section>
<section id="bayesian-regression" class="slide level2">
<h2>Bayesian Regression</h2>
<p><span class="math display">\[
y_i \sim D(f(\eta_i),\theta)
\]</span></p>
<ul>
<li><span class="math inline">\(f\)</span>: inverse of the link function</li>
<li><span class="math inline">\(\eta_i\)</span>: linear predictor</li>
<li><span class="math inline">\(D\)</span>: distribution of the response variable (family) <span class="math display">\[
\eta = X\beta + Zu
\]</span></li>
<li><span class="math inline">\(\beta,u,\theta\)</span>: model parameters; <span class="math inline">\(\beta\)</span> - fixed effects, <span class="math inline">\(u\)</span> - random effects</li>
</ul>
</section>
<section id="the-problem---predicting-a-continuous-outcome" class="slide level2">
<h2>The Problem - Predicting a Continuous Outcome</h2>
<ul>
<li>We have data: <span class="math inline">\((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\)</span></li>
<li><span class="math inline">\(x_i\)</span> represents a set of features (predictors, independent variables). Could be a vector (e.g., <span class="math inline">\(x_i = [age, weight, height]\)</span>).</li>
<li><span class="math inline">\(y_i\)</span> is a continuous target variable (dependent variable, response). We want to predict <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>.</li>
<li><strong>Example:</strong> Predicting house price (<span class="math inline">\(y\)</span>) based on size, location, number of bedrooms (<span class="math inline">\(x\)</span>).</li>
</ul>
</section>
<section id="the-linear-regression-model-review" class="slide level2">
<h2>The Linear Regression Model (Review)</h2>
<ul>
<li><p>We assume a <em>linear</em> relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
  y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \ldots + \beta_px_{ip} + \epsilon_i
  \]</span></p>
<ul>
<li><span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span> are the <em>coefficients</em> (parameters) we need to estimate. <span class="math inline">\(\beta_0\)</span> is the intercept.</li>
<li><span class="math inline">\(x_{ij}\)</span> is the <span class="math inline">\(j\)</span>-th feature of the <span class="math inline">\(i\)</span>-th data point.</li>
<li><span class="math inline">\(\epsilon_i\)</span> is the error term, assumed to be normally distributed: <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span></li>
<li><span class="math inline">\(\sigma^2\)</span> is the variance of the error (noise).</li>
</ul></li>
<li><p>In matrix form: <span class="math inline">\(y = X\beta + \epsilon\)</span></p></li>
</ul>
</section>
<section id="from-frequentist-to-bayesian" class="slide level2">
<h2>From Frequentist to Bayesian</h2>
<ul>
<li><strong>Frequentist Approach (e.g., Ordinary Least Squares):</strong>
<ul>
<li>Find the <em>single best</em> set of coefficients (<span class="math inline">\(\beta\)</span>) that minimize the sum of squared errors.</li>
<li>Coefficients (<span class="math inline">\(\beta\)</span>) are treated as fixed, unknown constants.</li>
</ul></li>
<li><strong>Bayesian Approach:</strong>
<ul>
<li>Treat the coefficients (<span class="math inline">\(\beta\)</span>) and the variance (<span class="math inline">\(\sigma^2\)</span>) as <em>random variables</em>.</li>
<li>We define <em>prior distributions</em> for <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>, representing our initial beliefs.</li>
<li>We use <em>Bayes’ Theorem</em> to update these beliefs based on the data.</li>
<li>We obtain <em>posterior distributions</em> for <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>, representing our updated beliefs.</li>
</ul></li>
</ul>
</section>
<section id="bayes-theorem---the-core" class="slide level2">
<h2>Bayes’ Theorem - The Core</h2>
<ul>
<li><p>Bayes’ Theorem tells us how to update our beliefs:</p>
<p><span class="math display">\[
  P(\theta | Data) = [ P(Data | \theta)P(\theta) ] / P(Data)
  \]</span></p>
<ul>
<li><span class="math inline">\(P(\theta | Data):\)</span> Posterior distribution - Our updated belief about the parameters () <em>after</em> seeing the data. This is what we want!</li>
<li><span class="math inline">\(P(Data | \theta):\)</span> Likelihood function - The probability of observing the data <em>given</em> specific parameter values.</li>
<li><span class="math inline">\(P(\theta):\)</span> Prior distribution - Our initial belief about the parameters <em>before</em> seeing the data.</li>
<li><span class="math inline">\(P(Data):\)</span> Evidence (marginal likelihood) - The probability of the data (a normalizing constant). Often difficult to compute directly.</li>
</ul></li>
</ul>
</section>
<section id="bayesian-linear-regression---example" class="slide level2 smaller">
<h2>Bayesian Linear Regression - Example</h2>
<ul>
<li><strong>Likelihood:</strong> <span class="math inline">\(P(Data | \beta, \sigma^2)\)</span>
<ul>
<li>Since <span class="math inline">\(y = X\beta + \epsilon\)</span> and <span class="math inline">\(\epsilon \sim N(0, \sigma^2)\)</span>:</li>
<li><span class="math inline">\(y_i | \beta, \sigma^2 \sim N(X_i\beta, \sigma^2)\)</span> (The data is normally distributed around the predicted value.)</li>
<li>The likelihood function is the product of these normal distributions for all data points.</li>
</ul></li>
<li><strong>Priors:</strong> <span class="math inline">\(P(\beta, \sigma^2)\)</span> (Often factored as <span class="math inline">\(P(\beta)P(\sigma^2)\)</span> - assuming prior independence)
<ul>
<li><strong>For <span class="math inline">\(\beta\)</span> (coefficients):</strong>
<ul>
<li>Common choice: <em>Normal prior</em>: <span class="math inline">\(\beta \sim N(μ_0, Σ_0)\)</span>
<ul>
<li><span class="math inline">\(μ_0\)</span>: Prior mean (vector). Often set to 0 (no prior preference for any particular coefficient value).</li>
<li><span class="math inline">\(Σ_0\)</span>: Prior covariance matrix. Controls the prior uncertainty. A diagonal matrix with large values means high uncertainty (a “weak” prior).</li>
</ul></li>
</ul></li>
<li><strong>For <span class="math inline">\(\sigma^2\)</span> (variance):</strong>
<ul>
<li>Common choice: <em>Inverse Gamma prior</em>: <span class="math inline">\(\sigma^2 \sim InvGamma(a, b)\)</span>
<ul>
<li><span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are hyperparameters controlling the shape and scale of the distribution. Carefully chosen to be <em>non-informative</em> or <em>weakly informative</em>.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="the-posterior-distribution" class="slide level2">
<h2>The Posterior Distribution</h2>
<ul>
<li><p>Applying Bayes’ Theorem:</p>
<p><span class="math display">\[
  P(\beta, \sigma^2 | Data) \propto P(Data | \beta, \sigma^2)P(\beta)P(\sigma^2)
  \]</span></p></li>
<li><p>The posterior is <em>proportional</em> to the product of the likelihood and the priors.</p></li>
<li><p><strong>Crucially:</strong> For certain choices of priors (like the Normal prior for <span class="math inline">\(\beta\)</span> and Inverse Gamma for <span class="math inline">\(\sigma^2\)</span>), the posterior has a <em>known form</em> (conjugate priors). This makes calculations easier. In other cases, we use approximation techniques.</p></li>
</ul>
</section>
<section id="inference-and-prediction" class="slide level2">
<h2>Inference and Prediction</h2>
<ul>
<li><strong>Inference:</strong> We’re interested in the posterior distribution <span class="math inline">\(P(\beta, \sigma^2 | Data)\)</span>. This tells us:
<ul>
<li>The most likely values of the coefficients (<span class="math inline">\(\beta\)</span>).</li>
<li>The uncertainty associated with these estimates (captured by the spread of the posterior). The uncertainty in the noise variance (<span class="math inline">\(\sigma^2\)</span>).</li>
</ul></li>
<li><strong>Prediction:</strong> Given a new input <span class="math inline">\(x^*\)</span>, we want to predict the corresponding output <span class="math inline">\(y^*\)</span>.
<ul>
<li><p>We use the <em>posterior predictive distribution</em>: <span class="math inline">\(P(y^* | x^*, Data)\)</span></p></li>
<li><p>This distribution integrates over the posterior distribution of the parameters:</p>
<p><span class="math display">\[
  P(y^* | x^*, Data) = ∫ P(y^* | x^*, \beta, \sigma^2)P(\beta, \sigma^2 | Data) d\beta d\sigma^2
  \]</span></p></li>
<li><p>This gives us a <em>distribution</em> over possible values of <span class="math inline">\(y^*\)</span>, not just a single point estimate. This captures the uncertainty in our prediction.</p></li>
</ul></li>
</ul>
</section>
<section id="advantages-of-bayesian-linear-regression" class="slide level2">
<h2>Advantages of Bayesian Linear Regression</h2>
<ul>
<li><strong>Uncertainty Quantification:</strong> Provides a full probability distribution over the parameters and predictions, not just point estimates. This is crucial for decision-making.</li>
<li><strong>Incorporation of Prior Knowledge:</strong> Allows us to incorporate prior beliefs about the parameters. This can be helpful when data is scarce or when we have expert knowledge.</li>
<li><strong>Regularization:</strong> The prior acts as a natural regularizer, preventing overfitting (especially with informative priors). Similar in effect to ridge regression or lasso.</li>
<li><strong>Model Comparison:</strong> Bayesian methods provide a framework for comparing different models (e.g., different sets of features) using the evidence (marginal likelihood).</li>
</ul>
</section>
<section id="disadvantages-and-challenges" class="slide level2">
<h2>Disadvantages and Challenges</h2>
<ul>
<li><strong>Computational Complexity:</strong> Calculating the posterior (especially the evidence) can be computationally expensive, especially for complex models and large datasets. Approximation techniques like Markov Chain Monte Carlo (MCMC) are often required.</li>
<li><strong>Prior Specification:</strong> Choosing appropriate priors can be challenging. Poorly chosen priors can lead to biased results. Sensitivity analysis (checking how the results change with different priors) is important.</li>
<li><strong>Interpretation:</strong> While uncertainty quantification is a strength, interpreting posterior distributions can sometimes be less intuitive than interpreting point estimates.</li>
</ul>
</section>
<section id="bodyfat-example" class="slide level2">
<h2>bodyfat example</h2>
<ul>
<li>Body fat is hard to measure</li>
<li>Can be predicted from abdominal circumference?</li>
<li>Use The data set bodyfat can be found from the library BAS.</li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a></a>bodyfat <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"data/bodyfat.csv"</span>)</span>
<span id="cb1-2"><a></a>bodyfat[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">8</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>  Bodyfat Abdomen
1    12.3    85.2
2     6.1    83.0
3    25.3    87.9
4    10.4    86.4
5    28.7   100.0</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a></a><span class="fu">plot</span>(bodyfat<span class="sc">$</span>Abdomen,bodyfat<span class="sc">$</span>Bodyfat,<span class="at">pch=</span><span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-1-1.png" width="960" class="r-stretch"></section>
<section id="classical-linear-regression" class="slide level2">
<h2>Classical Linear Regression</h2>
<p><span class="math display">\[
y_i = \alpha + \beta x_i + \epsilon_i, \quad i = 1,\cdots, 252.
\]</span></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a></a>bodyfat.lm <span class="ot">=</span> <span class="fu">lm</span>(Bodyfat <span class="sc">~</span> Abdomen, <span class="at">data =</span> bodyfat)</span>
<span id="cb4-2"><a></a><span class="fu">summary</span>(bodyfat.lm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Bodyfat ~ Abdomen, data = bodyfat)

Residuals:
     Min       1Q   Median       3Q      Max 
-19.0160  -3.7557   0.0554   3.4215  12.9007 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -39.28018    2.66034  -14.77   &lt;2e-16 ***
Abdomen       0.63130    0.02855   22.11   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.877 on 250 degrees of freedom
Multiple R-squared:  0.6617,    Adjusted R-squared:  0.6603 
F-statistic: 488.9 on 1 and 250 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section>
<section id="classical-linear-regression-1" class="slide level2">
<h2>Classical Linear Regression</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a></a><span class="fu">plot</span>(bodyfat<span class="sc">$</span>Abdomen, bodyfat<span class="sc">$</span>Bodyfat, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb6-2"><a></a><span class="fu">abline</span>(bodyfat.lm, <span class="at">col=</span><span class="st">"red"</span>, <span class="at">lwd=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-3-1.png" width="960" class="r-stretch"></section>
<section id="bayesian-simple-linear-regression-using-the-reference-prior" class="slide level2 smaller">
<h2>Bayesian Simple Linear Regression Using the Reference Prior</h2>
<p>Likelihood: <span class="math display">\[
Y_i~|~x_i, \alpha, \beta,\sigma^2~ \sim~ \textsf{Normal}(\alpha + \beta x_i, \sigma^2),\qquad i = 1,\cdots, n.
\]</span> Priors: <span class="math display">\[
p(\alpha,\beta \mid \sigma^2) \propto 1, \quad p(\sigma^2) \propto 1/\sigma^2.
\]</span> Posterior: <span class="math display">\[
\beta~|~y_1,\cdots,y_n ~\sim~ \textsf{t}\left(n-2,\ \hat{\beta},\ \frac{\hat{\sigma}^2}{\text{S}_{xx}}\right) = \textsf{t}\left(n-2,\  \hat{\beta},\ (\text{se}_{\beta})^2\right)
\]</span> <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\beta},\ (\text{se}_{\beta})^2\)</span> are MLE estimates! For derivation, see Section 6.1.4 of <a href="https://statswithr.github.io/book/">An Introduction to Bayesian Thinking</a></p>
<p>The Bayesian posterior credible intervals for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> under the reference prior, are numerically equivalent to the confidence intervals from the classical frequentist OLS analysis!</p>
</section>
<section id="plot" class="slide level2">
<h2>Plot</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a></a>ind <span class="ot">=</span> <span class="fu">order</span>(bodyfat<span class="sc">$</span>Abdomen)</span>
<span id="cb7-2"><a></a>x <span class="ot">=</span> bodyfat<span class="sc">$</span>Abdomen[ind]</span>
<span id="cb7-3"><a></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">Abdomen =</span> x)</span>
<span id="cb7-4"><a></a>ymean <span class="ot">=</span> <span class="fu">predict</span>(bodyfat.lm, <span class="at">newdata =</span> df, <span class="at">interval =</span> <span class="st">"confidence"</span>)</span>
<span id="cb7-5"><a></a>ypred <span class="ot">=</span> <span class="fu">predict</span>(bodyfat.lm, <span class="at">newdata =</span> df, <span class="at">interval =</span> <span class="st">"prediction"</span>)</span>
<span id="cb7-6"><a></a><span class="fu">plot</span>(bodyfat<span class="sc">$</span>Abdomen, bodyfat<span class="sc">$</span>Bodyfat, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb7-7"><a></a><span class="fu">lines</span>(x, ymean[,<span class="dv">1</span>], <span class="at">col=</span><span class="st">"red"</span>, <span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb7-8"><a></a><span class="fu">lines</span>(x, ymean[,<span class="dv">2</span>], <span class="at">col=</span><span class="st">"grey"</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb7-9"><a></a><span class="fu">lines</span>(x, ymean[,<span class="dv">3</span>], <span class="at">col=</span><span class="st">"grey"</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb7-10"><a></a><span class="fu">lines</span>(x, ypred[,<span class="dv">2</span>], <span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb7-11"><a></a><span class="fu">lines</span>(x, ypred[,<span class="dv">3</span>], <span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb7-12"><a></a>labels <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"Data"</span>, <span class="st">"Mean"</span>, <span class="st">"Confidence"</span>, <span class="st">"Prediction"</span>)</span>
<span id="cb7-13"><a></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="at">legend =</span> labels, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>, <span class="st">"grey"</span>, <span class="st">"blue"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-4-1.png" width="960" class="r-stretch"></section>
<section id="informative-priors" class="slide level2">
<h2>Informative Priors</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a></a><span class="fu">library</span>(brms)</span>
<span id="cb8-2"><a></a>priors <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb8-3"><a></a>  <span class="fu">prior</span>(<span class="fu">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="at">class =</span> <span class="st">"b"</span>),       <span class="co"># Prior for coefficients (slope)</span></span>
<span id="cb8-4"><a></a>  <span class="fu">prior</span>(<span class="fu">normal</span>(<span class="dv">0</span>, <span class="dv">25</span>), <span class="at">class =</span> <span class="st">"Intercept"</span>), <span class="co"># Prior for the intercept</span></span>
<span id="cb8-5"><a></a>  <span class="fu">prior</span>(<span class="fu">student_t</span>(<span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">100</span>), <span class="at">class =</span> <span class="st">"sigma"</span>) <span class="co"># Prior for the residual standard deviation</span></span>
<span id="cb8-6"><a></a>)</span>
<span id="cb8-7"><a></a>bodyfat.brms <span class="ot">=</span> <span class="fu">brm</span>(Bodyfat <span class="sc">~</span> Abdomen, <span class="at">data =</span> bodyfat,<span class="at">family =</span> <span class="fu">gaussian</span>(),<span class="at">prior =</span> priors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 3.7e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.014 seconds (Warm-up)
Chain 1:                0.013 seconds (Sampling)
Chain 1:                0.027 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 5e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.016 seconds (Warm-up)
Chain 2:                0.013 seconds (Sampling)
Chain 2:                0.029 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 3e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.017 seconds (Warm-up)
Chain 3:                0.012 seconds (Sampling)
Chain 3:                0.029 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 2e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.019 seconds (Warm-up)
Chain 4:                0.013 seconds (Sampling)
Chain 4:                0.032 seconds (Total)
Chain 4: </code></pre>
</div>
</div>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a></a><span class="fu">summary</span>(bodyfat.brms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> Family: gaussian 
  Links: mu = identity; sigma = identity 
Formula: Bodyfat ~ Abdomen 
   Data: bodyfat (Number of observations: 252) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Regression Coefficients:
          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept   -39.30      2.67   -44.47   -33.94 1.00     4061     2807
Abdomen       0.63      0.03     0.57     0.69 1.00     4082     3052

Further Distributional Parameters:
      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sigma     4.90      0.22     4.48     5.36 1.00     3732     2907

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
</div>
</section>
<section id="plot-1" class="slide level2">
<h2>Plot</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a></a><span class="fu">pp_check</span>(bodyfat.brms, <span class="at">ndraws =</span> <span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-6-1.png" width="960" class="r-stretch"></section>
<section id="plot-all-parameters" class="slide level2">
<h2>Plot all parameters</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a></a><span class="fu">plot</span>(bodyfat.brms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-7-1.png" width="960" class="r-stretch"></section>
<section id="plot-predictions" class="slide level2">
<h2>Plot Predictions</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a></a>new_data <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">Abdomen =</span> x)</span>
<span id="cb14-2"><a></a>predictions <span class="ot">=</span> <span class="fu">posterior_predict</span>(bodyfat.brms, <span class="at">newdata =</span> new_data)</span>
<span id="cb14-3"><a></a>quntiles <span class="ot">=</span> <span class="fu">apply</span>(predictions, <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="fu">quantile</span>(x, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>,<span class="fl">0.975</span>)))</span>
<span id="cb14-4"><a></a><span class="fu">plot</span>(bodyfat<span class="sc">$</span>Abdomen, bodyfat<span class="sc">$</span>Bodyfat, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb14-5"><a></a><span class="fu">lines</span>(x, quntiles[<span class="dv">1</span>,], <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb14-6"><a></a><span class="fu">lines</span>(x, quntiles[<span class="dv">3</span>,], <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb14-7"><a></a><span class="fu">lines</span>(x, quntiles[<span class="dv">2</span>,], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-8-1.png" width="960" class="r-stretch"></section>
<section id="small-data-example" class="slide level2">
<h2>Small Data Example</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a></a><span class="fu">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb15-2"><a></a>true_intercept <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb15-3"><a></a>true_slope <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb15-4"><a></a>true_sigma <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb15-5"><a></a></span>
<span id="cb15-6"><a></a><span class="co"># Number of data points (intentionally small)</span></span>
<span id="cb15-7"><a></a>n_data <span class="ot">&lt;-</span> <span class="dv">15</span></span>
<span id="cb15-8"><a></a></span>
<span id="cb15-9"><a></a><span class="co"># Generate predictor (x)</span></span>
<span id="cb15-10"><a></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_data, <span class="at">min =</span> <span class="sc">-</span><span class="dv">5</span>, <span class="at">max =</span> <span class="dv">5</span>)</span>
<span id="cb15-11"><a></a></span>
<span id="cb15-12"><a></a><span class="co"># Generate outcome (y) with noise</span></span>
<span id="cb15-13"><a></a>y <span class="ot">&lt;-</span> true_intercept <span class="sc">+</span> true_slope <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(n_data, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> true_sigma)</span>
<span id="cb15-14"><a></a></span>
<span id="cb15-15"><a></a><span class="co"># Combine into a data frame</span></span>
<span id="cb15-16"><a></a>sim_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb15-17"><a></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-9-1.png" width="960" class="r-stretch"></section>
<section id="classical-linear-regression-2" class="slide level2">
<h2>Classical Linear Regression</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a></a>ols_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> sim_data)</span>
<span id="cb16-2"><a></a><span class="fu">summary</span>(ols_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x, data = sim_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-6.0837 -1.5346 -0.8869  1.9904  4.4709 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   3.5984     0.7468   4.818 0.000336 ***
x             1.6563     0.2350   7.047 8.71e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.795 on 13 degrees of freedom
Multiple R-squared:  0.7925,    Adjusted R-squared:  0.7766 
F-statistic: 49.66 on 1 and 13 DF,  p-value: 8.712e-06</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a></a>x_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(sim_data<span class="sc">$</span>x), <span class="fu">max</span>(sim_data<span class="sc">$</span>x), <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb18-2"><a></a></span>
<span id="cb18-3"><a></a><span class="co"># Get OLS predictions</span></span>
<span id="cb18-4"><a></a>ols_predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(ols_model, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_vals), <span class="at">interval =</span> <span class="st">"confidence"</span>)</span>
<span id="cb18-5"><a></a>ols_predictions <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(ols_predictions) <span class="co"># Convert to data frame</span></span>
<span id="cb18-6"><a></a>ols_predictions<span class="sc">$</span>x <span class="ot">&lt;-</span> x_vals</span>
<span id="cb18-7"><a></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb18-8"><a></a><span class="fu">lines</span>(x_vals,ols_predictions<span class="sc">$</span>fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/simols-1.png" width="960" class="r-stretch"></section>
<section id="bayesian-linear-regression" class="slide level2">
<h2>Bayesian Linear Regression</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a></a>priors <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb19-2"><a></a>  <span class="fu">prior</span>(<span class="fu">normal</span>(<span class="dv">5</span>, <span class="dv">1</span>), <span class="at">class =</span> <span class="st">"Intercept"</span>),  <span class="co"># Prior for intercept</span></span>
<span id="cb19-3"><a></a>  <span class="fu">prior</span>(<span class="fu">normal</span>(<span class="dv">2</span>, <span class="dv">1</span>), <span class="at">class =</span> <span class="st">"b"</span>),       <span class="co"># Prior for slope</span></span>
<span id="cb19-4"><a></a>  <span class="fu">prior</span>(<span class="fu">student_t</span>(<span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">5</span>), <span class="at">class =</span> <span class="st">"sigma"</span>) <span class="co"># Prior for sigma (wider than the true value)</span></span>
<span id="cb19-5"><a></a>)</span>
<span id="cb19-6"><a></a></span>
<span id="cb19-7"><a></a><span class="co"># Fit the Bayesian model using brms</span></span>
<span id="cb19-8"><a></a>bayes_model <span class="ot">&lt;-</span> <span class="fu">brm</span>(y <span class="sc">~</span> x,<span class="at">data =</span> sim_data,<span class="at">family =</span> <span class="fu">gaussian</span>(),<span class="at">prior =</span> priors,<span class="at">seed =</span> <span class="dv">123</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 3.5e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.35 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.013 seconds (Warm-up)
Chain 1:                0.01 seconds (Sampling)
Chain 1:                0.023 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 1e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.013 seconds (Warm-up)
Chain 2:                0.011 seconds (Sampling)
Chain 2:                0.024 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 1e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.013 seconds (Warm-up)
Chain 3:                0.011 seconds (Sampling)
Chain 3:                0.024 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 1e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.014 seconds (Warm-up)
Chain 4:                0.01 seconds (Sampling)
Chain 4:                0.024 seconds (Total)
Chain 4: </code></pre>
</div>
</div>
</section>
<section id="summary-1" class="slide level2">
<h2>Summary</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a></a><span class="fu">summary</span>(bayes_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> Family: gaussian 
  Links: mu = identity; sigma = identity 
Formula: y ~ x 
   Data: sim_data (Number of observations: 15) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Regression Coefficients:
          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept     4.71      0.73     3.36     6.20 1.00     2856     2479
x             1.68      0.26     1.16     2.21 1.00     2984     2130

Further Distributional Parameters:
      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sigma     3.22      0.72     2.17     5.00 1.00     2474     2322

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
</div>
</section>
<section id="plot-2" class="slide level2">
<h2>Plot</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb23-2"><a></a>bayes_estimates <span class="ot">&lt;-</span> <span class="fu">summary</span>(bayes_model)<span class="sc">$</span>fixed</span>
<span id="cb23-3"><a></a><span class="fu">print</span>(<span class="st">"Bayesian Estimates:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Bayesian Estimates:"</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a></a><span class="fu">print</span>(bayes_estimates)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>          Estimate Est.Error l-95% CI u-95% CI     Rhat Bulk_ESS Tail_ESS
Intercept 4.712777 0.7325443 3.358504 6.198919 1.001263 2855.809 2478.723
x         1.680288 0.2634961 1.161723 2.207987 1.001100 2984.003 2129.753</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a></a><span class="co"># Get Bayesian predictions (posterior draws of the expected value, epred)</span></span>
<span id="cb27-2"><a></a>bayes_predictions <span class="ot">&lt;-</span> <span class="fu">posterior_epred</span>(bayes_model, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_vals))</span>
<span id="cb27-3"><a></a>quntiles <span class="ot">=</span> <span class="fu">apply</span>(bayes_predictions, <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="fu">quantile</span>(x, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>,<span class="fl">0.975</span>)))</span>
<span id="cb27-4"><a></a>bayes_pred_summary<span class="ot">=</span> <span class="fu">as.data.frame</span>(<span class="fu">t</span>(quntiles)) <span class="co">#col.names=c("lwr","med","upr")</span></span>
<span id="cb27-5"><a></a><span class="fu">colnames</span>(bayes_pred_summary) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"lower"</span>,<span class="st">"med"</span>,<span class="st">"upper"</span>)</span>
<span id="cb27-6"><a></a>bayes_pred_summary<span class="sc">$</span>x <span class="ot">&lt;-</span> x_vals</span>
<span id="cb27-7"><a></a></span>
<span id="cb27-8"><a></a><span class="co"># Plot the data, OLS fit, and Bayesian fit</span></span>
<span id="cb27-9"><a></a><span class="fu">ggplot</span>(sim_data) <span class="sc">+</span></span>
<span id="cb27-10"><a></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y),<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb27-11"><a></a>  <span class="co"># True regression line (for comparison)</span></span>
<span id="cb27-12"><a></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> true_intercept, <span class="at">slope =</span> true_slope, <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb27-13"><a></a>  <span class="co"># OLS regression line and confidence interval</span></span>
<span id="cb27-14"><a></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> ols_predictions, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> fit), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb27-15"><a></a>  <span class="fu">geom_ribbon</span>(<span class="at">data =</span> ols_predictions, <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">ymin =</span> lwr, <span class="at">ymax =</span> upr), <span class="at">fill =</span> <span class="st">"red"</span>, <span class="at">alpha =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb27-16"><a></a>  <span class="co"># Bayesian regression line and credible interval</span></span>
<span id="cb27-17"><a></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> bayes_pred_summary, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> med), <span class="at">color =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb27-18"><a></a>  <span class="fu">geom_ribbon</span>(<span class="at">data =</span> bayes_pred_summary, <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper), <span class="at">fill =</span> <span class="st">"blue"</span>, <span class="at">alpha =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb27-19"><a></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Bayesian vs. OLS Regression (Small Sample Size)"</span>,</span>
<span id="cb27-20"><a></a>       <span class="at">x =</span> <span class="st">"Predictor (x)"</span>,</span>
<span id="cb27-21"><a></a>       <span class="at">y =</span> <span class="st">"Outcome (y)"</span>) <span class="sc">+</span></span>
<span id="cb27-22"><a></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/simplot-1.png" width="960" class="r-stretch"></section>
<section id="sparsity-inducing-prior" class="slide level2">
<h2>Sparsity Inducing Prior</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a></a><span class="co"># Simulate data with some true coefficients and some noise</span></span>
<span id="cb28-2"><a></a><span class="fu">set.seed</span>(<span class="dv">7</span>)  <span class="co"># For reproducibility</span></span>
<span id="cb28-3"><a></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>        <span class="co"># Number of observations</span></span>
<span id="cb28-4"><a></a>p <span class="ot">&lt;-</span> <span class="dv">20</span>         <span class="co"># Number of predictors (some relevant, some not)</span></span>
<span id="cb28-5"><a></a>true_beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="sc">-</span><span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>) <span class="co"># Only first 3 are truly non-zero</span></span>
<span id="cb28-6"><a></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p), n, p)</span>
<span id="cb28-7"><a></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> true_beta <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>) <span class="co"># Adding noise</span></span>
<span id="cb28-8"><a></a><span class="co"># Fit a standard linear regression (OLS)</span></span>
<span id="cb28-9"><a></a>ols_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X<span class="dv">-1</span>)</span>
<span id="cb28-10"><a></a></span>
<span id="cb28-11"><a></a><span class="co"># Fit a Bayesian linear regression with shrinkage (using the 'BAS' package)</span></span>
<span id="cb28-12"><a></a><span class="co"># Install if you don't have it: install.packages("BAS")</span></span>
<span id="cb28-13"><a></a><span class="fu">library</span>(BAS)</span>
<span id="cb28-14"><a></a></span>
<span id="cb28-15"><a></a><span class="co"># Define prior (we'll use a g-prior which is common for shrinkage)</span></span>
<span id="cb28-16"><a></a><span class="co"># 'gprior' is common for shrinkage</span></span>
<span id="cb28-17"><a></a><span class="co"># 'bic' is another option that often works well</span></span>
<span id="cb28-18"><a></a><span class="co"># 'hyper-g' is another good choice</span></span>
<span id="cb28-19"><a></a>bas_model <span class="ot">&lt;-</span> <span class="fu">bas.lm</span>(y <span class="sc">~</span> X<span class="dv">-1</span>, <span class="at">prior =</span> <span class="st">"g-prior"</span>, <span class="at">modelprior =</span> <span class="fu">uniform</span>())</span>
<span id="cb28-20"><a></a></span>
<span id="cb28-21"><a></a><span class="co"># Examine coefficients</span></span>
<span id="cb28-22"><a></a>ols_coef <span class="ot">&lt;-</span> <span class="fu">coef</span>(ols_model)</span>
<span id="cb28-23"><a></a>bas_coef <span class="ot">&lt;-</span> <span class="fu">coefficients</span>(bas_model) <span class="co"># These are *posterior means*</span></span>
<span id="cb28-24"><a></a></span>
<span id="cb28-25"><a></a><span class="co"># Plot the coefficients to visualize shrinkage</span></span>
<span id="cb28-26"><a></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span>p, true_beta, <span class="at">type =</span> <span class="st">"h"</span>, <span class="at">ylab =</span> <span class="st">"Coefficient Value"</span>, <span class="at">xlab =</span> <span class="st">"Predictor"</span>, <span class="at">main =</span> <span class="st">"True Coefficients"</span>)</span>
<span id="cb28-27"><a></a><span class="fu">points</span>(<span class="dv">1</span><span class="sc">:</span>p, ols_coef, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">pch =</span> <span class="dv">16</span>) <span class="co"># OLS</span></span>
<span id="cb28-28"><a></a><span class="fu">points</span>(<span class="dv">1</span><span class="sc">:</span>p, bas_coef<span class="sc">$</span>postmean, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">pch =</span> <span class="dv">16</span>) <span class="co"># Bayesian</span></span>
<span id="cb28-29"><a></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb28-30"><a></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"True"</span>, <span class="st">"OLS"</span>, <span class="st">"Bayesian"</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/sparsityprior-1.png" width="960" class="r-stretch"></section>
<section id="what-are-random-intercepts-models" class="slide level2">
<h2>What are Random Intercepts Models?</h2>
<ul>
<li>Extension of Linear Regression: Deals with <em>clustered</em> or <em>hierarchical</em> data.</li>
<li>Clustered Data: Observations are grouped within clusters (e.g., students within schools, patients within hospitals, measurements over time within individuals).</li>
<li>Problem with Standard Regression: Ignores the clustering, leading to:
<ul>
<li>Underestimated standard errors (and thus, inflated Type I error rates - false positives).</li>
<li>Incorrect conclusions about the significance of predictors.</li>
</ul></li>
<li>Random Intercepts: Assume each cluster has its own <em>intercept</em>, which is drawn from a distribution (usually normal). This accounts for between-cluster variability.</li>
</ul>
</section>
<section id="the-model-equation" class="slide level2">
<h2>The Model Equation</h2>
<ul>
<li><p>Level 1 (Individual Level):</p>
<p><span class="math display">\[
  y_{ij} = \beta_{0j} + \beta_1x_{ij} + \epsilon_{ij}
  \]</span></p>
<ul>
<li><span class="math inline">\(y_{ij}\)</span>: Outcome for individual <span class="math inline">\(i\)</span> in cluster <span class="math inline">\(j\)</span>.</li>
<li><span class="math inline">\(x_{ij}\)</span>: Predictor for individual <span class="math inline">\(i\)</span> in cluster <span class="math inline">\(j\)</span>.</li>
<li><span class="math inline">\(\beta_{0j}\)</span>: Intercept for cluster <span class="math inline">\(j\)</span>.</li>
<li><span class="math inline">\(\beta_1\)</span>: Effect of <span class="math inline">\(x\)</span> (assumed to be <em>fixed</em> - the same across all clusters).</li>
<li><span class="math inline">\(\epsilon_{ij}\)</span>: Individual-level error, <span class="math inline">\(\epsilon_{ij} \sim N(0, \sigma^2_e)\)</span>.</li>
</ul></li>
</ul>
</section>
<section id="level-2-cluster-level" class="slide level2">
<h2>Level 2 (Cluster Level)</h2>
<p><span class="math display">\[
\beta_{0j} = \gamma + u_{j}
\]</span></p>
<ul>
<li><p><span class="math inline">\(\gamma\)</span>: Overall average intercept.</p></li>
<li><p><span class="math inline">\(u_{j}\)</span>: Random effect (deviation) of cluster <span class="math inline">\(j\)</span>’s intercept from the overall average, <span class="math inline">\(u_{j} \sim N(0, \sigma^2_u)\)</span>.</p></li>
<li><p><strong>Combined Equation:</strong></p>
<p><span class="math display">\[
  y_{ij} = (\gamma+ u_{j}) + \beta_1x_{ij}  + \epsilon_{ij}
  \]</span></p></li>
</ul>
</section>
<section id="key-parameters-interpretation" class="slide level2">
<h2>Key Parameters &amp; Interpretation</h2>
<ul>
<li><span class="math inline">\(\gamma\)</span> (Fixed Intercept): The average outcome when the predictor <span class="math inline">\(x\)</span> is zero (and across all clusters).</li>
<li><span class="math inline">\(\beta_1\)</span> (Fixed Slope): The change in the outcome for a one-unit increase in <span class="math inline">\(x\)</span>, <em>within</em> any given cluster.</li>
<li><span class="math inline">\(\sigma^2_u\)</span> (Random Intercept Variance): The variance of the cluster-specific intercepts. Indicates the amount of between-cluster variability. A larger <span class="math inline">\(\sigma^2_u\)</span> means more variation between clusters.</li>
<li><span class="math inline">\(\sigma^2_e\)</span> (Residual Variance): The variance of the individual-level errors. Indicates within-cluster variability.</li>
<li>Intraclass Correlation Coefficient (ICC):
<ul>
<li><span class="math inline">\(ICC = \sigma^2_u / (\sigma^2_u + \sigma^2_e)\)</span></li>
<li>Represents the proportion of total variance that is due to between-cluster differences. Ranges from 0 to 1. Higher ICC means more clustering.</li>
</ul></li>
</ul>
</section>
<section id="example-student-test-scores-in-schools" class="slide level2">
<h2>Example: Student Test Scores in Schools</h2>
<ul>
<li><strong>Data:</strong> We have data on student test scores (<span class="math inline">\(score\)</span>) and a student-level predictor (e.g., hours of study, <span class="math inline">\(studyhours\)</span>). Students are nested within schools.</li>
<li><strong>Research Question:</strong> Does the number of study hours predict test scores, accounting for the fact that students are clustered within schools?</li>
<li><strong>Model:</strong>
<ul>
<li><span class="math inline">\(score_{ij} = \gamma + \beta_1(studyhours)_{ij} + u_{j} + \epsilon_{ij}\)</span></li>
<li><span class="math inline">\(u_{j} \sim N(0, \sigma^2_u)\)</span></li>
<li><span class="math inline">\(\epsilon_{ij} \sim N(0, \sigma^2_e)\)</span></li>
</ul></li>
</ul>
</section>
<section id="r-demonstration---setup-and-data" class="slide level2">
<h2>R Demonstration - Setup and Data</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a></a><span class="co"># Install and load necessary packages</span></span>
<span id="cb29-2"><a></a><span class="fu">library</span>(lme4)</span>
<span id="cb29-3"><a></a><span class="fu">library</span>(lmerTest)</span>
<span id="cb29-4"><a></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb29-5"><a></a></span>
<span id="cb29-6"><a></a><span class="co"># Create some simulated data (for reproducibility)</span></span>
<span id="cb29-7"><a></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb29-8"><a></a>n_schools <span class="ot">&lt;-</span> <span class="dv">30</span>  <span class="co"># Number of schools</span></span>
<span id="cb29-9"><a></a>n_students_per_school <span class="ot">&lt;-</span> <span class="dv">20</span> <span class="co"># Students per school</span></span>
<span id="cb29-10"><a></a>n_total <span class="ot">&lt;-</span> n_schools <span class="sc">*</span> n_students_per_school</span>
<span id="cb29-11"><a></a></span>
<span id="cb29-12"><a></a>school_effects <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_schools, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">2</span>) <span class="co"># School random effects</span></span>
<span id="cb29-13"><a></a>student_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb29-14"><a></a>  <span class="at">school_id =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_schools, <span class="at">each =</span> n_students_per_school),</span>
<span id="cb29-15"><a></a>  <span class="at">studyhours =</span> <span class="fu">rnorm</span>(n_total, <span class="at">mean =</span> <span class="dv">10</span>, <span class="at">sd =</span> <span class="dv">3</span>),</span>
<span id="cb29-16"><a></a>  <span class="at">error =</span> <span class="fu">rnorm</span>(n_total, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb29-17"><a></a>)</span>
<span id="cb29-18"><a></a></span>
<span id="cb29-19"><a></a><span class="co"># Generate scores based on the model</span></span>
<span id="cb29-20"><a></a>student_data<span class="sc">$</span>score <span class="ot">&lt;-</span> <span class="dv">50</span> <span class="sc">+</span> <span class="fl">1.5</span> <span class="sc">*</span> student_data<span class="sc">$</span>studyhours <span class="sc">+</span></span>
<span id="cb29-21"><a></a>                     school_effects[student_data<span class="sc">$</span>school_id] <span class="sc">+</span> student_data<span class="sc">$</span>error</span>
<span id="cb29-22"><a></a><span class="fu">head</span>(student_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>  school_id studyhours      error    score
1         1  11.279393 -0.6785031 65.11963
2         1   9.114786 -1.8555717 60.69566
3         1  12.685377  0.5332594 68.44037
4         1  12.634400  0.3102303 68.14088
5         1  12.464743 -1.3538343 66.22233
6         1  12.065921 -1.9429564 65.03497</code></pre>
</div>
</div>
</section>
<section id="plot-3" class="slide level2">
<h2>Plot</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a></a><span class="co"># Plot the data and use school_id for color and add a linear model to fit the data for each school</span></span>
<span id="cb31-2"><a></a><span class="fu">ggplot</span>(student_data, <span class="fu">aes</span>(<span class="at">x =</span> studyhours, <span class="at">y =</span> score, <span class="at">color =</span> <span class="fu">as.factor</span>(school_id))) <span class="sc">+</span></span>
<span id="cb31-3"><a></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb31-4"><a></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb31-5"><a></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-11-1.png" width="960" class="r-stretch"></section>
<section id="r-demonstration---fit-the-model" class="slide level2">
<h2>R Demonstration - Fit the Model</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a></a><span class="co"># Fit the random intercepts model using lmer()</span></span>
<span id="cb32-2"><a></a>model <span class="ot">&lt;-</span> <span class="fu">lmer</span>(score <span class="sc">~</span> studyhours <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> school_id), <span class="at">data =</span> student_data)</span>
<span id="cb32-3"><a></a></span>
<span id="cb32-4"><a></a><span class="co"># View the model summary</span></span>
<span id="cb32-5"><a></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Linear mixed model fit by REML. t-tests use Satterthwaite's method [
lmerModLmerTest]
Formula: score ~ studyhours + (1 | school_id)
   Data: student_data

REML criterion at convergence: 1846.4

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.71647 -0.68621  0.01679  0.64918  2.99847 

Random effects:
 Groups    Name        Variance Std.Dev.
 school_id (Intercept) 3.845    1.961   
 Residual              1.014    1.007   
Number of obs: 600, groups:  school_id, 30

Fixed effects:
             Estimate Std. Error        df t value Pr(&gt;|t|)    
(Intercept)  49.84593    0.38800  38.91559   128.5   &lt;2e-16 ***
studyhours    1.50785    0.01432 569.50545   105.3   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
           (Intr)
studyhours -0.371</code></pre>
</div>
</div>
</section>
<section id="r-demonstration---interpreting-the-output" class="slide level2">
<h2>R Demonstration - Interpreting the Output</h2>
<ul>
<li><strong>Fixed Effects:</strong>
<ul>
<li>Intercept <span class="math inline">\((\gamma)\)</span>: Estimated average score when <span class="math inline">\(studyhours\)</span> is 0.</li>
<li>studyhours <span class="math inline">\((\beta_1)\)</span>: Estimated increase in score for each additional hour of study. The p-value indicates if this effect is statistically significant.</li>
</ul></li>
<li><strong>Random Effects:</strong>
<ul>
<li>Variance (school_id) <span class="math inline">\(\sigma^2_u\)</span>: Estimated variance of the school-specific intercepts.</li>
<li>Variance (Residual) <span class="math inline">\(\sigma^2_e\)</span>: Estimated variance of the individual-level errors.</li>
</ul></li>
</ul>
</section>
<section id="icc-calculation" class="slide level2">
<h2>ICC Calculation*</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a></a><span class="fu">VarCorr</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> Groups    Name        Std.Dev.
 school_id (Intercept) 1.9609  
 Residual              1.0072  </code></pre>
</div>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a></a>sigma2u <span class="ot">&lt;-</span> <span class="fl">2.384</span>   <span class="co">#From the output of VarCorr(model), school_id variance</span></span>
<span id="cb36-2"><a></a>sigma2e <span class="ot">&lt;-</span> <span class="fl">0.962</span>   <span class="co">#From the output of VarCorr(model), residual variance</span></span>
<span id="cb36-3"><a></a>icc <span class="ot">&lt;-</span> sigma2u <span class="sc">/</span> (sigma2u <span class="sc">+</span> sigma2e)</span>
<span id="cb36-4"><a></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"ICC:"</span>, <span class="fu">round</span>(icc, <span class="dv">3</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "ICC: 0.712"</code></pre>
</div>
</div>
</section>
<section id="r-demonstration---visualizing-the-model" class="slide level2">
<h2>R Demonstration - Visualizing the Model</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a></a><span class="co"># Get predicted values for each student</span></span>
<span id="cb38-2"><a></a>student_data<span class="sc">$</span>predicted <span class="ot">&lt;-</span> <span class="fu">predict</span>(model)</span>
<span id="cb38-3"><a></a></span>
<span id="cb38-4"><a></a><span class="co"># Plot observed vs. predicted values, colored by school</span></span>
<span id="cb38-5"><a></a><span class="fu">ggplot</span>(student_data, <span class="fu">aes</span>(<span class="at">x =</span> score, <span class="at">y =</span> predicted, <span class="at">color =</span> <span class="fu">factor</span>(school_id))) <span class="sc">+</span></span>
<span id="cb38-6"><a></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb38-7"><a></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span>  <span class="co"># Add a diagonal line</span></span>
<span id="cb38-8"><a></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Observed vs. Predicted Scores"</span>,</span>
<span id="cb38-9"><a></a>       <span class="at">x =</span> <span class="st">"Observed Score"</span>,</span>
<span id="cb38-10"><a></a>       <span class="at">y =</span> <span class="st">"Predicted Score"</span>,</span>
<span id="cb38-11"><a></a>       <span class="at">color =</span> <span class="st">"School ID"</span>) <span class="sc">+</span></span>
<span id="cb38-12"><a></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span> </span>
<span id="cb38-13"><a></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)  <span class="co">#remove legend because too many schools</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/lmeviz-1.png" width="960" class="r-stretch"></section>
<section id="plot-random-intercepts-for-each-school" class="slide level2">
<h2>Plot random intercepts for each school</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a></a><span class="fu">plot</span>(school_effects,<span class="fu">ranef</span>(model)<span class="sc">$</span>school_id[,<span class="dv">1</span>], <span class="at">pch=</span><span class="dv">16</span>, <span class="at">xlab=</span><span class="st">"True school effect"</span>, <span class="at">ylab=</span><span class="st">"Estimated school effect"</span>)</span>
<span id="cb39-2"><a></a><span class="fu">abline</span>(<span class="at">a=</span><span class="dv">0</span>,<span class="at">b=</span><span class="dv">1</span>,<span class="at">col=</span><span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-14-1.png" width="960" class="r-stretch"></section>
<section id="plot-random-intercepts-for-each-school-1" class="slide level2">
<h2>Plot random intercepts for each school</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a></a>ranef_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">ranef</span>(model)<span class="sc">$</span>school_id)</span>
<span id="cb40-2"><a></a>ranef_df<span class="sc">$</span>school_id <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(ranef_df)</span>
<span id="cb40-3"><a></a></span>
<span id="cb40-4"><a></a><span class="fu">ggplot</span>(ranef_df, <span class="fu">aes</span>(<span class="at">x =</span> school_id, <span class="at">y =</span> <span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>)) <span class="sc">+</span></span>
<span id="cb40-5"><a></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb40-6"><a></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb40-7"><a></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Random Intercepts by School"</span>,</span>
<span id="cb40-8"><a></a>       <span class="at">x =</span> <span class="st">"School ID"</span>,</span>
<span id="cb40-9"><a></a>       <span class="at">y =</span> <span class="st">"Random Intercept (Deviation from Overall Mean)"</span>) <span class="sc">+</span></span>
<span id="cb40-10"><a></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/lmeviz1-1.png" width="960" class="r-stretch"></section>
<section id="comparing-to-a-simple-linear-model" class="slide level2">
<h2>Comparing to a Simple Linear Model</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a></a><span class="co"># Fit a simple linear regression (ignoring clustering)</span></span>
<span id="cb41-2"><a></a>simple_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(score <span class="sc">~</span> studyhours, <span class="at">data =</span> student_data)</span>
<span id="cb41-3"><a></a><span class="co"># summary(simple_model) </span></span>
<span id="cb41-4"><a></a></span>
<span id="cb41-5"><a></a><span class="co"># Compare standard errors</span></span>
<span id="cb41-6"><a></a><span class="fu">coef</span>(<span class="fu">summary</span>(model))  <span class="co"># Random intercepts model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>             Estimate Std. Error        df  t value     Pr(&gt;|t|)
(Intercept) 49.845931 0.38799971  38.91559 128.4690 9.116905e-53
studyhours   1.507846 0.01431791 569.50545 105.3119 0.000000e+00</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a></a><span class="fu">coef</span>(<span class="fu">summary</span>(simple_model)) <span class="co"># Simple linear model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            Estimate Std. Error  t value      Pr(&gt;|t|)
(Intercept) 50.10072 0.31835002 157.3762  0.000000e+00
studyhours   1.48248 0.03043469  48.7102 2.579971e-210</code></pre>
</div>
</div>
<ul>
<li>Notice how the standard error for <span class="math inline">\(studyhours\)</span> is <em>smaller</em> in the simple linear model. This is because it doesn’t account for the clustering, leading to an overestimation of the precision of the estimate.</li>
</ul>
</section>
<section id="model-extensions-and-considerations" class="slide level2">
<h2>Model Extensions and Considerations</h2>
<ul>
<li><strong>Random Slopes:</strong> Allow the <em>effect</em> of <span class="math inline">\(studyhours\)</span> to vary across schools (not just the intercept). This is done by adding <span class="math inline">\((studyhours | school_id)\)</span> to the <span class="math inline">\(lmer\)</span> formula.</li>
<li><strong>Crossed Random Effects:</strong> When observations have multiple sources of clustering (e.g., students nested within schools AND neighborhoods).</li>
<li><strong>More Complex Models:</strong> Can include multiple predictors at both the individual and cluster levels.</li>
<li><strong>Model Selection:</strong> Use AIC, BIC, or likelihood ratio tests to compare different models (e.g., with and without random intercepts, with and without random slopes).</li>
<li><strong>Assumptions:</strong> Linearity, normality of residuals and random effects, independence of errors (within clusters, after accounting for the random effects), homoscedasticity. Check these assumptions using diagnostic plots.</li>
<li><strong>Software:</strong> <span class="math inline">\(lme4\)</span> is the most common R package. Other options include <span class="math inline">\(nlme\)</span>, <span class="math inline">\(brms\)</span> (for Bayesian mixed models), and <span class="math inline">\(glmmTMB\)</span>.</li>
</ul>
</section>
<section id="sweet-spot-for-bayesian-regression" class="slide level2">
<h2>Sweet Spot For Bayesian Regression</h2>
<ul>
<li>When you have a good prior knowledge about the parameters (e.g.&nbsp;from previous studies, expert opinion, etc.)</li>
<li>When you want to quantify uncertainty in your estimates</li>
<li>When you know that many of the predictors are not important (sparsity)</li>
<li>When dealing with small sample sizes</li>
<li>Hierarchical data (e.g.&nbsp;students within schools, patients within hospitals)
<ul>
<li>Modeling nested relationships</li>
<li>Sharing Information Across Groups</li>
<li>When you want to compare multiple models</li>
</ul></li>
</ul>
</section>
<section id="examples-where-bayesian-regression-is-useful" class="slide level2">
<h2>Examples where Bayesian Regression is Useful</h2>
<ul>
<li>Medical Trials: Incorporating prior knowledge about treatment effects.</li>
<li>Marketing: Predicting customer behavior with sparse data.</li>
<li>Educational studies: Analyzing student performance within classrooms and schools, while accounting for variations in teacher effectiveness and school resources.</li>
<li>Social sciences: Examining individual behavior within different communities or regions, while accounting for contextual factors and cultural differences.</li>
<li>Ecology: Modeling species populations within different habitats or ecosystems, while considering environmental variations and species interactions</li>
</ul>
</section>
<section>
<section id="bayesian-additive-regression-trees-bart" class="title-slide slide level1 center">
<h1>Bayesian Additive Regression Trees (BART)</h1>

</section>
<section id="core-ideas-of-bart" class="slide level2">
<h2>Core Ideas of BART</h2>
<ul>
<li>BART is a non-parametric Bayesian model used for both regression and classification.<br>
</li>
<li>Its core idea is to represent a complex relationship between predictors (<span class="math inline">\(X\)</span>) and a response variable (<span class="math inline">\(Y\)</span>) as a <em>sum of many simple regression trees</em>.<br>
</li>
<li>Each tree is a “weak learner.” It’s typically a small decision tree, often with only a - - Ensemble Power: The <em>sum</em> of the predictions from many of these weak learners creates a powerful and flexible model. This is similar in spirit to boosted trees (like XGBoost or LightGBM), but BART’s Bayesian nature provides important advantages.</li>
</ul>
</section>
<section id="the-model" class="slide level2 smaller">
<h2>The Model</h2>
<p><span class="math display">\[
Y_i = f(X_i) + \epsilon_i
f(X_i) = \sum_{j=1}^m g(X_i; T_j, M_j)
\epsilon_i ~ N(0, \sigma^2)
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(f(X_i)\)</span> is the overall prediction function, which is the sum of the tree predictions.</li>
<li><span class="math inline">\(\epsilon_i\)</span> is the error term, assumed to be normally distributed with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>.</li>
<li><span class="math inline">\(m\)</span> is the number of trees in the ensemble (a crucial hyperparameter).</li>
<li><span class="math inline">\(g(X_i; T_j, M_j)\)</span> is the prediction from the j-th tree.
<ul>
<li><span class="math inline">\(T_j\)</span> represents the <em>tree structure</em> (the splitting rules). It defines the splits based on the predictor variables and their values.</li>
<li><span class="math inline">\(M_j = {\mu_{1j}, \mu_{2j}, ..., \mu_{b_j j}}\)</span> represents the set of <em>terminal node parameters</em> (the “leaf values”) for the j-th tree. <span class="math inline">\(b_j\)</span> is the number of terminal nodes (leaves) in tree <span class="math inline">\(T_j\)</span>. Each leaf gets assigned a constant value (), which is the prediction for any observation that falls into that leaf.</li>
</ul></li>
</ul>
</section>
<section id="diagram" class="slide level2">
<h2>Diagram</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource default number-lines code-with-copy"><code class="sourceCode default"><span id="cb45-1"><a></a>graph TB</span>
<span id="cb45-2"><a></a>    subgraph Input</span>
<span id="cb45-3"><a></a>        X[Predictors]</span>
<span id="cb45-4"><a></a>    end</span>
<span id="cb45-5"><a></a>    subgraph BARTModel</span>
<span id="cb45-6"><a></a>        Tree1((Tree 1)) --&gt; Sum</span>
<span id="cb45-7"><a></a>        Tree2((Tree 2)) --&gt; Sum</span>
<span id="cb45-8"><a></a>        TreeM((Tree M)) --&gt; Sum</span>
<span id="cb45-9"><a></a>        Sum[Sum of Trees]</span>
<span id="cb45-10"><a></a>    end</span>
<span id="cb45-11"><a></a>    subgraph Output</span>
<span id="cb45-12"><a></a>        fX["f(X) - Prediction"]</span>
<span id="cb45-13"><a></a>    end</span>
<span id="cb45-14"><a></a>Input --&gt; BARTModel</span>
<span id="cb45-15"><a></a>BARTModel --&gt; Output</span>
<span id="cb45-16"><a></a>fX --&gt; Y["Y= f(X) + ε"]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    subgraph Input
        X[Predictors]
    end
    subgraph BARTModel
        Tree1((Tree 1)) --&gt; Sum
        Tree2((Tree 2)) --&gt; Sum
        TreeM((Tree M)) --&gt; Sum
        Sum[Sum of Trees]
    end
    subgraph Output
        fX["f(X) - Prediction"]
    end
Input --&gt; BARTModel
BARTModel --&gt; Output
fX --&gt; Y["Y= f(X) + ε"]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="single-node" class="slide level2">
<h2>Single Node</h2>
<p><em>Example of a single tree (<span class="math inline">\(T_j\)</span>):</em></p>
<div class="cell" data-reveal="true" data-fig-width="12" data-layout-align="default">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource default number-lines code-with-copy"><code class="sourceCode default"><span id="cb46-1"><a></a>graph TD</span>
<span id="cb46-2"><a></a>    A[X1 &lt; 3] --&gt; B{Yes};</span>
<span id="cb46-3"><a></a>    A --&gt; C{No};</span>
<span id="cb46-4"><a></a>    B --&gt; D[X2 &gt; 7];</span>
<span id="cb46-5"><a></a>    C --&gt; E[X2 &gt; 7];</span>
<span id="cb46-6"><a></a>    D --&gt; F("$$\mu_1$$");</span>
<span id="cb46-7"><a></a>    D --&gt; G("$$\mu2$$");</span>
<span id="cb46-8"><a></a>    E --&gt; H("$$\mu3$$");</span>
<span id="cb46-9"><a></a>    E --&gt; I("$$\mu4$$");</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A[X1 &lt; 3] --&gt; B{Yes};
    A --&gt; C{No};
    B --&gt; D[X2 &gt; 7];
    C --&gt; E[X2 &gt; 7];
    D --&gt; F("$$\mu_1$$");
    D --&gt; G("$$\mu2$$");
    E --&gt; H("$$\mu3$$");
    E --&gt; I("$$\mu4$$");
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="bayesian-priors-the-bayesian-part" class="slide level2 smaller">
<h2>Bayesian Priors (The Bayesian Part)</h2>
<ul>
<li><strong>Prior on Tree Structure (<span class="math inline">\(T_j\)</span>):</strong> prefer <em>smaller trees</em>: regularization and prevents overfitting. The prior for node splitting and choosing a particular predictor variable for splitting.<br>
</li>
<li><strong>Prior on Leaf Parameters (<span class="math inline">\(M_j\)</span>):</strong> A common choice is a normal prior: <span class="math display">\[
  \mu_{kj} ~ N(0, \sigma_\mu^2)
  \]</span> Prior 0: the trees are encouraged to make small “corrections” rather than large jumps.</li>
<li><strong>Prior on Error Variance (^2):</strong> An inverse-gamma prior is often used: <span class="math display">\[
  \sigma^2 ~ InverseGamma(\nu/2, \nu\lambda/2)
  \]</span>
<ul>
<li><span class="math inline">\(\nu\)</span> (degrees of freedom) and <span class="math inline">\(\lambda\)</span> (scale) are hyperparameters. This prior allows the model to learn the appropriate level of noise in the data.</li>
</ul></li>
</ul>
</section>
<section id="inference-mcmc" class="slide level2">
<h2>Inference (MCMC)</h2>
<ul>
<li>Because of the complex structure and the priors, we can’t directly calculate the posterior distribution of the parameters <span class="math inline">\((T_j, M_j, \sigma^2)\)</span>.<br>
</li>
<li>Instead, BART uses Markov Chain Monte Carlo (MCMC) methods, specifically a Metropolis-within-Gibbs sampler, to draw samples from the posterior distribution.</li>
</ul>
<p>The MCMC algorithm iteratively updates: <span class="math inline">\(T_j\)</span> and <span class="math inline">\(M_j\)</span></p>
</section>
<section id="iterative-updates" class="slide level2 smaller scrollable">
<h2>Iterative Updates</h2>
<ol type="1">
<li><strong>Each Tree (T_j and M_j):</strong> For each tree, the algorithm proposes changes to the tree structure (growing, pruning, changing splitting rules) and the leaf values. These proposals are accepted or rejected based on how well they improve the fit to the data, <em>given the current state of all the other trees</em>. This is the “backfitting” part of BART – each tree is adjusted to account for the predictions of the other trees.</li>
<li><strong>The Error Variance (^2):</strong> The error variance is updated based on the residuals (the difference between the observed <span class="math inline">\(Y\)</span> values and the current model predictions).</li>
</ol>
<p>The result of the MCMC process is a collection of samples from the posterior distribution of the model parameters.</p>
</section>
<section id="example-in-r-bart-package" class="slide level2">
<h2>Example in R (BART package)</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a></a><span class="co"># Install the BART package if you haven't already</span></span>
<span id="cb47-2"><a></a><span class="co"># install.packages("BART")</span></span>
<span id="cb47-3"><a></a><span class="fu">library</span>(BART)</span>
<span id="cb47-4"><a></a><span class="fu">library</span>(MASS) <span class="co">#For Boston Housing</span></span>
<span id="cb47-5"><a></a></span>
<span id="cb47-6"><a></a><span class="co"># Load the Boston Housing dataset</span></span>
<span id="cb47-7"><a></a><span class="fu">data</span>(Boston)</span>
<span id="cb47-8"><a></a>X <span class="ot">&lt;-</span> Boston[, <span class="sc">-</span><span class="dv">14</span>]  <span class="co"># Predictors (all columns except 'medv')</span></span>
<span id="cb47-9"><a></a>Y <span class="ot">&lt;-</span> Boston[, <span class="dv">14</span>]   <span class="co"># Response variable ('medv' - median house value)</span></span>
<span id="cb47-10"><a></a></span>
<span id="cb47-11"><a></a><span class="co"># Fit the BART model</span></span>
<span id="cb47-12"><a></a><span class="co">#  - ntree: Number of trees (m in the formulas)</span></span>
<span id="cb47-13"><a></a><span class="co">#  - ndpost: Number of posterior samples to keep (after burn-in)</span></span>
<span id="cb47-14"><a></a><span class="co">#  - nskip: Number of burn-in iterations to discard</span></span>
<span id="cb47-15"><a></a>bart_model <span class="ot">&lt;-</span> <span class="fu">gbart</span>(<span class="at">x.train =</span> X, <span class="at">y.train =</span> Y, <span class="at">ntree =</span> <span class="dv">50</span>, <span class="at">ndpost =</span> <span class="dv">1000</span>, <span class="at">nskip =</span> <span class="dv">500</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>*****Calling gbart: type=1
*****Data:
data:n,p,np: 506, 13, 0
y1,yn: 1.467194, -10.632806
x1,x[n*p]: 0.006320, 7.880000
*****Number of Trees: 50
*****Number of Cut Points: 100 ... 100
*****burn,nd,thin: 500,1000,1
*****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,1.59099,3,4.38629,22.5328
*****sigma: 4.745298
*****w (weights): 1.000000 ... 1.000000
*****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,13,0
*****printevery: 100

MCMC
done 0 (out of 1500)
done 100 (out of 1500)
done 200 (out of 1500)
done 300 (out of 1500)
done 400 (out of 1500)
done 500 (out of 1500)
done 600 (out of 1500)
done 700 (out of 1500)
done 800 (out of 1500)
done 900 (out of 1500)
done 1000 (out of 1500)
done 1100 (out of 1500)
done 1200 (out of 1500)
done 1300 (out of 1500)
done 1400 (out of 1500)
time: 1s
trcnt,tecnt: 1000,0</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a></a><span class="co"># Make predictions on the training data (or you could use a separate test set)</span></span>
<span id="cb49-2"><a></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(bart_model, <span class="at">newdata =</span> X)  <span class="co"># This returns a matrix: rows=observations, cols=posterior samples</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>*****In main of C++ for bart prediction
tc (threadcount): 1
number of bart draws: 1000
number of trees in bart sum: 50
number of x columns: 13
from x,np,p: 13, 506
***using serial code</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a></a><span class="co"># Calculate the mean prediction for each observation</span></span>
<span id="cb51-2"><a></a>mean_predictions <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(predictions)</span>
<span id="cb51-3"><a></a></span>
<span id="cb51-4"><a></a><span class="co"># Calculate 95% credible intervals</span></span>
<span id="cb51-5"><a></a>credible_intervals <span class="ot">&lt;-</span> <span class="fu">apply</span>(predictions, <span class="dv">2</span>, quantile, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="example-in-r-bart-package-1" class="slide level2">
<h2>Example in R (BART package)</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a></a><span class="co"># Plot predictions vs. actual values (with credible intervals)</span></span>
<span id="cb52-2"><a></a><span class="fu">plot</span>(Y, mean_predictions, <span class="at">main =</span> <span class="st">"BART Predictions vs. Actual Values"</span>,</span>
<span id="cb52-3"><a></a>     <span class="at">xlab =</span> <span class="st">"Actual Median House Value"</span>, <span class="at">ylab =</span> <span class="st">"Predicted Median House Value"</span>,<span class="at">pch=</span><span class="dv">16</span>, <span class="at">cex=</span><span class="fl">0.5</span>)</span>
<span id="cb52-4"><a></a><span class="fu">segments</span>(Y, credible_intervals[<span class="dv">1</span>, ], Y, credible_intervals[<span class="dv">2</span>, ], <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb52-5"><a></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"red"</span>)  <span class="co"># Add a y=x line for reference</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-19-1.png" width="960" class="r-stretch"></section>
<section id="example-in-r-bart-package-2" class="slide level2">
<h2>Example in R (BART package)</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a></a><span class="co"># Variable Importance</span></span>
<span id="cb53-2"><a></a>var_importance <span class="ot">&lt;-</span> bart_model<span class="sc">$</span>varcount  <span class="co"># Number of times each variable was used for splitting</span></span>
<span id="cb53-3"><a></a>var_importance_mean <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(var_importance)</span>
<span id="cb53-4"><a></a><span class="fu">barplot</span>(var_importance_mean, <span class="at">names.arg =</span> <span class="fu">colnames</span>(X), <span class="at">las =</span> <span class="dv">2</span>,</span>
<span id="cb53-5"><a></a>        <span class="at">main =</span> <span class="st">"Variable Importance"</span>, <span class="at">ylab =</span> <span class="st">"Mean Split Count"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-20-1.png" width="960" class="r-stretch"></section>
<section id="example-in-r-bart-package-3" class="slide level2">
<h2>Example in R (BART package)</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a></a><span class="co"># Check convergence (trace plots - should look like "fuzzy caterpillars")</span></span>
<span id="cb54-2"><a></a><span class="fu">plot</span>(bart_model<span class="sc">$</span>sigma, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">main =</span> <span class="st">"Trace Plot of Sigma"</span>) <span class="co">#sigma plot</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-21-1.png" width="960" class="r-stretch"></section>
<section id="example-in-r-bart-package-4" class="slide level2">
<h2>Example in R (BART package)</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a></a><span class="co"># Example for prediction with new data</span></span>
<span id="cb55-2"><a></a>new_data <span class="ot">&lt;-</span> X[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,] <span class="co"># Lets pretend these are five new datapoints</span></span>
<span id="cb55-3"><a></a>new_predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(bart_model, <span class="at">newdata =</span> new_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>*****In main of C++ for bart prediction
tc (threadcount): 1
number of bart draws: 1000
number of trees in bart sum: 50
number of x columns: 13
from x,np,p: 13, 5
***using serial code</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a></a>new_mean_predictions <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(new_predictions)</span>
<span id="cb57-2"><a></a>new_credible_intervals <span class="ot">&lt;-</span> <span class="fu">apply</span>(new_predictions, <span class="dv">2</span>, quantile, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span>
<span id="cb57-3"><a></a><span class="fu">print</span>(new_mean_predictions) <span class="co">#print mean prediction for new data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 23.27826 21.08262 33.92599 33.88669 34.63011</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a></a><span class="fu">print</span>(new_credible_intervals) <span class="co"># print the CI for new data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]     [,2]     [,3]     [,4]     [,5]
2.5%  20.57538 18.51734 31.90270 31.40777 32.50151
97.5% 25.87818 22.85859 35.95392 36.70508 36.66815</code></pre>
</div>
</div>
</section>
<section id="key-advantages-of-bart" class="slide level2">
<h2>Key Advantages of BART</h2>
<ul>
<li><strong>Flexibility:</strong> Can capture complex non-linear relationships and interactions between predictors.</li>
<li><strong>Regularization:</strong> The Bayesian priors and the sum-of-trees structure provide built-in regularization, reducing the risk of overfitting.</li>
<li><strong>Uncertainty Quantification:</strong> Provides credible intervals, giving a measure of uncertainty in the predictions.</li>
<li><strong>Variable Importance:</strong> Provides a natural way to assess the importance of different predictors.</li>
<li><strong>Good Default Performance:</strong> Often performs well with minimal tuning of hyperparameters (although tuning can further improve performance).</li>
</ul>
</section>
<section id="disadvantages-and-considerations" class="slide level2">
<h2>Disadvantages and Considerations**</h2>
<ul>
<li><strong>Computational Cost:</strong> MCMC sampling can be computationally expensive, especially for large datasets and a large number of trees.</li>
<li><strong>Interpretability:</strong> While variable importance is helpful, the complex ensemble of trees can be difficult to interpret directly (compared to, say, a linear regression model). Partial dependence plots can help.</li>
<li><strong>Hyperparameter Tuning:</strong> While BART often works well with defaults, tuning <span class="math inline">\(ntree\)</span>, <span class="math inline">\(ndpost\)</span>, <span class="math inline">\(nskip\)</span>, and the prior parameters can further improve performance. Cross-validation is recommended for this. The <span class="math inline">\(BART\)</span> package provides functions for this.</li>
<li><strong>Missing data</strong>: Handling missing data in BART could require imputation strategies before modelling.</li>
</ul>
</section>
<section id="mcmc-training-process" class="slide level2">
<h2>MCMC Training Process</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource default number-lines code-with-copy"><code class="sourceCode default"><span id="cb61-1"><a></a>graph LR</span>
<span id="cb61-2"><a></a>    subgraph TrainingData</span>
<span id="cb61-3"><a></a>        X_train[Training Predictors]</span>
<span id="cb61-4"><a></a>        Y_train[Training Response]</span>
<span id="cb61-5"><a></a>    end</span>
<span id="cb61-6"><a></a>    subgraph BARTModel</span>
<span id="cb61-7"><a></a>        Trees["Multiple Trees (Tj, Mj)"]</span>
<span id="cb61-8"><a></a>    end</span>
<span id="cb61-9"><a></a>     subgraph MCMC</span>
<span id="cb61-10"><a></a>        MCMCIterations[MCMC Samples of Trees]</span>
<span id="cb61-11"><a></a>    end</span>
<span id="cb61-12"><a></a>    subgraph NewData</span>
<span id="cb61-13"><a></a>        X_new[New Predictors]</span>
<span id="cb61-14"><a></a>    end</span>
<span id="cb61-15"><a></a>    subgraph Prediction</span>
<span id="cb61-16"><a></a>        Predictions[Multiple Predictions]</span>
<span id="cb61-17"><a></a>        MeanPrediction[Mean Prediction]</span>
<span id="cb61-18"><a></a>        CredibleIntervals[Credible Intervals]</span>
<span id="cb61-19"><a></a>    end</span>
<span id="cb61-20"><a></a></span>
<span id="cb61-21"><a></a>    TrainingData --&gt; BARTModel</span>
<span id="cb61-22"><a></a>    BARTModel --&gt; MCMCIterations</span>
<span id="cb61-23"><a></a>    X_new --&gt; MCMCIterations</span>
<span id="cb61-24"><a></a>    MCMCIterations --&gt; Predictions</span>
<span id="cb61-25"><a></a>    Predictions --&gt; MeanPrediction</span>
<span id="cb61-26"><a></a>    Predictions --&gt; CredibleIntervals</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    subgraph TrainingData
        X_train[Training Predictors]
        Y_train[Training Response]
    end
    subgraph BARTModel
        Trees["Multiple Trees (Tj, Mj)"]
    end
     subgraph MCMC
        MCMCIterations[MCMC Samples of Trees]
    end
    subgraph NewData
        X_new[New Predictors]
    end
    subgraph Prediction
        Predictions[Multiple Predictions]
        MeanPrediction[Mean Prediction]
        CredibleIntervals[Credible Intervals]
    end

    TrainingData --&gt; BARTModel
    BARTModel --&gt; MCMCIterations
    X_new --&gt; MCMCIterations
    MCMCIterations --&gt; Predictions
    Predictions --&gt; MeanPrediction
    Predictions --&gt; CredibleIntervals
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section></section>
<section>
<section id="temporal-data-filtering-event-detection-pandemics" class="title-slide slide level1 center">
<h1>Temporal Data: Filtering, Event Detection, Pandemics</h1>

</section>
<section id="example-history-of-pandemics" class="slide level2 smaller">
<h2>Example: History of Pandemics</h2>
<p><em>Bill Gates: 12/11/2009: “I’m most worried about a worldwide Pandemic”</em></p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;">Early-period Pandemics</th>
<th style="text-align: left;">Dates</th>
<th style="text-align: left;">Size of Mortality</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Plague of Athens</td>
<td style="text-align: left;">430 BC</td>
<td style="text-align: left;">25% population.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Black Death</td>
<td style="text-align: left;">1347</td>
<td style="text-align: left;">30% Europe</td>
</tr>
<tr class="odd">
<td style="text-align: left;">London Plague</td>
<td style="text-align: left;">1666 2</td>
<td style="text-align: left;">0% population</td>
</tr>
</tbody>
</table>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;">Recent Flu Epidemics</th>
<th style="text-align: center;">Dates 1900-2010</th>
<th style="text-align: center;">Size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Spanish Flu</td>
<td style="text-align: center;">1918-19 40-</td>
<td style="text-align: center;">50 million</td>
</tr>
<tr class="even">
<td style="text-align: left;">Asian Flu</td>
<td style="text-align: center;">H2N2, 1957-58 2</td>
<td style="text-align: center;">million</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Hong Kong Flu</td>
<td style="text-align: center;">H3N2, 1968-69 6</td>
<td style="text-align: center;">million</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Spanish Flu killed more than WW1</p>
<p>H1N1 Flu 2009: <span class="math inline">\(18,449\)</span> people killed World wide:</p>
</section>
<section id="seir-epidemic-models" class="slide level2">
<h2>SEIR Epidemic Models</h2>
<p>Growth <em>“self-reinforcing”</em>: More likely if more infectants</p>
<ul>
<li><p>An individual comes into contact with disease at rate <span class="math inline">\(\beta_1\)</span></p></li>
<li><p>The susceptible individual contracts the disease with probability <span class="math inline">\(\beta_2\)</span></p></li>
<li><p>Each infectant becomes infectious with rate <span class="math inline">\(\alpha\)</span> per unit time</p></li>
<li><p>Each infectant recovers with rate <span class="math inline">\(\gamma\)</span> per unit time</p></li>
</ul>
<p><span class="math inline">\(S_t + E_t + I_t + R_t = N\)</span></p>
</section>
<section id="current-models-seir" class="slide level2">
<h2>Current Models: SEIR</h2>
<p>susceptible-exposed-infectious-recovered model</p>
<p>Dynamic models that extend earlier models to include exposure and recovery.</p>
<p>The coupled SEIR model:<br>
<span class="math inline">\(\dot{S} = -\beta S I\)</span><br>
<span class="math inline">\(\dot{E} = \beta S I - \alpha E\)</span><br>
<span class="math inline">\(\dot{I} = \alpha E -\gamma I\)</span><br>
<span class="math inline">\(\dot{R} = \gamma I\)</span><br>
</p>
</section>
<section id="infectious-disease-models" class="slide level2">
<h2>Infectious disease models</h2>
<p>Daniel Bernoulli’s (1766) first model of disease transmission in smallpox:</p>
<p><em>“I wish simply that, in matters which so closely concern the well being of the human race, no decision shall be made without all knowledge which a little analysis and calculation can provide”</em></p>
<ul>
<li>R.A. Ross, (Nobel Medicine winner, 1902) – math model of malaria transmission, which ultimately lead to malaria control.</li>
</ul>
<p><em>Ross-McDonald model</em></p>
<ul>
<li>Kermack and McKendrick: susceptible-infectious-recovered (SIR)</li>
</ul>
<p>London Plague 1665-1666; Cholera: London 1865, Bombay, 1906.</p>
</section>
<section id="example-london-plague-1666-village-eyam-nr.-sheffield" class="slide level2">
<h2>Example: London Plague, 1666: Village Eyam nr. Sheffield</h2>
<p>Model of transmission from Infectants, <span class="math inline">\(I\)</span>, to susceptibles, <span class="math inline">\(S\)</span>.</p>
<table class="caption-top">
<thead>
<tr class="header">
<th>Date 1666 Su</th>
<th>sceptibles In</th>
<th style="text-align: left;">fectives</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Initial</td>
<td>254</td>
<td style="text-align: left;">7</td>
</tr>
<tr class="even">
<td>July 3</td>
<td>235 15</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td>July 19</td>
<td>201 22</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td>Aug 3 1</td>
<td>53 29</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td>Aug 19</td>
<td>121 21</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td>Sept 3</td>
<td>108 8</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td>Sept 19</td>
<td>97 8</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td>Oct 3</td>
<td>–</td>
<td style="text-align: left;">–</td>
</tr>
<tr class="odd">
<td>Oct 19</td>
<td>83 0</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Initial Population <span class="math inline">\(N=261=S_0\)</span>; Final population <span class="math inline">\(S_\infty = 83\)</span>.</p>
</section>
<section id="modeling-growth-si" class="slide level2">
<h2>Modeling Growth: SI</h2>
<p>Coupled Differential eqn <span class="math inline">\(\dot{S} = - \beta SI , \dot{I} = ( \beta S - \alpha ) I\)</span></p>
<ul>
<li>Estimates <span class="math inline">\(\frac{\beta}{\alpha} = 6.54 \times 10^{-3} , \frac{\alpha}{\beta} = 1.53\)</span>.</li>
</ul>
<p><span class="math display">\[
\frac{ \hat{\beta} }{\alpha} = \frac{  \ln ( S_0 / S_\infty ) }{S_0 - S_\infty}
\]</span> Predicted maximum <span class="math inline">\(30.4\)</span>, very close to observed 29</p>
<p>Key: <span class="math inline">\(S\)</span> and <span class="math inline">\(I\)</span> are observed and <span class="math inline">\(\alpha , \beta\)</span> are estimated in <em>hindsight</em></p>
</section>
<section id="transmission-rates-r_0-for-1918-episode" class="slide level2">
<h2>Transmission Rates <span class="math inline">\(R_0\)</span> for 1918 Episode</h2>
<ul>
<li>1918-19 influenza pandemic:<br>
</li>
</ul>
<table class="caption-top">
<tbody>
<tr class="odd">
<td style="text-align: left;">Mills et al.&nbsp;2004:</td>
<td style="text-align: left;">45 US cities</td>
<td style="text-align: left;">3 (2-4)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Viboud et al.&nbsp;2006:</td>
<td style="text-align: left;">England and Wales</td>
<td style="text-align: left;">1.8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Massad et al.&nbsp;2007:</td>
<td style="text-align: left;">Sao Paulo Brazil</td>
<td style="text-align: left;">2.7</td>
</tr>
<tr class="even">
<td style="text-align: left;">Nishiura, 2007:</td>
<td style="text-align: left;">Prussia, Germany</td>
<td style="text-align: left;">3.41</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Chowell et al., 2006:</td>
<td style="text-align: left;">Geneva, Switzerland</td>
<td style="text-align: left;">2.7-3.8</td>
</tr>
<tr class="even">
<td style="text-align: left;">Chowell et al., 2007:</td>
<td style="text-align: left;">San Francisco</td>
<td style="text-align: left;">2.7-3.5</td>
</tr>
</tbody>
</table>
<p>The larger the <span class="math inline">\(R_0\)</span> the more severe the epidemic.</p>
<p>Transmission parameters vary substantially from epidemic to epidemic</p>
</section>
<section id="boat-localization-example" class="slide level2 smaller">
<h2>Boat Localization Example</h2>
<p>Localization with measurement update</p>
<ul>
<li><p>A boat sails from one island to another</p></li>
<li><p>Boat is trying to identify its location <span class="math inline">\(\theta \sim N(m_0, C_0)\)</span></p></li>
<li><p>Using a sequence of measurements to one of the islands <span class="math inline">\(x_1,\ldots,x_n\)</span></p></li>
</ul>
<p>Measurements are noisy due to dilution of precision <a href="http://www.sailingmates.com/your-gps-can-kill-you/" class="uri">http://www.sailingmates.com/your-gps-can-kill-you/</a></p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="fignick/GooglePointsAndErrors.png"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="fignick/gps-sat.png"></p>
</div>
</div>
</div>
</section>
<section id="reckoning" class="slide level2">
<h2>Reckoning</h2>
<p>Localization with no measurement updates is called reckoning</p>

<img data-src="fignick/traverse-board-2.jpg" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">source: http://www.hakaimagazine.com/article-short/traversing-seas</p></section>
<section id="kalman-filter" class="slide level2">
<h2>Kalman Filter</h2>
<p><span class="math display">\[
\theta \sim N(m_0, C_0)
\]</span> <span class="math display">\[
x_t = \theta + w_t,~~~w_t \sim N(0,\sigma^2)
\]</span> <span class="math display">\[
x_1,x_2,\ldots \mid \theta \sim N(\theta,\sigma^2)
\]</span> The prior variance <span class="math inline">\(C_0\)</span> might be quite large if you are very uncertain about your guess <span class="math inline">\(m_0\)</span><br>
Given the measurements <span class="math inline">\(x^n = (x_1,\ldots,x_n)\)</span>, you update your opinion about <span class="math inline">\(\theta\)</span> computing its posterior density, using the Bayes formula</p>
</section>
<section id="normal-model" class="slide level2">
<h2>Normal Model</h2>
<p><span class="math display">\[
f(x) = \dfrac{1}{\sqrt{2 \pi \sigma^2}} \exp^{-\dfrac{1}{2}\dfrac{(x-\mu)^2}{\sigma^2}}
\]</span> Or multivariate equivalent <span class="math display">\[
f(x) = (2 \pi)^{-k/2} |\Sigma|^{-1/2}\exp^{-0.5(x-\mu)^T\Sigma^{-1}(x-\mu)}
\]</span></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a></a><span class="fu">curve</span>(<span class="fu">exp</span>(<span class="sc">-</span>x<span class="sc">*</span>x),<span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>, <span class="at">bty=</span><span class="st">"newdataframe &lt;- na.omit(dataframe)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/normal-1.png" width="960" class="r-stretch"></section>
<section id="the-conjugate-prior-for-the-normal-distribution" class="slide level2">
<h2>The Conjugate Prior for the Normal Distribution</h2>
<p>We will look at the Gaussian distribution from a Bayesian point of view. In the standard form, the likelihood has two parameters, the mean <span class="math inline">\(\mu\)</span> and the variance <span class="math inline">\(\sigma^2\)</span> <span class="math display">\[
p(x^n | \mu, \sigma^2) \propto \dfrac{1}{\sigma^n}\exp\left(-\dfrac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right)
\]</span></p>
</section>
<section id="normal-prior" class="slide level2">
<h2>Normal Prior</h2>
<p>In case when we know the variance <span class="math inline">\(\sigma^2\)</span>, but do not know mean <span class="math inline">\(\mu\)</span>, we assume <span class="math inline">\(\mu\)</span> is random. To have conjugate prior we choose <span class="math display">\[
p(\mu | \mu_0, \sigma_0) \propto \dfrac{1}{\sigma_0}\exp\left(-\dfrac{1}{2\sigma_0^2}(\mu-\mu_0^2)\right)
\]</span> In practice, when little is known about <span class="math inline">\(\mu\)</span>, it is common to set the location hyper-parameter to zero and the scale to some large value.</p>
</section>
<section id="normal-model-with-unknown-mean-known-variance" class="slide level2">
<h2>Normal Model with Unknown Mean, Known Variance</h2>
<p>Suppose we wish to estimate a model where the likelihood of the data is normal with an unknown mean <span class="math inline">\(\mu\)</span> and a known variance <span class="math inline">\(\sigma^2\)</span>.<br>
Our parameter of interest is <span class="math inline">\(\mu\)</span>. We can use a conjugate Normal prior on <span class="math inline">\(\mu\)</span>, with mean <span class="math inline">\(\mu_0\)</span> and variance <span class="math inline">\(\sigma_0^2\)</span>. <span class="math display">\[
\begin{aligned}
p(\mu| x^n, \sigma^2) &amp; \propto p(x^n | \mu, \sigma^2)p(\mu) ~~~\mbox{(Bayes rule)}\\
N(\mu_1,\tau_1)     &amp; = N(\mu, \sigma^2)\times N(\mu_0, \sigma_0^2)
\end{aligned}
\]</span></p>
</section>
<section id="useful-identity" class="slide level2 smaller">
<h2>Useful Identity</h2>
<p>One of the most useful algebraic tricks for calculating posterior distribution is <strong>completing the square</strong>.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><span class="math display">\[
\dfrac{(x-\mu_1)^2}{\sigma_1} + \dfrac{(x-\mu_2)^2}{\sigma_2} = \dfrac{(x - \mu_3)^2}{\sigma_3} + \dfrac{(\mu_1-\mu_2)^2}{\sigma_1 + \sigma_2}
\]</span> where <span class="math display">\[
\mu_3 = \sigma_3 (\mu_1/\sigma_1 + \mu_2/\sigma_2)
\]</span> and <span class="math display">\[
\sigma_3 = (1/\sigma_1 + 1/\sigma_2)^{-1}
\]</span></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Prior: <span class="math display">\[
\theta \sim \frac{e^{-\frac{(\theta-\mu )^2}{2 \sigma ^2}}}{\sqrt{2 \pi } \sigma }
\]</span> Likelihood: <span class="math display">\[
x \mid \theta \sim \frac{e^{-\frac{(\theta-y )^2}{2 r^2}}}{\sqrt{2 \pi } r}
\]</span> Posterior mean: <span class="math display">\[
\frac{x  \sigma ^2+\mu  r^2}{r^2+\sigma ^2}
\]</span> Posterior variance: <span class="math display">\[
\frac{1}{\frac{1}{r^2}+\frac{1}{\sigma ^2}}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="prior-likelihood-posterior" class="slide level2">
<h2>Prior, Likelihood, Posterior</h2>

<img data-src="fig/prior-meas-post.svg" style="width:80.0%" class="r-stretch"></section>
<section id="after-n-steps" class="slide level2 smaller">
<h2>After <span class="math inline">\(n\)</span> steps</h2>
<p><span class="math display">\[
\begin{aligned}
p(\mu  | x^n) &amp; \propto \prod_{i=1}^{n}\dfrac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{(x_i - \mu )^2}{2\sigma^2}\right)\times\dfrac{1}{\sqrt{2\pi \sigma_0^2}}\exp\left(-\frac{(\mu  - \mu_0)^2}{2\sigma_0^2}\right)\\
&amp; \propto \exp\left(-\sum_{i=1}^{n}\frac{(x_i - \mu )^2}{2\sigma^2} - \frac{(\mu  - \mu_0)^2}{2\sigma_0^2}\right)\\
&amp; = \exp\left(-\dfrac{1}{2}\left[\sum_{i=1}^{n}\frac{(x_i - \mu )^2}{\sigma^2} + \frac{(\mu  - \mu_0)^2}{\sigma_0^2}\right]\right)\\
&amp; = \exp\left(-\dfrac{1}{2\sigma^2 \sigma_0^2}\left[\sigma_0^2\sum_{i=1}^{n}(x_i - \mu )^2 + \sigma^2 (\mu  - \mu_0)^2\right]\right)\\
&amp; = \exp\left(-\dfrac{1}{2\sigma^2 \sigma_0^2}\left[\sigma_0^2\sum_{i=1}^{n}(x_i^2 - 2\mu x_i+ \mu ^2) + \sigma^2 (\mu ^2 - 2\mu \mu_0 + \mu_0^2)\right]\right)\\\end{aligned}
\]</span></p>
</section>
<section id="after-n-steps-1" class="slide level2 smaller">
<h2>After <span class="math inline">\(n\)</span> steps</h2>
<p>We can multiply the <span class="math inline">\(2\mu x_i\)</span> term in the summation by <span class="math inline">\(n/n\)</span> in order to get the equations in terms of the sufficient statistic <span class="math inline">\(\bar{x}^n\)</span> <span class="math display">\[
\begin{aligned}
p(\mu  | x^n) &amp; \propto \exp\left(-\dfrac{1}{2\sigma^2 \sigma_0^2}\left[\sigma_0^2\sum_{i=1}^{n}(x_i^2 - \dfrac{n}{n}2\mu x_i+ \mu ^2) + \sigma^2 (\mu ^2 - 2\mu \mu_0 + \mu_0^2)\right]\right)\\
&amp; = \exp\left(-\dfrac{1}{2\sigma^2 \sigma_0^2}\left[\sigma_0^2\sum_{i=1}^{n}x_i^2 - \sigma_0^22\mu n\bar{x}^n+ \tau_n^0n\mu ^2 + \sigma^2 \mu ^2 - 2\mu \mu_0\sigma^2 + \mu_0^2\sigma^2\right]\right)\end{aligned}
\]</span> set <span class="math inline">\(k = \sigma_0^2\sum_{i=1}^{n}x_i^2 + \mu_0^2\sigma^2\)</span> (they do not contain <span class="math inline">\(\mu\)</span>) <span class="math display">\[
p(\mu  | x^n)   \propto \exp\left(-\dfrac{1}{2}\left[\mu ^2\left(\dfrac{1}{\sigma_0^2} + \dfrac{n}{\sigma^2}\right) - 2\mu\left(\dfrac{\mu_0}{\sigma_0^2} + \dfrac{n\bar{x}^n}{\sigma^2}\right) + k\right]\right)
\]</span></p>
</section>
<section id="after-n-steps-2" class="slide level2 smaller">
<h2>After <span class="math inline">\(n\)</span> steps</h2>
<p>Let’s multiply by <span class="math display">\[
\dfrac{1/\sigma_0^2 + n/\sigma^2}{1/\sigma_0^2 + n/\sigma^2}
\]</span> Now <span class="math display">\[
p(\mu  | x^n)   \propto \exp\left(-\dfrac{1}{2}\left(1/\sigma_0^2 + n/\sigma^2\right)\left(\mu  - \dfrac{\mu_0/\sigma_0^2 + n\bar{x}^n/\sigma^2}{1/\sigma_0^2 + n/\sigma^2}\right)^2\right)
\]</span></p>
<p><span class="math display">\[
p(\mu  | x^n)   \propto \exp\left(-\dfrac{1}{2}\left(1/\sigma_0^2 + n/\sigma^2\right)\left(\mu  - \dfrac{\mu_0/\sigma_0^2 + n\bar{x}^n/\sigma^2}{1/\sigma_0^2 + n/\sigma^2}\right)^2\right)
\]</span></p>
</section>
<section id="after-n-steps-3" class="slide level2 smaller">
<h2>After <span class="math inline">\(n\)</span> steps</h2>
<ul>
<li><p>Posterior mean: <span class="math inline">\(\mu_n  =  \dfrac{\mu_0/\sigma_0^2 + n\bar{x}^n/\sigma^2}{1/\sigma_0^2 + n/\sigma^2}\)</span></p></li>
<li><p>Posterior variance: <span class="math inline">\(\sigma_n^2 = \left(1/\sigma_0^2 + n/\sigma^2\right)^{-1}\)</span></p></li>
<li><p>Posterior precision:: <span class="math inline">\(\tau_n^2 = 1/\sigma_0^2 + n/\sigma^2\)</span></p></li>
</ul>
<p>Posterior Precision is just the sum of the prior precision and the data precision.</p>
</section>
<section id="posterior-mean" class="slide level2">
<h2>Posterior Mean</h2>
<p><span class="math display">\[
\begin{aligned}
\mu_n  &amp; =  \dfrac{\mu_0/\sigma_0^2 + n\bar{x}^n/\sigma^2}{1/\sigma_0^2 + n/\sigma^2}\\
&amp; = \dfrac{\mu_0\sigma^2}{\sigma^2 + n\sigma_0^2} + \dfrac{\sigma_0^2n\bar{x}^n}{\sigma^2 + n\sigma_0^2}\end{aligned}
\]</span></p>
<ul>
<li><p>As <span class="math inline">\(n\)</span> increases, data mean dominates prior mean.</p></li>
<li><p>As <span class="math inline">\(\sigma_0^2\)</span> decreases (less prior variance, greater prior precision), our prior mean becomes more important.</p></li>
</ul>
</section>
<section id="a-state-space-model" class="slide level2">
<h2>A state space model</h2>
<p>A state space model consists of two equations: <span class="math display">\[
\begin{aligned}
Z_t&amp;=HS_t+w_t\\
S_{t+1} &amp;= FS_t + v_t\end{aligned}
\]</span> where <span class="math inline">\(S_t\)</span> is a state vector of dimension <span class="math inline">\(m\)</span>, <span class="math inline">\(Z_t\)</span> is the observed time series, <span class="math inline">\(F\)</span>, <span class="math inline">\(G\)</span>, <span class="math inline">\(H\)</span> are matrices of parameters, <span class="math inline">\(\{w_t\}\)</span> and <span class="math inline">\(\{v_t\}\)</span> are <span class="math inline">\(iid\)</span> random vectors satisfying <span class="math display">\[
\mbox{E}(w_t)=0, \hspace{0.5cm} \mbox{E}(v_t)=0, \hspace{0.5cm}\mathrm{cov}(v_t)=V, \hspace{0.5cm} \mathrm{cov}(w_t)=W
\]</span> and <span class="math inline">\(\{w_t\}\)</span> and <span class="math inline">\(\{v_t\}\)</span> are independent.</p>
</section>
<section id="state-space-models" class="slide level2">
<h2>State Space Models</h2>
<ul>
<li><p>State space models consider a time series as the output of a dynamic system perturbed by random disturbances.</p></li>
<li><p>Natural interpretation of a time series as the combination of several components, such as trend, seasonal or regressive components.</p></li>
<li><p>Computations can be implemented by recursive algorithms.</p></li>
</ul>
</section>
<section id="types-of-inference" class="slide level2">
<h2>Types of Inference</h2>
<ul>
<li><p>Model building versus inferring unknown variable. Assume a linear model <span class="math inline">\(Z = HS + \epsilon\)</span></p></li>
<li><p>Model building: know signal <span class="math inline">\(S\)</span>, observe <span class="math inline">\(Z\)</span>, infer <span class="math inline">\(H\)</span> (a.k.a. model identification, learning)</p></li>
<li><p>Estimation: know <span class="math inline">\(H\)</span>, observe <span class="math inline">\(Z\)</span>, estimate <span class="math inline">\(S\)</span></p></li>
<li><p>Hypothesis testing: unknown takes one of few possible values; aim at small probability of incorrect decision</p></li>
<li><p>Estimation: aim at a small estimation error</p></li>
</ul>
</section>
<section id="time-series-estimation-tasks" class="slide level2">
<h2>Time Series Estimation Tasks</h2>
<ul>
<li><p>Filtering: To recover the state vector <span class="math inline">\(S_t\)</span> given <span class="math inline">\(Z^t\)</span></p></li>
<li><p>Prediction: To predict <span class="math inline">\(S_{t+h}\)</span> or <span class="math inline">\(Z_{t+h}\)</span> for <span class="math inline">\(h &gt; 0\)</span>, given <span class="math inline">\(Z^t\)</span></p></li>
<li><p>Smoothing: To estimate <span class="math inline">\(S_t\)</span> given <span class="math inline">\(Z^T\)</span> , where <span class="math inline">\(T &gt; t\)</span></p></li>
</ul>
</section>
<section id="property-of-multivariate-normal" class="slide level2">
<h2>Property of Multivariate Normal</h2>
<p>Under normality, we have</p>
<ul>
<li><p>that normal prior plus normal likelihood results in a normal posterior,</p></li>
<li><p>that if the random vector <span class="math inline">\((X, Y )\)</span> are jointly normal</p></li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
X\\
Y
\end{bmatrix}
\sim N\left(
\begin{bmatrix}
\mu_x\\
\mu_y
\end{bmatrix},
\begin{bmatrix}
\Sigma_{xx}&amp;\Sigma_{xy}\\
\Sigma_{yx}&amp;\Sigma_{yy}\\
\end{bmatrix}\right),
\]</span></p>
<ul>
<li>then the conditional distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y = y\)</span> is normal</li>
</ul>
<p><span class="math display">\[
X|Y = y\sim N\left[\mu_x+\Sigma_{xy}\Sigma_{yy}^{-1}(y-\mu_y),\Sigma_{xx}-\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}\right].
\]</span></p>
</section>
<section id="from-state-space-model" class="slide level2">
<h2>From State Space Model</h2>
<p><span class="math display">\[
\begin{aligned}
S_{t+1}^t &amp;= FS_t\\
Z_{t+1}^t &amp;=HS_{t+1}^t\\
P_{t+1}^t&amp;=FP_tF^T+GQG^T\\
V_{t+1}^t&amp;=HP_{t+1}^tH^T+R\\
C_{t+1}^t&amp;=HP_{t+1}^t
\end{aligned}
\]</span></p>
<ul>
<li><p><span class="math inline">\(P_{t+j}^t\)</span> = conditional covariance matrix of <span class="math inline">\(S_{t+j}\)</span> given <span class="math inline">\(\{Z_t , Z_{t-1} , \cdots\}\)</span> for <span class="math inline">\(j \geq 0\)</span></p></li>
<li><p><span class="math inline">\(S_{t+j}^t\)</span> = conditional mean of <span class="math inline">\(S_{t+j}\)</span> given <span class="math inline">\(\{Z_t , Z_{t-1} , \cdots\}\)</span></p></li>
<li><p><span class="math inline">\(V_{t+1}^t\)</span> = conditional variance of <span class="math inline">\(Z_{t+1}\)</span> given <span class="math inline">\(Z^t = \{Z_t , Z_{t-1} , \cdots\}\)</span></p></li>
<li><p><span class="math inline">\(C_{t+1}^t\)</span> = conditional covariance between <span class="math inline">\(Z_{t+1}\)</span> and <span class="math inline">\(S_{t+1}\)</span></p></li>
</ul>
</section>
<section id="joint-conditional-distribution-ps_t1-z_t1-zt" class="slide level2">
<h2>Joint conditional distribution <span class="math inline">\(P(S_{t+1}, Z_{t+1} | Z^t)\)</span></h2>
<p><span class="math display">\[
\begin{bmatrix}
S_{t+1}\\
Z_{t+1}
\end{bmatrix}_t
\sim N
\left( \begin{bmatrix}
S_{t+1}^t\\
Z_{t+1}^t
\end{bmatrix},
\begin{bmatrix}
P_{t+1}^t &amp; P_{t+1}^tH'\\
HP_{t+1}^t &amp; HP_{t+1}^tH'+R
\end{bmatrix} \right)
\]</span></p>
</section>
<section id="ps_t1-z_t1" class="slide level2 smaller">
<h2><span class="math inline">\(P(S_{t+1}| Z_{t+1})\)</span></h2>
<p>Finally, when <span class="math inline">\(Z_{t+1}\)</span> becomes available, we may use the property of nromality to update the distribution of <span class="math inline">\(S_{t+1}\)</span> . More specifically, <span class="math display">\[
S_{t+1}=S_{t+1}^t+P_{t+1}^tH^T[HP_{t+1}^tH^T+R]^{-1}(Z_{t+1}-Z_{t+1}^t)
\]</span> and <span class="math display">\[
P_{t+1}=P_{t+1}^t-P_{t+1}^tH^T[HP_{t+1}^tH'+R]^{-1}HP_{t+1}^t.
\]</span> Predictive residual: <span class="math display">\[
R_{t+1}^t=Z_{t+1}-Z_{t+1}^t=Z_{t+1}-HS_{t+1}^t \ne 0
\]</span> means there is new information about the system so that the state vector should be modified. The contribution of <span class="math inline">\(r_{t+1}^t\)</span> to the state vector, of course, needs to be weighted by the variance of <span class="math inline">\(r_{t+1}^t\)</span> and the conditional covariance matrix of <span class="math inline">\(S_{t+1}\)</span>.</p>
</section>
<section id="kalman-filter-1" class="slide level2">
<h2>Kalman filter</h2>
<ul>
<li><p>Predict: <span class="math display">\[
\begin{aligned}
S_{t+1}^t &amp;= FS_t\\
Z_{t+1}^t &amp;=HS_{t+1}^t\\
P_{t+1}^t&amp;=FP_tF^T+GQG^T\\
V_{t+1}^t&amp;=HP_{t+1}^tH^T+R
\end{aligned}
\]</span></p></li>
<li><p>Update: <span class="math display">\[
\begin{aligned}
S_{t+1|t+1}=&amp; S_{t+1}^t+P_{t+1}^tH^T[HP_{t+1}^tH^T+R]^{-1}(Z_{t+1}-Z_{t+1}^t)\\
P_{t+1|t+1}=&amp; P_{t+1}^t-P_{t+1}^tH^T[HP_{t+1}^tH^T+R]^{-1}HP_{t+1}^t
\end{aligned}
\]</span></p></li>
</ul>
</section>
<section id="kalman-filter-2" class="slide level2">
<h2>Kalman filter</h2>
<ul>
<li><p>starts with initial prior information <span class="math inline">\(S_{0}\)</span> and <span class="math inline">\(P_{0}\)</span></p></li>
<li><p>predicts <span class="math inline">\(Z_{1}^0\)</span> and <span class="math inline">\(V_{1}^0\)</span></p></li>
<li><p>Once the observation <span class="math inline">\(Z_1\)</span> is available, uses the updating equations to compute <span class="math inline">\(S_{1}\)</span> and <span class="math inline">\(P_{1}\)</span></p></li>
</ul>
<p><span class="math inline">\(S_{1|1}\)</span> and <span class="math inline">\(P_{1|1}\)</span> is the prior for the next observation.</p>
<p>This is the <span style="color: red">Kalman recusion</span>.</p>
</section>
<section id="kalman-filter-3" class="slide level2">
<h2>Kalman filter</h2>
<ul>
<li><p>effect of the initial values <span class="math inline">\(S_{0}\)</span> and <span class="math inline">\(P_{0}\)</span> is decresing as <span class="math inline">\(t\)</span> increases</p></li>
<li><p>for a stationary time series, all eigenvalues of the coefficient matrix <span class="math inline">\(F\)</span> are less than one in modulus</p></li>
<li><p>Kalman filter recursion ensures that the effect of the initial values indeed vanishes as <span class="math inline">\(t\)</span> increases</p></li>
<li><p>uncertainty about the state is always normal</p></li>
</ul>
</section>
<section id="local-trend-model" class="slide level2">
<h2>Local Trend Model</h2>
<p><span class="math display">\[
\begin{aligned}
y_t =&amp; \mu_t + e_t,~e_t \sim N(0,\sigma_e^2)\\\
\mu_{t+1} =&amp; \mu_t + \eta_t,~ \eta_t \sim N(0,\sigma_{\eta}^2 )
\end{aligned}
\]</span></p>
<ul>
<li><p><span class="math inline">\(\{e_t\}\)</span> and <span class="math inline">\(\{\eta_t\}\)</span> are iid Gaussian white noise</p></li>
<li><p><span class="math inline">\(\mu_0\)</span> is given (possible as a distributed value)</p></li>
<li><p>trend <span class="math inline">\(\mu_t\)</span> is not observable</p></li>
<li><p>we observe some noisy version of the trend <span class="math inline">\(y_t\)</span></p></li>
<li><p>such a model can be used to analyze realized volatility: <span class="math inline">\(\mu_t\)</span> is the log volatility and <span class="math inline">\(y_t\)</span> is constructed from high frequency transactions data</p></li>
</ul>
</section>
<section id="local-trend-model-1" class="slide level2">
<h2>Local Trend Model</h2>
<p><span class="math display">\[
\begin{aligned}
y_t =&amp; \mu_t + e_t,~e_t \sim N(0,\sigma_e^2)\\\
\mu_{t+1} =&amp; \mu_t + \eta_t,~ \eta_t \sim N(0,\sigma_{\eta}^2 )
\end{aligned}
\]</span></p>
<ul>
<li><p>if <span class="math inline">\(\sigma_e=0\)</span>, then we have ARIMA(0,1,0) model</p></li>
<li><p>if <span class="math inline">\(\sigma_e &gt; 0\)</span>, then we have ARIMA(0,1,1) model, satisfying</p></li>
</ul>
<p><span class="math display">\[
(1-B)y_t = (1-\theta B)a_t, ~ a_t \sim N(0,\sigma_a^2)
\]</span> <span class="math inline">\(\sigma_a\)</span> and <span class="math inline">\(\theta\)</span> are determined by <span class="math inline">\(\sigma_e\)</span> and <span class="math inline">\(\sigma_{\eta}\)</span> <span class="math display">\[
(1-B)y_t = \eta_{t-1} + e_t - e_{t-1}
\]</span></p>
</section>
<section id="liner-regression-time-dependent-parameters" class="slide level2">
<h2>Liner Regression (time dependent parameters)</h2>
<p><span class="math display">\[
\begin{aligned}
y_t  &amp;= \alpha_t + \beta_t \, x_t + \epsilon_t   \qquad &amp; \epsilon_t \, \sim N(0,\sigma^2) \\
\alpha_t &amp;= \quad \alpha_{t-1}  + \epsilon_t^{\alpha} \qquad &amp; \epsilon_t^{\alpha} \sim N(0,\sigma_{\alpha}^2) \\
\beta_t  &amp;= \quad \beta_{t-1}   + \epsilon_t^{\beta}   \qquad &amp; \epsilon_t^{\beta} \sim N(0, \sigma_{\beta}^2) \\
\end{aligned}
\]</span></p>
<p>dlm Package</p>
<ul>
<li><p><code>dlmModARMA</code>: for an ARMA process, potentially multivariate</p></li>
<li><p><code>dlmModPoly</code>: for an <span class="math inline">\(n^{th}\)</span> order polynomial</p></li>
<li><p><code>dlmModReg</code> : for Linear regression</p></li>
<li><p><code>dlmModSeas</code>: for periodic – Seasonal factors</p></li>
<li><p><code>dlmModTrig:</code> for periodic – Trigonometric form</p></li>
</ul>
</section>
<section id="local-linear-trend" class="slide level2">
<h2>Local Linear Trend</h2>
<p><span class="math display">\[
\begin{aligned}
y_t &amp;= \qquad \quad \mu_t  + \upsilon_t  \quad &amp;\upsilon_t \sim N(0,V) \\
\mu_t &amp;= \mu_{t-1}  + \delta_{t-1} + \omega_t^{\mu} \quad &amp; \omega_t^{\mu} \sim N(0,W^{\mu}) \\
\delta_t &amp;= \qquad \,\, \, \delta_{t-1} + \omega_t^{\delta} \quad &amp; \omega_t^{\delta} \sim N(0,W^{\delta}) \\
\end{aligned}
\]</span></p>
</section>
<section id="simple-exponential-smoothing-with-additive-errors" class="slide level2">
<h2>Simple exponential smoothing with additive errors</h2>
<p><span class="math display">\[
x_t = \ell_{t-1} + \varepsilon_t
\]</span> <span class="math display">\[
\ell_t=\ell_{t-1}+\alpha \varepsilon_t.
\]</span></p>
</section>
<section id="holts-linear-method-with-additive-errors" class="slide level2">
<h2>Holt’s linear method with additive errors</h2>
<p><span class="math display">\[
\begin{aligned}
y_t&amp;=\ell_{t-1}+b_{t-1}+\varepsilon_t\\
\ell_t&amp;=\ell_{t-1}+b_{t-1}+\alpha \varepsilon_t\\
b_t&amp;=b_{t-1}+\beta \varepsilon_t, \end{aligned}
\]</span></p>
</section>
<section id="relation-to-arma-models" class="slide level2">
<h2>Relation to ARMA models</h2>
<p>Consider relation with ARMA models. The basic relations are</p>
<ul>
<li><p>an ARMA model can be put into a state space form in “infinite" many ways;</p></li>
<li><p>for a given state space model in, there is an ARMA model.</p></li>
</ul>
</section>
<section id="state-space-model-to-arma-model" class="slide level2">
<h2>State space model to ARMA model</h2>
<p>The second possibility is that there is an observational noise. Then, the same argument gives <span class="math display">\[
(1+\alpha_1B+\cdots+\alpha_mB^m)(Z_{t+m}-\epsilon_{t+m})=(1-\theta_1B-\cdots -\theta_{m-1}B^{m-1})a_{t+m}
\]</span> By combining <span class="math inline">\(\epsilon_t\)</span> with <span class="math inline">\(a_t\)</span> , the above equation is an ARMA<span class="math inline">\((m, m)\)</span> model.</p>
</section>
<section id="arma-model-to-state-space-model-ar2" class="slide level2">
<h2>ARMA model to state space model: AR(2)</h2>
<p><span class="math display">\[
Z_t=\phi_1Z_{t-1}+\phi_2Z_{t-2}+a_t
\]</span> For such an AR(2) process, to compute the forecasts, we need <span class="math inline">\(Z_{t-1}\)</span> and <span class="math inline">\(Z_{t-2}\)</span> . Therefore, it is easily seen that <span class="math display">\[
\begin{bmatrix}
Z_{t+1}\\
Z_t
\end{bmatrix}
=
\begin{bmatrix}
\phi_1 &amp; \phi_2\\
1 &amp; 0
\end{bmatrix}
\begin{bmatrix}
Z_t\\
Z_{t-1}
\end{bmatrix}
+
\begin{bmatrix}
1\\
0
\end{bmatrix}
e_t,
\]</span> where <span class="math inline">\(e_t = a_{t+1}\)</span> and <span class="math display">\[
Z_t=[1, 0]S_t
\]</span> where <span class="math inline">\(S_t = (Z_t , Z_{t-1})^T\)</span> and there is no observational noise.</p>
</section>
<section id="arma-model-to-state-space-model-ma2" class="slide level2">
<h2>ARMA model to state space model: MA(2)</h2>
<p><span class="math display">\[
Z_t=a_t-\theta_1a_{t-1}-\theta_2a_{t-2}
\]</span> <u>Method 1:</u><br>
<span class="math display">\[
\begin{bmatrix}
a_t\\
a_{t-1}
\end{bmatrix}
=
\begin{bmatrix}
0 &amp; 0\\
1 &amp; 0
\end{bmatrix}
\begin{bmatrix}
a_{t-1}\\
a_{t-2}
\end{bmatrix}
+
\begin{bmatrix}
1\\
0
\end{bmatrix}
a_t
\]</span></p>
<p><span class="math display">\[
Z_t=[-\theta_1, -\theta_2]S_{t-1} + a_t
\]</span> Here the innovation <span class="math inline">\(a_t\)</span> shows up in both the state transition equation and the observation equation. The state vector is of dimension 2.<br>
</p>
</section>
<section id="arma-model-to-state-space-model-ma2-1" class="slide level2">
<h2>ARMA model to state space model: MA(2)</h2>
<p><u>Method 2</u>: For an MA(2) model, we have <span class="math display">\[
\begin{aligned}
Z_{t}^t&amp;=Z_t\\
Z_{t+1}^t&amp;=-\theta_1a_t-\theta_2a_{t-1}\\
Z_{t+2}^t&amp;= -\theta_2a_t
\end{aligned}
\]</span> Let <span class="math inline">\(S_t = (Z_t , -\theta_1 a_t - \theta_2 a_{t-1} , -\theta_2 a_t )^T\)</span> . Then, <span class="math display">\[
S_{t+1}=
\begin{bmatrix}
0 &amp; 1&amp; 0\\
0&amp; 0&amp; 1\\
0&amp; 0&amp; 0
\end{bmatrix}
S_t+
\begin{bmatrix}
1\\
-\theta_1\\
-\theta_2
\end{bmatrix}
a_{t+1}
\]</span> and <span class="math display">\[
Z_t=[1,0,0]S_t
\]</span> Here the state vector is of dimension 3, but there is no observational noise.</p>
</section>
<section id="arma-model-to-state-space-model-akaikes-approach" class="slide level2">
<h2>ARMA model to state space model: Akaike’s approach</h2>
<p>Consider ARMA<span class="math inline">\((p, q)\)</span> process, let <span class="math inline">\(m = max\{p, q + 1\}\)</span>, <span class="math inline">\(\phi_i = 0\)</span> for <span class="math inline">\(i &gt; p\)</span> and <span class="math inline">\(\theta_j = 0\)</span> for <span class="math inline">\(j &gt; q\)</span>. <span class="math display">\[
S_t = (Z_t , Z_{t+1}^t , Z_{t+2}^t ,\cdots , Z_{t+m-1}^t )^T
\]</span> where <span class="math inline">\(Z_{t+\ell}^t\)</span> is the conditional expectation of <span class="math inline">\(Z_{t+\ell}\)</span> given <span class="math inline">\(\Psi_t = \{Z_t , Z_{t-1} , \cdots\}\)</span>. By using the updating equation <span class="math inline">\(f\)</span> forecasts (recall what we discussed before) <span class="math display">\[
Z_{t+1}(\ell -1)=Z_t(\ell)+\psi_{\ell-1}a_{t+1},
\]</span></p>
</section>
<section id="arma-model-to-state-space-model-akaikes-approach-1" class="slide level2">
<h2>ARMA model to state space model: Akaike’s approach</h2>
<p><span class="math display">\[
S_t = (Z_t , Z_{t+1}^t , Z_{t+2}^t ,\cdots , Z_{t+m-1}^t )^T
\]</span> &nbsp; <span class="math display">\[
S_{t+1}=FS_t+Ga_{t+1}
\]</span></p>
<p><span class="math display">\[
Z_t=[1,0, \cdots ,0]S_t
\]</span> where <span class="math display">\[
F=
\left[
\begin{array}{c|cccc}
0 &amp;1&amp; 0&amp; \cdots&amp; 0\\
0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; &amp; \\
\phi_m &amp; \phi_{m-1} &amp; \cdots &amp; \phi_2 &amp; \phi_1
\end{array}\right], G=
\begin{bmatrix}
1\\
\psi_1\\
\psi_2\\
\vdots\\
\psi_{m-1}
\end{bmatrix}
\]</span> The matrix <span class="math inline">\(F\)</span> is call a companion matrix of the polynomial <span class="math inline">\(1 - \phi_1 B - \cdots - \phi_m B^m\)</span>.</p>
</section>
<section id="arma-model-to-state-space-model-aokis-method" class="slide level2 smaller">
<h2>ARMA model to state space model: Aoki’s Method</h2>
<p>Two-step procedure: First, consider the MA<span class="math inline">\((q)\)</span> part: <span class="math display">\[
W_t = a_t - \theta_1 a_{t-1} - \cdots - \theta_q a_{t-q}
\]</span> <span class="math display">\[
\begin{bmatrix}
a_t\\
a_{t-1}\\
\vdots\\
a_{t-q+1}
\end{bmatrix}
=
\begin{bmatrix}
0&amp;0&amp;\cdots &amp; 0&amp;0\\
1&amp;0&amp; \cdots&amp;0&amp;0\\
\vdots &amp; &amp; &amp; &amp;\\
0 &amp; 0 &amp; \cdots &amp; 1 &amp; 0
\end{bmatrix}
\begin{bmatrix}
a_{t-1}\\
a_{t-2}\\
\vdots\\
a_{t-q}
\end{bmatrix}
+
\begin{bmatrix}
1\\
0\\
\vdots\\
0
\end{bmatrix}
a_t
\]</span></p>
<p><span class="math display">\[
W_t=[-\theta_1,-\theta_2, \cdots, -\theta_q]S_t+a_t
\]</span></p>
</section>
<section id="arma-model-to-state-space-model-aokis-method-1" class="slide level2">
<h2>ARMA model to state space model: Aoki’s Method</h2>
<p>First, consider the AR<span class="math inline">\((p)\)</span> part: <span class="math display">\[
Z_t = \phi_1 Z_{t-1} + ... + \phi_p Z_{t-p} + W_t
\]</span> Define state-space vector as <span class="math display">\[
S_t=(Z_{t-1},Z_{t-2}, \cdots , Z_{t-p},a_{t-1}, \cdots, a_{t-q})'
\]</span> Then, we have <span class="math display">\[
\begin{bmatrix}
Z-t\\
Z_{t-1}\\
\vdots\\
Z_{t-p+1}\\
a_t\\
a_{t-1}\\
\vdots\\
a_{t-q+1}
\end{bmatrix}
=
\left[
\begin{array}{cccc|cccc}
\phi_1&amp;\phi_2&amp;\cdots&amp;\phi_p&amp;-\theta_1&amp;-\theta_2&amp;\cdots&amp;-\theta_q\\
1&amp;0&amp;\cdots&amp;0&amp;0&amp;0&amp;\cdots&amp;0\\
\vdots &amp;&amp;&amp;&amp;\vdots&amp;&amp;&amp;\\
0&amp;\cdots&amp;1&amp;0&amp;0&amp;0&amp;\cdots&amp;0\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;0&amp;\cdots&amp;0\\
0&amp;0&amp;\cdots&amp;0&amp;1&amp;0&amp;\cdots&amp;0\\
\vdots &amp;&amp;&amp;&amp;0&amp;&amp;&amp;\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;1&amp;0
\end{array}\right]
\begin{bmatrix}
Z_{t-1}\\
Z_{t-2}\\
\vdots\\
Z_{t-p}\\
a_{t-1}\\
a_{t-2}\\
\vdots\\
a_{t-q}
\end{bmatrix}+
\begin{bmatrix}
1\\
0\\
\vdots\\
0\\
1\\
0\\
\vdots\\
0
\end{bmatrix}a_t
\]</span> and <span class="math display">\[
Z_t=[\phi_1,\cdots,\phi_p,-\theta_1,\cdots,-\theta_q]S_t+a_t
\]</span></p>
</section>
<section id="mle-estimation" class="slide level2">
<h2>MLE Estimation</h2>
<p>Innovations are given by <span class="math display">\[
\epsilon_t = Z_t - HS_t^{t-1}
\]</span> can be shown that <span class="math inline">\(\mathrm{var}(\epsilon_t) = \Sigma_t\)</span>, where <span class="math display">\[
\Sigma_t = HP_t^{t-1}H^T + R
\]</span> Incomplete Data Likelihood: <span class="math display">\[
-\ln L(\Theta) = \dfrac{1}{2}\sum_{t=1}^{n}\log|\Sigma_t(\Theta)| + \dfrac{1}{2}\sum_{t=1}^{n}\epsilon_t(\Theta)^T\Sigma(\Theta)^{-1}\epsilon_t(\Theta)
\]</span> Here <span class="math inline">\(\Theta = (F, Q, R)\)</span>. Use BFGS to find a sequence of <span class="math inline">\(\Theta\)</span>’s and stop when stagnation happens.</p>
</section>
<section id="kalman-smoother" class="slide level2">
<h2>Kalman Smoother</h2>
<ul>
<li><p>Input: initial distribution <span class="math inline">\(X_0\)</span> and data <span class="math inline">\(Z_1,...,Z_T\)</span></p></li>
<li><p>Algorithm: forward-backward pass</p></li>
<li><p>Forward pass: Kalman filter: compute <span class="math inline">\(S_{t+1}^t\)</span> and <span class="math inline">\(S_{t+1}^{t+1}\)</span> for <span class="math inline">\(0 \le t &lt; T\)</span></p></li>
<li><p>Backward pass: Compute <span class="math inline">\(S_t^T\)</span> for <span class="math inline">\(0 \le t &lt; T\)</span></p></li>
</ul>
</section>
<section id="backward-pass" class="slide level2">
<h2>Backward Pass</h2>
<ul>
<li><p>Compute <span class="math inline">\(X_t^T\)</span> given <span class="math inline">\(S_{t+1}^T \sim N(m_{t+1}^T,C_{t+1}^T)\)</span></p></li>
<li><p>Reverse arrow: <span class="math inline">\(S_t^t \leftarrow X_{t+1}^t\)</span></p></li>
<li><p>Same as incorporating measurement in filter</p></li>
<li><p>Compute joint <span class="math inline">\((S_t^t, S_{t+1}^t)\)</span></p></li>
<li><p>Compute conditional <span class="math inline">\((S_t^t \mid S_{t+1}^t )\)</span></p></li>
<li><p>New: <span class="math inline">\(S_{t+1}\)</span> is not “known”, we only know its distribution: <span class="math inline">\(S_{t+1} \sim S_{t+1}^T\)</span></p></li>
<li><p>“Uncondition” on <span class="math inline">\(S_{t+1}\)</span> to compute <span class="math inline">\(S_t^T\)</span> using laws of total expectation and variance</p></li>
</ul>
</section>
<section id="kalman-smoother-1" class="slide level2">
<h2>Kalman Smoother</h2>
<p>A smoothed version of data (an estimate, based on the entire data set) If <span class="math inline">\(S_n\)</span> and <span class="math inline">\(P_n\)</span> obtained via Kalman recursions, then for <span class="math inline">\(t=n,..,1\)</span> <span class="math display">\[
\begin{aligned}
S_{t-1}^t &amp;= S_{t-1} + J_{t-1}(S_t^n - S_t^{t-1})\\
P_{t-1}^n &amp;= P^{t-1} + J_{t-1}(P_t^n - P_t^{t-1})J^T_{t-1}\\
J_{t-1} &amp; = P_{t-1}F^T[P_t^{t-1}]^{-1}
\end{aligned}
\]</span></p>
</section>
<section id="kalman-and-histogran-filter-shortciomings" class="slide level2">
<h2>Kalman and Histogran Filter Shortciomings</h2>
<p>Kalman:</p>
<ul>
<li><p>linear dynamics</p></li>
<li><p>linear measurement model</p></li>
<li><p>normal errors</p></li>
<li><p>unimodal uncertainty</p></li>
</ul>
<p>Histogram:</p>
<ul>
<li><p>discrete states</p></li>
<li><p>approximation</p></li>
<li><p>inefficient in memory</p></li>
</ul>
</section>
<section id="mcmc-financial-econometrics" class="slide level2">
<h2>MCMC Financial Econometrics</h2>
<p>Set of tools for inference and pricing in continuous-time models.</p>
<ul>
<li><p>Simulation-based and provides a unified approach to state and parameter inference. Can also be applied sequentially.</p></li>
<li><p>Can handle Estimation and Model risk. Important implications for financial decision making</p></li>
<li><p>Bayesian inference. Uses conditional probability to solve an inverse problem and estimates expectations using Monte Carlo.</p></li>
</ul>
</section>
<section id="filtering-smoothing-learning-and-prediction" class="slide level2 smaller">
<h2>Filtering, Smoothing, Learning and Prediction</h2>
<p>Data <span class="math inline">\(y_{t}\)</span> depends on a , <span class="math inline">\(x_{t}\)</span>. <span class="math display">\[
\begin{aligned}
\text{Observation equation} &amp;  \text{:\ }y_{t}=f\left(  x_{t},\varepsilon
_{t}^{y}\right)  \\
\text{State evolution} &amp;  \text{: }x_{t+1}=g\left(  x_{t},\varepsilon
_{t+1}^{x}\right)  ,\end{aligned}
\]</span></p>
<ul>
<li><p>Posterior distribution of <span class="math inline">\(p\left(x_{t}|y^{t}\right)\)</span> where <span class="math inline">\(y^{t}=\left(
y_{1},...,y_{t}\right)\)</span></p></li>
<li><p>Prediction and Bayesian updating.</p></li>
</ul>
<p><span class="math display">\[
p\left(  x_{t+1}|y^{t}\right)  =\int p\left(  x_{t+1}|x_{t}\right)  p\left(
x_{t}|y^{t}\right)  dx_{t},\label{Predict}%
\]</span> updated by Bayes rule</p>
<p><span class="math display">\[
\underset{\text{Posterior}}{\underbrace{p\left(  x_{t+1}|y^{t+1}\right)  }%
}\propto\underset{\text{Likelihood}}{\underbrace{p\left(  y_{t+1}%
|x_{t+1}\right)  }}\underset{\text{Prior}}{\underbrace{p\left(  x_{t+1}%
|y^{t}\right)  }}.\label{Update}%
\]</span></p>
</section>
<section id="nonlinear-model" class="slide level2">
<h2>Nonlinear Model</h2>
<ul>
<li><p>The observation and evolution dynamics are <span class="math display">\[
\begin{aligned}
y_t &amp; = \frac{x_t}{1+ x_t^2} + v_t \; , {\rm where} \; v_t \sim N(0,1) \\
x_t &amp; = x_{t-1} + w_t  \; , {\rm where} \; w_t \sim N(0,0.5)\end{aligned}
\]</span></p></li>
<li><p>Initial condition <span class="math inline">\(x_0 \sim N( 1 , 10 )\)</span></p></li>
</ul>
<p>Fundamental question:</p>
<p><em>How do the filtering distributions <span class="math inline">\(p(x_t|y^t)\)</span> propagate in time?</em></p>
</section>
<section id="nonlinear-y_t-x_t-1-x_t2-v_t" class="slide level2">
<h2>Nonlinear: <span class="math inline">\(y_t = x_t / (1+ x_t^2) + v_t\)</span></h2>

<img data-src="fig/pl-nonlinear1.svg" class="r-stretch"></section>
<section id="simulate-data" class="slide level2">
<h2>Simulate Data</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a></a><span class="fu">set.seed</span>(<span class="dv">8</span>)</span>
<span id="cb63-2"><a></a><span class="co"># MC sample size</span></span>
<span id="cb63-3"><a></a>N <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb63-4"><a></a></span>
<span id="cb63-5"><a></a><span class="co">#Posterior at time t=0 p(x[0]|y[0])=N(1,10)</span></span>
<span id="cb63-6"><a></a>x <span class="ot">=</span> <span class="fu">rnorm</span>(N,<span class="dv">1</span>,<span class="fu">sqrt</span>(<span class="dv">10</span>))</span>
<span id="cb63-7"><a></a><span class="fu">hist</span>(x,<span class="at">main=</span><span class="st">"Pr(x[0]|y[0])"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="05.regression_files/figure-revealjs/unnamed-chunk-24-1.png" width="960"></p>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a></a><span class="co">#Obtain draws from prior p(x[1]|y[0])</span></span>
<span id="cb64-2"><a></a>x1 <span class="ot">=</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(N,<span class="dv">0</span>,<span class="fu">sqrt</span>(<span class="fl">0.5</span>))</span>
<span id="cb64-3"><a></a><span class="fu">hist</span>(x1,<span class="at">main=</span><span class="st">"Pr(x[1]|y[0])"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="05.regression_files/figure-revealjs/unnamed-chunk-24-2.png" width="960"></p>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a></a><span class="co">#Likelihood function p(y[1]|x[1])</span></span>
<span id="cb65-2"><a></a>y1  <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb65-3"><a></a>ths <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">30</span>,<span class="dv">30</span>,<span class="at">length=</span><span class="dv">1000</span>)</span>
<span id="cb65-4"><a></a><span class="fu">plot</span>(ths,<span class="fu">dnorm</span>(y1,ths<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>ths<span class="sc">^</span><span class="dv">2</span>),<span class="dv">1</span>),<span class="at">type=</span><span class="st">"l"</span>,<span class="at">xlab=</span><span class="st">""</span>,<span class="at">ylab=</span><span class="st">""</span>)</span>
<span id="cb65-5"><a></a><span class="fu">title</span>(<span class="fu">paste</span>(<span class="st">"p(y[1]="</span>,y1,<span class="st">"|x[1])"</span>,<span class="at">sep=</span><span class="st">""</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="05.regression_files/figure-revealjs/unnamed-chunk-24-3.png" width="960"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="nonlinear-filtering" class="slide level2">
<h2>Nonlinear Filtering</h2>

<img data-src="fig/pl-nonlinearcps.svg" class="r-stretch"></section>
<section id="resampling" class="slide level2">
<h2>Resampling</h2>
<p>Key: resample and propagate particles</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a></a><span class="co"># Computing resampling weights</span></span>
<span id="cb66-2"><a></a>w <span class="ot">=</span> <span class="fu">dnorm</span>(y1,x1<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>x1<span class="sc">^</span><span class="dv">2</span>),<span class="dv">1</span>)</span>
<span id="cb66-3"><a></a><span class="co"># Resample to obtain draws from p(x[1]|y[1])</span></span>
<span id="cb66-4"><a></a>k <span class="ot">=</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>N,<span class="at">size=</span>N,<span class="at">replace=</span><span class="cn">TRUE</span>,<span class="at">prob=</span>w)</span>
<span id="cb66-5"><a></a>x <span class="ot">=</span> x1[k]</span>
<span id="cb66-6"><a></a><span class="fu">hist</span>(x,<span class="at">main=</span><span class="st">"Pr(x[1]|y[1])"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="05.regression_files/figure-revealjs/unnamed-chunk-25-1.png" width="960"></p>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a></a><span class="co">#Obtain draws from prior p(x[2]|y[1])</span></span>
<span id="cb67-2"><a></a>x2 <span class="ot">=</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(N,<span class="dv">0</span>,<span class="fu">sqrt</span>(<span class="fl">0.5</span>))</span>
<span id="cb67-3"><a></a><span class="fu">hist</span>(x2,<span class="at">main=</span><span class="st">"Pr(x[2]|y[1])"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="05.regression_files/figure-revealjs/unnamed-chunk-25-2.png" width="960"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="propagation-of-mc-error" class="slide level2">
<h2>Propagation of MC error</h2>

<img data-src="fig/pl-MCerror.svg" class="r-stretch"></section>
<section id="dynamic-linear-model-dlm-kalman-filter" class="slide level2">
<h2>Dynamic Linear Model (DLM): Kalman Filter</h2>
<p>Kalman filter for linear Gaussian systems</p>
<ul>
<li>FFBS (Filter Forward Backwards Sample)</li>
</ul>
<p>This determines the posterior distribution of the states</p>
<p><span class="math display">\[
p( x_t | y^t ) \; {\rm and} \; p( x_t | y^T )
\]</span> Also the joint distribution <span class="math inline">\(p( x^T | y^T )\)</span> of the hidden states.</p>
<ul>
<li><p>Discrete Hidden Markov Model HMM (Baum-Welch, Viterbi)</p></li>
<li><p>With parameters <em>known</em> the Kalman filter gives the exact recursions.</p></li>
</ul>
</section>
<section id="simulate-dlm" class="slide level2">
<h2>Simulate DLM</h2>
<p>Dynamic Linear Models <span class="math display">\[
y_t = x_t + v_t \; \; {\rm and} \; \; x_t = \alpha + \beta x_{t-1} + w_t
\]</span> Simulate Data</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a></a>T1<span class="ot">=</span><span class="dv">50</span>; alpha<span class="ot">=</span><span class="sc">-</span><span class="fl">0.01</span>; beta<span class="ot">=</span><span class="fl">0.98</span>; sW<span class="ot">=</span><span class="fl">0.1</span>; sV<span class="ot">=</span><span class="fl">0.25</span>; W<span class="ot">=</span>sW<span class="sc">^</span><span class="dv">2</span>; V<span class="ot">=</span>sV<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb68-2"><a></a></span>
<span id="cb68-3"><a></a>y <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="sc">*</span>T1)</span>
<span id="cb68-4"><a></a>ht <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="sc">*</span>T1)</span>
<span id="cb68-5"><a></a></span>
<span id="cb68-6"><a></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(<span class="dv">2</span><span class="sc">*</span>T1)){</span>
<span id="cb68-7"><a></a>    ht[t] <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">1</span>,alpha<span class="sc">+</span>beta<span class="sc">*</span>ht[t<span class="dv">-1</span>],sW)</span>
<span id="cb68-8"><a></a>    y[t] <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">1</span>,ht[t],sV)</span>
<span id="cb68-9"><a></a>}</span>
<span id="cb68-10"><a></a>ht <span class="ot">=</span> ht[(T1<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(<span class="dv">2</span><span class="sc">*</span>T1)]</span>
<span id="cb68-11"><a></a>y <span class="ot">=</span> y[(T1<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(<span class="dv">2</span><span class="sc">*</span>T1)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="exact-calculations" class="slide level2">
<h2>Exact calculations</h2>
<p>Kalman Filter recursions</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a></a>m<span class="ot">=</span><span class="fu">rep</span>(<span class="dv">0</span>,T1);C<span class="ot">=</span><span class="fu">rep</span>(<span class="dv">0</span>,T1); m0<span class="ot">=</span><span class="dv">0</span>; C0<span class="ot">=</span><span class="dv">100</span></span>
<span id="cb69-2"><a></a></span>
<span id="cb69-3"><a></a>R <span class="ot">=</span> C0<span class="sc">+</span>W; Q <span class="ot">=</span> R<span class="sc">+</span>V; A <span class="ot">=</span> R<span class="sc">/</span>Q</span>
<span id="cb69-4"><a></a>m[<span class="dv">1</span>] <span class="ot">=</span> m0<span class="sc">+</span>A<span class="sc">*</span>(y[<span class="dv">1</span>]<span class="sc">-</span>m0); C[<span class="dv">1</span>] <span class="ot">=</span> R<span class="sc">-</span>A<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>Q</span>
<span id="cb69-5"><a></a></span>
<span id="cb69-6"><a></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>T1){</span>
<span id="cb69-7"><a></a>    R    <span class="ot">=</span> C[t<span class="dv">-1</span>]<span class="sc">+</span>W</span>
<span id="cb69-8"><a></a>    Q    <span class="ot">=</span> R<span class="sc">+</span>V</span>
<span id="cb69-9"><a></a>    A    <span class="ot">=</span> R<span class="sc">/</span>Q</span>
<span id="cb69-10"><a></a>    m[t] <span class="ot">=</span> m[t<span class="dv">-1</span>]<span class="sc">+</span>A<span class="sc">*</span>(y[t]<span class="sc">-</span>m[t<span class="dv">-1</span>])</span>
<span id="cb69-11"><a></a>    C[t] <span class="ot">=</span> R<span class="sc">-</span>A<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>Q</span>
<span id="cb69-12"><a></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="dlm-data" class="slide level2">
<h2>DLM Data</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a></a><span class="fu">plot</span>(y,<span class="at">type=</span><span class="st">"b"</span>,<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">xlab=</span><span class="st">"Time"</span>,<span class="at">ylab=</span><span class="st">"y_t"</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">bty=</span><span class="st">'n'</span>)</span>
<span id="cb70-2"><a></a><span class="fu">lines</span>(m,<span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb70-3"><a></a><span class="fu">lines</span>(m<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(C),<span class="at">col=</span><span class="st">"grey"</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb70-4"><a></a><span class="fu">lines</span>(m<span class="dv">-2</span><span class="sc">*</span><span class="fu">sqrt</span>(C),<span class="at">col=</span><span class="st">"grey"</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-26-1.png" width="960" class="r-stretch"></section>
<section id="bootstrap-filter" class="slide level2">
<h2>Bootstrap Filter</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a></a>M <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb71-2"><a></a>h <span class="ot">=</span> <span class="fu">rnorm</span>(M,m0,<span class="fu">sqrt</span>(C0))</span>
<span id="cb71-3"><a></a>hs <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb71-4"><a></a></span>
<span id="cb71-5"><a></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T1){</span>
<span id="cb71-6"><a></a>    h1 <span class="ot">=</span> <span class="fu">rnorm</span>(M,alpha<span class="sc">+</span>beta<span class="sc">*</span>h,sW)</span>
<span id="cb71-7"><a></a>    w  <span class="ot">=</span> <span class="fu">dnorm</span>(y[t],h1,sV)</span>
<span id="cb71-8"><a></a>    w  <span class="ot">=</span> w<span class="sc">/</span><span class="fu">sum</span>(w)</span>
<span id="cb71-9"><a></a>    h  <span class="ot">=</span> <span class="fu">sample</span>(h1,<span class="at">size=</span>M,<span class="at">replace=</span>T,<span class="at">prob=</span>w)</span>
<span id="cb71-10"><a></a>    hs <span class="ot">=</span> <span class="fu">cbind</span>(hs,h)</span>
<span id="cb71-11"><a></a>}</span>
<span id="cb71-12"><a></a><span class="co"># Quantiles</span></span>
<span id="cb71-13"><a></a>q025 <span class="ot">=</span> <span class="cf">function</span>(x){<span class="fu">quantile</span>(x,<span class="fl">0.025</span>)}</span>
<span id="cb71-14"><a></a>q975 <span class="ot">=</span> <span class="cf">function</span>(x){<span class="fu">quantile</span>(x,<span class="fl">0.975</span>)}</span>
<span id="cb71-15"><a></a>h025 <span class="ot">=</span> <span class="fu">apply</span>(hs,<span class="dv">2</span>,q025)</span>
<span id="cb71-16"><a></a>h975 <span class="ot">=</span> <span class="fu">apply</span>(hs,<span class="dv">2</span>,q975)</span>
<span id="cb71-17"><a></a>sd   <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">apply</span>(hs,<span class="dv">2</span>,var))</span>
<span id="cb71-18"><a></a>m <span class="ot">=</span> <span class="fu">colMeans</span>(hs)</span>
<span id="cb71-19"><a></a><span class="fu">plot</span>(y,<span class="at">type=</span><span class="st">"b"</span>,<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">xlab=</span><span class="st">"Time"</span>,<span class="at">ylab=</span><span class="st">"y_t"</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">bty=</span><span class="st">'n'</span>)</span>
<span id="cb71-20"><a></a><span class="fu">lines</span>(m,<span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb71-21"><a></a><span class="fu">lines</span>(h025,<span class="at">col=</span><span class="st">"grey"</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb71-22"><a></a><span class="fu">lines</span>(h975,<span class="at">col=</span><span class="st">"grey"</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<!-- ## DLM Filtering

![](fig/pl-DLM2.svg) -->
<img data-src="05.regression_files/figure-revealjs/unnamed-chunk-27-1.png" width="960" class="r-stretch"></section>
<section id="streaming-data-how-do-parameter-distributions-change-in-time" class="slide level2">
<h2>Streaming Data: How do Parameter Distributions change in Time?</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><img data-src="fignick/BayesThm.jpg"> Bayes theorem: <span class="math display">\[
p(\theta \mid y^t) \propto p(y_t \mid \theta) \,
p(\theta \mid y^{t-1})
\]</span></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Online Dynamic Learning</p>
<ul>
<li>Real-time surveillance</li>
<li>Bayes means sequential updating of information</li>
<li>Update posterior density <span class="math inline">\(p(\theta \mid y_t)\)</span> with every new observation (<span class="math inline">\(t = 1, \ldots, T\)</span>) - “sequential learning”</li>
</ul>
</div>
</div>
</div>
</section>
<section id="galton-1877-first-particle-filter" class="slide level2">
<h2>Galton 1877: First Particle Filter</h2>

<img data-src="fignick/1877.jpg" class="r-stretch"></section>
<section id="streaming-data-online-learning" class="slide level2 scrollable">
<h2>Streaming Data: Online Learning</h2>
<p>Construct an essential state vector <span class="math inline">\(Z_{t+1}\)</span>. <span class="math display">\[
\begin{aligned}
p(Z_{t+1}|y^{t+1}) &amp;= \int p(Z_{t+1}|Z_t, y_{t+1}) \;d\mathbb{P}(Z_t|y^{t+1}) \\
&amp;\propto  \int \underbrace{p(Z_{t+1}|Z_t, y_{t+1})}_{propagate} \overbrace{ \underbrace{p(y_{t+1} | Z_t)}_{resample} \;d\mathbb{P}(Z_t|y^t)}\end{aligned}
\]</span></p>
<ol type="1">
<li><p><em>Re-sample</em> with weights proportional to <span class="math inline">\(p(y_{t+1} | Z_t^{(i)})\)</span> and generate <span class="math inline">\(\{Z_t^{\zeta(i)}\}_{i=1}^N\)</span></p></li>
<li><p><em>Propagate</em> with <span class="math inline">\(Z_{t+1}^{(i)} \sim
p(Z_{t+1}|Z_t^{\zeta(i)}, y_{t+1})\)</span> to obtain <span class="math inline">\(\{Z_{t+1}^{(i)}\}_{i=1}^N\)</span></p></li>
</ol>
<p>Parameters: <span class="math inline">\(p( \theta | Z_{t+1} )\)</span> drawn “offline”</p>
</section>
<section id="sample-resample" class="slide level2">
<h2>Sample – Resample</h2>

<img data-src="fig/sample-resample.svg" class="r-stretch quarto-figure-center"><p class="caption">sample-resample</p></section>
<section id="resample-sample" class="slide level2">
<h2>Resample – Sample</h2>

<img data-src="fig/resample-sample.svg" class="r-stretch quarto-figure-center"><p class="caption">resample-sample</p></section>
<section id="particle-methods-blind-propagation" class="slide level2">
<h2>Particle Methods: Blind Propagation</h2>

<img data-src="fignick/smc.png" class="r-stretch quarto-figure-center"><p class="caption">Propagate-Resample is replaced by Resample-Propagate</p></section>
<section id="traffic-problem" class="slide level2">
<h2>Traffic Problem</h2>

<img data-src="fignick/d_domain.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">State-Space</p></section>
<section id="wave-speed-propagation-is-a-mixture-distribution" class="slide level2">
<h2>Wave Speed Propagation is a Mixture Distribution</h2>
<p>Shock wave propagation speed is a mixture, when calculated using Godunov scheme <span class="math display">\[
w = \frac{q(\rho_l) - q(\rho_r)}{\rho_l-\rho_r}  \ \left[\frac{mi}{h}\right] = \left[\frac{veh}{h}\right]\left[\frac{mi}{veh}\right].
\]</span> Assume <span class="math inline">\(\rho_l \sim TN(32, 16, 0, 320)\)</span> and <span class="math inline">\(\rho_r \sim TN(48, 16, 0,320)\)</span><br>
<span class="math inline">\(q_c = 1600  \ veh/h\)</span>, <span class="math inline">\(\rho_c = 40 \ veh/mi\)</span>, and <span class="math inline">\(\rho_{jam} = 320 \ veh/mi\)</span><br>
</p>

<img data-src="fignick/sigma-mixture.png" style="width:70.0%" class="r-stretch"></section>
<section id="traffic-flow-speed-forecast-is-a-mixtrue-dsitribution" class="slide level2">
<h2>Traffic Flow Speed Forecast is a Mixtrue Dsitribution</h2>
<p><strong>Theorem</strong>: The solution (including numerical) to the LWR model with stochastic initial conditions is a mixture distribution.</p>

<img data-src="fignick/mixture.png" style="width:40.0%" class="r-stretch"><p>A moment based filters such as Kalman Filter or Extended Kalman Filter would not capture the mixture.</p>
</section>
<section id="problem-at-hand" class="slide level2">
<h2>Problem at Hand</h2>
<p>The Parameter Learning and State Estimation Problem</p>
<ul>
<li><p>Goal: given sparse sensor measurements, find the distribution over traffic state and underlying traffic flow parameters <span class="math inline">\(p(\theta_t, \phi|y_1, y_2,...,y_t); \ \phi=(q_c,\rho_c)\)</span></p></li>
<li><p>Parameters of the evolution equation (LWR) are stochastic</p></li>
<li><p>Distribution over state is a mixture</p></li>
<li><p>Can’t use moment based filters (KF, EKF,...)</p></li>
</ul>
</section>
<section id="data-assimilation-state-space-representation" class="slide level2">
<h2>Data Assimilation: State Space Representation</h2>

<img data-src="fignick/state-space.png" style="width:60.0%" class="r-stretch"><p>State space formulation allows to combine knowledge from analytical model with the one from field measurements, while taking model and measurement errors into account</p>
</section>
<section id="state-space-representation" class="slide level2">
<h2>State Space Representation</h2>
<ul>
<li><p>State vector <span class="math inline">\(\theta_t = ( \rho_{1t} , \ldots , \rho_{nt} )\)</span></p></li>
<li><p>Boundary conditionals <span class="math inline">\(\rho_{0t}\)</span> and <span class="math inline">\(\rho_{(n+1)t}\)</span></p></li>
<li><p>Underlying parameters <span class="math inline">\(\phi = (q_c, \rho_c)\)</span> are stochastic</p></li>
</ul>
<p><span class="math display">\[\begin{align}
\mbox{Observation: }&amp;y_{t+1} = H\theta_{t+1}  + v; \ v \sim N(0,V) \label{eqn-y}\\
\mbox{Evolution: }&amp;\theta_{t+1} = f_{\phi}(\theta_t) + w; \ w \sim N(0,W) \label{(eqn-x)}
\end{align}\]</span></p>
<p><span class="math inline">\(H: \mathbb{R}^{M} \rightarrow \mathbb{R}^k\)</span> in the measurement model. <span class="math inline">\(\phi  = (q_c,\rho_c, \rho_{max})\)</span>.</p>
<p>Parameter priors: <span class="math inline">\(q_c \sim N(\mu_q, \sigma_c^2)\)</span>, <span class="math inline">\(\rho_c = Uniform(\rho_{min}, \rho_{max})\)</span></p>
</section>
<section id="particle-parameter-learning" class="slide level2">
<h2>Particle Parameter Learning</h2>

<img data-src="fignick/hmm-learning.png" style="width:100.0%" class="r-stretch"></section>
<section id="sample-based-pdf-representation" class="slide level2">
<h2>Sample-based PDF Representation</h2>
<ul>
<li><p>Regions of high density: Many particles and Large weight of particles</p></li>
<li><p>Uneven partitioning</p></li>
<li><p>Discrete approximation for continuous pdf</p></li>
</ul>
<p><span class="math display">\[
p^{N}\left(  \theta_{t+1}|y^{t+1}\right) \propto \sum_{i=1}^{N}w_{t}^{\left(  i\right)  }p\left(  \theta_{t+1}|\theta_t^{\left(
i\right)  },y_{t+1}\right)
\]</span></p>
</section>
<section id="particle-filter" class="slide level2 smaller">
<h2>Particle Filter</h2>
<p>Bayes Rule: <span class="math display">\[
p(y_{t+1},\theta_{t+1}|\theta_t)=p(y_{t+1}|\theta_t)\,p(\theta_{t+1}|%
\theta_t,y_{t+1}).
\]</span></p>
<ul>
<li>Given a particle approximation to <span class="math inline">\(p^{N}\left(  \theta_t|y^{t}\right)\)</span> <span class="math display">\[
\begin{aligned}
p^{N}\left(  \theta_{t+1}|y^{t+1}\right)   &amp;  \propto\sum_{i=1}^{N}p\left(
y_{t+1}|\theta_t^{\left(  i\right)  }\right)  p\left(  \theta_{t+1}|\theta_t^{\left(
i\right)  },y_{t+1}\right) \label{Mixture2}\\
&amp;  =\sum_{i=1}^{N}w_{t}^{\left(  i\right)  }p\left(  \theta_{t+1}|\theta_t^{\left(
i\right)  },y_{t+1}\right)  \text{,}%
\end{aligned}
\]</span> where</li>
</ul>
<p><span class="math display">\[
w_{t}^{\left(  i\right)  }=\frac{p\left(  y_{t+1}|\theta_t^{\left(  i\right)
}\right)  }{\sum_{i=1}^{N}p\left(  y_{t+1}|\theta_t^{\left(  i\right)  }\right)
}\text{.}%
\]</span></p>
<ul>
<li>Essentially a mixture Kalman filter</li>
</ul>
</section>
<section id="particle-parameter-learning-1" class="slide level2 smaller">
<h2>Particle Parameter Learning</h2>
<p>Given particles (a.k.a. random draws) <span class="math inline">\((\theta^{(i)}_t,\phi^{(i)},s^{(i)}_t),\)</span> <span class="math inline">\(i=1,\ldots,N\)</span> <span class="math display">\[
p( \theta_t | y_{1:t} ) = \frac{1}{N} \sum_{i=1}^N \delta_{ \theta^{(i)} } \; .
\]</span></p>
<ul>
<li><p>First resample <span class="math inline">\((\theta^{k(i)}_t,\phi^{k(i)},s^{k(i)}_t)\)</span> with weights proportional to <span class="math inline">\(p(y_{t+1}|\theta^{k(i)}_t,\phi^{k(i)})\)</span> and <span class="math inline">\(s_t^{k(i)}=S(s^{(i)}_t,\theta^{k(i)}_t,y_{t+1})\)</span> and then propogate to <span class="math inline">\(p(\theta_{t+1}|y_{1:t+1})\)</span> by drawing <span class="math inline">\(\theta^{(i)}_{t+1}\,\)</span>from <span class="math inline">\(p(\theta_{t+1}|\theta^{k(i)}_t,\phi^{k(i)},y_{t+1}),\,i=1,\ldots,N\)</span>.</p></li>
<li><p>Next we update the sufficient statistic as</p></li>
</ul>
<p><span class="math display">\[
s_{t+1}=S(s_t^{k(i)},\theta^{(i)}_{t+1},y_{t+1}),
\]</span> for <span class="math inline">\(i=1,\ldots,N\)</span>, which represents a deterministic propogation.</p>
<ul>
<li>Finally, parameter learning is completed by drawing <span class="math inline">\(\phi^{(i)}\)</span> using <span class="math inline">\(p(\phi|s^{(i)}_{t+1})\)</span> for <span class="math inline">\(i=1,\ldots,N\)</span>.</li>
</ul>
</section>
<section id="streaming-data-online-learning-1" class="slide level2 scrollable">
<h2>Streaming Data: Online Learning</h2>
<p>Construct an essential state vector <span class="math inline">\(Z_{t+1}\)</span>. <span class="math display">\[
\begin{aligned}
p(Z_{t+1}|y^{t+1}) &amp;= \int p(Z_{t+1}|Z_t, y_{t+1}) \;d\mathbb{P}(Z_t|y^{t+1}) \\
&amp;\propto  \int \underbrace{p(Z_{t+1}|Z_t, y_{t+1})}_{propagate} \overbrace{ \underbrace{p(y_{t+1} | Z_t)}_{resample} \;d\mathbb{P}(Z_t|y^t)}
\end{aligned}
\]</span></p>
<ol type="1">
<li><p><em>Re-sample</em> with weights proportional to <span class="math inline">\(p(y_{t+1} | Z_t^{(i)})\)</span> and generate <span class="math inline">\(\{Z_t^{\zeta(i)}\}_{i=1}^N\)</span></p></li>
<li><p><em>Propagate</em> with <span class="math inline">\(Z_{t+1}^{(i)} \sim
p(Z_{t+1}|Z_t^{\zeta(i)}, y_{t+1})\)</span> to obtain <span class="math inline">\(\{Z_{t+1}^{(i)}\}_{i=1}^N\)</span></p></li>
</ol>
<p>Parameters: <span class="math inline">\(p( \theta | Z_{t+1} )\)</span> drawn “offline”</p>
</section>
<section id="resample-propagate" class="slide level2">
<h2>Resample – Propagate</h2>

<img data-src="fig/resample-sample.svg" class="r-stretch"></section>
<section id="algorithm" class="slide level2">
<h2>Algorithm</h2>
<p>These ingredients then define a particle filtering and learning algorithm for the sequence of joint posterior distributions <span class="math inline">\(p( \theta_t , \phi | y_{1:t} )\)</span>: <span class="math display">\[
\begin{aligned}
&amp;  \text{Step 1. (Resample) Draw an index } k_t \left(  i\right)  \sim
Mult_{N}\left(  w_{t}^{\left(  1\right)  },...,w_{t}^{\left(  N\right)
}\right), \\
&amp; \mbox{where the weights are given by } w_t^{(i)} \propto p(y_{t+1}|(\theta_t,\phi)^{(i)}), \   \text{ for }i=1,...,N\\
\text{ }  &amp;  \text{Step 2. (Propagate) Draw }\theta_{t+1}^{\left(  i\right)  }\sim
p\left(  \theta_{t+1}|\theta_t^{k_t \left(  i\right)  },y_{t+1}\right)  \text{ for
}i=1,...,N.\\
\text{ }  &amp;  \text{Step 3. (Update) } s_{t+1}^{(i)} =S(s_t^{k_t(i)},\theta_{t+1}^{(i)},y_{t+1})\\
\text{ }  &amp;  \text{Step 4. (Replenish) } \phi^{(i)} \sim p( \phi | s_{t+1}^{(i)} )
\end{aligned}
\]</span> There are a number of efficiency gains from such an approach, e.g.&nbsp;it does not suffer from degeneracy problems associated with traditional propagate-resample algorithms when <span class="math inline">\(y_{t+1}\)</span> is an outliers.</p>
</section>
<section id="obtaining-state-estimates-from-particles" class="slide level2 smaller">
<h2>Obtaining state estimates from particles</h2>
<ul>
<li>Any estimate of a function <span class="math inline">\(f(\theta_t)\)</span> can be calculated by discrete-approximation</li>
</ul>
<p><span class="math display">\[
\mbox{E}(f(\theta_t)) = \dfrac{1}{N}\sum_{j=1}^{N}w_t^{(j)}f(\theta_t^{(j)})
\]</span></p>
<ul>
<li>Mean:</li>
</ul>
<p><span class="math display">\[
\mbox{E}(\theta_t) = \dfrac{1}{N}\sum_{j=1}^{N}w_t^{(j)}\theta_t^{(j)}
\]</span></p>
<ul>
<li><p>MAP-estimate: particle with largest weight</p></li>
<li><p>Robust mean: mean within window around MAP-estimate</p></li>
</ul>
</section>
<section id="particle-filters-pluses" class="slide level2">
<h2>Particle Filters: Pluses</h2>
<ul>
<li><p>Estimation of full PDFs</p></li>
<li><p>Non-Gaussian distributions (multi-modal)</p></li>
<li><p>Non-linear state and observation model</p></li>
<li><p>Parallelizable</p></li>
</ul>
</section>
<section id="particle-filters-minuses" class="slide level2">
<h2>Particle Filters: Minuses</h2>
<ul>
<li><p>Degeneracy problem</p></li>
<li><p>High number of particles needed</p></li>
<li><p>Computationally expensive</p></li>
<li><p>Linear-Gaussian assumption is often sufficient</p></li>
</ul>
</section>
<section id="applications-localization" class="slide level2">
<h2>Applications: Localization</h2>
<ul>
<li><p>Track car position in given road map</p></li>
<li><p>Track car position from radio frequency measurements</p></li>
<li><p>Track aircraft position from estimated terrain elevation</p></li>
<li><p>Collision Avoidance (Prediction)</p></li>
</ul>
</section>
<section id="applications-model-estimation" class="slide level2">
<h2>Applications: Model Estimation</h2>
<ul>
<li><p>Tracking with multiple motion-models</p></li>
<li><p>Recovery of signal from noisy measurements</p></li>
<li><p>Neural Network model selection (on-line classification)</p></li>
</ul>
</section>
<section id="applications-other" class="slide level2">
<h2>Applications: Other</h2>
<ul>
<li><p>Visual Tracking</p></li>
<li><p>Prediction of (financial) time series</p></li>
<li><p>Quality control in semiconductor industry</p></li>
<li><p>Military applications: Target recognition from single or multiple images, Guidance of missiles</p></li>
<li><p>Reinforcement Learning</p></li>
</ul>
</section>
<section id="mixture-kalman-filter-for-traffic" class="slide level2 smaller">
<h2>Mixture Kalman Filter For Traffic</h2>
<p><span class="math display">\[
\begin{aligned}
\mbox{Observation: }&amp;y_{t+1} = Hx_{t+1}  + \gamma^Tz_{t+1} + v_{t+1} , \ v_{t+1} \sim N(0, V_{t+1})\\
\mbox{Evolution: }&amp;x_{t+1} = F_{\alpha_{t+1}}x_t + (1-F_{\alpha_{t+1}})\mu + \alpha_t\beta_{t} + \omega_{1} \\
&amp;\beta_{t+1} = \max(0,\beta_{t} + \omega_{2} \label{eqn-beta})\\
\mbox{Switching Evolution: }&amp;\alpha_{t+1} \sim p(\alpha_{t+1} |\alpha_{t},Z_{t})
\end{aligned}
\]</span> where <span class="math inline">\(z_t\)</span> is an exogenous variable that effects the sensor model, <span class="math inline">\(\mu\)</span> is an average free flow speed <span class="math display">\[
\alpha_t \in \{0,1,-1\}
\]</span> <span class="math display">\[
\omega = (\omega_{1}, \omega_{2})^T \sim N(0, W), \ v \sim N(0,V)
\]</span> <span class="math display">\[
F_{\alpha_t} = \left\{
\begin{aligned}
&amp;1,  \ \alpha_t \in \{1,-1\}\\
&amp;F, \  \alpha_t = 0\end{aligned}
\right.
\]</span> No boundary conditions estimation is needed. No capacity/critical density is needed.</p>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1300,

        height: 920,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>