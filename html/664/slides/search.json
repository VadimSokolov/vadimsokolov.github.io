[
  {
    "objectID": "test.html#slide-1",
    "href": "test.html#slide-1",
    "title": "Bayes AI",
    "section": "SLide 1",
    "text": "SLide 1\n\\[\\begin{align}\n\\mbox{Observation: }&y_{t+1} = H\\theta_{t+1}  + v; \\ v \\sim N(0,V) \\label{eqn-y}\\\\\n\\mbox{Evolution: }&\\theta_{t+1} = f_{\\phi}(\\theta_t) + w; \\ w \\sim N(0,W) \\label{(eqn-x)}\n\\end{align}\\]"
  },
  {
    "objectID": "06-mcmc.html#mcmc-simulation",
    "href": "06-mcmc.html#mcmc-simulation",
    "title": "Bayes AI",
    "section": "MCMC Simulation",
    "text": "MCMC Simulation\nSuppose that \\(X \\sim F_X ( x )\\) and let \\(Y = g (X)\\).\nHow do we find \\(F_Y ( y )\\) and \\(f_Y ( y )\\) ?\n\nvon Neumann\n\nGiven a uniform \\(U\\), how do we find \\(X= g(U)\\)?\n\nIn the bivariate case \\((X,Y) \\rightarrow (U,V)\\).\n\nWe need to find \\(f_{(U,V)} ( u , v )\\) from \\(f_{X,Y}(x,y)\\)\n\nApplications: Simulation, MCMC and PF."
  },
  {
    "objectID": "06-mcmc.html#transformations",
    "href": "06-mcmc.html#transformations",
    "title": "Bayes AI",
    "section": "Transformations",
    "text": "Transformations\nThe cdf identity gives \\[\nF_Y ( y) = \\mathbb{P} ( Y \\leq y ) = \\mathbb{P} ( g( X) \\leq y )\n\\]\n\nHence if the function \\(g ( \\cdot )\\) is monotone we can invert to get\n\n\\[\nF_Y ( y ) = \\int_{ g( x) \\leq y } f_X ( x ) dx\n\\]\n\nIf \\(g\\) is increasing \\(F_Y ( y ) = P( X \\leq g^{-1} ( y ) ) = F_X ( g^{-1} ( y ) )\\)\n\nIf \\(g\\) is decreasing \\(F_Y ( y ) = P( X \\geq g^{-1} ( y ) ) = 1 - F_X ( g^{-1} ( y ) )\\)"
  },
  {
    "objectID": "06-mcmc.html#transformation-identity",
    "href": "06-mcmc.html#transformation-identity",
    "title": "Bayes AI",
    "section": "Transformation Identity",
    "text": "Transformation Identity\n\nTheorem 1: Let \\(X\\) have pdf \\(f_X ( x)\\) and let \\(Y=g(X)\\). Then if \\(g\\) is a monotone function we have\n\n\\[\nf_Y ( y) = f_X ( g^{-1} ( y ) ) \\left  | \\frac{ d}{dy}  g^{-1} ( y ) \\right |\n\\] There’s also a multivariate version of this that we’ll see later.\n\nSuppose \\(X\\) is a continuous rv, what’s the pdf for \\(Y = X^2\\)?\nLet \\(X \\sim N ( 0 ,1 )\\), what’s the pdf for \\(Y = X^2\\)?"
  },
  {
    "objectID": "06-mcmc.html#probability-integral-transform",
    "href": "06-mcmc.html#probability-integral-transform",
    "title": "Bayes AI",
    "section": "Probability Integral Transform",
    "text": "Probability Integral Transform\ntheorem Suppose that \\(U \\sim U[0,1]\\), then for any continuous distribution function \\(F\\), the random variable \\(X= F^{-1} (U)\\) has distribution function \\(F\\).\n\nRemember that for \\(u \\in [0,1]\\), \\(\\mathbb{P} \\left ( U \\leq u \\right ) = u\\), so we have\n\n\\[\n\\mathbb{P} \\left (X \\leq x \\right )= \\mathbb{P} \\left ( F^{-1} (U) \\leq x \\right )= \\mathbb{P} \\left ( U \\leq F(x) \\right )=F(x)\n\\] Hence, \\(X = F_X^{-1}(U)\\)."
  },
  {
    "objectID": "06-mcmc.html#normal",
    "href": "06-mcmc.html#normal",
    "title": "Bayes AI",
    "section": "Normal",
    "text": "Normal\nSometimes thare are short-cut formulas to generate random draws\nNormal \\(N(0,I_2)\\): \\(x_1,x_2\\) uniform on \\([0,1]\\) then \\[\n\\begin{aligned}\ny_1 = & \\sqrt{-2\\log x_1}\\cos(2\\pi x_2)\\\\\ny_2 = & \\sqrt{-2\\log x_1}\\sin(2\\pi x_2)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#simulation-and-transformations",
    "href": "06-mcmc.html#simulation-and-transformations",
    "title": "Bayes AI",
    "section": "Simulation and Transformations",
    "text": "Simulation and Transformations\nAn important application is how to transform multiple random variables?\n\nSuppose that we have random variables:\n\n\\[\n( X , Y ) \\sim  f_{ X , Y} ( x , y )\n\\] A transformation of interest given by: \\[\nU = g ( X , Y )\n\\; \\; {\\rm and} \\; \\;\nV = h ( X , Y )\n\\]\n\nThe problem is how to compute \\(f_{ U , V } ( u , v )\\) ? Jacobian\n\n\\[\nJ = \\frac{ \\partial ( x , y ) }{ \\partial ( u , v ) }  = \\left |  \\begin{array}{cc}\n\\frac{ \\partial x }{ \\partial u} & \\frac{ \\partial x }{ \\partial v} \\\\\n\\frac{ \\partial y }{ \\partial u} & \\frac{ \\partial y }{ \\partial v}\n\\end{array} \\right |\n\\]"
  },
  {
    "objectID": "06-mcmc.html#bivariate-change-of-variable",
    "href": "06-mcmc.html#bivariate-change-of-variable",
    "title": "Bayes AI",
    "section": "Bivariate Change of Variable",
    "text": "Bivariate Change of Variable\n\nTheorem: (change of variable)\n\n\\[\nf_{ U , V } ( u , v ) = f_{ X , Y} ( h_1 ( u , v )  , h_2 ( u , v ) )\n\\left |  \\frac{ \\partial ( x , y ) }{ \\partial ( u , v ) } \\right |\n\\] The last term is the Jacobian.\nThis can be calculated in two ways.\n\\[\n\\left |  \\frac{ \\partial ( x , y ) }{ \\partial ( u , v ) } \\right | =\n1 / \\left |  \\frac{ \\partial ( u , v ) }{ \\partial ( x , y ) } \\right |\n\\]\n\nSo we don’t always need the inverse transformation \\(( x , y ) = ( g^{-1} ( u , v )  , h^{-1} ( u , v ) )\\)"
  },
  {
    "objectID": "06-mcmc.html#inequalities-and-identities",
    "href": "06-mcmc.html#inequalities-and-identities",
    "title": "Bayes AI",
    "section": "Inequalities and Identities",
    "text": "Inequalities and Identities\n\nMarkov\n\n\\[\n\\mathbb{P} \\left ( g( X ) \\geq c \\right ) \\leq \\frac{ \\mathbb{E} ( g(X) ) }{c }\n\\; \\; {\\rm  where} \\; \\;   g( X) \\geq 0\n\\]\n\nChebyshev\n\n\\[\n\\mathbb{P} \\left ( | X - \\mu | \\geq c \\right ) \\leq \\frac{ Var(X) }{c^2 }\n\\]\n\nJensen\n\n\\[\n\\mathbb{E} \\left ( \\phi ( X ) \\right ) \\leq \\phi \\left ( \\mathbb{E}( X ) \\right )\n\\]\n\nCauchy-Schwarz \\[\ncorr (X,Y) \\leq 1\n\\]\n\nChebyshev follows from Markov. Mike Steele and Cauchy-Schwarz."
  },
  {
    "objectID": "06-mcmc.html#markov-inequality",
    "href": "06-mcmc.html#markov-inequality",
    "title": "Bayes AI",
    "section": "Markov Inequality",
    "text": "Markov Inequality\nLet \\(f\\) be non-decreasing \\[\n\\begin{aligned}\nP ( Z &gt; t ) &= P ( f(Z) \\geq f(t) ) \\\\\n& = E \\left (  \\mathbb{I}  ( f( Z) \\geq f(t )  ) \\right ) \\\\\n& \\leq E \\left (  \\mathbb{I}  ( f( Z) \\geq f(t ) )  \\frac{f(Z)}{f(t) }  \\right ) \\\\\n& =  E\\left  (  \\frac{f(Z)}{f(t) }  \\right )\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#concentration-inequalities",
    "href": "06-mcmc.html#concentration-inequalities",
    "title": "Bayes AI",
    "section": "Concentration Inequalities",
    "text": "Concentration Inequalities\nLaw of Large Numbers \\[\n\\lim_{ n \\rightarrow \\infty } \\mathbb{P} \\left ( | Z - E(Z) | &gt; n \\epsilon  \\right ) = 0 \\; \\; \\forall \\epsilon &gt; 0\n\\]\nCentral Limt Theorem (CLT) \\[\n\\lim_{ n \\rightarrow \\infty } \\mathbb{P} \\left ( n^{- 1/2} ( | Z - E(Z) |  ) &gt;  \\epsilon  \\right ) = \\Phi ( x )\n\\]\nPosterior Concentration"
  },
  {
    "objectID": "06-mcmc.html#hoeffding-and-bernstein",
    "href": "06-mcmc.html#hoeffding-and-bernstein",
    "title": "Bayes AI",
    "section": "Hoeffding and Bernstein",
    "text": "Hoeffding and Bernstein\nLet \\(Z= \\sum_{i=1}^n X_i\\).\nHoeffding \\[\nP ( Z &gt; E(Z) +  t ) \\leq \\exp \\left  ( - \\frac{ t^2}{2n} \\right )\n\\]\nBernstein \\[\nP ( Z &gt; E(Z) +  t ) \\leq \\exp  \\left ( - \\frac{ t^2}{ 2 ( Var(Z) + t/3 ) } \\right )\n\\] Large Deviations (Varadhan)"
  },
  {
    "objectID": "06-mcmc.html#special-distributions",
    "href": "06-mcmc.html#special-distributions",
    "title": "Bayes AI",
    "section": "Special Distributions",
    "text": "Special Distributions\nSee Common Distributions\n\nBernoulli and Binomial\nHypergeometric\nPoisson\nNegative Binomial\nNormal Distribution\nGamma Distribution\nBeta Distribution\nMultinomial Distribution\nBivariate Normal Distribution\nWishart Distribution\n\n\\(\\ldots\\)"
  },
  {
    "objectID": "06-mcmc.html#example-markov-dependence",
    "href": "06-mcmc.html#example-markov-dependence",
    "title": "Bayes AI",
    "section": "Example: Markov Dependence",
    "text": "Example: Markov Dependence\n\nWe can always factor a joint distribution as\n\n\\[\np( X_n , X_{n-1} , \\ldots , X_1 )  = p( X_n | X_{n-1} , \\ldots , X_1 ) \\ldots p( X_2 | X_1 ) p( X_1 )\n\\]\n\nA process has the Markov Property if\n\n\\[\np( X_n | X_{n-1} , \\ldots , X_1 ) = p( X_n | X_{n-1} )\n\\]\n\nOnly the current history matter when determining the probabilities."
  },
  {
    "objectID": "06-mcmc.html#a-real-world-probability-model-hidden-markov-models",
    "href": "06-mcmc.html#a-real-world-probability-model-hidden-markov-models",
    "title": "Bayes AI",
    "section": "A real world probability model: Hidden Markov Models",
    "text": "A real world probability model: Hidden Markov Models\nAre stock returns a random walk?\nHidden Markov Models (Baum-Welch, Viterbi)\n\nDaily returns on the SP500 stock market index.\n\nBuild a hidden Markov model to predict the ups and downs.\n\nSuppose that stock market returns on the next four days are \\(X_1 , \\ldots , X_4\\).\nLet’s empirical determine conditionals and marginals"
  },
  {
    "objectID": "06-mcmc.html#sp500-data",
    "href": "06-mcmc.html#sp500-data",
    "title": "Bayes AI",
    "section": "SP500 Data",
    "text": "SP500 Data\nMarginal and Bivariate Distributions\n\nEmpirically, what do we get? Daily returns from \\(1948-2007\\).\n\n\n\n\n\\(x\\)\nDown\nUp\n\n\n\n\n\\(P( X_i ) = x\\)\n0.474\n0.526\n\n\n\n\nFinding \\(p( X_2 | X_1 )\\) is twice as much computational effort: counting \\(UU,UD,DU,DD\\) transitions.\n\n\n\n\n\\(X_i\\)\nDown\nUp\n\n\n\n\n\\(X_{i-1} = Down\\)\n0.519\n0.481\n\n\n\\(X_{i-1} = Up\\)\n0.433\n0.567"
  },
  {
    "objectID": "06-mcmc.html#conditioned-on-two-days",
    "href": "06-mcmc.html#conditioned-on-two-days",
    "title": "Bayes AI",
    "section": "Conditioned on two days",
    "text": "Conditioned on two days\n\nLet’s do \\(p( X_3 | X_2 , X_1 )\\)\n\n\n\n\n\\(X_{i-2}\\)\n\\(X_{i-1}\\)\nDown\nUp\n\n\n\n\nDown\nDown\n0.501\n0.499\n\n\nDown\nUp\n0.412\n0.588\n\n\nUp\nDown\n0.539\n0.461\n\n\nUp\nUp\n0.449\n0.551\n\n\n\n\nWe could do the distribution \\(p( X_2 , X_3 | X_1 )\\). This is a joint, marginal and conditional distribution all at the same time.\n\nJoint because more than one variable \\(( X_2 , X_3 )\\), marginal because it ignores \\(X_4\\) and conditional because its given \\(X_1\\)."
  },
  {
    "objectID": "06-mcmc.html#joint-probabilities",
    "href": "06-mcmc.html#joint-probabilities",
    "title": "Bayes AI",
    "section": "Joint Probabilities",
    "text": "Joint Probabilities\n\nUnder Markov dependence \\[\n\\begin{aligned}\nP( UUD ) & = p( X_1 = U) p( X_2 = U | X_1 = U) p( X_3 | X_2 = U , X_1 = U ) \\\\\n& = ( 0.526 ) ( 0.567 ) ( 0.433)\n\\end{aligned}\n\\]\nUnder independence we would have got \\[\n\\begin{aligned}\nP(UUD) & = P( X_1 = U) p( X_2 = U) p( X_3 = D ) \\\\\n& = (.526)(.526)(.474) \\\\\n& = 0.131\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#markov-chain-monte-carlo",
    "href": "06-mcmc.html#markov-chain-monte-carlo",
    "title": "Bayes AI",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\n\nThink od shuffling problem\nThere are 52! possible permutations of this deck.\nNaive approach: generate a vector with all possible permutations and draw an element of this vector with probability 1/52!.\n\n\n\\(52! \\approx 10^{68}\\)\n\nslightly less then number of particles in the observed universe (\\(10^{80}\\))."
  },
  {
    "objectID": "06-mcmc.html#bayes-rule",
    "href": "06-mcmc.html#bayes-rule",
    "title": "Bayes AI",
    "section": "Bayes rule",
    "text": "Bayes rule\n\\[\np(\\theta \\mid X,Y) = \\dfrac{p(Y \\mid \\theta,X)p(\\theta)}{\\int p(Y \\mid \\theta,X)p(\\theta)d\\theta}.\n\\]\n\nMCMC algorithms generate a Markov chain \\[\n\\left\\{ \\theta ^{\\left( g\\right)}\\right\\} _{g=1}^{G}\n\\] whose stationary distribution is \\(p\\left( \\theta \\mid X,Y\\right)\\). Thus, the key to Bayesian inference is simulation rather than optimization.\n\n\\[\n\\begin{split}\n\\widehat{E}\\left( f\\left( \\theta\\right)  \\mid X,Y\\right)&=G^{-1}\\sum_{g=1}^{G}f\\left( \\theta ^{\\left( g\\right) }\\right)\\\\\n& \\approx \\int f\\left( \\theta\\right) p\\left( \\theta \\mid X,Y\\right)d\\theta=E\\left( f\\left( \\theta\\right)  \\mid X,Y\\right).\n\\end{split}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#mcmc-for-discrete-random-variables",
    "href": "06-mcmc.html#mcmc-for-discrete-random-variables",
    "title": "Bayes AI",
    "section": "MCMC for Discrete Random Variables",
    "text": "MCMC for Discrete Random Variables\n\\[\nP(X_{k+1} = i \\mid X_k = j) = p_{ij}.\n\\]\n\nTransition matrix \\(P = \\{p_{ij}\\}_{i,j=1}^n\\) is column stochastic, i.e. \\(p_{ij} \\ge 0\\) and \\(\\sum_j p_{ij} = 1\\).\nLet \\(\\pi_k \\in R^n\\) be the distribution of the random variable \\(X_k\\) at time \\(k\\), \\(\\pi_{ki} = P(X_k=i)\\)\n\\(\\pi_{k+1} = P\\pi_k\\).\nThe limiting distribution is \\(\\pi = P\\pi\\), i.e. \\(\\pi\\) is an eigenvector of \\(P\\) with eigenvalue 1.\nThe limiting distribution is unique and it is the first eigenvector of \\(P\\). The limiting distribution is also called a stationary distribution."
  },
  {
    "objectID": "06-mcmc.html#mcmc-for-discrete-random-variables-1",
    "href": "06-mcmc.html#mcmc-for-discrete-random-variables-1",
    "title": "Bayes AI",
    "section": "MCMC for Discrete Random Variables",
    "text": "MCMC for Discrete Random Variables\n\nPower method \\(\\pi^{k+1} = P^k\\pi^0\\) to \\(\\pi\\) (Google’s PageRank).\nPerron-Frobenius theorem: if \\(p_{ij}&gt;0\\) then \\(\\pi\\) always exists and it is unique and \\[\n\\Vert \\pi^k - \\pi\\Vert \\le c |\\lambda_2|^k,\n\\]\n\\(\\lambda_2\\) is the second largest eigenvalue of \\(P\\) (the first eigenvalue is always 1) - \\(\\pi\\) as the average time of visiting vertex \\(i\\) under random walk.\n\nThe mixing time of the Markov chain is given by \\[\nT=\\dfrac{1}{\\log(1/\\lambda_2)}\n\\] It is roughly, number of steps over which deviation from equilibrium distribution decreases by factor \\(e\\)."
  },
  {
    "objectID": "06-mcmc.html#three-state-example",
    "href": "06-mcmc.html#three-state-example",
    "title": "Bayes AI",
    "section": "Three-State Example",
    "text": "Three-State Example\n\n\nCode\ngraph LR\n    1 --0.5--&gt; 1\n    1 --0.25--&gt; 2\n    1 --0.25--&gt; 3\n    2 --0.2--&gt; 1\n    2 --0.1--&gt; 2\n    2 --0.7--&gt; 3\n    3 --0.25--&gt; 2\n    3 --0.25--&gt; 1\n    3 --0.5--&gt; 3\n\n\n\n\n\ngraph LR\n    1 --0.5--&gt; 1\n    1 --0.25--&gt; 2\n    1 --0.25--&gt; 3\n    2 --0.2--&gt; 1\n    2 --0.1--&gt; 2\n    2 --0.7--&gt; 3\n    3 --0.25--&gt; 2\n    3 --0.25--&gt; 1\n    3 --0.5--&gt; 3\n\n\n\n\n\n\n\n P &lt;- rbind(c(0.50, 0.2, 0.25),\n            c(0.25, 0.1, 0.25),\n            c(0.25, 0.7, 0.50))\n\nCheck that it is column-stochastic\n\n\nCode\n colSums(P)\n\n\n[1] 1 1 1"
  },
  {
    "objectID": "06-mcmc.html#power-iterations",
    "href": "06-mcmc.html#power-iterations",
    "title": "Bayes AI",
    "section": "Power Iterations",
    "text": "Power Iterations\n\n# Power iterations\niterate.P &lt;- function(x, P, n) {\n    res &lt;- matrix(NA, n+1, length(x))\n    res[1,] &lt;- x\n    for (i in 1:n)\n        res[i+1,] &lt;- x &lt;- P %*% x\n    res\n}\n\nLet’s start with a random vector \\(x = (1, 0, 0)\\) and iterate \\(x \\leftarrow P x\\) for \\(n=10\\) steps.\n\nn  = 10\ny = iterate.P(c(1, 0, 0), P, n)"
  },
  {
    "objectID": "06-mcmc.html#power-iterations-1",
    "href": "06-mcmc.html#power-iterations-1",
    "title": "Bayes AI",
    "section": "Power Iterations",
    "text": "Power Iterations\nWe can compare the results to the eigenvector calculated using built-in (QR decomposition-based) method\n\n\nCode\nv &lt;- eigen(P, FALSE)$vectors[,1]\nv &lt;- v/sum(v) # normalize eigenvector\nknitr::kable(cbind(v, y[n+1,]), col.names=c(\"Eigenvector\", \"Power iterations\"), digits=3)\n\n\n\n\n\nEigenvector\nPower iterations\n\n\n\n\n0.32\n0.32\n\n\n0.22\n0.22\n\n\n0.46\n0.46\n\n\n\nPower iterations convergence to eigenvector. Each color corresponds to a component of the eigenvector (stationary distribution vector)"
  },
  {
    "objectID": "06-mcmc.html#power-iterations-2",
    "href": "06-mcmc.html#power-iterations-2",
    "title": "Bayes AI",
    "section": "Power Iterations",
    "text": "Power Iterations\n\n\nCode\nmatplot(0:n, y, type=\"l\", lty=1, xlab=\"Step\", ylab=\"y\", las=1)\nabline(h=v, lty=2, col=1:3)"
  },
  {
    "objectID": "06-mcmc.html#power-iterations-3",
    "href": "06-mcmc.html#power-iterations-3",
    "title": "Bayes AI",
    "section": "Power Iterations",
    "text": "Power Iterations\nWe can also find the stationary distribution by doing a simple random walk on the graph.\n\nrun &lt;- function(i, P, n) {\n  res &lt;- integer(n)\n  for (t in seq_len(n))\n  res[[t]] &lt;- i &lt;- sample(ncol(P), 1, pr=P[,i])\n  res\n}\nsamples &lt;- run(1, P, 300)"
  },
  {
    "objectID": "06-mcmc.html#power-iterations-4",
    "href": "06-mcmc.html#power-iterations-4",
    "title": "Bayes AI",
    "section": "Power Iterations",
    "text": "Power Iterations\nNow, we plot the fraction of time that we were in each state over time:\n\n\nCode\ncummean &lt;- function(x) cumsum(x) / seq_along(x)\nplot(cummean(samples == 1), type=\"l\", ylim=c(0, 1), xlab=\"Step\", ylab=\"y\", las=1)\nlines(cummean(samples == 2), col=2)\nlines(cummean(samples == 3), col=3)\nabline(h=v, lty=2, col=1:3)"
  },
  {
    "objectID": "06-mcmc.html#english-alphabet",
    "href": "06-mcmc.html#english-alphabet",
    "title": "Bayes AI",
    "section": "English Alphabet",
    "text": "English Alphabet\n\n\nCode\nlibrary(jsonlite)\n# http://norvig.com/mayzner.html\nbg = read_json(\"data/bigrams.json\", simplifyVector=T)\nnbg = nrow(bg)\nbgm = matrix(0,nrow=26,ncol=26)\nrownames(bgm) = letters; colnames(bgm) = letters\nfor (i in 1:nbg) { # from j to i \n  idx = match(unlist(strsplit(bg[i,1], split=\"\")), letters)\n  bgm[idx[2],idx[1]] = as.numeric(bg[i,2])\n}\n# View(bgm)\nbgm = bgm %*% diag(1/colSums(bgm))\n\nimage(bgm, axes=FALSE)\naxis(3, at=seq(0,1, length=26), labels=letters)\naxis(2, at=seq(1,0, length=26), labels=letters)\n\n\n\nCode\nev = eigen(bgm)\nv = ev$vectors[,1]\nv &lt;- as.numeric(v/sum(v))"
  },
  {
    "objectID": "06-mcmc.html#english-alphabet-1",
    "href": "06-mcmc.html#english-alphabet-1",
    "title": "Bayes AI",
    "section": "English Alphabet",
    "text": "English Alphabet\n\nCode\nbarplot(v, names.arg = letters, ylim=c(0,0.13))\nlf = read.csv(\"data/letterfreq.txt\")\nlf$freq = lf$freq/sum(lf$freq)\nlf = lf[order(lf$letter),]\nbarplot(lf$freq, names.arg = letters, ylim=c(0,0.13))\n\nbarplot(bgm[,5], names.arg = letters)\n\n\n\n\n\n\n\nFrom Bigrams\n\n\n\n\n\n\n\nFrom Google\n\n\n\n\n\n\n\nTransitions from letter e\n\n\n\n\n\n\nIndividual Letter Frequencies"
  },
  {
    "objectID": "06-mcmc.html#how-to-construct-transition-probabilities",
    "href": "06-mcmc.html#how-to-construct-transition-probabilities",
    "title": "Bayes AI",
    "section": "How to Construct Transition Probabilities?",
    "text": "How to Construct Transition Probabilities?\n\nThe reverse problem, is how to construct the transition probabilities so that the limiting distribution is \\(\\pi\\).\nFor example, it is easy to see that any row-stochastic matrix which is symmetric has a uniform limiting distribution, i.e. \\(\\pi = e\\), \\(e_i = 1/n\\), where \\(n\\) is the number of vertices.\n\\(e\\) is an eigenvector of \\(P^T\\), since \\(P^T\\) is row-stochastic, thus sum of each row equals to 1, i.e. \\(P^T e = e\\).\n\nGeneral case balance condition: \\[\n\\pi_i p_{ij} = \\pi_j p_{ji}, ~ i,j=1,\\ldots,n,\n\\] from which follows that \\[\n\\sum_i p_{ij}\\pi_i = \\pi_j \\sum_i p_{ji} = \\pi_j,~j=1,\\ldots,n.\n\\]"
  },
  {
    "objectID": "06-mcmc.html#how-to-construct-transition-probabilities-1",
    "href": "06-mcmc.html#how-to-construct-transition-probabilities-1",
    "title": "Bayes AI",
    "section": "How to Construct Transition Probabilities?",
    "text": "How to Construct Transition Probabilities?\nThus we need to find a Markov chain in which the transition probabilities satisfy the detailed balance condition (symmetry). Another way to write the condition is \\[\n\\dfrac{p_{ij}}{p_{ji}} = \\dfrac{\\pi_j}{\\pi_i}.\n\\] Say we have some “teaser” transition probabilities \\(p_{ij}^0\\) and we assume those are symmetric. We want to find new probabilities \\(p_{ij} = p_{ij}^0b_{ij}, ~ i\\ne j\\) and \\(p_{ii} = 1 - \\sum_{j:~j\\ne i} p_{ij}\\). We need to choose \\(b_{ij}\\) is such a way so that \\[\n\\dfrac{p_{ij}}{p_{ji}} = \\dfrac{p_{ij}^0b_{ij}}{p_{ji}^0b_{ji}} = \\dfrac{b_{ij}}{b_{ji}} = \\dfrac{\\pi_j}{\\pi_i}\n\\] We can choose \\[\nb_{ij} = F\\left(\\dfrac{\\pi_j}{\\pi_i}\\right)\n\\]\nFor the detailed balance condition to be satisfied, we need to choose \\(F: R_+ \\rightarrow [0,1]\\) such that \\[\n\\dfrac{F(z)}{F(1/z)} = z.\n\\]"
  },
  {
    "objectID": "06-mcmc.html#how-to-construct-transition-probabilities-2",
    "href": "06-mcmc.html#how-to-construct-transition-probabilities-2",
    "title": "Bayes AI",
    "section": "How to Construct Transition Probabilities?",
    "text": "How to Construct Transition Probabilities?\nAn example of such a function is \\(F(z) = \\min(z,1)\\). This leads to the Metropolis algorithm\n\nWhen at state \\(i\\), draw from \\(p^{0}_{ij}\\) to generate next state \\(j\\)\nCalculate \\(a_{ij} = \\min(1,\\pi_i/\\pi_j)\\) (acceptance probability)\nMove to \\(j\\) with probability \\(a_{ij}\\), stay at \\(i\\) otherwise\n\nWhen move happens, we say step is accepted, otherwise it is rejected. In a more general case (Metropolis-Hastings), when \\(p^0\\) is not symmetric, we calculate \\[\na_{ij} = \\min\\left(1,\\dfrac{\\pi_i p^0_{ji}}{\\pi_j p^0_{ji}}\\right).\n\\]"
  },
  {
    "objectID": "06-mcmc.html#continious-case",
    "href": "06-mcmc.html#continious-case",
    "title": "Bayes AI",
    "section": "Continious Case",
    "text": "Continious Case\nIn the continuous case we use variables \\(S\\) and \\(T\\) instead induces \\(i\\) and \\(j\\). The detailed balance condition becomes \\[\n\\pi(S)p(T \\mid S) = \\pi(T)p(S \\mid T).\n\\] Then \\[\n\\int p(T \\mid S)\\pi(S)dS = \\int p(S \\mid T)\\pi(T)dS = \\pi(T)\\int p(S\\mid T)dS = \\pi(T).\n\\]\nThe following conditions \\[\n\\forall S, \\forall T:\\pi(T)\\ne 0:p(T \\mid S) &gt;0\n\\] are sufficient to guarantee uniqueness of \\(\\pi(T)\\)."
  },
  {
    "objectID": "06-mcmc.html#metropolis-hastings-algorithm",
    "href": "06-mcmc.html#metropolis-hastings-algorithm",
    "title": "Bayes AI",
    "section": "Metropolis-Hastings Algorithm",
    "text": "Metropolis-Hastings Algorithm\nSpecifically, the MH algorithm repeats the following two steps \\(G\\) times: given \\(\\theta ^{\\left( g\\right) }\\) \\[\n\\begin{aligned}   \n&\\text{Step 1. Draw }\\theta^{\\prime} \\text{ from a proposal distribution,} p(\\theta^{\\prime}|\\theta ^{(g)}) \\\\  \n&\\text{Step 2. Accept } \\theta^{\\prime} \\text{ with probability } \\alpha \\left(\\theta ^{(g)},\\theta^{\\prime}\\right) \\text{,}\n\\end{aligned}\n\\] where \\[\n\\alpha \\left( \\theta ^{(g)},\\theta^{\\prime}\\right) =\\min \\left(     \\frac{\\pi(\\theta^{\\prime})}{\\pi (\\theta ^{(g)})}\\frac{q(\\theta^{(g)}|\\theta^{\\prime})}{q(\\theta^{\\prime}|\\theta ^{(g)})},1\\right) \\text{.}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#metropolis-hastings-algorithm-1",
    "href": "06-mcmc.html#metropolis-hastings-algorithm-1",
    "title": "Bayes AI",
    "section": "Metropolis-Hastings Algorithm",
    "text": "Metropolis-Hastings Algorithm\nImplementation of the accept-reject step:\n\nDraw a uniform random variable, \\(U\\sim U \\left[ 0,1\\right]\\), and set \\(\\theta ^{\\left( g+1\\right) }=\\theta^{\\prime}\\)\nif \\(U&lt;\\alpha \\left( \\theta ^{(g)},\\theta^{\\prime}\\right)\\), leaving \\(\\theta ^{\\left( g\\right) }\\) unchanged (\\(\\theta^{\\left( g+1\\right) }=\\theta^{(g)}\\)).\nIt is important to note that the denominator in the acceptance probability cannot be zero, provided the algorithm is started from a \\(\\pi\\) - positive point since \\(q\\) is always positive. The MH algorithmonly requires that \\(\\pi\\) can be evaluated up to proportionality.\nThe output of the algorithm, \\(\\left\\{ \\theta ^{\\left( g\\right) }\\right\\}_{g=1}^{\\infty }\\), is clearly a Markov chain. The key theoretical property is that the Markov chain, under mild regularity, has \\(\\pi \\left( \\theta\\right)\\) as its limiting distribution. We discuss two important specialcases that depend on the choice of \\(q\\)."
  },
  {
    "objectID": "06-mcmc.html#independence-mh",
    "href": "06-mcmc.html#independence-mh",
    "title": "Bayes AI",
    "section": "Independence MH",
    "text": "Independence MH\nOne special case draws a candidate independently of the previous state, \\(q(\\theta^{\\prime}|\\theta ^{(g)})=q(\\theta^{\\prime})\\). In this independence MH algorithm, the acceptance criterion simplifies to \\[\n\\alpha \\left( \\theta ^{(g)},\\theta^{\\prime}\\right) =\\min \\left( \\frac{\\pi        (\\theta^{\\prime})}{\\pi (\\theta ^{(g)})}\\frac{q(\\theta ^{(g)})}{q(\\theta       ^{\\prime})},1\\right).\n\\] Even though \\(\\theta\\) is drawn independently of the previous state, the sequence generated is not being independent, since \\(\\alpha\\) depends on previous draws. The criterion implies a new draw is always accepted if target density ratio \\(\\pi (\\theta^{\\prime})/\\pi (\\theta ^{(g)})\\), increases more than the proposal ratio, \\(q(\\theta ^{(g)})/q(\\theta^{\\prime})\\). When this is not satisfied, a balanced coin is flipped to decide whether or not toaccept the proposal."
  },
  {
    "objectID": "06-mcmc.html#random-walk-metropolis",
    "href": "06-mcmc.html#random-walk-metropolis",
    "title": "Bayes AI",
    "section": "Random-walk Metropolis",
    "text": "Random-walk Metropolis\nRandom-walk (RW) Metropolis is the polar opposite of the independence MH algorithm. It draws a candidate from the following RW model, \\[\n\\theta^{\\prime}=\\theta ^{\\left( g\\right) }+\\sigma \\varepsilon _{g+1},\n\\] where \\(\\varepsilon _{t}\\) is an independent, mean zero, and symmetric error term, typically taken to be a normal or \\(t-\\)distribution, and \\(\\sigma\\) is a scaling factor. The algorithm must be tuned via the choice of \\(\\sigma\\), the scaling factor. Symmetry implies that \\[\nq\\left( \\theta^{\\prime}|\\theta ^{\\left( g\\right) }\\right) =q\\left( \\theta    ^{\\left( g\\right) }|\\theta^{\\prime}\\right),\n\\] and \\[\n\\alpha \\left( \\theta ^{(g)},\\theta^{\\prime}\\right) =\\min \\left( \\pi    (\\theta^{\\prime})/\\pi (\\theta ^{(g)}),1\\right).\n\\]"
  },
  {
    "objectID": "06-mcmc.html#implementation",
    "href": "06-mcmc.html#implementation",
    "title": "Bayes AI",
    "section": "Implementation",
    "text": "Implementation\n\nset.seed(7)\nmh = function(target, proposal, n, x0) {\n  x = x0; p = length(x0)\n  samples = matrix(NA, nrow=n, ncol=p)\n  accept = rep(0, n)\n  for (i in 1:n) {\n    x_new = proposal(x)\n    a = min(1,target(x_new) / target(x))\n    # print(c(x, x_new, a))\n    if (runif(1) &lt; a) {accept[i]=1; x = x_new}\n    samples[i,] = x\n    # print(accept[i])\n  }\n  list(samples = samples, accept = accept)\n}"
  },
  {
    "objectID": "06-mcmc.html#apply",
    "href": "06-mcmc.html#apply",
    "title": "Bayes AI",
    "section": "Apply",
    "text": "Apply\nWe apply MCMC to the weighted sum of two normal distributions. This sort of distribution is fairly straightforward to sample from, but let’s draw samples with MCMC.\n\n\nCode\np &lt;- 0.4\nmu &lt;- c(-1, 2)\nsd &lt;- c(.5, 2)\ntarget &lt;- function(x)\n    p     * dnorm(x, mu[1], sd[1]) +\n    (1-p) * dnorm(x, mu[2], sd[2])\ncurve(target(x), col=\"red\", -4, 8, n=301, las=1)"
  },
  {
    "objectID": "06-mcmc.html#apply-1",
    "href": "06-mcmc.html#apply-1",
    "title": "Bayes AI",
    "section": "Apply",
    "text": "Apply\n\n\nCode\nset.seed(7)\nproposal = function(x) x + rnorm(1, 0, 0.25)\nr = mh(target, proposal, 2000, x0=0)\nhist(r$samples[r$accept==1,], freq=FALSE, breaks=35, col=\"lightblue\", main=\"Metropolis-Hastings\", xlab=\"x\")\ncurve(target(x), add=TRUE, col=\"red\", lwd=2)"
  },
  {
    "objectID": "06-mcmc.html#bivariate",
    "href": "06-mcmc.html#bivariate",
    "title": "Bayes AI",
    "section": "Bivariate",
    "text": "Bivariate\n\n\nCode\nset.seed(92)\nlibrary(ggplot2)\n# Metropolis-Hastings for Bivariate Normal distribution\n# Target distribution\nmu = c(0, 0)\nsigma = matrix(c(1, 0.5, 0.5, 1), 2, 2)\ntarget = function(x) exp(-0.5*t(x-mu) %*% solve(sigma) %*% (x-mu))\nlibrary(mixtools)\nellipse(mu, sigma, npoints = 1000, newplot = TRUE)\nproposal = function(x) x + rnorm(2, 0, .5)\nn = 5000 # Number of samples\nx = c(0, 0)\nsamples = matrix(0, n, 2)\nfor (i in 1:n) {\n  x_new = proposal(x)\n  a = target(x_new) / target(x)\n  if (i&lt;50) {\n  if (runif(1) &gt; a) {arrows(x[1], x[2], x_new[1], x_new[2], col= 'red')} else  {arrows(x[1], x[2], x_new[1], x_new[2], col= 'green'); x = x_new}\n  }\n  if (runif(1) &lt; a) x = x_new\n\n  samples[i,] = x\n}\n\n\n\nPaths of random samples generated by Metropolis algorithm. Red are the rejected steps and green are accepted ones"
  },
  {
    "objectID": "06-mcmc.html#bivariate-1",
    "href": "06-mcmc.html#bivariate-1",
    "title": "Bayes AI",
    "section": "Bivariate",
    "text": "Bivariate\n\n\nCode\nggplot(mapping = aes(x=samples[,1], y=samples[,2])) + geom_point()+ geom_density_2d(size=1)"
  },
  {
    "objectID": "06-mcmc.html#gibbs-samples-clifford-hammersley",
    "href": "06-mcmc.html#gibbs-samples-clifford-hammersley",
    "title": "Bayes AI",
    "section": "Gibbs Samples: Clifford-Hammersley",
    "text": "Gibbs Samples: Clifford-Hammersley\n\nClifford-Hammersley (CH) theorem: high-dimensional joint distribution, \\(p\\left( \\theta\\mid X,Y\\right)\\), is completely characterized by a larger number of lower dimensional conditional distributions. Given this characterization, MCMC methods iteratively sample from these conditional distributions using standard sampling methods.\nExample: bivariate posterior distribution \\(p\\left( \\theta_1,\\theta_{2} \\mid X,Y \\right)\\), e.g. \\(\\theta_1\\) as traditional static parameters and \\(\\theta_2\\) as latent variables.\n\nCH: \\(p\\left( \\theta_1,\\theta_2\\right)\\) is determined by \\(p\\left( \\theta_{1} \\mid \\theta_{2}\\right)\\) and \\(p\\left( \\theta_{2} \\mid \\theta_{1}\\right)\\), given \\(p\\left( \\theta_{1},\\theta_2\\right)\\), \\(p\\left( \\theta_{1}\\right)\\) and \\(p\\left( \\theta_{2}\\right)\\) have positive mass for all points"
  },
  {
    "objectID": "06-mcmc.html#besag-formula",
    "href": "06-mcmc.html#besag-formula",
    "title": "Bayes AI",
    "section": "Besag formula",
    "text": "Besag formula\nFor any pairs \\((\\theta_{1},\\theta_{2})\\) and \\((\\theta_1^{\\prime},\\theta_2^{\\prime})\\), \\[\n\\frac{p(\\theta_1,\\theta_2)}{p(\\theta_1^{\\prime},\\theta_2^{\\prime})}=\\frac{p(\\theta_1 \\mid \\theta_2^{\\prime})p(\\theta_2 \\mid \\theta_1)}{p\\left( \\theta_1^{\\prime} \\mid \\theta_2^{\\prime}\\right) p\\left( \\theta_2^{\\prime} \\mid \\theta_2\\right) }.\n\\] The proof uses the fact that \\(p\\left( \\theta_1,\\theta_2\\right)=p\\left( \\theta_2 \\mid \\theta_1\\right) p\\left( \\theta_1\\right)\\) (applied to both \\((\\theta_1,\\theta_2)\\) and \\((\\theta_1^{\\prime},\\theta_2^{\\prime})\\)) and the Bayes rule: \\[\np\\left( \\theta_1\\right) =\\frac{p\\left( \\theta_1 \\mid \\theta_2^{\\prime            }\\right) p\\left( \\theta_2^{\\prime}\\right) }{p\\left( \\theta_2^{\\prime} \\mid \\theta_1\\right) }\\text{.}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#clifford-hammersley-the-general-version",
    "href": "06-mcmc.html#clifford-hammersley-the-general-version",
    "title": "Bayes AI",
    "section": "Clifford-Hammersley: The general version",
    "text": "Clifford-Hammersley: The general version\n\nPartitioning a vector as \\(\\theta =\\left( \\theta_{1},\\theta_{2},\\theta_{3},\\ldots ,\\theta_{K}\\right)\\), then the general CH theorem states that \\[\np\\left( \\theta _{i} \\mid \\theta _{-i}\\right) := p\\left( \\theta    _{i} \\mid \\theta_1,\\theta_1,\\ldots,\\theta_{i-1},\\theta _{i+1},...,\\theta    _{K}\\right) ,\n\\] for \\(i=1,...,K\\), completely characterize \\(p\\left( \\theta_1,\\ldots,\\theta_{K}\\right)\\)."
  },
  {
    "objectID": "06-mcmc.html#latent-variable-models",
    "href": "06-mcmc.html#latent-variable-models",
    "title": "Bayes AI",
    "section": "Latent Variable Models",
    "text": "Latent Variable Models\n\nFixed parameters \\(\\theta\\), and latent variables, \\(x\\). In this case, then CH \\[\np\\left( \\theta,x \\mid y\\right)\n\\] is completely characterized by \\(p\\left( \\theta  \\mid x,y\\right)\\) and \\(p\\left( x \\mid \\theta ,y\\right)\\).\nThe distribution \\(p\\left( \\theta \\mid x,y\\right)\\) is the posterior distribution of the parameters, conditional on the observed data and the latent variables. Similarly, \\(p\\left( x \\mid \\theta,y\\right)\\) is the smoothing distribution of the latent variables."
  },
  {
    "objectID": "06-mcmc.html#gibbs-sampler",
    "href": "06-mcmc.html#gibbs-sampler",
    "title": "Bayes AI",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\n\nThe Gibbs sampler simulates multi-dimensional posterior distributions by iteratively sampling from the lower-dimensional conditional posteriors.\nUnlike the previous MH algorithms, the Gibbs sampler updates the chain one component at a time, instead of updating the entire vector.\nThis requires either that the conditional posteriors distributions is discrete, or a recognizable distribution (e.g. Normal) for which standard sampling algorithms apply, or that resampling methods, such as accept-reject, can be used."
  },
  {
    "objectID": "06-mcmc.html#gibbs-sampler-1",
    "href": "06-mcmc.html#gibbs-sampler-1",
    "title": "Bayes AI",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\nIn the case of \\(p\\left( \\theta_1,\\theta_2\\right)\\), given current draws,\\(\\left( \\theta_1^{\\left( g\\right) },\\theta_2^{\\left( g\\right)}\\right)\\), the Gibbs sampler consists of \\[\n\\begin{aligned}\n\\text{1. Draw }\\theta_1^{\\left( g+1\\right) } &\\sim &p\\left( \\theta   _1|\\theta_2^{\\left( g\\right) }\\right) \\\\    \n\\text{2. Draw }\\theta_2^{\\left( g+1\\right) } &\\sim &p\\left( \\theta_2|\\theta_1^{\\left( g+1\\right) }\\right) ,\n\\end{aligned}\n\\] repeating \\(G\\) times."
  },
  {
    "objectID": "06-mcmc.html#gibbs-sampler-2",
    "href": "06-mcmc.html#gibbs-sampler-2",
    "title": "Bayes AI",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\n\nTransition kernel from state \\(\\theta\\) to state \\(\\theta ^{\\prime}\\) is \\[\np\\left( \\theta ,\\theta^{\\prime}\\right) =p\\left( \\theta_{1}^{\\prime}|\\theta_2\\right) p\\left( \\theta_2^{\\prime}|\\theta_{1}^{\\prime}\\right)\n\\]\nThe limiting (statiobnary) distribution \\(\\pi\\), satisfies \\[\n\\pi \\left( \\theta^{\\prime}\\right) =\\int p\\left( \\theta ,\\theta^{\\prime}\\right)  d\\theta.\n\\]"
  },
  {
    "objectID": "06-mcmc.html#gibbs-sampler-3",
    "href": "06-mcmc.html#gibbs-sampler-3",
    "title": "Bayes AI",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\nIt is easy to verify that the stationary distribution of the Markov chain generated by the Gibbs sampler is the posterior distribution, \\(\\pi \\left(\\theta \\right) =p\\left( \\theta_1,\\theta_2\\right)\\): \\[\n\\begin{aligned}    \n\\int p\\left( \\theta ,\\theta^{\\prime}\\right) d\\theta &=&p\\left( \\theta_2^{\\prime}|\\theta_1^{\\prime    }\\right) \\int_{\\theta_2}\\int_{\\theta_1}p\\left( \\theta_1^{\\prime}|\\theta_2\\right) p\\left( \\theta_1,\\theta_2\\right) d\\theta_{1}d\\theta_2 \\\\\n&=&p\\left( \\theta_2^{\\prime}|\\theta_1^{\\prime}\\right) \\int_{\\theta_2}p\\left( \\theta_1^{\\prime}|\\theta_2\\right)  p\\left( \\theta_{2}\\right)  d\\theta_2 \\\\&=&p\\left( \\theta_2^{\\prime}|\\theta_1^{\\prime}\\right) p\\left( \\theta_{1}^{\\prime}\\right) =p\\left( \\theta_1^{\\prime},\\theta_2^{\\prime}\\right) =\\pi \\left( \\theta^{\\prime}\\right) \\text{.}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#gibbs-sampler-4",
    "href": "06-mcmc.html#gibbs-sampler-4",
    "title": "Bayes AI",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\nThe ergodic theorem holds: for a sufficiently integrable function \\(l\\) and for all starting points \\(\\theta ,\\) \\[\n\\underset{G\\rightarrow \\infty }{\\lim }\\frac{1}{G}\\sum_{g=1}^{G}f\\left(\\theta ^{\\left( g\\right) }\\right) =\\int f\\left( \\theta \\right) \\pi \\left(\\theta \\right) d\\theta =E\\left[ f\\left( \\theta \\right) \\right]\n\\] - Run for an initial length, often called the burn-in - Then a secondary sample of size \\(G\\) is created for Monte Carlo inference"
  },
  {
    "objectID": "06-mcmc.html#gibbs-sample-for-normal-gamma",
    "href": "06-mcmc.html#gibbs-sample-for-normal-gamma",
    "title": "Bayes AI",
    "section": "Gibbs sample for Normal-Gamma",
    "text": "Gibbs sample for Normal-Gamma\n\n\nCode\n# summary statistics of sample\nn &lt;- 30\nybar &lt;- 15\ns2 &lt;- 3\n\n# sample from the joint posterior (mu, tau | data)\nmu &lt;- tau &lt;- rep(NA, 11000)\nT &lt;- 1000    # burnin\ntau[1] &lt;- 1  # initialisation\nfor(i in 2:11000)\n{   \n    mu[i] &lt;- rnorm(n = 1, mean = ybar, sd = sqrt(1 / (n * tau[i - 1])))    \n    tau[i] &lt;- rgamma(n = 1, shape = n / 2, scale = 2 / ((n - 1) * s2 + n * (mu[i] - ybar)^2))\n}\nmu &lt;- mu[-(1:T)]   # remove burnin\ntau &lt;- tau[-(1:T)] # remove burnin\nhist(mu)\n\n\n\n\n\n\n\n\n\nCode\nhist(tau)"
  },
  {
    "objectID": "06-mcmc.html#hybrid-chains",
    "href": "06-mcmc.html#hybrid-chains",
    "title": "Bayes AI",
    "section": "Hybrid chains",
    "text": "Hybrid chains\n\nGiven a partition of the vector \\(\\theta\\) via CH, a hybrid MCMC algorithm updates the chain one subset at a time, either by direct draws (Gibbs steps) or via MH. - Thus, a hybrid algorithm combines the features of the MH algorithm and the Gibbs sampler, providing significant flexibility in designing MCMC algorithms for different models.\n\\(p\\left( \\theta_2|\\theta_1\\right)\\) is recognizable and can be directly sampled.\n\\(p\\left( \\theta_1|\\theta_2\\right)\\) can only be evaluated and not directly sampled.\nMH accept/reject based on \\[\n\\alpha \\left( \\theta_1^{(g)},\\theta_1^{\\prime}\\right) =\\min \\left(     \\frac{p\\left( \\theta_1^{\\prime}|\\theta_2^{\\left( g\\right) }\\right) }{p\\left( \\theta_1^{\\left ( g \\right ) }|\\theta_2^{\\left( g\\right) }\\right) }\\frac{q\\left( \\theta_1^{\\left( g\\right)        }|\\theta_1^{\\prime},\\theta_2^{\\left( g\\right) }\\right) }{q\\left( \\theta_1^{\\prime}|\\theta_1^{\\left( g\\right) },\\theta    _2^{\\left( g\\right) }\\right) },1\\right) .\n\\]"
  },
  {
    "objectID": "06-mcmc.html#hybrid-chains-1",
    "href": "06-mcmc.html#hybrid-chains-1",
    "title": "Bayes AI",
    "section": "Hybrid chains",
    "text": "Hybrid chains\nThe general hybrid algorithm is as follows. Given \\(\\theta_1^{\\left(g\\right) }\\) and \\(\\theta_2^{\\left( g\\right) }\\), for \\(g=1,\\ldots, G\\), \\[\n\\begin{aligned}    \n1.\\text{ Draw }\\theta_1^{\\left( g+1\\right) } &\\sim &MH\\left[ q\\left(    \\theta_1|\\theta_1^{\\left( g\\right) },\\theta_2^{\\left( g\\right)    }\\right) \\right] \\\\    \n2.\\text{ Draw }\\theta_2^{\\left( g+1\\right) } &\\sim &p\\left( \\theta_2|\\theta_1^{\\left( g+1\\right) }\\right) .\n\\end{aligned}\n\\] In higher dimensional cases, a hybrid algorithm consists of any combination of Gibbs and Metropolis steps. Hybrid algorithms significantly increase the applicability of MCMC methods, as the only requirement is that the model generates posterior conditionals that can either be sampled or evaluated."
  },
  {
    "objectID": "06-mcmc.html#hamiltonian-monte-carlo",
    "href": "06-mcmc.html#hamiltonian-monte-carlo",
    "title": "Bayes AI",
    "section": "Hamiltonian Monte-Carlo",
    "text": "Hamiltonian Monte-Carlo\n\nAdd a momentum to the Metropolis Sampling.\nGradient of the un-normalized posterior to better navigate the surface defined by the posterior.\nMomentum variable \\(r\\) for each model variable \\(\\theta\\). \\[\np(\\theta,r) \\propto \\exp\\left(L(\\theta) - 0.5 r^Tr\\right),\n\\]\n\\(L\\) is the logarithm of the joint density of the variables of interest (negative potential energy)\n\\(0.5 r^Tr\\) is the kinetic energy of the particle\n\\(p(\\theta,r)\\) is the total negative energy"
  },
  {
    "objectID": "06-mcmc.html#hamiltonian-monte-carlo-1",
    "href": "06-mcmc.html#hamiltonian-monte-carlo-1",
    "title": "Bayes AI",
    "section": "Hamiltonian Monte-Carlo",
    "text": "Hamiltonian Monte-Carlo\nWe can simulate the evolution over time of the Hamiltonian dynamics of this system via the “leapfrog” integrator, which proceeds according to the updates \\[\n\\begin{aligned}\nr^{+/2} & = r + (\\epsilon/2) \\nabla_{\\theta}L(\\theta) \\\\\n\\theta^{+} & = \\theta + (\\epsilon/2) r^{+/2}\\\\\nr^{+} & = r^{+/2} + (\\epsilon/2)\\nabla_{\\theta}L(\\theta^{+}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#hierarchical-bayesian-model",
    "href": "06-mcmc.html#hierarchical-bayesian-model",
    "title": "Bayes AI",
    "section": "Hierarchical Bayesian Model",
    "text": "Hierarchical Bayesian Model\nWe consider the following model: \\[\\begin{align*}\n    \\tau &\\sim \\mathrm{Gamma}(0.5, 0.5) \\\\\n    \\lambda_d &\\sim \\mathrm{Gamma}(0.5, 0.5) \\\\\n    \\beta_d &\\sim \\mathcal{N}(0, 20) \\\\\n    y_n &\\sim \\mathrm{Bernoulli}(\\sigma((\\tau \\lambda \\odot \\beta)^T x_n))),\n\\end{align*}\\]\n\n\\(\\tau\\) is a scalar global coefficient scale\n\\(\\lambda\\) is a vector of local scales\n\\(\\beta\\) is the vector of unscaled coefficients,\n\n\n\nCode\nimport tensorflow_probability.substrates.jax as tfp\nimport numpy as np\nimport jax.numpy as jnp\ntfd = tfp.distributions\nimport jax\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "06-mcmc.html#apply-to-iris-data",
    "href": "06-mcmc.html#apply-to-iris-data",
    "title": "Bayes AI",
    "section": "Apply to Iris Data",
    "text": "Apply to Iris Data\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np  \niris = pd.read_csv(\"data/iris.csv\")\nprint(iris.shape)\n\n\n(150, 5)\n\n\nCode\nprint(iris.head())\n\n\n   sepal.length  sepal.width  petal.length  petal.width variety\n0           5.1          3.5           1.4          0.2  Setosa\n1           4.9          3.0           1.4          0.2  Setosa\n2           4.7          3.2           1.3          0.2  Setosa\n3           4.6          3.1           1.5          0.2  Setosa\n4           5.0          3.6           1.4          0.2  Setosa\n\n\nCode\ny = iris['variety']==\"Setosa\"\nx = iris[\"sepal.length\"].values\nplt.scatter(x,y)\nx = np.c_[np.ones(150),x]\nprint(x.shape)\n\n\n(150, 2)"
  },
  {
    "objectID": "06-mcmc.html#log-density-function",
    "href": "06-mcmc.html#log-density-function",
    "title": "Bayes AI",
    "section": "Log Density Function",
    "text": "Log Density Function\nFor Hamiltonian MC, we only need to evaluate the joint log-density pointwise\n\n\nCode\ndef joint_log_prob(x, y, tau, lamb, beta):\n    lp = tfd.Gamma(0.5, 0.5).log_prob(tau)\n    lp += tfd.Gamma(0.5, 0.5).log_prob(lamb).sum() \n    lp += tfd.Normal(0., 20).log_prob(beta).sum() \n    logits = x @ (tau * lamb * beta)\n    lp += tfd.Bernoulli(logits).log_prob(y).sum() \n    return lp   \n\ntau = np.random.rand(1)[0]\nlamb = np.random.rand(2)\nbeta = np.random.rand(2)\njoint_log_prob(x, y, tau, lamb, beta)\n\n\nArray(-134.03787, dtype=float32)"
  },
  {
    "objectID": "06-mcmc.html#change-of-variables",
    "href": "06-mcmc.html#change-of-variables",
    "title": "Bayes AI",
    "section": "Change of Variables",
    "text": "Change of Variables\n\\[\nz\\triangleq T^{-1}(\\theta),\\qquad \\pi(z) = \\pi(\\theta) \\left| \\frac{\\partial T}{\\partial z} (z) \\right|,\n\\]\nTaking the logarithm of both sides, we get \\[\n\\log \\pi(z) = \\log \\pi(\\theta) + \\log \\left| \\frac{\\partial T }{\\partial z}(z) \\right|\n\\] Use \\(T(z)=e^z\\), and \\(\\log|\\frac{\\partial T}{\\partial z}(z)| = z\\)"
  },
  {
    "objectID": "06-mcmc.html#change-of-variables-in-code",
    "href": "06-mcmc.html#change-of-variables-in-code",
    "title": "Bayes AI",
    "section": "Change of Variables in Code",
    "text": "Change of Variables in Code\n\ndef unconstrained_joint_log_prob(x, y, theta):\n    ndims = x.shape[-1]\n    unc_tau, unc_lamb, beta = jnp.split(theta, [1, 1 + ndims])\n    unc_tau = unc_tau.reshape([]) \n    # Make unc_tau a scalar \n    tau = jnp.exp(unc_tau)\n    ldj = unc_tau\n    lamb = jnp.exp(unc_lamb)\n    ldj += unc_lamb.sum()\n    return joint_log_prob(x, y, tau, lamb, beta) + ldj"
  },
  {
    "objectID": "06-mcmc.html#lets-check-out-function",
    "href": "06-mcmc.html#lets-check-out-function",
    "title": "Bayes AI",
    "section": "Let’s check out function",
    "text": "Let’s check out function\n\nfrom functools import partial\ntarget_log_prob = partial(unconstrained_joint_log_prob, x, y)\ntheta = np.r_[tau,lamb,beta]\ntarget_log_prob(theta)\n\nArray(-904.6409, dtype=float32)"
  },
  {
    "objectID": "06-mcmc.html#automatic-differentiation",
    "href": "06-mcmc.html#automatic-differentiation",
    "title": "Bayes AI",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\n\n\nCode\ntarget_log_prob_and_grad = jax.value_and_grad(target_log_prob)\ntlp, tlp_grad = target_log_prob_and_grad(theta)\nprint(tlp)\n\n\n-904.6409\n\n\nCode\ntlp_grad\n\n\nArray([ -892.1147 ,  -100.52104,  -792.00354,  -233.65552, -1235.4655 ],      dtype=float32)"
  },
  {
    "objectID": "06-mcmc.html#hamiltonian-monte-carlo-2",
    "href": "06-mcmc.html#hamiltonian-monte-carlo-2",
    "title": "Bayes AI",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\n\n\nCode\ndef leapfrog_step(target_log_prob_and_grad, step_size, i, leapfrog_state):\n    z, m, tlp, tlp_grad = leapfrog_state\n    m += 0.5 * step_size * tlp_grad\n    z += step_size * m\n    tlp, tlp_grad = target_log_prob_and_grad(z)\n    m += 0.5 * step_size * tlp_grad\n    return z, m, tlp, tlp_grad\n\ndef hmc_step(target_log_prob_and_grad, num_leapfrog_steps, step_size, z, seed):\n    m_seed, mh_seed = jax.random.split(seed)\n    tlp, tlp_grad = target_log_prob_and_grad(z)\n    m = jax.random.normal(m_seed, z.shape)\n    energy = 0.5 * jnp.square(m).sum() - tlp\n    new_z, new_m, new_tlp, _ = jax.lax.fori_loop(\n        0,\n        num_leapfrog_steps,\n        partial(leapfrog_step, target_log_prob_and_grad, step_size),\n        (z, m, tlp, tlp_grad)) \n    new_energy = 0.5 * jnp.square(new_m).sum() - new_tlp\n    log_accept_ratio = energy - new_energy\n    is_accepted = jnp.log(jax.random.uniform(mh_seed, [])) &lt; log_accept_ratio\n    # select the proposed state if accepted\n    z = jnp.where(is_accepted, new_z, z)\n    hmc_output = {\"z\": z,\n                  \"is_accepted\": is_accepted,\n                  \"log_accept_ratio\": log_accept_ratio}\n    # hmc_output[\"z\"] has shape [num_dimensions]\n    return z, hmc_output\n\ndef hmc(target_log_prob_and_grad, num_leapfrog_steps, step_size, num_steps, z,\n        seed):\n    # create a seed for each step\n    seeds = jax.random.split(seed, num_steps)\n    # this will repeatedly run hmc_step and accumulate the outputs\n    _, hmc_output = jax.lax.scan(\n        partial(hmc_step, target_log_prob_and_grad, num_leapfrog_steps, step_size),\n        z, seeds)\n    # hmc_output[\"z\"] now has shape [num_steps, num_dimensions]\n    return hmc_output\n\ndef scan(f, state, xs):\n  output = []\n  for x in xs:\n    state, y = f(state, x)\n    output.append(y)\n  return state, jnp.stack(output)"
  },
  {
    "objectID": "06-mcmc.html#hmc",
    "href": "06-mcmc.html#hmc",
    "title": "Bayes AI",
    "section": "HMC",
    "text": "HMC\n\nnum_leapfrog_steps=30\nstep_size = 0.008\nfrom jax import random\nseed = random.PRNGKey(92)\nnum_samples=10000\nhmc_output = hmc(target_log_prob_and_grad, num_leapfrog_steps, step_size,\n    num_samples, theta, seed)"
  },
  {
    "objectID": "06-mcmc.html#inspect-the-results",
    "href": "06-mcmc.html#inspect-the-results",
    "title": "Bayes AI",
    "section": "Inspect the results",
    "text": "Inspect the results\n\n\nCode\nndims = x.shape[-1]\nthetap = hmc_output['z']\ntaup, lambp, betap = jnp.split(thetap, [1, 1 + ndims],axis=1)\nskip = 2000\nslope = betap[skip:,1]\nintercept = betap[skip:,0]\nplt.hist(slope,bins=50);\nplt.hist(intercept,bins=50);"
  },
  {
    "objectID": "06-mcmc.html#inspect-the-results-1",
    "href": "06-mcmc.html#inspect-the-results-1",
    "title": "Bayes AI",
    "section": "Inspect the results",
    "text": "Inspect the results\n\n\nCode\nprint(np.quantile(slope,[0.05,0.95,0.5]))\n\n\n[-14.34220815  -1.41647384  -5.76567411]\n\n\nCode\nprint(np.quantile(intercept,[0.05,0.95,0.5]))\n\n\n[17.46308355 39.23070431 28.39726067]\n\n\nCode\nprint(np.mean(intercept),np.mean(slope))\n\n\n28.044336 -6.913012\n\n\n\n\nCode\ny = iris$Species==\"setosa\"\nglm(y~iris$Sepal.Length, family=binomial)\n\n\n\nCall:  glm(formula = y ~ iris$Sepal.Length, family = binomial)\n\nCoefficients:\n      (Intercept)  iris$Sepal.Length  \n            27.83              -5.18  \n\nDegrees of Freedom: 149 Total (i.e. Null);  148 Residual\nNull Deviance:      191 \nResidual Deviance: 72   AIC: 76"
  },
  {
    "objectID": "06-mcmc.html#hierarchical-model",
    "href": "06-mcmc.html#hierarchical-model",
    "title": "Bayes AI",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nWe will use the Normal-Normal model with kown variance and unknowm mean \\[\n\\begin{aligned}\n\\mu \\sim & \\mathrm{Normal}(0,1) \\\\\nx\\mid \\mu \\sim & \\mathrm{Normal}(\\mu,1)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#lets-sample",
    "href": "06-mcmc.html#lets-sample",
    "title": "Bayes AI",
    "section": "Let’s sample",
    "text": "Let’s sample\n\n\nCode\nn =20\ndata= sort(rnorm(n,0))\n# hist(data)\nsamples = 15000\nproposal_width = 0.5\nmu_current = 0\nsigma = 1\nmu0 = 0\nsigma0 = 1\nposterior = c(mu_current)\nfor (i in 1:samples) {\n  mu_proposal = rnorm(1,mu_current, proposal_width)\n  likelihood_current =  prod(dnorm(data,mu_current,  sigma))\n  likelihood_proposal = prod(dnorm(data,mu_proposal, sigma))\n  prior_current =  dnorm(mu_current,mu0, sigma0)\n  prior_proposal = dnorm(mu_proposal,mu0, sigma0)\n\n  p_current = likelihood_current * prior_current\n  p_proposal = likelihood_proposal * prior_proposal\n  # Accept proposal?\n  p_accept = p_proposal / p_current\n  accept = (runif(1) &lt; p_accept)\n  if (accept) {\n      mu_current = mu_proposal\n      posterior = c(posterior,mu_current)\n  }\n}\nposterior = posterior[-(1:500)]\nplot(posterior, pch=16)\n\n\n\n\n\n\n\n\n\nCode\nmu_post = (mu0 / sigma0^2 + sum(data) / sigma^2) / (1. / sigma0^2 + n / sigma^2)\nsigma_post = sqrt((1. / sigma0^2 + n / sigma^2)^(-1))\nsprintf(\"Analytical mean: %.2f. MCMC Mean: %.2f\", mu_post, mean(posterior))\n\n\n[1] \"Analytical mean: 0.03. MCMC Mean: 0.03\"\n\n\nCode\nsprintf(\"Analytical sd: %.2f. MCMC sd: %.2f\", sigma_post, sd(posterior))\n\n\n[1] \"Analytical sd: 0.22. MCMC sd: 0.23\"\n\n\nCode\nx = seq(-1,1,by=0.01)\ndpost = dnorm(x,mu_post,sigma_post)\nhist(posterior, freq = F, breaks=30, ylim=c(0,2), main=\"Posterior Mean\")\nlines(density(posterior), ylim=c(0,2), main=\"Posterior Mean\")\nlines(x,dpost, type='l', lwd=3, col='red')"
  },
  {
    "objectID": "06-mcmc.html#lets-compare-to-hmc",
    "href": "06-mcmc.html#lets-compare-to-hmc",
    "title": "Bayes AI",
    "section": "Let’s compare to HMC",
    "text": "Let’s compare to HMC\n\n\nCode\nlibrary(brms)\nd = data.frame(y=data, sigma0=rep(1,20))\nfit &lt;- brm(data = d, \n            family = gaussian,\n            y | se(sigma0) ~ 1,\n    prior = c(prior(normal(0, 1), class = Intercept)),\n    iter = 1000, refresh = 0, chains = 4)\nposterior = as_draws_array(fit, variable = \"b_Intercept\")\nhist(posterior, freq = F, breaks=30, ylim=c(0,2), main=\"Posterior Mean\")\nlines(density(posterior), ylim=c(0,2), main=\"Posterior Mean\")\nlines(x,dpost, type='l', lwd=3, col='red')"
  },
  {
    "objectID": "06-mcmc.html#problems-of-metropolis-hastings-algorithm",
    "href": "06-mcmc.html#problems-of-metropolis-hastings-algorithm",
    "title": "Bayes AI",
    "section": "Problems of Metropolis-Hastings algorithm",
    "text": "Problems of Metropolis-Hastings algorithm\nMH is very simple and quite general. At the same time\n\ntoo large step size \\(\\sigma\\) leads to a large fraction of rejected samples, while too small \\(\\sigma\\) makes very small steps, thus it takes long time to ‘explore the distribution’ (check this in demonstration!)\nin high-dimensional spaces (very important use-case), MH explores the space very inefficiently because of it’s random-walk behavior. Guessing good direction in 1000 dimensions is incomparably harder than doing this for 2 dimensions!\nMH can’t travel long distances (significantly larger than \\(\\sigma\\)) between isolated local minimums\n\nSampling high-dimensional distributions with MH becomes very inefficient in practice. A more efficient scheme is called Hamiltonian Monte Carlo (HMC)."
  },
  {
    "objectID": "04-hypothesis.html#bayesian-hypothesis-testing",
    "href": "04-hypothesis.html#bayesian-hypothesis-testing",
    "title": "Bayes AI",
    "section": "Bayesian Hypothesis Testing",
    "text": "Bayesian Hypothesis Testing\n\nGiven sample of data, \\(y\\), generated from \\(p\\left( y \\mid \\theta\\right)\\) for \\(\\theta\\in\\Theta\\)\nDetermine if \\(\\theta\\) lies in \\(\\Theta_{0}\\) or in \\(\\Theta_{1}\\)\nTwo disjoint subsets of \\(\\Theta\\). In general, the hypothesis testing problem involves an action: accepting or rejecting a hypothesis.\nA null, \\(H_{0}\\), and alternative hypothesis, \\(H_{1}\\), which are defined as \\[\nH_{0}:\\theta\\in\\Theta_{0}\\;\\;\\mathrm{and}\\;\\;H_{1}%\n:\\theta\\in\\Theta_{1}\\text{.}%\n\\]"
  },
  {
    "objectID": "04-hypothesis.html#bayesian-hypothesis-testing-1",
    "href": "04-hypothesis.html#bayesian-hypothesis-testing-1",
    "title": "Bayes AI",
    "section": "Bayesian Hypothesis Testing",
    "text": "Bayesian Hypothesis Testing\n\n\\(\\Theta _{0}=\\theta_{0}\\): simple or “sharp” null hypothesis.\nComposite, otherwise\nSingle parameter: typical one-sided tests are of the form \\(H_{0}:\\theta&lt;\\theta_{0}\\) and \\(H_{1}:\\theta&gt;\\theta_{0}\\).\n\n\n\n\n\n\n\n\n\n\n\\(\\theta\\in\\Theta_{0}\\)\n\\(\\theta\\in\\Theta_{1}\\)\n\n\n\n\nAccept \\(H_{0}\\)\nCorrect decision\nType II error\n\n\nAccept \\(H_{1}\\)\nType I error\nCorrect decision"
  },
  {
    "objectID": "04-hypothesis.html#bayesian-hypothesis-testing-errors",
    "href": "04-hypothesis.html#bayesian-hypothesis-testing-errors",
    "title": "Bayes AI",
    "section": "Bayesian Hypothesis Testing: Errors",
    "text": "Bayesian Hypothesis Testing: Errors\nFormally, the probabilities of Type I (\\(\\alpha\\)) and Type II (\\(\\beta\\)) errors are defined as: \\[\n\\alpha=P \\left[  \\text{reject }H_{0} \\mid H_{0}\\text{\nis true }\\right]  \\text{ and }\\beta=P \\left[  \\text{accept\n}H_{0} \\mid H_{1}\\text{ is true }\\right]  \\text{.}%\n\\]\n\nIt is useful to think of the decision to accept or reject as a decision rule, \\(d\\left( y\\right)\\).\nIn many cases, the decision rules form a critical region \\(R\\), such that \\(d\\left( y\\right) =d_{1}\\) if \\(y\\in R\\).\nThese regions are often take the form of simple inequalities.\n\n\\[\\begin{align*}\n\\alpha_{\\theta}\\left(  d\\right)   &  =P \\left[  d\\left(  y\\right)\n=d_{1} \\mid \\theta\\right]  \\text{ if }\\theta\\in\\Theta_{0}\\text{ }(H_{0}\\text{ is true})\\\\\n\\beta_{\\theta}\\left(  d\\right)   &  =P \\left[  d\\left(  y\\right)\n=d_{0} \\mid \\theta\\right]  \\text{ if }\\theta\\in\\Theta_{1}\\text{ }(H_{1}\\text{ is true})\\text{.}%\n\\end{align*}\\] Errors explicitly depend on the decision and the true parameter value"
  },
  {
    "objectID": "04-hypothesis.html#bayesian-hypothesis-testing-2",
    "href": "04-hypothesis.html#bayesian-hypothesis-testing-2",
    "title": "Bayes AI",
    "section": "Bayesian Hypothesis Testing",
    "text": "Bayesian Hypothesis Testing\n\nThe size of the test (the probability of making a type I error) is defined as \\[\n\\alpha = \\underset{\\theta\\in\\Theta_{0}}{\\sup}~\\alpha_{\\theta}\\left( d\\right)\n\\]\nThe power is defined as \\(1-\\beta_{\\theta}\\left( d\\right)\\).\nIt is always possible to set either \\(\\alpha_{\\theta}\\left( d\\right)\\) or \\(\\beta_{\\theta }\\left( d\\right)\\) equal to zero, by finding a test that always rejects alternative or null, respectively.\nThe total probability of making an error is \\(\\alpha_{\\theta}\\left(d\\right) +\\beta_{\\theta}\\left(d\\right)\\), want to minimize"
  },
  {
    "objectID": "04-hypothesis.html#bayesian-hypothesis-testing-3",
    "href": "04-hypothesis.html#bayesian-hypothesis-testing-3",
    "title": "Bayes AI",
    "section": "Bayesian Hypothesis Testing",
    "text": "Bayesian Hypothesis Testing\n\nThe optimal action \\(d^*\\) minimizes the posterior expected loss, is \\(d^* = d_0 = 0\\) if the posterior probability of hypothesis \\(H_0\\) exceeds 1/2, and \\(d^* = d_1=1\\) else \\[\nd^* = 1\\left(  P \\left(  \\theta \\in \\Theta_0 \\mid y\\right) &lt; P \\left(  \\theta \\in \\Theta_1 \\mid y\\right)\\right)  = 1\\left(P \\left(  \\theta \\in \\Theta_0 \\mid y\\right)&lt;1/2\\right).\n\\]\nSimply speaking, the hypothesis with higher posterior probability is selected.\nMore data reduces the error probability\nSometimes we can derive optimal tests, those that minimize the sum of the errors."
  },
  {
    "objectID": "04-hypothesis.html#bayesian-hypothesis-testing-4",
    "href": "04-hypothesis.html#bayesian-hypothesis-testing-4",
    "title": "Bayes AI",
    "section": "Bayesian Hypothesis Testing",
    "text": "Bayesian Hypothesis Testing\n\nSimple hypothesis tests \\(H_{0}:\\theta=\\theta_{0}\\) versus \\(H_{1}:\\theta=\\theta_{1}\\) admit optimal tests\nDefining \\(d^{\\ast}\\) as a test accepting \\(H_{0}\\) if \\(a_{0}f\\left( y \\mid \\theta_{0}\\right) &gt;a_{1}f\\left( y \\mid \\theta_{1}\\right)\\) and \\(H_{1}\\) if \\(a_{0}f\\left( y \\mid \\theta_{0}\\right) &lt;a_{1}f\\left( y \\mid \\theta _{1}\\right)\\), for some \\(a_{0}\\) and \\(a_{1}\\).\nEither \\(H_{0}\\) or \\(H_{1}\\) can be accepted if \\(a_{0}f\\left(y \\mid \\theta_{0}\\right) =a_{1}f\\left( y \\mid \\theta_{1}\\right)\\). Then, for any other test \\(d\\), it is not hard to show that \\[\na_{0}\\alpha\\left(  d^{\\ast}\\right)  +a_{1}\\beta\\left(  d^{\\ast}\\right)  \\leq\na_{0}\\alpha\\left(  d\\right)  +a_{1}\\beta\\left(  d\\right),\n\\] where \\(\\alpha_{d}=\\alpha_{d}\\left( \\theta\\right)\\) and \\(\\beta_{d}=\\beta_{d}\\left( \\theta\\right)\\)."
  },
  {
    "objectID": "04-hypothesis.html#bayesian-hypothesis-testing-5",
    "href": "04-hypothesis.html#bayesian-hypothesis-testing-5",
    "title": "Bayes AI",
    "section": "Bayesian Hypothesis Testing",
    "text": "Bayesian Hypothesis Testing\n\nThis result highlights the optimality of tests defining rejection regions in terms of the likelihood ratio statistic, \\(f\\left( y \\mid \\theta_{0}\\right)/f\\left( y \\mid \\theta_{1}\\right)\\).\nIt turns out that the results are in fact stronger.\nIn terms of decision theoretic properties, tests that define rejection regions based on likelihood ratios are not only admissible decisions, but form a minimal complete class, the strongest property possible."
  },
  {
    "objectID": "04-hypothesis.html#bayesian-hypothesis-testing-6",
    "href": "04-hypothesis.html#bayesian-hypothesis-testing-6",
    "title": "Bayes AI",
    "section": "Bayesian Hypothesis Testing",
    "text": "Bayesian Hypothesis Testing\n\nTradeoff between the two goals of reducing type I and type II errors: decreasing \\(\\alpha\\) leads to an increase in \\(\\beta\\), and vice-versa.\nIt is common to fix \\(\\alpha_{\\theta}\\left( d\\right)\\), or \\(\\sup~\\alpha_{\\theta}\\left( d\\right)\\), and then find a test to minimize \\(\\beta_{d}\\left( \\theta\\right)\\).\nThis leads to “most powerful” tests.\nImportant result from decision theory: test procedures that use the same size level of \\(\\alpha\\) in problems with different sample sizes are inadmissible.\nThis is commonly done where significance is indicated by a fixed size, say 5%. The implications of this will be clearer below in examples."
  },
  {
    "objectID": "04-hypothesis.html#testing-fairness",
    "href": "04-hypothesis.html#testing-fairness",
    "title": "Bayes AI",
    "section": "Testing fairness",
    "text": "Testing fairness\nSuppose we are interested in testing \\(\\theta\\), the unknown probability of heads for possibly biased coin. Suppose, \\[\nH_0 :~\\theta=1/2 \\quad\\text{v.s.} \\quad  H_1 :~\\theta&gt;1/2.\n\\] An experiment is conducted and 9 heads and 3 tails are observed. This information is not sufficient to fully specify the model \\(p(y\\mid \\theta)\\). There are two approaches."
  },
  {
    "objectID": "04-hypothesis.html#testing-fairness-1",
    "href": "04-hypothesis.html#testing-fairness-1",
    "title": "Bayes AI",
    "section": "Testing fairness",
    "text": "Testing fairness\nScenario 1: Number of flips, \\(n = 12\\) is predetermined. Then number of heads \\(Y \\mid \\theta\\) is binomial \\(B(n, \\theta)\\), with probability mass function \\[\np(y\\mid \\theta)= {n \\choose x} \\theta^x(1−\\theta)^{n−x} = 220 \\cdot \\theta^9(1−\\theta)^3\n\\] For a frequentist, the p-value of the test is \\[\nP(Y \\geq 9\\mid H_0)=\\sum_{y=9}^{12} {12 \\choose y} (1/2)^y(1−1/2)^{12−y} = (1+12+66+220)/2^{12} =0.073,\n\\] and if you recall the classical testing, the \\(H_0\\) is not rejected at level \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "04-hypothesis.html#testing-fairness-2",
    "href": "04-hypothesis.html#testing-fairness-2",
    "title": "Bayes AI",
    "section": "Testing fairness",
    "text": "Testing fairness\nScenario 2: Number of tails (successes) \\(\\alpha = 3\\) is predetermined, i.e, the flipping is continued until 3 tails are observed. Then, \\(Y\\) - number of heads (failures) until 3 tails appear is Negative Binomial \\(NB(3, 1− \\theta)\\), \\[\np(y\\mid \\theta)= {\\alpha+y-1 \\choose \\alpha-1} \\theta^{y}(1−\\theta)^{\\alpha} = {3+9-1 \\choose 3-1} \\theta^9(1−\\theta)^3 = 55\\cdot \\theta^9(1−\\theta)^3.\n\\] For a frequentist, large values of \\(Y\\) are critical and the p-value of the test is \\[\nP(Y \\geq 9\\mid H_0)=\\sum_{y=9}^{\\infty} {3+y-1 \\choose 2} (1/2)^{x}(1/2)^{3} = 0.0327.\n\\] We used the following identity here \\[\n\\sum_{x=k}^{\\infty} {2+x \\choose 2}\\dfrac{1}{2^x} = \\dfrac{8+5k+k^2}{2^k}.\n\\] The hypothesis \\(H_0\\) is rejected, and this change in decision is not caused by observations.\nThis is the violation of the likelihoood principle."
  },
  {
    "objectID": "04-hypothesis.html#the-bayesian-approach",
    "href": "04-hypothesis.html#the-bayesian-approach",
    "title": "Bayes AI",
    "section": "The Bayesian Approach",
    "text": "The Bayesian Approach\nCompute the posterior distribution of each hypothesis \\(H_i: \\theta \\in \\Theta_i, ~ i=0,1\\). By Bayes rule \\[\nP \\left(  H_{i} \\mid y\\right)  =\\frac{p\\left(  y \\mid H_{i}\\right)  P \\left(  H_{i}\\right)  }{p\\left(  y\\right)}\n\\] where \\(P \\left( H_{i}\\right)\\) is the prior probability of \\(H_{i}\\), \\[\np\\left( y \\mid H_{i}\\right) =\\int_{\\theta \\in \\Theta_i} p\\left( y \\mid \\    \\theta\\right) p\\left( \\theta \\mid H_{i}\\right) d\\theta\n\\] is the marginal likelihood under \\(H_{i}\\), \\(p\\left( \\theta \\mid H_{i}\\right)\\) is the parameter prior under \\(H_{i}\\), and \\[\np(y)  = p\\left(y \\mid H_0\\right)P(H_0) + p\\left(y \\mid H_1\\right)P(H_1)\n\\]"
  },
  {
    "objectID": "04-hypothesis.html#the-bayesian-approach-1",
    "href": "04-hypothesis.html#the-bayesian-approach-1",
    "title": "Bayes AI",
    "section": "The Bayesian Approach",
    "text": "The Bayesian Approach\nIf the hypothesis are mutually exclusive, \\(P \\left( H_{0}\\right) =1-P \\left( H_{1}\\right)\\). \\[\n\\text{Odds}_{0,1}=\\frac{P \\left(  H_{0} \\mid y\\right)  }{P %\n\\left(  H_{1} \\mid y\\right)  }=\\frac{p\\left(  y \\mid H_{0}\\right)\n}{p\\left(  y \\mid H_{1}\\right)  }\\frac{P \\left(  H_{0}\\right)  }{P \\left(  H_{1}\\right)  }\\text{.}%\n\\]\nThe odds ratio updates the prior odds, \\(P \\left( H_{0}\\right) /P \\left( H_{1}\\right)\\), using the Bayes Factor, \\[\n\\mathcal{BF}_{0,1}=\\dfrac{p\\left(y \\mid H_{0}\\right)}{p\\left( y \\mid H_{1}\\right)}.\n\\] With exhaustive competing hypotheses\\(,\\) \\(P \\left( H_{0} \\mid y\\right)\\) simplifies to \\[\nP \\left(  H_{0} \\mid y\\right)  =\\left(  1+\\left(  \\mathcal{BF}_{0,1}\\right)  ^{-1}\\frac{\\left(  1-P \\left(  H_{0}\\right)\n\\right)  }{P \\left(  H_{0}\\right)  }\\right)  ^{-1}\\text{,}%\n\\] and with equal prior probability, \\[\np\\left( H_{0} \\mid y\\right) =\\left( 1+\\left( \\mathcal{BF}_{0,1}\\right) ^{-1}\\right) ^{-1}\n\\]"
  },
  {
    "objectID": "04-hypothesis.html#jeffreys-scale",
    "href": "04-hypothesis.html#jeffreys-scale",
    "title": "Bayes AI",
    "section": "Jeffreys’ Scale",
    "text": "Jeffreys’ Scale\nFor \\(\\mathcal{BF}_{0,1}&gt;1\\)\n\n\n\n\n\n\n\n\n\\(\\mathcal{BF}_{0,1}\\)\n\\(log10(\\mathcal{BF}_{0,1})\\)\nGrades of evidence\n\n\n\n\n\\(1\\) to \\(10^{1/2}\\)\n0 to 1/2\nBarely worth mentioning\n\n\n\\(10^{1/2}\\) to \\(10\\)\n1/2 to 1\nSubstantial\n\n\n\\(10\\) to \\(10^{3/2}\\)\n1 to 3/2\nStrong\n\n\n\\(10^{3/2}\\) to \\(10^{2}\\)\n3/2 to 2\nVery strong\n\n\n&gt; \\(10^{2}\\)\n&gt; 2\nDecisive"
  },
  {
    "objectID": "04-hypothesis.html#jeffreys-scale-1",
    "href": "04-hypothesis.html#jeffreys-scale-1",
    "title": "Bayes AI",
    "section": "Jeffreys’ Scale",
    "text": "Jeffreys’ Scale\nFor \\(\\mathcal{BF}_{0,1}&lt;1\\)\n\n\n\n\n\n\n\n\n\\(\\mathcal{BF}_{0,1}\\)\n$log\\(10(\\mathcal{BF}_{0,1})\\)\nGrades of evidence\n\n\n\n\n\\(1\\) to \\(10^{-1/2}\\)\n0 to -1/2\nBarely worth mentioning\n\n\n\\(10^{-1/2}\\) to \\(10^{-1}\\)\n-1/2 to -1\nSubstantial\n\n\n\\(10^{-1}\\) to \\(10^{-3/2}\\)\n-1 to -3/2\nStrong\n\n\n\\(10^{-3/2}\\) to \\(10^{-2}\\)\n-3/2 to -2\nVery strong\n\n\n&lt; \\(10^{-2}\\)\n&lt; -2\nDecisive"
  },
  {
    "objectID": "04-hypothesis.html#how-to-make-a-decision",
    "href": "04-hypothesis.html#how-to-make-a-decision",
    "title": "Bayes AI",
    "section": "How to make a decision",
    "text": "How to make a decision\n\nThe loss incurred when accepting the null (alternative) when the alternative is true (false) is \\(L\\left( d_{0} \\mid H_{1}\\right)\\) and \\(L\\left( d_{1} \\mid H_{0}\\right)\\), respectively.\nAssumes a zero loss of making a correct decision\nThe Bayesian will accept or reject based on the posterior expected loss \\[\n\\mathbb{E}\\left[  \\text{Loss}\\mid d_{0},y\\right]  =L\\left(  d_{0} \\mid H_{0}\\right)\nP \\left(  H_{0} \\mid y\\right)  +L\\left(  d_{0} \\mid H_{1}\\right)  P \\left(  H_{1} \\mid y\\right)  =L\\left( d_{0} \\mid H_{1}\\right)  P \\left(  H_{1} \\mid y\\right)  ,\n\\] vs \\[\n\\mathbb{E}\\left[  \\text{Loss} \\mid d_{1},y\\right]  =L\\left(  d_{1} \\mid H_{0}\\right)\nP \\left(  H_{0} \\mid y\\right)  +L\\left(  d_{1} \\mid H_{1}\\right)  P \\left(  H_{1} \\mid y\\right)  =L\\left( d_{1} \\mid H_{0}\\right)  P \\left(  H_{0} \\mid y\\right)  .\n\\]"
  },
  {
    "objectID": "04-hypothesis.html#how-to-make-a-decision-1",
    "href": "04-hypothesis.html#how-to-make-a-decision-1",
    "title": "Bayes AI",
    "section": "How to make a decision",
    "text": "How to make a decision\n\\[\n\\frac{L\\left(  d_{0} \\mid H_{1}\\right)  }{L\\left(  d_{1} \\mid H_{0}\\right)  }&lt;\\frac{P \\left(  H_{0} \\mid y\\right)  }{P \\left(  H_{1} \\mid y\\right)  }.\n\\] In the case of equal prior probabilities, this reduces to \\[\n\\frac{L\\left(  d_{0} \\mid H_{1}\\right)  }{L\\left(  d_{1} \\mid H_{0}\\right)  }&lt;\\frac{1}{\\mathcal{BF}_{0,1}}.\n\\]"
  },
  {
    "objectID": "04-hypothesis.html#signal-transmission-example",
    "href": "04-hypothesis.html#signal-transmission-example",
    "title": "Bayes AI",
    "section": "Signal Transmission Example",
    "text": "Signal Transmission Example\n\nRandom variable \\(X\\) is transmitted over a noisy communication channel. Assume that the received signal is given by \\[\nY=X+W,\n\\] where \\(W\\sim N(0,\\sigma^2)\\) is independent of \\(X\\).\nSuppose that \\(X=1\\) with probability \\(p\\), and \\(X=−1\\) with probability \\(1−p\\).\nThe goal is to decide between \\(X=1\\) and \\(X=−1\\) by observing the random variable \\(Y\\). We will assume symmetric loss and will accept the hypothesis with the higher posterior probability. This is also sometimes called the maximum a posteriori (MAP) test."
  },
  {
    "objectID": "04-hypothesis.html#signal-transmission-example-1",
    "href": "04-hypothesis.html#signal-transmission-example-1",
    "title": "Bayes AI",
    "section": "Signal Transmission Example",
    "text": "Signal Transmission Example\nWe assume that \\(H_0: ~ X = 1\\), thus \\(Y\\mid X_0 \\sim N(1,\\sigma^2)\\), and \\(Y\\mid X_1 \\sim N(-1,\\sigma^2)\\). The Bayes factor is simply the likelihood ratio \\[\n\\dfrac{p(y\\mid H_0)}{p(y \\mid H_1)} =  \\exp\\left( \\frac{2y}{\\sigma^2}\\right).\n\\] The propr odds are \\(p/(1-p)\\), thus the posterior odds are \\[\n\\exp\\left( \\frac{2y}{\\sigma^2}\\right)\\dfrac{p}{1-p}.\n\\] We choose \\(H_0\\) (true \\(X\\) is 1), if the posterior odds are greater than 1, i.e., \\[\ny &gt; \\frac{\\sigma^2}{2} \\log\\left( \\frac{p}{1-p}\\right) = c.\n\\]"
  },
  {
    "objectID": "04-hypothesis.html#signal-transmission-example-2",
    "href": "04-hypothesis.html#signal-transmission-example-2",
    "title": "Bayes AI",
    "section": "Signal Transmission Example",
    "text": "Signal Transmission Example\nFurther, we can calculate the error probabilities of our test. \\[\np(d_1\\mid H_0) = P(Y&lt;c\\mid X=1) = \\Phi\\left( \\frac{c-1}{\\sigma}\\right),\n\\] and \\[\np(d_0\\mid H_1) = P(Y&gt;c\\mid X=-1) = 1- \\Phi\\left( \\frac{c+1}{\\sigma}\\right).\n\\] Let’s plot the total error rate as a function of \\(p\\) and assuming \\(\\sigma=0.2\\) \\[\nP_e = p(d_1\\mid H_0) (1-p) + p(d_0\\mid H_1) p\n\\]"
  },
  {
    "objectID": "04-hypothesis.html#signal-transmission-example-3",
    "href": "04-hypothesis.html#signal-transmission-example-3",
    "title": "Bayes AI",
    "section": "Signal Transmission Example",
    "text": "Signal Transmission Example\n\n\nCode\nsigma &lt;- 0.2\np &lt;- seq(0.01,0.99,0.01)\nc &lt;- sigma^2/2*log(p/(1-p))\nPe &lt;- pnorm((c-1)/sigma)*(1-p) + (1-pnorm((c+1)/sigma))*p\nplot(p,Pe,type=\"l\",xlab=\"p\",ylab=\"Total Error Rate\")"
  },
  {
    "objectID": "04-hypothesis.html#hockey-hypothesis-testing-for-normal-mean",
    "href": "04-hypothesis.html#hockey-hypothesis-testing-for-normal-mean",
    "title": "Bayes AI",
    "section": "Hockey: Hypothesis Testing for Normal Mean",
    "text": "Hockey: Hypothesis Testing for Normal Mean\n\nGeneral manager of Washington Capitals (an NHL hockey team) thinks that their star center player Evgeny Kuznetsov is underperformed and is thinking of trading him to a different team.\nHe uses the number of goals per season as a metric of performance.\nPrior: a top forward scores on average 30 goals per season with a standard deviation of 5, \\(\\theta \\sim N(30,25)\\).\nData: In the 2022-2023 season Kuznetsov scored 12 goals and likelihood \\(X\\mid \\theta \\sim N(\\theta, 36)\\).\nKuznetsov’s performance was not stable over the years, thus high variance in the likelihood. Thus, the posterior is \\(N(23,15)\\).\n\n\n\nCode\nsigma2 = 36\nsigma02 = 25\nmu=30\ny=12\nk = sigma02 + sigma2\nmu1 = sigma2/k*mu + sigma02/k*y\nsigma21 = sigma2*sigma02/k\nmu1\n\n\n[1] 22.62295\n\n\nCode\nsigma21\n\n\n[1] 14.7541"
  },
  {
    "objectID": "04-hypothesis.html#hockey",
    "href": "04-hypothesis.html#hockey",
    "title": "Bayes AI",
    "section": "Hockey",
    "text": "Hockey\n\nThe manager thinks, that Kuznetsov simply had a bad year and his true performance is at least 24 goals per season \\[\nH_0: \\theta &gt; 24$, ~H_1: \\theta&lt;24\n\\] The posterior probability of \\(H_0\\) hypothesis is\n\n\n\nCode\na = 1-pnorm(24,mu1,sqrt(sigma21))\na\n\n\n[1] 0.3599834\n\n\nIt is less than 1/2, only 36%. Thus, we should reject the null hypothesis. The posterior odds in favor of the null hypothesis is\n\n\nCode\na/(1-a)\n\n\n[1] 0.5624594"
  },
  {
    "objectID": "04-hypothesis.html#add-loss",
    "href": "04-hypothesis.html#add-loss",
    "title": "Bayes AI",
    "section": "Add Loss",
    "text": "Add Loss\n\nIf underestimating (and trading) Kuznetsov is two times more costly than overestimating him (fans will be upset and team spirit might be affected) \\[\nL(d_1\\mid H_0) = 2L(d_0\\mid H_1)\n\\]\nThen we should accept the null when posterior odds are greater than 1/2.\n\nThe posterior odds are in favor of the null hypothesis. Thus, the manager should not trade Kuznetsov.\nKuznetsov was traded to Carolina Hurricanes towards the end of the 2023-2024 season."
  },
  {
    "objectID": "04-hypothesis.html#two-sided-test-for-normal-mean",
    "href": "04-hypothesis.html#two-sided-test-for-normal-mean",
    "title": "Bayes AI",
    "section": "Two-Sided Test for Normal Mean",
    "text": "Two-Sided Test for Normal Mean\n\n\\(H_0: \\theta = m_0\\), \\(p\\left( \\theta \\mid H_{0}\\right) =\\delta_{m_{0}}\\left( \\theta\\right)\\)\n\\(H_1: \\theta \\neq m_0\\), \\(p\\left( \\theta \\mid H_{1}\\right) = N\\left( m_{0},\\sigma^{2}/n_0\\right)\\)\nWhere \\(n\\) is the sample size and \\(\\sigma^2\\) is the variance (known) of the population. Observed samples are \\(Y = (y_1, y_2, \\ldots, y_n)\\) with \\[\ny_i \\sim N(\\theta, \\sigma^2).\n\\]"
  },
  {
    "objectID": "04-hypothesis.html#bayes-factor",
    "href": "04-hypothesis.html#bayes-factor",
    "title": "Bayes AI",
    "section": "Bayes Factor",
    "text": "Bayes Factor\nThe Bayes factor can be calculated analytically \\[\nBF_{0,1} = \\frac{p(Y\\mid \\theta = m_0, \\sigma^2 )}\n{\\int p(Y\\mid \\theta, \\sigma^2) p(\\theta \\mid m_0, n_0, \\sigma^2)\\, d \\theta}\n\\] \\[\n\\int p(Y\\mid \\theta, \\sigma^2) p(\\theta \\mid m_0, n_0, \\sigma^2)\\, d \\theta = \\frac{\\sqrt{n_0}\\exp\\left\\{-\\frac{n_0(m_0-\\bar y)^2}{2\\left(n_0+n\\right)\\sigma^2}\\right\\}}{\\sqrt{2\\pi}\\sigma^2\\sqrt{\\frac{n_0+n}{\\sigma^2}}}\n\\] \\[\np(Y\\mid \\theta = m_0, \\sigma^2 ) = \\frac{\\exp\\left\\{-\\frac{(\\bar y-m_0)^2}{2 \\sigma ^2}\\right\\}}{\\sqrt{2 \\pi } \\sigma }\n\\]"
  },
  {
    "objectID": "04-hypothesis.html#bayes-factor-1",
    "href": "04-hypothesis.html#bayes-factor-1",
    "title": "Bayes AI",
    "section": "Bayes Factor",
    "text": "Bayes Factor\nThus, the Bayes factor is \\[\nBF_{0,1} = \\frac{\\sigma\\sqrt{\\frac{n_0+n}{\\sigma^2}}e^{-\\frac{(m_0-\\bar y)^2}{2\\left(n_0+n\\right)\\sigma^2}}}{\\sqrt{n_0}}\n\\]\n\\[\nBF_{0,1} =\\left(\\frac{n + n_0}{n_0} \\right)^{1/2} \\exp\\left\\{-\\frac 1 2 \\frac{n }{n + n_0} Z^2 \\right\\}\n\\]\n\\[\nZ =  \\frac{(\\bar{Y} - m_0)}{\\sigma/\\sqrt{n}}\n\\]\nOne way to interpret the scaling factor \\(n_0\\) is ro look at the standard effect size \\[\n\\delta = \\frac{\\theta - m_0}{\\sigma}.\n\\] The prior of the standard effect size is \\[\n\\delta \\mid H_1 \\sim N(0, 1/n_0).\n\\] This allows us to think about a standardized effect independent of the units of the problem."
  },
  {
    "objectID": "04-hypothesis.html#argon-discovery",
    "href": "04-hypothesis.html#argon-discovery",
    "title": "Bayes AI",
    "section": "Argon discovery",
    "text": "Argon discovery\n\nIn 1894, Lord Rayleigh and Sir William Ramsay discovered a new gas, argon, by removing all the nitrogen and oxygen from a sample of air.\n\n\n\nCode\nair =    c(2.31017, 2.30986, 2.31010, 2.31001, 2.31024, 2.31010, 2.31028, 2.31028)\ndecomp = c(2.30143, 2.29890, 2.29816, 2.30182, 2.29869, 2.29940, 2.29849, 2.29889)\n\n\n\nOur null hypothesis is that the mean of the difference equals to zero.\nWe assume that measurements made in the lab have normal errors, this the normal likelihood. We empirically calculate the standard deviation of our likelihood."
  },
  {
    "objectID": "04-hypothesis.html#argon-discovery-1",
    "href": "04-hypothesis.html#argon-discovery-1",
    "title": "Bayes AI",
    "section": "Argon discovery",
    "text": "Argon discovery\nThe Bayes factor is\n\n\nCode\ny = air - decomp\nn = length(y)\nm0 = 0\nsigma = sqrt(var(air) + var(decomp))\nn0 = 1\nZ = (mean(y) - m0)/(sigma/sqrt(n))\nBF = sqrt((n + n0)/n0)*exp(-0.5*n/(n + n0)*Z^2)\nBF\n\n\n[1] 1.924776e-91\n\n\nWe have extremely strong evidence in favor \\(H_1: \\theta \\ne 0\\) hypothesis. \\(P(H_0\\mid y) = 1\\).\n\n\nCode\na = 1/(1+BF)\na\n\n\n[1] 1"
  },
  {
    "objectID": "04-hypothesis.html#ab-testing-google-search",
    "href": "04-hypothesis.html#ab-testing-google-search",
    "title": "Bayes AI",
    "section": "AB Testing: Google Search",
    "text": "AB Testing: Google Search\n\nGoogle is testing new search algorithm (Algo2)\nWant to know if it is any better compare to the old one (Algo1)\nCollected data on 2500 search requests for each algorithm\n\n\n\n\n\nAlgo1\nAlgo2\n\n\n\n\nsuccess\n1755\n1818\n\n\nfailure\n745\n682\n\n\ntotal\n2500\n2500"
  },
  {
    "objectID": "04-hypothesis.html#ab-testing-google-search-1",
    "href": "04-hypothesis.html#ab-testing-google-search-1",
    "title": "Bayes AI",
    "section": "AB Testing: Google Search",
    "text": "AB Testing: Google Search\n\nAssume binomial likelihood and use conjugate beta prior\nIndependent beta priors on the click-through rates\n\\[\np_1\\sim Beta(\\alpha_1,\\beta_1), ~p_2\\sim Beta(\\alpha_2,\\beta_2).\n\\] The posterior for \\(p_1\\) and and \\(p_2\\) are independent betas \\[\np(p_1, p_1 \\mid y) \\propto p_1^{\\alpha_1 + 1755 - 1} (1-p_1)^{\\beta_1 + 745 - 1}\\times p_2^{\\alpha_2 + 1818 - 1} (1-p_2)^{\\beta_2 + 682 - 1}.\n\\]"
  },
  {
    "objectID": "04-hypothesis.html#ab-testing-google-search-2",
    "href": "04-hypothesis.html#ab-testing-google-search-2",
    "title": "Bayes AI",
    "section": "AB Testing: Google Search",
    "text": "AB Testing: Google Search\nThe easiest way to explore this posterior is via Monte Carlo simulation of the posterior.\n\n\nCode\nset.seed(92) #Kuzy\ny1 &lt;- 1755; n1 &lt;- 2500; alpha1 &lt;- 1; beta1 &lt;- 1\ny2 &lt;- 1818; n2 &lt;- 2500; alpha2 &lt;- 1; beta2 &lt;- 1\nm = 10000\np1 &lt;- rbeta(m, y1 + alpha1, n1 - y1 + beta1)\np2 &lt;- rbeta(m, y2 + alpha2, n2 - y2 + beta2)\nrd &lt;- p2 - p1\nplot(density(rd), main=\"Posterior Difference in Click-Through Rates\", xlab=\"p2 - p1\", ylab=\"Density\")\nq = quantile(rd, c(.05, .95))\nprint(q)\n\n\n         5%         95% \n0.003700223 0.046539157 \n\n\nCode\nabline(v=q,col=\"red\")"
  },
  {
    "objectID": "04-hypothesis.html#lindleys-paradox",
    "href": "04-hypothesis.html#lindleys-paradox",
    "title": "Bayes AI",
    "section": "Lindley’s Paradox",
    "text": "Lindley’s Paradox\nOften evidence which, for a Bayesian statistician, strikingly supports the null leads to rejection by standard classical procedures.\n\nDo Bayes and Classical always agree?\n\nBayes computes the probability of the null being true given the data \\(p ( H_0 | D )\\). That’s not the p-value. Why?\n\nSurely they agree asymptotically?\nHow do we model the prior and compute likelihood ratios \\(L ( H_0 | D )\\) in the Bayesianwork?"
  },
  {
    "objectID": "04-hypothesis.html#bayes-t-ratio",
    "href": "04-hypothesis.html#bayes-t-ratio",
    "title": "Bayes AI",
    "section": "Bayes \\(t\\)-ratio",
    "text": "Bayes \\(t\\)-ratio\nEdwards, Lindman and Savage (1963)\nSimple approximation for the likelihood ratio. \\[\nL ( p_0 ) \\approx \\sqrt{2 \\pi} \\sqrt{n} \\exp \\left ( - \\frac{1}{2} t^2 \\right )\n\\]\n\nKey: Bayes test will have the factor \\(\\sqrt{n}\\)\n\nThis will asymptotically favour the null.\n\nThere is only a big problem when \\(2 &lt; t &lt; 4\\) – but this is typically the most interesting case!"
  },
  {
    "objectID": "04-hypothesis.html#coin-tossing",
    "href": "04-hypothesis.html#coin-tossing",
    "title": "Bayes AI",
    "section": "Coin Tossing",
    "text": "Coin Tossing\nIntuition: Imagine a coin tossing experiment and you want to determine whether the coin is “fair” \\(H_0 : p = \\frac{1}{2}\\).\nThere are four experiments.\n\n\n\nExpt\n1\n2\n3\n4\n\n\n\n\nn\n50\n100\n400\n10,000\n\n\nr\n32\n60\n220\n5098\n\n\n\\(L(p_0)\\)\n0.81\n1.09\n2.17\n11.68"
  },
  {
    "objectID": "04-hypothesis.html#coin-tossing-1",
    "href": "04-hypothesis.html#coin-tossing-1",
    "title": "Bayes AI",
    "section": "Coin Tossing",
    "text": "Coin Tossing\nImplications:\n\nClassical: In each case the \\(t\\)-ratio is approx 2. They we just \\(H_0\\) ( a fair coin) at the 5% level in each experiment.\nBayes: \\(L ( p_0 )\\) grows to infinity and so they is overwhelming evidence for \\(H _ 0\\). Connelly shows that the Monday effect disappears when you compute the Bayes version."
  },
  {
    "objectID": "02-utility.html#probability-and-psychology",
    "href": "02-utility.html#probability-and-psychology",
    "title": "Bayes AI",
    "section": "Probability and Psychology",
    "text": "Probability and Psychology\nHow do people form probabilities or expectations in reality?\nPsychologists have categorized many different biases that people have in their beliefs or judgments.\n\n\n\nLoss Aversion\n\n\nThe most important finding of Kahneman and Tversky is that people are loss averse.\n\n\n\nUtilities are defined over gains and losses rather than over final (or terminal) wealth, an idea first proposed by Markowitz. This is a violation of the EU postulates. Let \\((x,y)\\) denote a bet with gain \\(x\\) with probability \\(y\\).\nTo illustrate this subjects were asked:\nIn addition to whatever you own, you have been given $1000, now choose between the gambles \\(A = ( 1000 , 0.5 )\\) and \\(B = ( 500 , 1 )\\).\n\\(B\\) was the more popular choice."
  },
  {
    "objectID": "02-utility.html#example",
    "href": "02-utility.html#example",
    "title": "Bayes AI",
    "section": "Example",
    "text": "Example\nThe same subjects were then asked: In addition to whatever you own, you have been given $2000, now choose between the gambles \\(C = ( -1000 , 0.5 )\\) and \\(D = ( -500 , 1 )\\).\n\nThis time \\(C\\) was more popular.\nThe key here is that their final wealth positions are identical yet people chose differently. The subjects are apparently focusing only on gains and losses.\n\nWhen they are not given any information about prior winnings, they choose \\(B\\) over \\(A\\) and \\(C\\) over \\(D\\). Clearly for a risk averse people this is the rational choice.\n\nThis effect is known as loss aversion."
  },
  {
    "objectID": "02-utility.html#representativeness",
    "href": "02-utility.html#representativeness",
    "title": "Bayes AI",
    "section": "Representativeness",
    "text": "Representativeness\n\n\n\nRepresentativeness\n\n\nWhen people try to determine the probability that evidence \\(A\\) was generated by model \\(B\\), they often use the representative heuristic. This means that they evaluate the probability by the degree to which \\(A\\) reflects the essential characteristics of \\(B\\).\n\n\n\n\nA common bias is base rate neglect or ignoring prior evidence.\nFor example, in tossing a fair coin the sequence HHTHTHHTHH with seven heads is likely to appear and yet people draw conclusions from too few data points and think 7 heads is representative of the true process and conclude \\(p=0.7\\)."
  },
  {
    "objectID": "02-utility.html#expected-utility-eu-theory-normative",
    "href": "02-utility.html#expected-utility-eu-theory-normative",
    "title": "Bayes AI",
    "section": "Expected Utility (EU) Theory: Normative",
    "text": "Expected Utility (EU) Theory: Normative\nLet \\(P,Q\\) be two probability distributions or risky gambles/lotteries.\n\\(p P + (1 - p ) Q\\) is the compound or mixture lottery.\nThe rational agent (You) will have preferences between gambles.\n\nWe write \\(P \\succeq Q\\) if and only if You strictly prefer \\(P\\) to \\(Q\\). If two lotteries are indifferent we write \\(P \\sim Q\\).\nEU – a number of plausible axioms – completeness, transitivity, continuity and independence – then preferences are an expectation of a utility function.\nThe theory is a normative one and not necessarily descriptive. It suggests how a rational agent should formulate beliefs and preferences and not how they actually behave.\nExpected utility \\(U(P)\\) of a risky gamble is then\n\n\\[\nP \\succeq Q \\; \\; \\iff \\; \\; U (P) \\geq U (Q )\n\\]"
  },
  {
    "objectID": "02-utility.html#key-facts",
    "href": "02-utility.html#key-facts",
    "title": "Bayes AI",
    "section": "Key Facts",
    "text": "Key Facts\nThe two key facts then are uniqueness of probability and existence of expected utility. Formally,\n\nIf \\(P \\succeq R \\succeq Q\\) and \\(w P + (1 - w ) Q \\sim R\\) then \\(w\\) is unique.\nThere exists an expected utility \\(U(\\cdot )\\) such that \\(P \\succeq Q \\; \\; \\iff \\; \\; U (P) \\geq U (Q)\\). Furthermore \\[\nU \\left (w P + (1 - w ) Q \\right ) = wU (P) +(1 - w ) U(Q)\n\\] for any \\(P, Q\\) and \\(0 \\leq w \\leq 1\\).\n\nThis implies that \\(U\\) is additive and it is also unique up to affine transformation."
  },
  {
    "objectID": "02-utility.html#st.-petersburg-paradox",
    "href": "02-utility.html#st.-petersburg-paradox",
    "title": "Bayes AI",
    "section": "St. Petersburg Paradox",
    "text": "St. Petersburg Paradox\nWhat are you willing to pay to enter the following game?\n\nI toss a fair game and when the first head appears, on the \\(T\\)th toss, I pay you \\(\\$2^T\\) dollars.\nFirst, probability of first head on \\(T\\)th toss is \\(2^{-T}\\)\n\n\\[\n\\begin{aligned}\nE ( X) & = \\sum_{T=1}^{\\infty}  2^T 2^{-T} \\\\\n    & = 2 ( 1/2) + 4 (1/4) + 8(1/8) + \\ldots \\\\\n    & = 1 + 1 + 1+  \\ldots \\rightarrow \\infty\n\\end{aligned}\n\\] - Bernoulli (1754) constructed utility to value bets with \\(E( u(X) )\\).\nSome examples of utility functions are,\n\n\\(U(x) = V_0 (1-x^{-\\alpha})\\), \\(\\alpha &gt; 0\\), which gives an expected utility of \\(V_0 \\left(1-\\frac{1}{2^{\\alpha+1}-1}\\right)\\)\nLog utility, \\(U(x) = \\log(x)\\), with expected value \\(2 \\log(2)\\).\n\nNotice that after obtaining an expected utility value, you’ll have to find the corresponding reward/dollar amount."
  },
  {
    "objectID": "02-utility.html#attitudes-to-risk",
    "href": "02-utility.html#attitudes-to-risk",
    "title": "Bayes AI",
    "section": "Attitudes to Risk",
    "text": "Attitudes to Risk\nTwo gambles\n\nget \\(P_1\\) for sure\nget \\(P_2 = P_1+k\\) and \\(P_3 = P_1-k\\) with probability 1/2.\n\n\nRisk neutral: indifferent about fair bets, has linear utility.\nRisk averse: prefers certainty over fair bets, concave utility.\nRisk loving: prefers fair bets over certainty, convex utility.\n\nThen we will compare the utility of those gambles"
  },
  {
    "objectID": "02-utility.html#attitudes-to-risk-1",
    "href": "02-utility.html#attitudes-to-risk-1",
    "title": "Bayes AI",
    "section": "Attitudes to Risk",
    "text": "Attitudes to Risk\nThe solution depends on your risk preferences:\n\nRisk neutral: a risk neutral person is indifferent about fair bets.\nLinear Utility\nRisk averse: a risk averse person prefers certainty over fair bets. \\[\n\\mathbb{E}( U(X) ) &lt; U \\left ( \\mathbb{E}(X) \\right ) \\; .\n\\] Concave utility\nRisk loving: a risk loving person prefer fair bets over certainty.\n\nDepends on your preferences."
  },
  {
    "objectID": "02-utility.html#ellsberg-paradox",
    "href": "02-utility.html#ellsberg-paradox",
    "title": "Bayes AI",
    "section": "Ellsberg Paradox",
    "text": "Ellsberg Paradox\nProbability is counter-intuitive!!!\nTwo urns\n\n100 balls with 50 red and 50 blue.\nA mix of red and blue but you don’t know the proportion.\n\n\nWhich urn would you like to bet on?\nPeople don’t like the “uncertainty” about the distribution of red/blue balls in the second urn."
  },
  {
    "objectID": "02-utility.html#allais-paradox",
    "href": "02-utility.html#allais-paradox",
    "title": "Bayes AI",
    "section": "Allais Paradox",
    "text": "Allais Paradox\nYou have to make a choice between the following gambles\nFirst compare the “Gambles”\n\n\n\n\n\n\n\n\n\nExperiment 1\n\n\n\n\n\n\n\nGamble \\(G_1\\)\n\nGamble \\(G_2\\)\n\n\n\nWin\nChance\nWin\nChance\n\n\n$25m\n0\n$25m\n0.1\n\n\n$5m\n1\n$5m\n0.89\n\n\n$0m\n0\n$0m\n0.01\n\n\n\n\n\n\n\n\nExperiment 2\n\n\n\n\n\n\n\nGamble \\(G_3\\)\n\nGamble \\(G_4\\)\n\n\n\nWin\nChance\nWin\nChance\n\n\n$25\n0\n$25m\n0.1\n\n\n$5\n0.11\n$5m\n0\n\n\n$0m\n0.89\n$0m\n0.9\n\n\n\n\n\n\nIf \\(G_1 \\geq G_2\\) then \\(G_3 \\geq G_4\\) and vice-versa."
  },
  {
    "objectID": "02-utility.html#solution-expected-utility",
    "href": "02-utility.html#solution-expected-utility",
    "title": "Bayes AI",
    "section": "Solution: Expected Utility",
    "text": "Solution: Expected Utility\nGiven (subjective) probabilities \\(P = ( p_1 , p_2 , p_3 )\\). Write \\(E ( U | P )\\) for expected utility. W.l.o.g. set \\(u ( 0 ) = 0\\) and for the high prize set \\(u(\\$25 \\; {\\rm million} ) = 1\\). Which leaves one free parameter \\(u = u (\\$5 \\; {\\rm million} )\\).\n\nHence to compare gambles with probabilities \\(P\\) and \\(Q\\) we look at the difference \\[\nE ( u | P ) - E ( u | Q ) = ( p_2 - q_2 ) u + ( p_3 - q_3 )\n\\]\nFor comparing \\(G_1\\) and \\(G_2\\) we get \\[\n\\begin{aligned}\n  E ( u | G_1 ) - E ( u | G_2 ) &= 0.11 u - 0.1 \\\\\n  E ( u | G_3 ) - E ( u | G_4 ) &= 0.11 u - 0.1\n  \\end{aligned}\n\\] The order is the same, given your \\(u\\).\nIf your utility satisfies \\(u &lt; 0.1/0.11 = 0.909\\) you take the “riskier” gamble."
  },
  {
    "objectID": "02-utility.html#power-utility",
    "href": "02-utility.html#power-utility",
    "title": "Bayes AI",
    "section": "Power Utility",
    "text": "Power Utility\nPower and log-utilities\n\nConstant relative risk aversion (CRRA).\nAdvantage that the optimal rule is unaffected by wealth effects. The CRRA utility of wealth takes the form\n\n\\[\nU_\\gamma (W) = \\frac{ W^{1-\\gamma} -1 }{1-\\gamma}\n\\] - The special case \\(U(W) = \\log (W )\\) for \\(\\gamma = 1\\).\nThis leads to a myopic Kelly criterion rule."
  },
  {
    "objectID": "02-utility.html#kelly-criterion",
    "href": "02-utility.html#kelly-criterion",
    "title": "Bayes AI",
    "section": "Kelly Criterion",
    "text": "Kelly Criterion\nKelly Criterion corresponds to betting under binary uncertainty. - Consider a sequence of i.i.d. bets where\n\\[\np ( X_t = 1 ) = p \\; \\; {\\rm and} \\; \\; p ( X_t = -1 ) = q=1-p\n\\] The optimal allocation is \\(\\omega^\\star = p - q = 2 p - 1\\).\n\nMaximising the expected long-run growth rate leads to the solution\n\n\\[\n\\begin{aligned}\n\\max_\\omega \\mathbb{E} \\left ( \\ln ( 1 + \\omega W_T ) \\right )\n& = p \\ln ( 1 + \\omega ) + (1 -p) \\ln (1 - \\omega ) \\\\\n& \\leq p \\ln p + q \\ln q + \\ln 2 \\; {\\rm and} \\; \\omega^\\star = p - q\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "02-utility.html#kelly-criterion-1",
    "href": "02-utility.html#kelly-criterion-1",
    "title": "Bayes AI",
    "section": "Kelly Criterion",
    "text": "Kelly Criterion\n\nLet \\(p\\) denote the probability of a gain and \\(O = (1-p)/p\\) the odds. We can generalize the rule to the case of asymmetric payouts \\((a,b)\\) where\n\n\\[\np ( X_t = 1 ) = p \\; \\; {\\rm and} \\; \\; p ( X_t = -1 ) = q=1-p\n\\] - Then the expected utility function is\n\\[\np \\ln ( 1 + b \\omega ) + (1 -p) \\ln (1 - a \\omega )\n\\] - The optimal solution is\n\\[\n\\omega^\\star = \\frac{bp - a q}{ab} = \\frac{p-q}{\\sigma}\n\\]"
  },
  {
    "objectID": "02-utility.html#kelly-criterion-2",
    "href": "02-utility.html#kelly-criterion-2",
    "title": "Bayes AI",
    "section": "Kelly Criterion",
    "text": "Kelly Criterion\n\nIf \\(a=b=1\\) this reduces to the pure Kelly criterion.\nA common case occurs when \\(a=1\\) and market odds \\(b=O\\). The rule becomes \\[\n\\omega^\\star = \\frac{p \\cdot O  -q }{O}\n\\]\n\nTwo possible market opportunities: one where it offers you \\(4/1\\) when you have personal odds of \\(3/1\\) and a second one when it offers you \\(12/1\\) while you think the odds are \\(9/1\\).\nIn expected return these two scenarios are identical both offering a 33% gain.\nIn terms of maximizing long-run growth, however, they are not identical."
  },
  {
    "objectID": "02-utility.html#example-1",
    "href": "02-utility.html#example-1",
    "title": "Bayes AI",
    "section": "Example",
    "text": "Example\n\nTable below shows the Kelly criteria advises an allocation that is twice as much capital to the lower odds proposition: \\(1/16\\) weight versus \\(1/40\\).\n\n\n\n\nMarket\nYou\n\\(p\\)\n\\(\\omega^\\star\\)\n\n\n\n\n\\(4/1\\)\n\\(3/1\\)\n\\(1/4\\)\n\\(1/16\\)\n\n\n\\(12/1\\)\n\\(9/1\\)\n\\(1/10\\)\n\\(1/40\\)\n\n\n\n\nThe optimal allocation \\(\\omega^\\star = ( p O - q ) / O\\) is\n\n\\[\n\\frac{ (1/4) \\times 4 - (3/4) }{4} = \\frac{1}{16} \\; {\\rm and} \\;\n    \\frac{ (1/10) \\times 12 - (9/10) }{12} = \\frac{1}{40}\n\\]"
  },
  {
    "objectID": "02-utility.html#parrandos-paradoxes",
    "href": "02-utility.html#parrandos-paradoxes",
    "title": "Bayes AI",
    "section": "Parrando’s Paradoxes",
    "text": "Parrando’s Paradoxes\nTwo losing bets can be combined to a winner\n\n\n\n\n\n\n\nBernoulli market: \\(1+f\\) or \\(1-f\\) with \\(p=0.51\\) and \\(f = 0.05\\)\nPositive Expectation\nCaveat: Growth governed by the median/entropy \\[\n\\begin{aligned}\n  p \\log( 1 + f) & + (1-p)\\log(1-f) \\\\\n   & = -0.00025 &lt; 0\n  \\end{aligned}\n\\] Brownian Ratchets and cross-entropy of Markov processes\n\n\n\n\nTwo Losing Bets+Volatility"
  },
  {
    "objectID": "02-utility.html#parrando",
    "href": "02-utility.html#parrando",
    "title": "Bayes AI",
    "section": "Parrando",
    "text": "Parrando\n\n\n\n\n\n\n\n\n\nOptimal allocation \\(\\omega\\)\n\n\n\n\n\n\n\nEx Ante vs Ex Post Realisation"
  },
  {
    "objectID": "02-utility.html#breiman-kelly-merton-rule",
    "href": "02-utility.html#breiman-kelly-merton-rule",
    "title": "Bayes AI",
    "section": "Breiman-Kelly-Merton Rule",
    "text": "Breiman-Kelly-Merton Rule\nKelly Criterion: Optimal wager in binary setting \\[\n\\omega^\\star = \\frac{p \\cdot O  -q }{O}\n\\]\nMerton’s Rule: in continuous setting is Kelly \\[\n\\omega^\\star = \\frac{1}{\\gamma} \\frac{\\mu}{\\sigma^2}\n\\]\n\n\\(\\mu\\): (excess) expected return\n\\(\\sigma\\): volatility\n\\(\\gamma\\): risk aversion\n\\(\\omega^\\star\\): optimal position size\n\\(p= Prob(Up), q = Prob(Down), O = Odds\\)"
  },
  {
    "objectID": "02-utility.html#example-kelly-criterion-sp500",
    "href": "02-utility.html#example-kelly-criterion-sp500",
    "title": "Bayes AI",
    "section": "Example: Kelly Criterion S&P500:",
    "text": "Example: Kelly Criterion S&P500:\nConsider logarithmic utility (CRRA with \\(\\gamma=1\\)). This is a pure Kelly rule.\n\nWe assume iid log-normal stock returns with an annualized expected excess return of \\(5.7\\)% and a volatility of 16% which is consistent will long-run equity returns. In our continuous time formulation \\(\\omega^\\star = 0.057/0.16^2 = 2.22\\) and the Kelly criterion which imply that the investor borrows 122% of wealth to invest a total of 220% in stocks. This is a the risk-profile of the Kelly criterion.\nOne also sees that the allocation is highly sensitive to estimation error in \\(\\hat{\\mu}\\). We consider dynamic learning in a later section and show how the long horizon and learning affects the allocation today."
  },
  {
    "objectID": "02-utility.html#fractional-kelly",
    "href": "02-utility.html#fractional-kelly",
    "title": "Bayes AI",
    "section": "Fractional Kelly",
    "text": "Fractional Kelly\nThe fractional Kelly rule leads to a more realistic allocation.\n\nSuppose that \\(\\gamma = 3\\). Then the informational ratio is \\[\n\\frac{\\mu}{\\sigma^2} = \\frac{0.057}{0.16} = 0.357 \\; {\\rm and} \\;\n  \\omega^\\star = \\frac{1}{3} \\frac{0.057}{0.16^2} = 74.2\\%\n\\]\nAn investor with such a level of risk aversion then has a more reasonable \\(74.2\\)% allocation.\nThis analysis ignores the equilibrium implications. If every investor acted this way, then this would drive up prices and drive down the equity premium of \\(5.7\\)%."
  },
  {
    "objectID": "02-utility.html#rule",
    "href": "02-utility.html#rule",
    "title": "Bayes AI",
    "section": "60-40 Rule",
    "text": "60-40 Rule\n\n\n\n\n\n\n\n\n\n.\n\n\n\n\n\n\n\n."
  },
  {
    "objectID": "02-utility.html#keynes",
    "href": "02-utility.html#keynes",
    "title": "Bayes AI",
    "section": "Keynes",
    "text": "Keynes\nOptimal Bayes Rebalancing\n\n\n\n\n\n\n\n\n\nOptimal allocation \\(\\omega\\)\n\n\n\n\n\n\n\nKeynes vs Universal vs Cash"
  },
  {
    "objectID": "02-utility.html#winners-curse",
    "href": "02-utility.html#winners-curse",
    "title": "Bayes AI",
    "section": "Winner’s Curse",
    "text": "Winner’s Curse\nImmediately after you have win, you should feel a little regret!\nClaiming racehorse whose value is uncertain\n\n\n\nValue\nOutcome\n\n\n\n\n0\nhorse never wins\n\n\n50,000\nhorse improves\n\n\n\nSimple expected value tells you \\[\nE(X) = \\frac{1}{2} \\cdot 0 + \\frac{1}{2} \\cdot 50,000 = \\$25,000.\n\\] In a $20,000 claiming race (you can buy the horse for this fixed fee ahead of time from the owner) it looks like a simple decision to claim the horse.\nAsymmetric information!"
  },
  {
    "objectID": "02-utility.html#lemons-problem",
    "href": "02-utility.html#lemons-problem",
    "title": "Bayes AI",
    "section": "Lemon’s Problem",
    "text": "Lemon’s Problem\n\nAsymmetric information.\nProposed by George Akerlof in his 1970 paper “The Market for Lemons: Quality Uncertainty and the Market Mechanism.”\nThe lemons principle: low-value cars force high-value cars out of the market because of the asymmetrical information\nSeller does not know what the true value of a used car is and, not willing to pay a premium o\nSellers are not willing to sell below the premium price so this results in only lemons being sold."
  },
  {
    "objectID": "02-utility.html#lemons-problem-1",
    "href": "02-utility.html#lemons-problem-1",
    "title": "Bayes AI",
    "section": "Lemon’s Problem",
    "text": "Lemon’s Problem\n\nSuppose that a dealer pays $20K for a car and wants to sell for $25K, a lemon is only worth $5K.\nLet’s first suppose only 10% of cars are lemons, the customer’s calculations are \\[\nE (X)= \\frac{9}{10} \\cdot 25 + \\frac{1}{10} \\cdot 5 = \\$ 23 K\n\\]\nDealer is missing $2000. Therefore, they should try and persuade the customer its not a lemon by offering a warranty for example."
  },
  {
    "objectID": "02-utility.html#lemons-problem-2",
    "href": "02-utility.html#lemons-problem-2",
    "title": "Bayes AI",
    "section": "Lemon’s Problem",
    "text": "Lemon’s Problem\n\nThe more interesting case is when \\(p=0.5\\). The customer now values the car at \\[\nE (X)  = \\frac{1}{2} \\cdot 25 + \\frac{1}{2} \\cdot 5 = \\$ 15K\n\\]\nThis is lower than the $20K – the reservation price that the dealer would have for a good car. Now what type of car and at what price do they sell?\nAt $15K dealer is only willing to sell a lemon.\nBut then if the customer computes a conditional expectation \\[\nE ( X \\mid L ) = 1 \\cdot  5 = \\$ 5K\n\\] Therefore only lemons sell, at $ 5K, even if the dealer has a perfectly good car the customer is not willing to buy!\n\nAgain what should the dealer do?"
  },
  {
    "objectID": "02-utility.html#medical-testing",
    "href": "02-utility.html#medical-testing",
    "title": "Bayes AI",
    "section": "Medical Testing",
    "text": "Medical Testing\n\nA patient goes to see a doctor.\nThe doctor performs a test which is 95% sensitive – that is 95 percent of people who are sick test positive and 99% specific – that is 99 percent of the healthy people test negative.\nThe doctor also knows that only 1 percent of the people in the country are sick. Now the question is: if the patient tests positive, what are the chances the patient is sick? The intuitive answer is 99 percent, but the correct answer is 66 percent."
  },
  {
    "objectID": "02-utility.html#decision-trees-medical-testing",
    "href": "02-utility.html#decision-trees-medical-testing",
    "title": "Bayes AI",
    "section": "Decision Trees: Medical Testing",
    "text": "Decision Trees: Medical Testing\n\n\\(D=1\\) that indicates you have a disease\n\\(T=1\\) that indicates you tested positive\n\n\n\nCode\nflowchart LR\n  D[D] --&gt;|0.02| D1(D=1)\n  D --&gt;|0.98| D0(D=0)\n  D1 --&gt;|0.95| D1T1(T=1)\n  D1 --&gt;|0.05| D1T0(T=0)\n  D0 --&gt;|0.01| D0T1(T=1)\n  D0 --&gt;|0.99| D0T0(T=0)\n\n\n\n\n\n\nflowchart LR\n  D[D] --&gt;|0.02| D1(D=1)\n  D --&gt;|0.98| D0(D=0)\n  D1 --&gt;|0.95| D1T1(T=1)\n  D1 --&gt;|0.05| D1T0(T=0)\n  D0 --&gt;|0.01| D0T1(T=1)\n  D0 --&gt;|0.99| D0T0(T=0)\n\n\n\n\nFigure 1: Medical Diagnostics Decision Tree."
  },
  {
    "objectID": "02-utility.html#medical-testing-intuition",
    "href": "02-utility.html#medical-testing-intuition",
    "title": "Bayes AI",
    "section": "Medical Testing: Intuition",
    "text": "Medical Testing: Intuition\n\nImagine that the above story takes place in a small town, with \\(1,000\\) people.\nPrior: 20 people, are sick, and 980 are healthy.\nAminister the test to everyone: 19 of the 20 sick people test positive, 9.8 of the healthy people test positive, we round it to 10.\nNow if the doctor sends everyone who tests positive to the national hospital, there will be 10 healthy and 19 sick patients. 1 to 2 ratio or 66 percent of the patients are healthy."
  },
  {
    "objectID": "02-utility.html#medical-testing-with-utility",
    "href": "02-utility.html#medical-testing-with-utility",
    "title": "Bayes AI",
    "section": "Medical Testing: With utility",
    "text": "Medical Testing: With utility\n\nThe decision problem is to treat \\(a_T\\) or not to treat \\(a_N\\)\n\n\nUtility of the test and the treatment.\n\n\nA/S\n\\(a_T\\)\n\\(a_N\\)\n\n\n\n\n\\(D_0\\)\n90\n100\n\n\n\\(D_1\\)\n90\n0\n\n\n\nThen expected (unconditional) utility of the treatment is 90 and no treatment is 98. A huge difference. Given our prior knowledge, we should not treat everyone.\nHow does the utility will change when our probability of disease changes?"
  },
  {
    "objectID": "02-utility.html#medical-testing-with-utility-1",
    "href": "02-utility.html#medical-testing-with-utility-1",
    "title": "Bayes AI",
    "section": "Medical Testing: With utility",
    "text": "Medical Testing: With utility\n\n\nCode\np = seq(0,1,0.01)\nplot(p, 100*(1-p), type = \"l\", xlab = \"p\", ylab = \"$E[U(a)]$\")\nabline(h=90, col=\"red\")\nlegend(\"bottomleft\", legend = c(\"$E[U(a_N)]$\", \"$E[U(a_T)]$\"), col = c(\"black\", \"red\"), lty = 1, bty='n')\n\n\n\nExpected utility of the treatment and no treatment as a function of the prior probability of disease.The crossover point is. \\[\n100(1-p) = 90, ~p = 0.1\n\\]"
  },
  {
    "objectID": "02-utility.html#medical-testing-with-utility-2",
    "href": "02-utility.html#medical-testing-with-utility-2",
    "title": "Bayes AI",
    "section": "Medical Testing: With utility",
    "text": "Medical Testing: With utility\nThe gap of of \\(0.9-100(1-p)\\) is the expected gain from treatment.\n\n\nCode\nplot(p, 90-100*(1-p), type = \"l\", xlab = \"p\", ylab = \"Utility gain from treatment\")"
  },
  {
    "objectID": "02-utility.html#medical-testing-the-value-of-test",
    "href": "02-utility.html#medical-testing-the-value-of-test",
    "title": "Bayes AI",
    "section": "Medical Testing: The value of test",
    "text": "Medical Testing: The value of test\nWe will need to calculate the posterior probabilities\n\n\nCode\n# P(D | T = 0) = P(T = 0 | D) P(D) / P(T = 0)\npdt0 = 0.05*0.02/(0.05*0.02 + 0.99*0.98) \nprint(pdt0)\n\n\n[1] 0.001029654\n\n\nCode\n# Expected utility given the test is negative \n# E[U(a_N | T=0)]\nUN0 = pdt0*0 + (1-pdt0)*100\nprint(UN0)\n\n\n[1] 99.89703\n\n\nCode\n# E[U(a_T | T=0)]\nUT0 = pdt0*90 + (1-pdt0)*90\nprint(UT0)\n\n\n[1] 90\n\n\nGiven test is negative, our best action is not to treat. Our utility is 100. What if the test is positive?"
  },
  {
    "objectID": "02-utility.html#medical-testing-the-value-of-test-1",
    "href": "02-utility.html#medical-testing-the-value-of-test-1",
    "title": "Bayes AI",
    "section": "Medical Testing: The value of test",
    "text": "Medical Testing: The value of test\n\n\nCode\n# P(D | T = 1) = P(T = 1 | D) P(D) / P(T = 1)\npdt = 0.95*0.02/(0.95*0.02 + 0.01*0.98)\nprint(pdt)\n\n\n[1] 0.6597222\n\n\nCode\n# E[U(a_N | T=1)]\nUN1 = pdt*0 + (1-pdt)*100\nprint(UN1)\n\n\n[1] 34.02778\n\n\nCode\n# E[U(a_T | T=1)]\nUT1 = pdt*90 + (1-pdt)*90\nprint(UT1)\n\n\n[1] 90\n\n\nThe best option is to treat now! Given the test our strategy is to treat if the test is positive and not treat if the test is negative."
  },
  {
    "objectID": "02-utility.html#medical-testing-the-value-of-test-2",
    "href": "02-utility.html#medical-testing-the-value-of-test-2",
    "title": "Bayes AI",
    "section": "Medical Testing: The value of test",
    "text": "Medical Testing: The value of test\nLet’s calculate the expected utility of this strategy.\n\n\nCode\n# P(T=1) = P(T=1 | D) P(D) + P(T=1 | D=0) P(D=0)\npt = 0.95*0.02 + 0.01*0.98\nprint(pt)\n\n\n[1] 0.0288\n\n\nCode\n# P(T=0) = P(T=0 | D) P(D) + P(T=0 | D=0) P(D=0)\npt0 = 0.05*0.02 + 0.99*0.98\nprint(pt0)\n\n\n[1] 0.9712\n\n\nCode\n# Expected utility of the strategy\npt*UT1 + pt0*UN0\n\n\n[1] 99.612\n\n\nThe utility of out strategy of 100 is above of the strategy prior to testing (98), this difference of 2 is called the value of information."
  },
  {
    "objectID": "02-utility.html#nash-equilibrium",
    "href": "02-utility.html#nash-equilibrium",
    "title": "Bayes AI",
    "section": "Nash Equilibrium",
    "text": "Nash Equilibrium\n\nWhen multiple decision makers interact with each other, meaning the decision of one player changes the state of the “world” and thus affects the decision of another player - Nash equilibrium: a set of strategies where no player can improve their payoff by unilaterally changing their strategy, assuming others keep their strategies constant.\nNo player has an incentive to deviate from their current strategy, given the strategies of the other players."
  },
  {
    "objectID": "02-utility.html#nash-equilibrium-1",
    "href": "02-utility.html#nash-equilibrium-1",
    "title": "Bayes AI",
    "section": "Nash Equilibrium",
    "text": "Nash Equilibrium\n\nPrisoner’s Dilemma: Two prisoners must decide whether to cooperate with each other or defect. The Nash equilibrium is for both to defect, even though they would be better off if they both cooperated.\nPricing Strategies: Firms in a market choose prices to maximize profits, taking into account their competitors’ pricing decisions. The equilibrium is the set of prices where no firm can increase profits by changing its price unilaterally.\nTraffic Flow: Drivers choose routes to minimize travel time, based on their expectations of other drivers’ choices. The equilibrium is the pattern of traffic flow where no driver can reduce their travel time by choosing a different route."
  },
  {
    "objectID": "02-utility.html#marble-game",
    "href": "02-utility.html#marble-game",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nTwo players \\(A\\) and \\(B\\) have both a red and a blue marble. They present one marble to each other. The payoff table is as follows:\n\nIf both present red, \\(A\\) wins $3.\nIf both present blue, \\(A\\) wins $1.\nIf the colors do not match, \\(B\\) wins $2\n\nThe tit-for-tat strategy, where you cooperate until your opponent defects. Then you match his last response."
  },
  {
    "objectID": "02-utility.html#marble-game-1",
    "href": "02-utility.html#marble-game-1",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nNash equilibrium will also allow us to study the concept of a randomized strategy (ie. picking a choice with a certain probability) which turns out to be optimal in many game theory problems.\nFirst, assume that the players have a \\(\\frac{1}{2}\\) probability of playing Red or Blue. Thus each player has the same expected payoff \\(E(A) = 1\\) \\[\\begin{align*}\n    E(A) &= \\frac{1}{4} \\cdot 3 + \\frac{1}{4} \\cdot 1 =1 \\\\\n    E(B) &= \\frac{1}{4} \\cdot 2 + \\frac{1}{4} \\cdot 2 =1\n\\end{align*}\\]"
  },
  {
    "objectID": "02-utility.html#marble-game-2",
    "href": "02-utility.html#marble-game-2",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nWe might go one step further and look at the risk (and measured by a standard deviation) and calculate the variances of each players payout \\[\\begin{align*}\n    Var (A) & = (1-1)^2 \\cdot \\frac{1}{4} +(3-1)^2 \\cdot \\frac{1}{4} + (0-1)^2 \\cdot \\frac{1}{2} = 1.5 \\\\\n    Var(B) & = 1^2 \\cdot \\frac{1}{2} + (2-1)^2 \\cdot \\frac{1}{2} = 1\n\\end{align*}\\] Therefore, under this scenario, if you are risk averse, player \\(B\\) position is favored."
  },
  {
    "objectID": "02-utility.html#marble-game-3",
    "href": "02-utility.html#marble-game-3",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nThe matrix of probabilities with equally likely choices is given by\n\n\n\n\\(A,B\\)\nProbability\n\n\n\n\n\\(P( red, red )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( red, blue )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( blue, red )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( blue, blue )\\)\n(1/2)(1/2)=1/4\n\n\n\nNow they is no reason to assume ahead of time that the players will decide to play \\(50/50\\). We will show that there’s a mixed strategy (randomized) that is a Nash equilibrium that is, both players won’t deviate from the strategy."
  },
  {
    "objectID": "02-utility.html#marble-game-4",
    "href": "02-utility.html#marble-game-4",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nWe’ll prove that the following equilibrium happens:\n\n\\(A\\) plays Red with probability 1/2 and blue 1/2\n\\(B\\) plays Red with probability 1/4 and blue 3/4\n\nIn this case the expected payoff to playing Red equals that of playing Blue for each player. We can simply calculate: \\(A\\)’s expected payoff is 3/4 and \\(B\\)’s is $1 \\[\nE(A) = \\frac{1}{8} \\cdot 3 + \\frac{3}{8} \\cdot 1 = \\frac{3}{4}\n\\] Moreover, \\(E(B) =1\\), thus \\(E(B) &gt; E(A)\\). We see that \\(B\\) is the favored position. It is simple that if I know that you are going to play this strategy and vice-versa, neither of us will deviate from this strategy – hence the Nash equilibrium concept."
  },
  {
    "objectID": "02-utility.html#marble-game-5",
    "href": "02-utility.html#marble-game-5",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nNash equilibrium probabilities are: \\(p=P( A \\; red )= 1/2, p_1 = P( B \\; red ) = 1/4\\) with payout matrix\n\n\n\n\\(A,B\\)\nProbability\n\n\n\n\n\\(P( red, red )\\)\n(1/2)(1/4)=1/8\n\n\n\\(P( red, blue )\\)\n(1/2)(3/4)=3/8\n\n\n\\(P( blue, red )\\)\n(1/2)(1/4)=1/8\n\n\n\\(P( blue, blue )\\)\n(1/2)(3/4)=3/8\n\n\n\nWe have general payoff probabilities: \\(p=P( A \\; red ), p_1 = P( B \\; red )\\)\n\\[\\begin{align*}\n    f_A ( p , p_1 ) =& 3 p p_1 + ( 1 -p ) ( 1 - p_1 ) \\\\\n    f_B ( p , p_1 ) =& 2 \\{ p(1 - p_1) + ( 1 -p ) p_1 \\}\n\\end{align*}\\]"
  },
  {
    "objectID": "02-utility.html#marble-game-6",
    "href": "02-utility.html#marble-game-6",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nTo find the equilibrium point \\[\\begin{align*}\n    ( \\partial / \\partial p ) f_A ( p , p_1 ) =& 3 p_1 - ( 1 - p_1 ) = 4 p_1 -1 \\; \\; \\mathrm{so} \\; \\; p_1= 1/4 \\\\\n    ( \\partial / \\partial p_1 ) f_B ( p , p_1 ) =& 2 ( 1 - 2p ) \\; \\; \\mathrm{so} \\; \\; p= 1/2\n\\end{align*}\\]\nMuch research has been directed to repeated games versus the one-shot game and is too large a topic to discuss further.\n What are the drawbacks of the equilibrium analysis?"
  },
  {
    "objectID": "01-intro.html#random-facts",
    "href": "01-intro.html#random-facts",
    "title": "Bayes AI",
    "section": "Random facts",
    "text": "Random facts\nOn this Day (January 27):\n\n1888: The National Geographic Society is founded in Washington, D.C.\n1945: The Red Army liberates the Auschwitz-Birkenau concentration camp\n1967: The United States, United Kingdom, and Soviet Union sign the Outer Space Treaty in Washington, D.C.\n1973: The Paris Peace Accords officially end the Vietnam War.\n2010: Apple Inc. unveils the iPad."
  },
  {
    "objectID": "01-intro.html#the-first-thoughts-about-artificial-intelligence",
    "href": "01-intro.html#the-first-thoughts-about-artificial-intelligence",
    "title": "Bayes AI",
    "section": "The first thoughts about artificial intelligence",
    "text": "The first thoughts about artificial intelligence\n\n\n\n\n\n\n\nHephaestus created for himself Android robots, such as a giant human-like robot of Talos.\nPygmalion revived Galatea.\nJehovah and Allah - pieces of clay\nParticularly pious and learned rabbis rabbis could create golems.\nAlbert the Great made an artificial speaking head (which very upset Thomas Aquinas)."
  },
  {
    "objectID": "01-intro.html#mechanical-machines",
    "href": "01-intro.html#mechanical-machines",
    "title": "Bayes AI",
    "section": "Mechanical machines",
    "text": "Mechanical machines\nRobots and Automatic Machines Were Generally Very Inventive: Al-Jazari (XII Century)\n\nHesdin Castle (Robert II of Artois), Leonardo’s robot…"
  },
  {
    "objectID": "01-intro.html#mechanical-machines-1",
    "href": "01-intro.html#mechanical-machines-1",
    "title": "Bayes AI",
    "section": "Mechanical machines",
    "text": "Mechanical machines\nJaquet-Droz automata (XVIII century):"
  },
  {
    "objectID": "01-intro.html#mechanical-machines-2",
    "href": "01-intro.html#mechanical-machines-2",
    "title": "Bayes AI",
    "section": "Mechanical machines",
    "text": "Mechanical machines\n\nBut this is in mechanics, in mathematics/logic AI it was quite rudimentary for a long time\n\n\nLogic machine of Ramon Llull (XIII-XIV centuries)\nStarting with Dr. Frankenstein, further AI in the literature appears constantly …"
  },
  {
    "objectID": "01-intro.html#turing-test",
    "href": "01-intro.html#turing-test",
    "title": "Bayes AI",
    "section": "Turing Test",
    "text": "Turing Test\n\nAI as a science begins with a Turing test (1950).\nThe ides of the Turing test is to check if a machine can imitate a human in a conversation.\nThe original formulation was more nuanced."
  },
  {
    "objectID": "01-intro.html#shennons-theseus",
    "href": "01-intro.html#shennons-theseus",
    "title": "Bayes AI",
    "section": "Shennon’s Theseus",
    "text": "Shennon’s Theseus\n\nYouTube Video\nEarly 1950s, Claude Shannon (The father of Information Theory) demonstrates Theseus\nA life-sized magnetic mouse controlled by relay circuits, learns its way around a maze."
  },
  {
    "objectID": "01-intro.html#stanford-cart",
    "href": "01-intro.html#stanford-cart",
    "title": "Bayes AI",
    "section": "Stanford Cart",
    "text": "Stanford Cart\n\n\nYouTube Video\nTakes 2.6-second for signal to travel from earth to the moon\nLatest iterations is automated with 3D vision capabilities\nPause after each meter of movement and take 10-15 minutes to reassess its surroundings and reevaluate its decided path.\nIn 1979, this cautious version of the cart successfully made its way 20 meters through a chair-strewn room in five hours without human intervention."
  },
  {
    "objectID": "01-intro.html#turing-test-1",
    "href": "01-intro.html#turing-test-1",
    "title": "Bayes AI",
    "section": "Turing Test",
    "text": "Turing Test\nIt takes a lot to create an AI system:\n\nProcessing of a natural language\nSensors and actuators\nRepresentation of knowledge\nInference from the existing knowledge\nTraining on experience (Machine Learning)."
  },
  {
    "objectID": "01-intro.html#dartmouth-workshop",
    "href": "01-intro.html#dartmouth-workshop",
    "title": "Bayes AI",
    "section": "Dartmouth workshop",
    "text": "Dartmouth workshop\n\nAI as a science appeared in 1956 at the Dartmouth workshop.\nIt was organized by John McCarthy, Marvin Minsky, Claude Shennon and Nathaniel Rochester.\nIt was probably the most ambitious grant proposal in the history of computer science."
  },
  {
    "objectID": "01-intro.html#dartmouth-workshop-1",
    "href": "01-intro.html#dartmouth-workshop-1",
    "title": "Bayes AI",
    "section": "Dartmouth workshop",
    "text": "Dartmouth workshop\n We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer."
  },
  {
    "objectID": "01-intro.html#great-hopes",
    "href": "01-intro.html#great-hopes",
    "title": "Bayes AI",
    "section": "1956-1960: Great hopes",
    "text": "1956-1960: Great hopes\n\nOptimistic time. It seemed a that we were almost there…\nAllen Newell, Herbert A. Simon, and Cliff Shaw: Logic Theorist.\nAutomated reasoning.\nIt was able to prof most of the Principia Mathematica, in some places even more elegant than Russell and Whitehead."
  },
  {
    "objectID": "01-intro.html#big-hopes",
    "href": "01-intro.html#big-hopes",
    "title": "Bayes AI",
    "section": "1956-1960: Big Hopes",
    "text": "1956-1960: Big Hopes\n\nGeneral Problem Solver - a program that tried to think as a person\nA lot of programs that have been able to do some limited things (MicroWorlds):\n\nAnalogy (IQ tests with multiple choice questions)\nStudent (algebraic verbal tasks)\nBlocks World (rearranged 3D blocks)."
  },
  {
    "objectID": "01-intro.html#sknowledge-based-systems",
    "href": "01-intro.html#sknowledge-based-systems",
    "title": "Bayes AI",
    "section": "1970s:Knowledge Based Systems",
    "text": "1970s:Knowledge Based Systems\n\nThe bottom line: to accumulate a fairly large set of rules and knowledge about the subject area, then draw conclusions.\nFirst success: MYCIN - Diagnosis of blood infections:\n\nabout 450 rules\nThe results are like an experienced doctor and significantly better than beginner doctors."
  },
  {
    "objectID": "01-intro.html#commercial-applications-industry-ai",
    "href": "01-intro.html#commercial-applications-industry-ai",
    "title": "Bayes AI",
    "section": "1980-2010: Commercial applications Industry AI",
    "text": "1980-2010: Commercial applications Industry AI\n\nThe first AI department was at Dec (Digital Equipment Corporation). It is argued that by 1986 he saved the Dec about  $ 10 million per year.\nThe boom ended by the end of the 80s, when many companies could not live up to high expectations."
  },
  {
    "objectID": "01-intro.html#data-mining-machine-learning",
    "href": "01-intro.html#data-mining-machine-learning",
    "title": "Bayes AI",
    "section": "1990-2010: DATA MINING, MACHINE LEARNING",
    "text": "1990-2010: DATA MINING, MACHINE LEARNING\n\nIn recent decades, the main emphasis has shifted to machine training and search for patterns in the data.\nEspecially - with the development of the Internet.\nNot too many people remember the original AI ideas, but Machine Learning is now everywhere.\nBut Robotics flourishes and uses Machine Learning at every step."
  },
  {
    "objectID": "01-intro.html#rule-based-system-vs-bayes",
    "href": "01-intro.html#rule-based-system-vs-bayes",
    "title": "Bayes AI",
    "section": "Rule-Based System vs Bayes",
    "text": "Rule-Based System vs Bayes\n\nSince 1956, the field of artificial intelligence (AI) has undergone significant transformations\ntraditional AI was mostly focused on rule-based systems and boolean logic programming, with limited learning capabilities. It lead to them being brittle in changing environments.\nOn the other hand, emerging AI is focused on modeling uncertainties, pattern matching, and deep learning.\nAll of those are data-driven approaches.\nThese approaches are more adaptable and can handle complex and unstructured data. They are also more data-dependent and lack interpretability."
  },
  {
    "objectID": "01-intro.html#rule-based-system-vs-bayes-1",
    "href": "01-intro.html#rule-based-system-vs-bayes-1",
    "title": "Bayes AI",
    "section": "Rule-Based System vs Bayes",
    "text": "Rule-Based System vs Bayes\n\n\n\n\n\n\n\n\n\nOld AI\n\n\n If rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n\n \n\n\n\n\n\nNew AI\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\n\n\nBayesian approach is a powerful statistical framework based on the work of Thomas Bayes and later Laplace.\nIt provides a probabilistic approach to reasoning and learning\nAllowing us to update our beliefs about the world as we gather new data.\nThis makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information."
  },
  {
    "objectID": "01-intro.html#definition",
    "href": "01-intro.html#definition",
    "title": "Bayes AI",
    "section": "DEFINITION",
    "text": "DEFINITION\n\nHow to determine “learning”?\n\n\n\n\nDefinition:\n\n\nThe computer program learns as the data is accumulating relative to a certain problem class \\(T\\) and the target function of \\(P\\) if the quality of solving these problems (relative to \\(P\\)) improves with gaining new experience.\n\n\n\n\nThe definition is very (too?) General.\nWhat specific examples can be given?"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml",
    "href": "01-intro.html#tasks-and-concepts-of-ml",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML",
    "text": "Tasks and concepts of ML"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-supervised-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-supervised-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: Supervised Learning",
    "text": "Tasks and concepts of ML: Supervised Learning\n\ntraining sample – a set of examples, each of which consists of input features (attributes) and the correct “answers” - the response variable\nLearn a rule that maps input features to the response variable\nThen this rule is applied to new examples (deployment)\nThe main thing is to train a model that explains not only examples from the training set, but also new examples (generalizes)\nOtherwise - overfitting"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: unsupervised learning",
    "text": "Tasks and concepts of ML: unsupervised learning\nThere are no correct answers, only data, e.g. clustering:\n\nWe need to divide the data into pre -unknown classes to some extent similar:\n\nhighlight the family of genes from the sequences of nucleotides\ncluster users and personalize the application for them\ncluster the mass spectrometric image to parts with different composition"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning-1",
    "href": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning-1",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: unsupervised learning",
    "text": "Tasks and concepts of ML: unsupervised learning\n\nDimensionality reduction: data have a high dimension, it is necessary to reduce it, select the most informative features so that all of the above algorithms can work\nMatrix Competition: There is a sparse matrix, we must predict what is in the missing positions.\nAnomaly detection: find anomalies in the data, e.g. fraud detection.\nOften the outputs answers are given for a small part of the data, then we call it semi -supervised Learning."
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-reinforcement-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-reinforcement-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: reinforcement learning",
    "text": "Tasks and concepts of ML: reinforcement learning\n\nMulti-armed bandits: there is a certain set of actions, each of which leads to random results, you need to get as much rewards possible\nExploration vs.Exploitation: how and when to proceed from the study of the new to use what has already studied\nCredit Assignment: You get rewarded at the very end (won the game), and we must somehow distribute this reward on all the moves that led to victory."
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-active-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-active-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: active learning",
    "text": "Tasks and concepts of ML: active learning\n\nActive Learning - how to choose the following (relatively expensive) test\nBoosting - how to combine several weak classifiers so that it turns out good\nModel Selection - where to draw a line between models with many parameters and with a few.\nRanking: response list is ordered (internet search)"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai",
    "href": "01-intro.html#tasks-and-concepts-of-ai",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI",
    "text": "Tasks and concepts of AI"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-reasoning",
    "href": "01-intro.html#tasks-and-concepts-of-ai-reasoning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Reasoning",
    "text": "Tasks and concepts of AI: Reasoning\n\nBayesian networks: given conditional probabilities, calculate the probability of the event\no1 by OpenAI: a family of AI models that are designed to perform complex reasoning tasks, such as math, coding, and science. o1 models placed among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME)\nGemini 2.0: model for the agentic era"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-representation",
    "href": "01-intro.html#tasks-and-concepts-of-ai-representation",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Representation",
    "text": "Tasks and concepts of AI: Representation\n\nKnowledge Graphs: a graph database that uses semantic relationships to represent knowledge\nEmbeddings: a way to represent data in a lower-dimensional space\nTransformers: a deep learning model that uses self-attention to process sequential data"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-generation",
    "href": "01-intro.html#tasks-and-concepts-of-ai-generation",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\nIn shadows of data, uncertainty reigns,\nBayesian whispers, where knowledge remains.\nWith prior beliefs, we start our quest,\nUpdating with evidence, we strive for the best.\nA dance of the models, predictions unfold,\nInferences drawn, from the new and the old.\nThrough probabilities, we find our way,\nIn the world of AI, it’s the Bayesian sway.\nSo gather your data, let prior thoughts flow,\nIn the realm of the unknown, let your insights grow.\nFor in this approach, with each little clue,\nWe weave understanding, both rich and true.\nMusic"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-generation-1",
    "href": "01-intro.html#tasks-and-concepts-of-ai-generation-1",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\n\nfrom openai import OpenAI\nclient = OpenAI(api_key=\"your-api-key\")\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"a hockey player trying to understand the Bayes rule\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1,\n)\n\nprint(response.data[0].url)"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-generation-2",
    "href": "01-intro.html#tasks-and-concepts-of-ai-generation-2",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\nA humorous and illustrative scene of a hockey player sitting on a bench in full gear, holding a hockey stick in one hand and a whiteboard marker in th"
  },
  {
    "objectID": "01-intro.html#chess-and-ai",
    "href": "01-intro.html#chess-and-ai",
    "title": "Bayes AI",
    "section": "Chess and AI",
    "text": "Chess and AI\nOld AI: Deep Blue (1997) vs. Garry Kasparov\n\nKasparov vs IBM’s DeepBlue in 1997"
  },
  {
    "objectID": "01-intro.html#alphago-zero",
    "href": "01-intro.html#alphago-zero",
    "title": "Bayes AI",
    "section": "AlphaGo Zero",
    "text": "AlphaGo Zero\n\nRemove all human knowledge from training process - only uses self play,\nTakes raw board as input and neural network predicts the next move.\nUses Monte Carlo tree search to evaluate the position.\nThe algorithm was able to beat AlphaGo 100-0. The algorithm was then used to play chess and shogi and was able to beat the best human players in those games as well.\n\n\nAlpha GO vs Lee Sedol: Move 37 by AlphaGo in Game Two"
  },
  {
    "objectID": "01-intro.html#probability-in-machine-learning",
    "href": "01-intro.html#probability-in-machine-learning",
    "title": "Bayes AI",
    "section": "Probability in machine learning",
    "text": "Probability in machine learning\n\nIn all methods and approaches, it is useful not only generate an answer, but also evaluate how confident in this answer, how well the model describes the data, how these values ​​will change in further experiments, etc.\nTherefore, the central role in machine learning is played by the theory of probability - and we will also actively use it."
  },
  {
    "objectID": "01-intro.html#references",
    "href": "01-intro.html#references",
    "title": "Bayes AI",
    "section": "References",
    "text": "References\n\nChristopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2007.\nKevin Murphy, Machine Learning: A Probabilistic Perspective, MIT Press, 2013.\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd ed., Springer, 2009."
  },
  {
    "objectID": "01-intro.html#probability",
    "href": "01-intro.html#probability",
    "title": "Bayes AI",
    "section": "Probability",
    "text": "Probability\nSubjective Probability (de Finetti, Ramsey, Savage, von Neumann, ... )\nPrinciple of Coherence:\nA set of subjective probability beliefs must avoid sure loss\n\nIf an event \\(A\\) is certain to occur, it has probability 1\nEither an event \\(A\\) occurs or it does not. \\[\nP(A) = 1 - P(\\mbox{not }A)\n\\]\nIf two events are mutually exclusive (both cannot occur simultaneously) then \\[\nP(A \\mbox{ or } B) = P(A) + P(B)\n\\]\nJoint probability, when events are independent \\[\nP(A \\mbox{ and } B) = P( A) P(B)\n\\]"
  },
  {
    "objectID": "01-intro.html#conditional-joint-and-marginal-distributions",
    "href": "01-intro.html#conditional-joint-and-marginal-distributions",
    "title": "Bayes AI",
    "section": "Conditional, Joint and Marginal Distributions",
    "text": "Conditional, Joint and Marginal Distributions\nUse probability to describe outcomes involving more than one variable at a time. Need to be able to measure what we think will happen to one variable relative to another\nIn general the notation is ...\n\n\\(P(X=x, Y=y )\\) is the joint probability that \\(X =x\\) and \\(Y=y\\)\n\\(P(X=x  \\mid  Y=y )\\) is the conditional probability that \\(X\\) equals \\(x\\) given \\(Y=y\\)\n\\(P(X=x)\\) is the marginal probability of \\(X=x\\)"
  },
  {
    "objectID": "01-intro.html#conditional-joint-and-marginal-distributions-1",
    "href": "01-intro.html#conditional-joint-and-marginal-distributions-1",
    "title": "Bayes AI",
    "section": "Conditional, Joint and Marginal Distributions",
    "text": "Conditional, Joint and Marginal Distributions\nRelationship between the joint and conditional ... \\[\n\\begin{aligned}\nP(x,y) & = P(x) P(y \\mid x) \\\\\n& =  P(y) P(x \\mid y)\n\\end{aligned}\n\\]\nRelationship between the joint and marginal ... \\[\n\\begin{aligned}\nP(x) & = \\sum_y P(x,y) \\\\\nP(y) & =  \\sum_x P(x,y)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-rule",
    "href": "01-intro.html#bayes-rule",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nThe computation of \\(P(x \\mid y)\\) from \\(P(x)\\) and \\(P(y \\mid x)\\) is called Bayes theorem ... \\[\nP(x \\mid y) = \\frac{P(y,x)}{P(y)} = \\frac{P(y,x)}{\\sum_x P(y,x)} = \\frac{P(y \\mid x)P(x)}{\\sum_x P(y \\mid x)P(x)}\n\\]\nThis shows now the conditional distribution is related to the joint and marginal distributions.\nYou’ll be given all the quantities on the r.h.s."
  },
  {
    "objectID": "01-intro.html#bayes-rule-1",
    "href": "01-intro.html#bayes-rule-1",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nKey fact: \\(P(x \\mid y)\\) is generally different from \\(P(y \\mid x)\\)!\nExample: Most people would agree \\[\n\\begin{aligned}\nPr  & \\left ( Practice \\; hard  \\mid  Play \\; in \\; NBA \\right ) \\approx  1\\\\\nPr  & \\left ( Play \\; in \\; NBA  \\mid  Practice \\; hard  \\right ) \\approx  0\n\\end{aligned}\n\\]\nThe main reason for the difference is that \\(P( Play \\; in \\; NBA ) \\approx 0\\)."
  },
  {
    "objectID": "01-intro.html#independence",
    "href": "01-intro.html#independence",
    "title": "Bayes AI",
    "section": "Independence",
    "text": "Independence\nTwo random variable \\(X\\) and \\(Y\\) are independent if \\[\nP(Y = y  \\mid X = x) = P (Y = y)\n\\] for all possible \\(x\\) and \\(y\\) values. Knowing \\(X=x\\) tells you nothing about \\(Y\\)!\nExample: Tossing a coin twice. What’s the probability of getting \\(H\\) in the second toss given we saw a \\(T\\) in the first one?"
  },
  {
    "objectID": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models",
    "href": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models",
    "title": "Bayes AI",
    "section": "Bookies vs Betters: The Battle of Probabilistic Models",
    "text": "Bookies vs Betters: The Battle of Probabilistic Models\n\nimageSource: The Secret Betting Strategy That Beats Online Bookmakers"
  },
  {
    "objectID": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models-1",
    "href": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models-1",
    "title": "Bayes AI",
    "section": "Bookies vs Betters: The Battle of Probabilistic Models",
    "text": "Bookies vs Betters: The Battle of Probabilistic Models\n\nBookies set odds that reflect their best guess on probabilities of a win, draw, or loss. Plus their own margin\nBookies have risk aversion bias. When many people bet for an underdog (more popular team)\nBookies hedge their bets by offering more favorable odds to the opposed team\nSimple algorithm: calculate average odds across many bookies and find outliers with large deviation from the mean"
  },
  {
    "objectID": "01-intro.html#odds-oddschecker",
    "href": "01-intro.html#odds-oddschecker",
    "title": "Bayes AI",
    "section": "Odds: Oddschecker",
    "text": "Odds: Oddschecker\nWe can express probabilities in terms of Odds via \\[\nO(A) = \\frac{ 1- P(A) }{ P(A) }\n\\; \\; {\\rm or} \\; \\; P(A) = \\frac{ 1 }{ 1 + O(A) }\n\\]\n\nFor example if \\(O(A) = 1\\) then for ever $1 bet you will payout $1. An event with probability \\(\\frac{1}{2}\\).\nIf \\(O(A) = 2\\) or \\(2:1\\), then for a $1 bet you’ll payback $3.\n\nIn terms of probability \\(P = \\frac{1}{3}\\)."
  },
  {
    "objectID": "01-intro.html#envelope-paradox",
    "href": "01-intro.html#envelope-paradox",
    "title": "Bayes AI",
    "section": "Envelope Paradox",
    "text": "Envelope Paradox\nThe following problem is known as the “exchange paradox”.\n\nA swami puts \\(m\\) dollars in one envelope and \\(2 m\\) in another. He hands on envelope to you and one to your opponent.\nThe amounts are placed randomly and so there is a probability of \\(\\frac{1}{2}\\) that you get either envelope.\nYou open your envelope and find \\(x\\) dollars. Let \\(y\\) be the amount in your opponent’s envelope."
  },
  {
    "objectID": "01-intro.html#envelope-paradox-1",
    "href": "01-intro.html#envelope-paradox-1",
    "title": "Bayes AI",
    "section": "Envelope Paradox",
    "text": "Envelope Paradox\nYou know that \\(y = \\frac{1}{2} x\\) or \\(y = 2 x\\). You are thinking about whether you should switch your opened envelope for the unopened envelope of your friend. It is tempting to do an expected value calculation as follows \\[\nE( y) = \\frac{1}{2} \\cdot  \\frac{1}{2} x + \\frac{1}{2} \\cdot 2 x = \\frac{5}{4} x &gt; x\n\\] Therefore, it looks as if you should switch no matter what value of \\(x\\) you see. A consequence of this, following the logic of backwards induction, that even if you didn’t open your envelope that you would want to switch!"
  },
  {
    "objectID": "01-intro.html#bayes-rule-2",
    "href": "01-intro.html#bayes-rule-2",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\nWhere’s the flaw in this argument? Use Bayes rule to update the probabilities of which envelope your opponent has! Assume \\(p(m)\\) of dollars to be placed in the envelope by the swami.\nSuch an assumption then allows us to calculate an odds ratio \\[\n\\frac{ p \\left ( y = \\frac{1}{2} x | x \\right ) }{ p \\left ( y = 2 x | x \\right ) }\n\\] concerning the likelihood of which envelope your opponent has.\nThen, the expected value is given by\n\n\\[\nE(y) =  p \\left ( y = \\frac{1}{2} x \\; \\vert \\;  x \\right ) \\cdot  \\frac{1}{2} x +\np \\left ( y = 2 x | x \\right ) \\cdot 2 x\n\\] and the condition \\(E( y) &gt; x\\) becomes a decision rule."
  },
  {
    "objectID": "01-intro.html#prisoners-dilemma",
    "href": "01-intro.html#prisoners-dilemma",
    "title": "Bayes AI",
    "section": "Prisoner’s Dilemma",
    "text": "Prisoner’s Dilemma\nThree prisoners \\(A , B , C\\).\nEach believe are equally likely to be set free.\nPrisoner \\(A\\) goes to the warden \\(W\\) and asks if s/he is getting axed.\n\nThe Warden can’t tell \\(A\\) anything about him.\nHe provides the new information: \\(WB\\) = “\\(B\\) is to be executed”"
  },
  {
    "objectID": "01-intro.html#prisoners-dilemma-1",
    "href": "01-intro.html#prisoners-dilemma-1",
    "title": "Bayes AI",
    "section": "Prisoner’s Dilemma",
    "text": "Prisoner’s Dilemma\nUniform Prior Probabilities: \\[\n\\begin{array}{c|ccc}\nPrior & A  & B  & C  \\\\\\hline\nP ( {\\rm Pardon} ) & 0.33 & 0.33 & 0.33\n\\end{array}\n\\]\nPosterior: Compute \\(P ( A | WB )\\)?\n What happens if \\(C\\) overhears the conversation?\n Compute \\(P ( C | WB )\\)?"
  },
  {
    "objectID": "01-intro.html#game-show-problem",
    "href": "01-intro.html#game-show-problem",
    "title": "Bayes AI",
    "section": "Game Show Problem",
    "text": "Game Show Problem\nNamed after the host of the long-running TV show, Let’s make a Deal.\n\nA contestant is given the choice of 3 doors.\n\nThere is a prize (a car, say) behind one of the doors and something worthless behind the other two doors: two goats.\n\nThe optimal strategy is counter-intuitive"
  },
  {
    "objectID": "01-intro.html#puzzle",
    "href": "01-intro.html#puzzle",
    "title": "Bayes AI",
    "section": "Puzzle",
    "text": "Puzzle\nThe game is as follows:\n\nYou pick a door.\nMonty then opens one of the other two doors, revealing a goat.\nYou have the choice of switching doors.\n\n Is it advantageous to switch?\n Assume you pick door \\(A\\) at random. Then \\(P(A) = ( 1 /3 )\\).\nYou need to figure out \\(P( A | MB )\\) after Monte reveals \\(B\\) is a goat."
  },
  {
    "objectID": "01-intro.html#bayes-rule-3",
    "href": "01-intro.html#bayes-rule-3",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nIn its simplest form.\n\nTwo events \\(A\\) and \\(B\\). Bayes rule \\[\nP ( A | B ) = \\frac{P (  A \\cap  B )}{ P ( B )}\n= \\frac{P ( B | A ) P ( A )}{ P ( B )}\n\\]\nLaw of Total Probability \\[\nP ( B ) = P ( B | A ) P ( A ) + P ( B | \\bar{A} ) P ( \\bar{A} )\n\\] Hence we can calculate the denominator of Bayes rule."
  },
  {
    "objectID": "01-intro.html#bayes-theorem",
    "href": "01-intro.html#bayes-theorem",
    "title": "Bayes AI",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nMany problems in decision making can be solved using Bayes rule.\n\nAI: Rule-based decision making.\nIt’s counterintuitive! But gives the “right” answer.\n\nBayes Rule: \\[\n\\mbox{P}(A|B) = \\frac{\\mbox{P}(A \\cap B)}{\\mbox{P}(B)} = \\frac{  \\mbox{P}(B|A) \\mbox{P}(A)}{ \\mbox{P}(B)}\n\\] Law of Total Probability: \\[\n\\mbox{P}(B) =  \\mbox{P}(B|A) \\mbox{P}(A ) +  \\mbox{P}(B| \\bar{A} ) \\mbox{P}(\\bar{A} )\n\\]"
  },
  {
    "objectID": "01-intro.html#apple-watch",
    "href": "01-intro.html#apple-watch",
    "title": "Bayes AI",
    "section": "Apple Watch",
    "text": "Apple Watch\nThe Apple Watch Series 4 can perform a single-lead ECG and detect atrial fibrillation. The software can correctly identify 98% of cases of atrial fibrillation (true positives) and 99% of cases of non-atrial fibrillation (true negatives).\nHowever, what is the probability of a person having atrial fibrillation when atrial fibrillation is identified by the Apple Watch Series 4?\nBayes’ Theorem: \\[\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n\\]"
  },
  {
    "objectID": "01-intro.html#apple-watch-1",
    "href": "01-intro.html#apple-watch-1",
    "title": "Bayes AI",
    "section": "Apple Watch",
    "text": "Apple Watch\n\n\n\nPredicted\natrial fibrillation\nno atrial fibrillation\n\n\n\n\natrial fibrillation\n1960\n980\n\n\nno atrial fibrillation\n40\n97020\n\n\n\n\\[\n0.6667\n=\n\\frac{0.98\\cdot 0.02}{\n0.0294}\n\\]\nThe conditional probability of having atrial fibrillation when the Apple Watch Series 4 detects atrial fibrillation is about 67%."
  },
  {
    "objectID": "01-intro.html#abraham-wald",
    "href": "01-intro.html#abraham-wald",
    "title": "Bayes AI",
    "section": "Abraham Wald",
    "text": "Abraham Wald\nHow Abraham Wald improved aircraft survivability. Raw Reports from the Field\n\n\n\nType of damage suffered\nReturned (316 total)\nShot down (60 total)\n\n\n\n\nEngine\n29\n?\n\n\nCockpit\n36\n?\n\n\nFuselage\n105\n?\n\n\nNone\n146\n0\n\n\n\nThis fact would allow Wald to estimate: \\[\nP(\\text{damage on fuselage} \\mid \\text{returns safely}) = 105/316 \\approx 32\\%\n\\] You need the inverse probability : \\[\nP(\\text{returns safely} \\mid \\text{damage on fuselage})\n\\] Completely different!"
  },
  {
    "objectID": "01-intro.html#abraham-wald-1",
    "href": "01-intro.html#abraham-wald-1",
    "title": "Bayes AI",
    "section": "Abraham Wald",
    "text": "Abraham Wald\nImputation: fill-in missing data.\n\n\n\nType of damage suffered\nReturned (316 total)\nShot down (60 total)\n\n\n\n\nEngine\n29\n31\n\n\nCockpit\n36\n21\n\n\nFuselage\n105\n8\n\n\nNone\n146\n0\n\n\n\nThen Wald got: \\[\n\\begin{aligned}\nP(\\text{returns safely} \\mid \\text{damage on fuselage}) & =\\frac{105}{105+8}\\approx 93\\%\\\\\nP(\\text{returns safely} \\mid \\text{damage on engine}) & =\\frac{29}{29+31}\\approx 48\\%\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "01-intro.html#personalization-conditional-probability",
    "href": "01-intro.html#personalization-conditional-probability",
    "title": "Bayes AI",
    "section": "“Personalization\" \\(=\\)”Conditional Probability\"",
    "text": "“Personalization\" \\(=\\)”Conditional Probability\"\n\nConditional probability is how AI systems express judgments in a way that reflects their partial knowledge.\nPersonalization runs on conditional probabilities, all of which must be estimated from massive data sets in which you are the conditioning event.\n\n Many Business Applications!! Suggestions vs Search…."
  },
  {
    "objectID": "01-intro.html#probability-as-evidence",
    "href": "01-intro.html#probability-as-evidence",
    "title": "Bayes AI",
    "section": "Probability as Evidence",
    "text": "Probability as Evidence\nevidence: known facts about criminal (e.g. blood type, DNA, ...)\nsuspect: matches a trait with evidence at scene of crime\nLet \\(G\\) denote the event that the suspect is the criminal.\nBayes computes the conditional probability of guilt\n\\[\nP ( G | {\\rm evidence} )\n\\] Evidence \\(E\\): suspect and criminal possess a common trait"
  },
  {
    "objectID": "01-intro.html#probability-as-evidence-1",
    "href": "01-intro.html#probability-as-evidence-1",
    "title": "Bayes AI",
    "section": "Probability as Evidence",
    "text": "Probability as Evidence\nBayes Theorem yields \\[\nP ( G | {\\rm evidence} )\n= \\frac{ P ( {\\rm evidence} | G ) P ( G ) }{ P ( {\\rm evidence} )}\n\\]\nIn terms of relative odds \\[\n\\frac{ P ( I | {\\rm evidence} ) }{ P ( G | {\\rm evidence} ) }\n= \\frac{ P ( {\\rm evidence} | I ) }{ P ( {\\rm evidence} | G ) }\n\\frac{ P ( I ) }{ P ( G ) }\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-factors",
    "href": "01-intro.html#bayes-factors",
    "title": "Bayes AI",
    "section": "Bayes Factors",
    "text": "Bayes Factors\nThere are two terms:\n\nPrior Odds of Guilt \\(O ( G ) = P ( I ) / P ( G )\\) ?\n\nHow many people on the island?\nSensitivity “what if” analysis?\n\nThe Bayes factor \\[\n\\frac{ P ( {\\rm evidence} | I ) }{ P ( {\\rm evidence} | G ) }\n\\] is common to all observers and updates everyone’s initials odds"
  },
  {
    "objectID": "01-intro.html#prosecutors-fallacy",
    "href": "01-intro.html#prosecutors-fallacy",
    "title": "Bayes AI",
    "section": "Prosecutor’s Fallacy",
    "text": "Prosecutor’s Fallacy\nThe most common fallacy is confusing \\[\nP ( {\\rm evidence} | G ) \\; \\; {\\rm with} \\; \\;\nP ( G | {\\rm evidence} )\n\\]\nBayes rule yields \\[\nP ( G | {\\rm evidence} ) = \\frac{ P ( {\\rm evidence} | G ) p( G )}{ P ( {\\rm evidence}  )}\n\\] Your assessment of \\(P( G )\\) will matter."
  },
  {
    "objectID": "01-intro.html#island-problem",
    "href": "01-intro.html#island-problem",
    "title": "Bayes AI",
    "section": "Island Problem",
    "text": "Island Problem\nSuppose there’s a criminal on a island of \\(N+1\\) people.\n\nLet \\(I\\) denote innocence and \\(G\\) guilt.\nEvidence \\(E\\): the suspect matches a trait with the criminal.\nThe probabilities are \\[\np(E|I)=p\\;\\;\\mathrm{and}\\;\\;p(E|G)=1\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-factor",
    "href": "01-intro.html#bayes-factor",
    "title": "Bayes AI",
    "section": "Bayes factor",
    "text": "Bayes factor\nBayes factors are likelihood ratios\n\nThe Bayes factor is given by \\[\n\\frac{p(E|I)}{p(E|G)}=p\n\\]\nIf we start with a uniform prior distribution we have\n\n\\[\np(I)=\\frac{1}{N+1}\\;\\;\\mathrm{and}\\;\\;odds(I)=N\n\\]\n\nPriors will matter!"
  },
  {
    "objectID": "01-intro.html#island-problem-1",
    "href": "01-intro.html#island-problem-1",
    "title": "Bayes AI",
    "section": "Island Problem",
    "text": "Island Problem\nPosterior Probability related to Odds \\[\np(I|y)=\\frac\n{1}{1+odds(I|y)}%\n\\]\n\nProsecutors’ fallacy\n\nThe posterior probability \\(p(I|y)\\neq p(y|I)=p\\).\n\nSuppose that \\(N=10^{3}\\) and \\(p=10^{-3}\\). Then\n\n\\[\np( I|y) = \\frac{1}{1 + 10^3 \\cdot 10^{-3}} = \\frac{1}{2}\n\\]\nThe odds on innocence are \\(odds(I|y)=1\\).\nThere’s a \\(50/50\\) chance that the criminal has been found."
  },
  {
    "objectID": "01-intro.html#sally-clark-case-independence-or-bayes",
    "href": "01-intro.html#sally-clark-case-independence-or-bayes",
    "title": "Bayes AI",
    "section": "Sally Clark Case: Independence or Bayes?",
    "text": "Sally Clark Case: Independence or Bayes?\nSally Clark was accused and convicted of killing her two children\nThey could have both died of SIDS.\n\nThe chance of a family which are non-smokers and over 25 having a SIDS death is around 1 in 8,500.\nThe chance of a family which has already had a SIDS death having a second is around 1 in 100.\nThe chance of a mother killing her two children is around 1 in 1,000,000."
  },
  {
    "objectID": "01-intro.html#bayes-or-independence",
    "href": "01-intro.html#bayes-or-independence",
    "title": "Bayes AI",
    "section": "Bayes or Independence",
    "text": "Bayes or Independence\n\nUnder Bayes \\[\n\\begin{aligned}\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)   &  = P \\left(\n\\mathrm{first} \\; \\mathrm{SIDS} \\right)  P \\left(  \\mathrm{Second} \\; \\;\n\\mathrm{SIDS} | \\mathrm{first} \\; \\mathrm{SIDS} \\right) \\\\\n&  = \\frac{1}{8500} \\cdot \\frac{1}{100} = \\frac{1}{850,000}\n\\end{aligned}\n\\] The \\(\\frac{1}{100}\\) comes from taking into account genetics.\nIndependence, as the court did, gets you\n\n\\[\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)  = (1/8500) (1/8500) = (1/73,000,000)\n\\]\n\nBy Bayes rule\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{P( E \\cap I)}{P( E \\cap G)}\n\\] \\(P( E \\cap I) = P(E|I )P(I)\\) needs discussion of \\(p(I)\\)."
  },
  {
    "objectID": "01-intro.html#comparison",
    "href": "01-intro.html#comparison",
    "title": "Bayes AI",
    "section": "Comparison",
    "text": "Comparison\n\nHence putting these two together gives the odds of guilt as\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{1/850,000}{1/1,000,000} = 1.15\n\\] In terms of posterior probabilities\n\\[\np( G|E) = \\frac{1}{1 + O(G|E)} = 0.465\n\\]\n\nIf you use independence\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{1}{73} \\; {\\rm and} \\; p( G|E) \\approx 0.99\n\\] The suspect looks guilty."
  },
  {
    "objectID": "01-intro.html#oj-simpson",
    "href": "01-intro.html#oj-simpson",
    "title": "Bayes AI",
    "section": "OJ Simpson",
    "text": "OJ Simpson\nThe O.J. Simpson trial was possibly the trail of the century\nThe murder of his wife Nicole Brown Simpson, and a friend, Ron Goldman, in June 1994 and the trial dominated the TV networks\n\nDNA evidence and probability: \\(p( E| G)\\)\nBayes Theorem: \\(p( G | E )\\)\nProsecutor’s Fallacy: \\(p( G|E ) \\neq p(E|G)\\)\n\nOdds ratio \\[\n\\frac{ p( I|E) }{ p ( G | E ) } = \\frac{ p( E|I )}{ p( E|G) } \\frac{ p(I) }{p(G ) }\n\\] Prior odds conditioned on background information."
  },
  {
    "objectID": "01-intro.html#oj-simpson-bayes-theorem",
    "href": "01-intro.html#oj-simpson-bayes-theorem",
    "title": "Bayes AI",
    "section": "OJ Simpson: Bayes Theorem",
    "text": "OJ Simpson: Bayes Theorem\nSuppose that you are a juror in a murder case of a husband who is accused of killing his wife.\nThe husband is known is have battered her in the past.\nConsider the three events:\n\n\\(G\\) “husband murders wife in a given year”\n\\(M\\) “wife is murdered in a given year”\n\\(B\\) “husband is known to batter his wife”"
  },
  {
    "objectID": "01-intro.html#oj-simpson-bayes-theorem-1",
    "href": "01-intro.html#oj-simpson-bayes-theorem-1",
    "title": "Bayes AI",
    "section": "OJ Simpson: Bayes Theorem",
    "text": "OJ Simpson: Bayes Theorem\n\nOnly \\(1/10\\)th of one percent of husbands who batter their wife actually murder them.\n\nConditional on eventually murdering their wife, there a one in ten chance it happens in a given year.\nIn 1994, 5000 women were murdered, 1500 by their husband\nGiven a population of 100 million women at the time \\[\np( M | I ) = \\frac{ 3500 }{ 10^8 } \\approx \\frac{1}{30,000} .\n\\] We’ll also need \\(p( M | I , B )  = p( M | I )\\)"
  },
  {
    "objectID": "01-intro.html#oj-simpson-prosecutors-fallacy",
    "href": "01-intro.html#oj-simpson-prosecutors-fallacy",
    "title": "Bayes AI",
    "section": "OJ Simpson: Prosecutor’s Fallacy",
    "text": "OJ Simpson: Prosecutor’s Fallacy\n\nLet \\(G =\\) Guilt and \\(E=\\) Evidence\nProsecutor’s Fallacy: \\(P(G|E) \\neq P(E|G)\\).\nDNA evidence gives \\(P( E | I )\\) – the \\(p\\)-value.\n\nWhat’s the “match probability” for a rare event?\nBayes theorem in Odds \\[\n\\frac{p(G|M,B)}{p(I|M,B)} = \\frac{p(M|G,B)}{p(M|I,B)} \\frac{p(G|B)}{p(I|B)}\n\\]"
  },
  {
    "objectID": "01-intro.html#oj-simpson-bayes-theorem-2",
    "href": "01-intro.html#oj-simpson-bayes-theorem-2",
    "title": "Bayes AI",
    "section": "OJ Simpson: Bayes Theorem",
    "text": "OJ Simpson: Bayes Theorem\nBy assumption,\n\n\\(p(M|G,B)=1\\)\n\\(p(M|I,B)= \\frac{1}{30,000}\\)\n\\(p( G|B) = \\frac{1}{1000}\\) and so\n\n\\[\n\\frac{p(G|B)}{p(I|B)} = \\frac{1}{999}\n\\]\nTherefore, \\[\n\\frac{p(G|M,B)}{p(I|M,B)} \\approx 30 \\; {\\rm and} \\; p(G|M,B) = \\frac{30}{31} \\approx 97\\%\n\\] More than a 50/50 chance that your spouse murdered you!"
  },
  {
    "objectID": "01-intro.html#fallacy-p-g-b-neq-p-g-b-m",
    "href": "01-intro.html#fallacy-p-g-b-neq-p-g-b-m",
    "title": "Bayes AI",
    "section": "Fallacy \\(p ( G | B ) \\neq p( G | B , M )\\)",
    "text": "Fallacy \\(p ( G | B ) \\neq p( G | B , M )\\)\nThe defense stated to the press: in any given year\n“Fewer than 1 in 2000 of batterers go on to murder their wives”.\n\nNow estimate \\(p( M | \\bar{G} , B ) = p( M| \\bar{G} ) = \\frac{1}{20,000}\\).\nThe Bayes factor is then\n\n\\[\n\\frac{ p( G | M , B ) }{ p( \\bar{G} | M , B ) } = \\frac{ 1/999 }{1 /20,000} = 20\n\\] which implies posterior probabilities\n\\[\np( \\bar{G} | M , B ) = \\frac{1}{1+20} \\; {\\rm and} \\; p( G | M , B ) = \\frac{20}{21}\n\\] Hence its over 95% chance that O.J. is guilty based on this information!\nDefense intended this information to exonerate O.J."
  },
  {
    "objectID": "01-intro.html#base-rate-fallacies",
    "href": "01-intro.html#base-rate-fallacies",
    "title": "Bayes AI",
    "section": "Base Rate Fallacies",
    "text": "Base Rate Fallacies\n“Witness” 80 % certain saw a “checker” \\(C\\) taxi in the accident.\n\nWhat’s your \\(P ( C | E )\\) ?\nNeed \\(P ( C )\\). Say \\(P( C ) = 0.2\\) and \\(P( E  | C) = 0.8\\).\nThen your posterior is\n\n\\[\nP ( C | E ) = \\frac{0.8 \\cdot 0.2}{ 0.8 \\cdot 0.2 + 0.2 \\cdot 0.8 } = 0.5\n\\]\nTherefore \\(O ( C ) = 1\\) a 50/50 bet."
  },
  {
    "objectID": "01-intro.html#updating-fallacies",
    "href": "01-intro.html#updating-fallacies",
    "title": "Bayes AI",
    "section": "Updating Fallacies",
    "text": "Updating Fallacies\nMost people don’t update quickly enough in light of new data\nWards Edwards 1960s\nWhen you have a small sample size, Bayes rule still updates probabilities\n\nTwo players: either 70 % A or 30 % A\nObserve \\(A\\) beats \\(B\\) 3 times out of 4.\nWhat’s \\(P ( A = 70 \\% \\; {\\rm player} )\\) ?"
  },
  {
    "objectID": "01-intro.html#conditional-independence",
    "href": "01-intro.html#conditional-independence",
    "title": "Bayes AI",
    "section": "Conditional Independence",
    "text": "Conditional Independence\n\nConsider three variables a,b,c\nConditional distribution of a given b and c, is p(a|b,c)\nIf p(a|b,c) does not depend on value of b, we can write p(a|b,c) = p(a|c)\nWe say that a is conditionally independent of b given c\n\nWe can use the telescoping property of conditional probabilities to write the joint probability distribution as a product of conditional probabilities. This is the essence of the chain rule of probability. It is given by \\[\np(x_1, x_2, \\ldots, x_n) = p(x_1)p(x_2 \\mid x_1)p(x_3 \\mid x_1, x_2) \\ldots p(x_n \\mid x_1, x_2, \\ldots, x_{n-1}).\n\\]\nright hand side can be simplified if some of the variables are conditionally independent"
  },
  {
    "objectID": "01-intro.html#graphical-representation",
    "href": "01-intro.html#graphical-representation",
    "title": "Bayes AI",
    "section": "Graphical Representation",
    "text": "Graphical Representation\nWhen two nodes are connected they are not independent. Consider the following three cases:\n\n\n\n\n\n\n\n\n\nLine Structure\n\n\n\n\n\\[\np(b\\mid c,a) = p(b\\mid c),~ p(a,b,c) = p(a)p(c\\mid a)p(b\\mid c)\n\\]\n\n\n\n\n\n\n\nLambda Structure\n\n\n\n\n\\[\np(a\\mid b,c) = p(a\\mid c), ~ p(a,b,c) = p(a\\mid c)p(b\\mid c)p(c)\n\\]\n\n\n\n\n\n\n\nV-structure\n\n\n\n\n\\[\np(a\\mid b) = p(a),~ p(a,b,c) = p(c\\mid a,b)p(a)p(b)\n\\]"
  },
  {
    "objectID": "01-intro.html#derived-assumptions",
    "href": "01-intro.html#derived-assumptions",
    "title": "Bayes AI",
    "section": "Derived Assumptions",
    "text": "Derived Assumptions\n\n\n\n\n\n\n\n\n\nLine Structure\n\n\n\n\n\\(a\\) and \\(b\\) connected through \\(c\\). Thus, \\(a\\) can influence \\(b\\). However, once \\(c\\) is known, \\(a\\) and \\(b\\) are independent.\n\n\n\n\n\n\n\nLambda Structure\n\n\n\n\n\\(a\\) can influence \\(b\\) through \\(c\\), but once \\(c\\) is known, \\(a\\) and \\(b\\) are independent.\n\n\n\n\n\n\n\nV-structure\n\n\n\n\n\\(a\\) and \\(b\\) are independent, but once \\(c\\) is known, \\(a\\) and \\(b\\) are not independent. You can formally derive these independencies from the graph by comparing \\(p(a,b\\mid c)\\) and \\(p(a\\mid c)p(b\\mid c)\\)."
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics",
    "href": "01-intro.html#bayes-home-diagnostics",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\n\nAlarm system sends me a text notification when some motion inside my house is detected. - - Prior: during an earthquake alarm is triggered in 10% of the cases.\nI get text message and assess \\(p(b\\mid a)\\) is high and I start driving back home\nWhile driving I hear on the radio about a small earthquake in our area, need to calculate \\(p(b \\mid a,r)\\)\n\n\\(b\\) = burglary, \\(e\\) = earthquake, \\(a\\) = alarm, and \\(r\\) = radio message about small earthquake.\nThe joint distribution is then given by \\[\n  p(b,e,a,r) = p(r \\mid a,b,e)p(a \\mid b,e)p(b\\mid e)p(e).\n\\] Since we know the causal relations, we can simplify this expression \\[\np(b,e,a,r) = p(r \\mid e)p(a \\mid b,e)p(b)p(e).\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-1",
    "href": "01-intro.html#bayes-home-diagnostics-1",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\nThe joint distribution is defined by\n\n\n\n\\(p(a=1 \\mid b,e)\\)\nb\ne\n\n\n\n\n0\n0\n0\n\n\n0.1\n0\n1\n\n\n1\n1\n0\n\n\n1\n1\n1"
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-2",
    "href": "01-intro.html#bayes-home-diagnostics-2",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\nGraphically, we can represent the relations between the variables known as a Directed Acyclic Graph (DAG), which is known as Bayesian network.\n\n\nCode\ngraph TB\n    b((b)) --&gt; a((a))\n    e((e)) --&gt; a\n    e --&gt; r((r))\n\n\n\n\n\n\ngraph TB\n    b((b)) --&gt; a((a))\n    e((e)) --&gt; a\n    e --&gt; r((r))\n\n\n\n\nFigure 1: Bayesian network for alarm ."
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-3",
    "href": "01-intro.html#bayes-home-diagnostics-3",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\n\nCalculate \\(p(a=0 \\mid b,e)\\), from \\[\np(a=1 \\mid b,e) + p(a=0 \\mid b,e) = 1.\n\\] Also know \\(p(r=1 \\mid e=1) = 0.5\\) and \\(p(r=1 \\mid e=0) = 0\\), \\(p(b) = 2\\cdot10^{-4}\\) and \\(p(e) = 10^{-2}\\) (historical data)\n\nGraph allowed us to have a more compact representation of the joint probability distribution. The original naive representations requires specifying \\(2^4\\) parameters."
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-4",
    "href": "01-intro.html#bayes-home-diagnostics-4",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\nTo answer our original question, calculate \\[\np(b \\mid a) = \\dfrac{p(a \\mid b)p(b)}{p(a)},~~p(b) = p(a=1 \\mid b=1)p(b=1) + p(a=1 \\mid b=0)p(b=0).\n\\] We have everything but \\(p(a \\mid b)\\). This is obtained by marginalizing \\(p(a=1 \\mid b,e)\\), to yield \\[\np(a \\mid b) = p(a \\mid b,e=1)p(e=1) + p(a \\mid b,e=0)p(e=0).\n\\] We can calculate \\[\np(a=1 \\mid b=1) = 1, ~p(a=1 \\mid b=0) = 0.1*10^{-2} + 0 = 10^{-3}.\n\\] This leads to \\(p(b \\mid a) = 2\\cdot10^{-4}/(2\\cdot10^{-4} + 10^{-3}(1-2\\cdot10^{-4})) = 1/6\\)."
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-5",
    "href": "01-intro.html#bayes-home-diagnostics-5",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\n\nResult is somewhat counterintuitive.\nWe get such a low probability of burglary because its prior is very low compared to prior probability of an earthquake.\nWhat will happen to posterior if we live in an area with higher crime rates, say \\(p(b) = 10^{-3}\\). \\[\np(a \\mid b) = \\dfrac{p(b)}{p(b) + 10^{-3}(1-p(b))}\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-6",
    "href": "01-intro.html#bayes-home-diagnostics-6",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\n\n\nCode\nprior &lt;- seq(0, .1, length.out = 200)\npost &lt;- prior / (prior + 0.001 * (1 - prior))\nplot(prior, post, type = \"l\", lwd = 3, col = \"red\")\n\n\n\n\nFigure 2: Relationship between the prior and posterior"
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-7",
    "href": "01-intro.html#bayes-home-diagnostics-7",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\nNow, suppose that you hear on the radio about a small earthquake while driving. Then, using Bayesian conditioning, \\[\np(b=1 \\mid a=1,r=1) =  \\dfrac{p(a,r  \\mid  b)p(b)}{p(a,r)}\n\\] and \\[\np(a,r  \\mid  b)p(b) = \\dfrac{\\sum_e p(b=1,e,a=1,r=1)}{\\sum_b\\sum_ep(b,e,a=1,r=1)}\n\\] \\[\n=\\dfrac{\\sum_ep(r=1 \\mid e)p(a=1 \\mid b=1,e)p(b=1)p(e)}{\\sum_b\\sum_ep(r=1 \\mid e)p(a=1 \\mid b,e)p(b)p(e)}\n\\] which is \\(\\approx 2\\%\\) in our case. This effect is called explaining away, namely when new information explains some previously known fact."
  },
  {
    "objectID": "01-intro.html#a-random-image",
    "href": "01-intro.html#a-random-image",
    "title": "Bayes AI",
    "section": "A Random Image",
    "text": "A Random Image"
  },
  {
    "objectID": "03-conjugate.html#epl-odds",
    "href": "03-conjugate.html#epl-odds",
    "title": "Bayes AI",
    "section": "EPL Odds",
    "text": "EPL Odds"
  },
  {
    "objectID": "03-conjugate.html#english-premier-league-epl",
    "href": "03-conjugate.html#english-premier-league-epl",
    "title": "Bayes AI",
    "section": "English Premier League: EPL",
    "text": "English Premier League: EPL\nCalculate Odds for the possible scores in a match? \\[\n0-0, \\; 1-0, \\; 0-1, \\; 1-1, \\; 2-0, \\ldots\n\\] Let \\(X=\\) Goals scored by Arsenal\n\\(Y=\\) Goals scored by Liverpool\nWhat’s the odds of a team winning? \\(\\; \\; \\;  P \\left ( X&gt; Y \\right )\\) Odds of a draw? \\(\\; \\; \\;  P \\left ( X = Y \\right )\\)\nz1 = rpois(100,0.6) z2 = rpois(100,1.4) sum(z1&lt;z2)/100 # Team 2 wins sum(z1=z2)/100 # Draw"
  },
  {
    "objectID": "03-conjugate.html#chelsea-epl-2017",
    "href": "03-conjugate.html#chelsea-epl-2017",
    "title": "Bayes AI",
    "section": "Chelsea EPL 2017",
    "text": "Chelsea EPL 2017\nLet’s take a historical set of data on scores Then estimate \\(\\lambda\\) with the sample mean of the home and away scores\n\n\n\nhome team\nresults\n\nvisit team\n\n\n\n\nChelsea\n2\n1\nWest Ham\n\n\nChelsea\n5\n1\nSunderland\n\n\nWatford\n1\n2\nChelsea\n\n\nChelsea\n3\n0\nBurnley\n\n\n\\(\\dots\\)"
  },
  {
    "objectID": "03-conjugate.html#epl-chelsea",
    "href": "03-conjugate.html#epl-chelsea",
    "title": "Bayes AI",
    "section": "EPL Chelsea",
    "text": "EPL Chelsea\n\n\n\n\n\n\n\n\n\nChelsea against\n\n\n\n\n\n\n\nChelsea for\n\n\n\n\n\nOur Poisson model fits the empirical data!!"
  },
  {
    "objectID": "03-conjugate.html#epl-attack-and-defence-strength",
    "href": "03-conjugate.html#epl-attack-and-defence-strength",
    "title": "Bayes AI",
    "section": "EPL: Attack and Defence Strength",
    "text": "EPL: Attack and Defence Strength\nEach team gets an “attack” strength and “defence” weakness rating Adjust home and away average goal estimates"
  },
  {
    "objectID": "03-conjugate.html#epl-hull-vs-manu-poisson-distribution",
    "href": "03-conjugate.html#epl-hull-vs-manu-poisson-distribution",
    "title": "Bayes AI",
    "section": "EPL: Hull vs ManU: Poisson Distribution",
    "text": "EPL: Hull vs ManU: Poisson Distribution\nManU Average away goals \\(=1.47\\). Prediction: \\(1.47 \\times 1.46 \\times 1.37 = 2.95\\)\nAttack strength times Hull’s defense weakness times average\nHull Average home goals \\(=1.47\\). Prediction: \\(1.47 \\times 0.85 \\times 0.52 = 0.65\\). Simulation\n\n\n\nTeam\nExpected Goals\n0\n1\n2\n3\n4\n5\n\n\n\n\nMan U\n2.95\n7\n22\n26\n12\n11\n13\n\n\nHull City\n0.65\n49\n41\n10\n0\n0\n0"
  },
  {
    "objectID": "03-conjugate.html#epl-predictions",
    "href": "03-conjugate.html#epl-predictions",
    "title": "Bayes AI",
    "section": "EPL Predictions",
    "text": "EPL Predictions\nA model is only as good as its predictions\n\nIn our simulation Man U wins 88 games out of 100, we should bet when odds ratio is below 88 to 100.\nMost likely outcome is 0-3 (12 games out of 100)\nThe actual outcome was 0-1 (they played on August 27, 2016)\nIn out simulation 0-1 was the fourth most probable outcome (9 games out of 100)"
  },
  {
    "objectID": "03-conjugate.html#bayesian-methods",
    "href": "03-conjugate.html#bayesian-methods",
    "title": "Bayes AI",
    "section": "Bayesian Methods",
    "text": "Bayesian Methods\nModern Statistical/Machine Learning\n\nBayes Rule and Probabilistic Learning\nComputationally challenging: MCMC and Particle Filtering\nMany applications in Finance:\n\nAsset pricing and corporate finance problems.\nLindley, D.V. Making Decisions\nBernardo, J. and A.F.M. Smith Bayesian Theory"
  },
  {
    "objectID": "03-conjugate.html#bayesian-books",
    "href": "03-conjugate.html#bayesian-books",
    "title": "Bayes AI",
    "section": "Bayesian Books",
    "text": "Bayesian Books\n\nHierarchical Models and MCMC\nBayesian Nonparametrics\n\nMachine Learning\n\nDynamic State Space Models \\(\\ldots\\)"
  },
  {
    "objectID": "03-conjugate.html#popular-books",
    "href": "03-conjugate.html#popular-books",
    "title": "Bayes AI",
    "section": "Popular Books",
    "text": "Popular Books\nMcGrayne (2012): The Theory that would not Die\n\nHistory of Bayes-Laplace\nCode breaking\nBayes search: Air France \\(\\ldots\\)"
  },
  {
    "objectID": "03-conjugate.html#nate-silver-538-and-nyt",
    "href": "03-conjugate.html#nate-silver-538-and-nyt",
    "title": "Bayes AI",
    "section": "Nate Silver: 538 and NYT",
    "text": "Nate Silver: 538 and NYT\nSilver (2012): The Signal and The Noise\n\nPresidential Elections\nBayes dominant methodology\nPredicting College Basketball/Oscars \\(\\ldots\\)"
  },
  {
    "objectID": "03-conjugate.html#things-to-know",
    "href": "03-conjugate.html#things-to-know",
    "title": "Bayes AI",
    "section": "Things to Know",
    "text": "Things to Know\nExplosion of Models and Algorithms starting in 1950s\n\nBayesian Regularisation and Sparsity\nHierarchical Models and Shrinkage\nHidden Markov Models\nNonlinear Non-Gaussian State Space Models\n\nAlgorithms\n\nMonte Carlo Method (von Neumann and Ulam, 1940s)\nMetropolis-Hastings (Metropolis, 1950s)\nGibbs Sampling (Geman and Geman, Gelfand and Smith, 1980s)\nSequential Particle Filtering"
  },
  {
    "objectID": "03-conjugate.html#probabilistic-reasoning",
    "href": "03-conjugate.html#probabilistic-reasoning",
    "title": "Bayes AI",
    "section": "Probabilistic Reasoning",
    "text": "Probabilistic Reasoning\n\nBayesian Probability (Ramsey, 1926, de Finetti, 1931)\n\nBeta-Binomial Learning: Black Swans\nElections: Nate Silver\nBaseball: Kenny Lofton and Derek Jeter\n\nMonte Carlo (von Neumann and Ulam, Metropolis, 1940s)\nShrinkage Estimation (Lindley and Smith, Efron and Morris, 1970s)"
  },
  {
    "objectID": "03-conjugate.html#bayesian-inference",
    "href": "03-conjugate.html#bayesian-inference",
    "title": "Bayes AI",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nKey Idea: Explicit use of probability for summarizing uncertainty.\n\nA probability distribution for data given parameters \\[\nf(y| \\theta ) \\; \\; \\text{Likelihood}\n\\]\nA probability distribution for unknown parameters \\[\np(\\theta) \\; \\; \\text{Prior}\n\\]\nInference for unknowns conditional on observed data\n\nInverse probability (Bayes Theorem);\nFormal decision making (Loss, Utility)"
  },
  {
    "objectID": "03-conjugate.html#posterior-inference",
    "href": "03-conjugate.html#posterior-inference",
    "title": "Bayes AI",
    "section": "Posterior Inference",
    "text": "Posterior Inference\nBayes theorem to derive posterior distributions \\[\n\\begin{aligned}\np( \\theta | y ) & = \\frac{p(y| \\theta)p( \\theta)}{p(y)} \\\\\np(y) & = \\int p(y| \\theta)p( \\theta)d \\theta\n\\end{aligned}\n\\] Allows you to make probability statements\n\nThey can be very different from p-values!\n\nHypothesis testing and Sequential problems\n\nMarkov chain Monte Carlo (MCMC) and Filtering (PF)"
  },
  {
    "objectID": "03-conjugate.html#coin-example",
    "href": "03-conjugate.html#coin-example",
    "title": "Bayes AI",
    "section": "Coin Example",
    "text": "Coin Example\n\nWhat if we gamble against unfair coin flips or the person who performs the flips is trained to get the side he wants?\nIn this case, we need to estimate the probability of heads \\(\\theta\\) from the data. Suppose we have observed 10 flips \\[\ny = \\{H, T, H, H, H, T, H, T, H, H\\},\n\\]\nThe frequency-based answer would be \\(\\theta = 3/10 = 0.3\\), but this is not a good estimate."
  },
  {
    "objectID": "03-conjugate.html#coin-example-1",
    "href": "03-conjugate.html#coin-example-1",
    "title": "Bayes AI",
    "section": "Coin Example",
    "text": "Coin Example\n\nBayes approach gives us more flexibility.\nPrior belief: the coin is fair, but we are not sure.\nWe can model this belief by a prior distribution, we discretize the variable\n\n\n\nCode\ntheta &lt;- seq(0, 1, by = 0.1)\nprior = c(0, 0.024, 0.077, 0.132, 0.173, 0.188, 0.173, 0.132, 0.077, 0.024, 0)\nbarplot(prior, names.arg = theta, xlab = \"theta\", ylab = \"prior\")\n\n\n\nPrior distribution"
  },
  {
    "objectID": "03-conjugate.html#coin-example-2",
    "href": "03-conjugate.html#coin-example-2",
    "title": "Bayes AI",
    "section": "Coin Example",
    "text": "Coin Example\nUse Bayes rule to update our prior belief. The posterior distribution is given by \\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}.\n\\] The denominator is the marginal likelihood, which is given by \\[\np(y) = \\sum_{\\theta} p(y \\mid \\theta) p(\\theta).\n\\] The likelihood is given by the Binomial distribution \\[\np(y \\mid \\theta) \\propto \\theta^3 (1 - \\theta)^7.\n\\]\nNotice, that the posterior distribution depends only on the number of positive and negative cases. Those numbers are sufficient for the inference about \\(\\theta\\). The posterior distribution is given by"
  },
  {
    "objectID": "03-conjugate.html#coin-example-3",
    "href": "03-conjugate.html#coin-example-3",
    "title": "Bayes AI",
    "section": "Coin Example",
    "text": "Coin Example\n\n\nCode\nlikelihood &lt;- function(theta, n, Y) {\n  theta^Y * (1 - theta)^(n - Y)\n}\nposterior &lt;- likelihood(theta, 10,3) * prior\nposterior &lt;- posterior / sum(posterior) # normalize\nbarplot(posterior, names.arg = theta, xlab = \"theta\", ylab = \"posterior\")\n\n\n\nPosterior distribution"
  },
  {
    "objectID": "03-conjugate.html#coin-example-4",
    "href": "03-conjugate.html#coin-example-4",
    "title": "Bayes AI",
    "section": "Coin Example",
    "text": "Coin Example\nIf you are to keep collecting more observations and say observe a sequence of 100 flips, then the posterior distribution will be more concentrated around the value of \\(\\theta = 0.3\\).\n\n\nCode\nposterior &lt;- likelihood(theta, 100,30) * prior\nposterior &lt;- posterior / sum(posterior) # normalize\nbarplot(posterior, names.arg = theta, xlab = \"theta\", ylab = \"posterior\")\n\n\n\nPosterior distribution for n=100This demonstrates that for large sample sizes, the frequentist approach and Bayes approach agree."
  },
  {
    "objectID": "03-conjugate.html#conjugate-priors",
    "href": "03-conjugate.html#conjugate-priors",
    "title": "Bayes AI",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\n\nDefinition: Let \\(F\\) denote the class of distributions \\(f ( y | \\theta )\\).\n\nA class \\(\\Pi\\) of prior distributions is conjugate for \\(F\\) if the posterior distribution is in the class \\(\\Pi\\) for all \\(f \\in F , \\pi \\in \\Pi , y \\in Y\\).\n\nExample: Binomial/Beta:\n\nSuppose that \\(Y_1 , \\ldots , Y_n \\sim Ber ( p )\\).\nLet \\(p \\sim Beta ( \\alpha , \\beta )\\) where \\(( \\alpha , \\beta )\\) are known hyper-parameters.\nThe beta-family is very flexible\nPrior mean \\(E ( p ) = \\frac{\\alpha}{ \\alpha + \\beta }\\)."
  },
  {
    "objectID": "03-conjugate.html#bayes-learning-beta-binomial",
    "href": "03-conjugate.html#bayes-learning-beta-binomial",
    "title": "Bayes AI",
    "section": "Bayes Learning: Beta-Binomial",
    "text": "Bayes Learning: Beta-Binomial\nHow do I update my beliefs about a coin toss?\nLikelihood for Bernoulli \\[\np\\left(  y|\\theta\\right)  =\\prod_{t=1}^{T}p\\left(  y_{t}|\\theta\\right)\n=\\theta^{\\sum_{t=1}^{T}y_{t}}\\left(  1-\\theta\\right)  ^{T-\\sum_{t=1}^{T}y_{t}}.\n\\] Initial prior distribution \\(\\theta\\sim\\mathcal{B}\\left(  a,A\\right)\\) given by \\[\np\\left(  \\theta|a,A\\right)  =\\frac{\\theta^{a-1}\\left(  1-\\theta\\right)\n^{A-1}}{B\\left(  a,A\\right)  }\n\\]"
  },
  {
    "objectID": "03-conjugate.html#bayes-learning-beta-binomial-1",
    "href": "03-conjugate.html#bayes-learning-beta-binomial-1",
    "title": "Bayes AI",
    "section": "Bayes Learning: Beta-Binomial",
    "text": "Bayes Learning: Beta-Binomial\nUpdated posterior distribution is also Beta \\[\np\\left(\n\\theta|y\\right)  \\sim\\mathcal{B}\\left(  a_{T},A_{T}\\right)  \\; {\\rm and} \\;\na_{T}=a+\\sum_{t=1}^{T}y_{t} , A_{T}=A+T-\\sum_{t=1}^{T}y_{t}\n\\] The posterior mean and variance are \\[\nE\\left[  \\theta|y\\right]  =\\frac{a_{T}}{a_{T}+A_{T}}\\text{ and }var\\left[\n\\theta|y\\right]  =\\frac{a_{T}A_{T}}{\\left(  a_{T}+A_{T}\\right)  ^{2}\\left(\na_{T}+A_{T}+1\\right)  }\n\\]"
  },
  {
    "objectID": "03-conjugate.html#binomial-beta",
    "href": "03-conjugate.html#binomial-beta",
    "title": "Bayes AI",
    "section": "Binomial-Beta",
    "text": "Binomial-Beta\n\\(p ( p | \\bar{y} )\\) is the posterior distribution for \\(p\\)\n\\(\\bar{y}\\) is a sufficient statistic.\n\nBayes theorem gives \\[\n\\begin{aligned}\np ( p | y )\n& \\propto f ( y | p ) p ( p | \\alpha , \\beta )\\\\\n& \\propto p^{\\sum y_i} (1 - p )^{n - \\sum y_i } \\cdot p^{\\alpha - 1} ( 1 - p )^{\\beta - 1} \\\\\n& \\propto p^{ \\alpha + \\sum y_i - 1 } ( 1 - p )^{ n - \\sum y_i + \\beta - 1} \\\\\n& \\sim Beta ( \\alpha + \\sum y_i , \\beta + n - \\sum y_i )\n\\end{aligned}\n\\]\nThe posterior mean is a shrinkage estimator\n\nCombination of sample mean \\(\\bar{y}\\) and prior mean \\(E( p )\\)\n\\[\nE(p|y) = \\frac{\\alpha + \\sum_{i=1}^n y_i}{\\alpha + \\beta + n} = \\frac{n}{n+ \\alpha +\\beta} \\bar{y} + \\frac{\\alpha + \\beta}{\\alpha + \\beta+n} \\frac{\\alpha}{\\alpha+\\beta}\n\\]"
  },
  {
    "objectID": "03-conjugate.html#binomial-beta-1",
    "href": "03-conjugate.html#binomial-beta-1",
    "title": "Bayes AI",
    "section": "Binomial-Beta",
    "text": "Binomial-Beta\nLet \\(p_i\\) be the death rate proportion under treatment \\(i\\).\n\nTo compare treatment \\(A\\) to \\(B\\) directly compute \\(P ( p_1 &gt; p_2 | D )\\).\nPrior \\(beta ( \\alpha , \\beta )\\) with prior mean \\(E ( p ) = \\frac{\\alpha}{\\alpha + \\beta }\\).\n\nPosterior \\(beta ( \\alpha + \\sum x_i , \\beta + n - \\sum x_i )\\)\n\nFor \\(A\\), \\(beta ( 1 , 1 ) \\rightarrow beta ( 8 , 94 )\\)\n\nFor \\(B\\), \\(beta ( 1 , 1 ) \\rightarrow beta ( 2 , 100 )\\)\n\nInference: \\(P ( p_1 &gt; p_2 | D ) \\approx 0.98\\)"
  },
  {
    "objectID": "03-conjugate.html#black-swans",
    "href": "03-conjugate.html#black-swans",
    "title": "Bayes AI",
    "section": "Black Swans",
    "text": "Black Swans\nTaleb, The Black Swan: the Impact of the Highly Improbable\nSuppose you’re only see a sequence of White Swans, having never seen a Black Swan.\nWhat’s the Probability of Black Swan event sometime in the future?\nSuppose that after \\(T\\) trials you have only seen successes \\(( y_1 , \\ldots , y_T ) = ( 1 , \\ldots , 1 )\\). The next trial being a success has \\[\np( y_{T+1} =1 | y_1 , \\ldots , y_T ) = \\frac{T+1}{T+2}\n\\] For large \\(T\\) is almost certain. Here \\(a=A=1\\)."
  },
  {
    "objectID": "03-conjugate.html#black-swans-1",
    "href": "03-conjugate.html#black-swans-1",
    "title": "Bayes AI",
    "section": "Black Swans",
    "text": "Black Swans\nPrinciple of Induction (Hume)\nThe probability of never seeing a Black Swan is given by \\[\np( y_{T+1} =1 , \\ldots ,  y_{T+n} = 1 | y_1 , \\ldots , y_T ) = \\frac{ T+1 }{ T+n+1 } \\rightarrow 0\n\\]\nBlack Swan will eventually happen – don’t be surprised when it actually happens."
  },
  {
    "objectID": "03-conjugate.html#bayesian-learning-poisson-gamma",
    "href": "03-conjugate.html#bayesian-learning-poisson-gamma",
    "title": "Bayes AI",
    "section": "Bayesian Learning: Poisson-Gamma",
    "text": "Bayesian Learning: Poisson-Gamma\nPoisson/Gamma: Suppose that \\(Y_1 , \\ldots , Y_n \\mid \\lambda \\sim Poi ( \\lambda )\\).\nLet \\(\\lambda \\sim Gamma  ( \\alpha , \\beta )\\)\n\\(( \\alpha , \\beta )\\) are known hyper-parameters.\n\nThe posterior distribution is\n\n\\[\n\\begin{aligned}\np ( \\lambda | y ) & \\propto\n\\exp ( - n \\lambda ) \\lambda^{ \\sum y_i } \\lambda^{ \\alpha - 1 } \\exp ( - \\beta \\lambda ) \\\\\n& \\sim Gamma ( \\alpha + \\sum y_i , n + \\beta )\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "03-conjugate.html#example-clinical-trials",
    "href": "03-conjugate.html#example-clinical-trials",
    "title": "Bayes AI",
    "section": "Example: Clinical Trials",
    "text": "Example: Clinical Trials\nNovick and Grizzle: A Bayesian Approach to the Analysis of Data from Clinical Trials\nFour treatments for duodenal ulcers.\nDoctors assess the state of the patient.\nSequential data\n(\\(\\alpha\\)-spending function, can only look at prespecified times).\n\n\n\nTreat\nExcellent\nFair\nDeath\n\n\n\n\nA\n76\n17\n7\n\n\nB\n89\n10\n1\n\n\nC\n86\n13\n1\n\n\nD\n88\n9\n3\n\n\n\nConclusion: Cannot reject at the 5% level\nConjugate binomial/beta model+sensitivity analysis."
  },
  {
    "objectID": "03-conjugate.html#sensitivity-analysis",
    "href": "03-conjugate.html#sensitivity-analysis",
    "title": "Bayes AI",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\nImportant to do a sensitivity analysis.\n\n\n\nTreat\nExcellent\nFair\nDeath\n\n\n\n\nA\n76\n17\n7\n\n\nB\n89\n10\n1\n\n\nC\n86\n13\n1\n\n\nD\n88\n9\n3\n\n\n\nPoisson-Gamma, prior \\(\\Gamma ( m , z)\\) and \\(\\lambda_i\\) be the expected death rate.\nCompute \\(P \\left ( \\frac{ \\lambda_1 }{ \\lambda_2 } &gt; c | D \\right )\\)\n\n\n\nProb\n( 0 , 0 )\n( 100, 2)\n( 200 , 5)\n\n\n\n\n\\(P \\left ( \\frac{ \\lambda_1 }{ \\lambda_2 } &gt; 1.3 | D \\right )\\)\n0.95\n0.88\n0.79\n\n\n\\(P \\left ( \\frac{ \\lambda_1 }{ \\lambda_2 } &gt; 1.6  | D \\right )\\)\n0.91\n0.80\n0.64"
  },
  {
    "objectID": "03-conjugate.html#bayesian-learning-normal-normal",
    "href": "03-conjugate.html#bayesian-learning-normal-normal",
    "title": "Bayes AI",
    "section": "Bayesian Learning: Normal-Normal",
    "text": "Bayesian Learning: Normal-Normal\nUsing Bayes rule we get \\[\np( \\mu | y ) \\propto p( y| \\mu ) p( \\mu )\n\\]\n\nPosterior is given by\n\n\\[\np( \\mu | y ) \\propto \\exp \\left ( - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n ( y_i - \\mu )^2 -\n\\frac{1}{2 \\tau^2} ( \\mu - \\mu_0 )^2 \\right )\n\\] Hence \\(\\mu | y \\sim N \\left ( \\hat{\\mu}_B , V_{\\mu} \\right )\\) where\n\\[\n\\hat{\\mu}_B = \\frac{ n / \\sigma^2 }{ n / \\sigma^2 + 1 / \\tau^2 } \\bar{y}\n+  \\frac{ 1 / \\tau^2 }{ n / \\sigma^2 + 1 / \\tau^2 }\\mu_0 \\; \\;\n{\\rm and} \\; \\;  V_{\\mu}^{-1} = \\frac{n}{ \\sigma^2 } + \\frac{1}{\\tau^2}\n\\] A shrinkage estimator."
  },
  {
    "objectID": "03-conjugate.html#sat-scores",
    "href": "03-conjugate.html#sat-scores",
    "title": "Bayes AI",
    "section": "SAT Scores",
    "text": "SAT Scores\nSAT (\\(200-800\\)): 8 high schools and estimate effects.\n\n\n\nSchool\nEstimated \\(y_j\\)\nSt. Error \\(\\sigma_j\\)\nAverage Treatment \\(\\theta_i\\)\n\n\n\n\nA\n28\n15\n?\n\n\nB\n8\n10\n?\n\n\nC\n-3\n16\n?\n\n\nD\n7\n11\n?\n\n\nE\n-1\n9\n?\n\n\nF\n1\n11\n?\n\n\nG\n18\n10\n?\n\n\nH\n12\n18\n?\n\n\n\n\n\\(\\theta_j\\) average effects of coaching programs\n\\(y_j\\) estimated treatment effects, for school \\(j\\), standard error \\(\\sigma_j\\)."
  },
  {
    "objectID": "03-conjugate.html#estimates",
    "href": "03-conjugate.html#estimates",
    "title": "Bayes AI",
    "section": "Estimates",
    "text": "Estimates\nTwo programs appear to work (improvements of 18 and 28)\n\nLarge standard errors. Overlapping Confidence Intervals?\nClassical hypothesis test fails to reject the hypothesis that the \\(\\theta_j\\)’s are equal.\nPooled estimate has standard error of \\(4.2\\) with\n\n\\[\n\\hat{\\theta} = \\frac{  \\sum_j ( y_j / \\sigma_j^2 ) }{ \\sum_j ( 1 / \\sigma_j^2 ) } = 7.9\n\\]\n\nNeither separate or pooled seems sensible.\n\nBayesian shrinkage!"
  },
  {
    "objectID": "03-conjugate.html#hierarchical-model",
    "href": "03-conjugate.html#hierarchical-model",
    "title": "Bayes AI",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nHierarchical Model (\\(\\sigma_j^2\\) known) is given by \\[\n\\bar{y}_j | \\theta_j \\sim N ( \\theta_j , \\sigma_j^2 )\n\\] Unequal variances–differential shrinkage.\n\nPrior Distribution: \\(\\theta_j \\sim N ( \\mu , \\tau^2 )\\) for \\(1 \\leq j \\leq 8\\).\n\nTraditional random effects model.\nExchangeable prior for the treatment effects.\nAs \\(\\tau \\rightarrow 0\\) (complete pooling) and as \\(\\tau \\rightarrow \\infty\\) (separate estimates).\n\nHyper-prior Distribution: \\(p( \\mu , \\tau^2 ) \\propto 1 / \\tau\\).\n\nThe posterior \\(p( \\mu , \\tau^2 | y )\\) can be used to “estimate” \\(( \\mu , \\tau^2 )\\)."
  },
  {
    "objectID": "03-conjugate.html#posterior",
    "href": "03-conjugate.html#posterior",
    "title": "Bayes AI",
    "section": "Posterior",
    "text": "Posterior\nJoint Posterior Distribution \\(y = ( y_1 , \\ldots , y_J )\\) \\[\np( \\theta , \\mu , \\tau | y )\n\\propto  p( y| \\theta )   p( \\theta | \\mu , \\tau )p( \\mu , \\tau )\n\\] \\[\n\\propto p( \\mu , \\tau^2)\n\\prod_{i=1}^8 N ( \\theta_j | \\mu , \\tau^2 ) \\prod_{j=1}^8\nN ( y_j | \\theta_j )\n\\] \\[\n\\propto \\tau^{-9} \\exp \\left ( - \\frac{1}{2} \\sum_j \\frac{1}{\\tau^2} ( \\theta_j - \\mu )^2 -\n\\frac{1}{2} \\sum_j \\frac{1}{\\sigma_j^2} ( y_j - \\theta_j )^2 \\right )\n\\] MCMC!"
  },
  {
    "objectID": "03-conjugate.html#posterior-inference-1",
    "href": "03-conjugate.html#posterior-inference-1",
    "title": "Bayes AI",
    "section": "Posterior Inference",
    "text": "Posterior Inference\nReport posterior quantiles\n\n\n\nSchool\n2.5%\n25%\n50%\n75%\n97.5%\n\n\n\n\nA\n-2\n6\n10\n16\n32\n\n\nB\n-5\n4\n8\n12\n20\n\n\nC\n-12\n3\n7\n11\n22\n\n\nD\n-6\n4\n8\n12\n21\n\n\nE\n-10\n2\n6\n10\n19\n\n\nF\n-9\n2\n6\n10\n19\n\n\nG\n-1\n6\n10\n15\n27\n\n\nH\n-7\n4\n8\n13\n23\n\n\n\\(\\mu\\)\n-2\n5\n8\n11\n18\n\n\n\\(\\tau\\)\n0.3\n2.3\n5.1\n8.8\n21\n\n\n\nSchools \\(A\\) and \\(G\\) are similar!"
  },
  {
    "objectID": "03-conjugate.html#bayesian-shrinkage",
    "href": "03-conjugate.html#bayesian-shrinkage",
    "title": "Bayes AI",
    "section": "Bayesian Shrinkage",
    "text": "Bayesian Shrinkage\nBayesian shrinkage provides a way of modeling complex datasets.\n\nBaseball batting averages: Stein’s Paradox\nBatter-pitcher match-up: Kenny Lofton and Derek Jeter\nBayes Elections\nToxoplasmosis\nBayes MoneyBall\nBayes Portfolio Selection"
  },
  {
    "objectID": "03-conjugate.html#example-baseball",
    "href": "03-conjugate.html#example-baseball",
    "title": "Bayes AI",
    "section": "Example: Baseball",
    "text": "Example: Baseball\nBatter-pitcher match-up?\nPrior information on overall ability of a player.\nSmall sample size, pitcher variation.\n\nLet \\(p_i\\) denote batter’s ability. Observed number of hits \\(y_i\\)\n\n\\[\n(y_i | p_i ) \\sim Bin ( T_i , p_i ) \\; \\; {\\rm with} \\; \\; p_i \\sim Be ( \\alpha , \\beta )\n\\] where \\(T_i\\) is the number of at-bats against pitcher \\(i\\). A priori \\(E( p_i ) = \\alpha / (\\alpha+\\beta ) = \\bar{p}_i\\).\n\nThe extra heterogeneity leads to a prior variance \\(Var (p_i ) = \\bar{p}_i (1 - \\bar{p}_i ) \\phi\\) where \\(\\phi = ( \\alpha + \\beta + 1 )^{-1}\\)."
  },
  {
    "objectID": "03-conjugate.html#sports-data-baseball",
    "href": "03-conjugate.html#sports-data-baseball",
    "title": "Bayes AI",
    "section": "Sports Data: Baseball",
    "text": "Sports Data: Baseball\nKenny Lofton hitting versus individual pitchers.\n\n\n\n\n\n\n\n\n\nPitcher\nAt-bats\nHits\nObsAvg\n\n\n\n\nJ.C. Romero\n9\n6\n.667\n\n\nS. Lewis\n5\n3\n.600\n\n\nB. Tomko\n20\n11\n.550\n\n\nT. Hoffman\n6\n3\n.500\n\n\nK. Tapani\n45\n22\n.489\n\n\nA. Cook\n9\n4\n.444\n\n\nJ. Abbott\n34\n14\n.412\n\n\nA.J. Burnett\n15\n6\n.400\n\n\nK. Rogers\n43\n17\n.395\n\n\nA. Harang\n6\n2\n.333\n\n\n\n\n\n\n\n\nPitcher\nAt-bats\nHits\nObsAvg\n\n\n\n\nK. Appier\n49\n15\n.306\n\n\nR. Clemens\n62\n14\n.226\n\n\nC. Zambrano\n9\n2\n.222\n\n\nN. Ryan\n10\n2\n.200\n\n\nE. Hanson\n41\n7\n.171\n\n\nE. Milton\n19\n1\n.056\n\n\nM. Prior\n7\n0\n.000\n\n\nM. Prior\n7\n0\n.000\n\n\nTotal\n7630\n2283\n.299"
  },
  {
    "objectID": "03-conjugate.html#baseball-kenny-lofton",
    "href": "03-conjugate.html#baseball-kenny-lofton",
    "title": "Bayes AI",
    "section": "Baseball: Kenny Lofton",
    "text": "Baseball: Kenny Lofton\nKenny Lofton (career \\(.299\\) average, and current \\(.308\\) average for 2006 season) was facing the pitcher Milton (current record 1 for 19).\n\nIs putting in a weaker player really a better bet?\nOver-reaction to bad luck?\n\n\\(\\mathbb{P}\\left ( \\leq 1 \\; {\\rm hit \\; in \\; } 19 \\; {\\rm attempts} | p = 0.3 \\right ) = 0.01\\)\nAn unlikely 1-in-100 event."
  },
  {
    "objectID": "03-conjugate.html#baseball-kenny-lofton-1",
    "href": "03-conjugate.html#baseball-kenny-lofton-1",
    "title": "Bayes AI",
    "section": "Baseball: Kenny Lofton",
    "text": "Baseball: Kenny Lofton\nBayes solution: shrinkage. Borrow strength across pitchers\nBayes estimate: use the posterior mean\nLofton’s batting estimates that vary from \\(.265\\) to \\(.340\\).\nThe lowest being against Milton.\n\\(.265 &lt; .275\\)\nConclusion: resting Lofton against Milton was justified!!"
  },
  {
    "objectID": "03-conjugate.html#bayes-batter-pitcher-match-up",
    "href": "03-conjugate.html#bayes-batter-pitcher-match-up",
    "title": "Bayes AI",
    "section": "Bayes Batter-pitcher match-up",
    "text": "Bayes Batter-pitcher match-up\nHere’s our model again ...\n\nSmall sample sizes and pitcher variation.\nLet \\(p_i\\) denote Lofton’s ability. Observed number of hits \\(y_i\\)\n\n\\[\n(y_i | p_i ) \\sim Bin ( T_i , p_i ) \\; \\; {\\rm with} \\; \\; p_i \\sim Be ( \\alpha , \\beta )\n\\] where \\(T_i\\) is the number of at-bats against pitcher \\(i\\).\nEstimate \\(( \\alpha , \\beta )\\)"
  },
  {
    "objectID": "03-conjugate.html#example-derek-jeter",
    "href": "03-conjugate.html#example-derek-jeter",
    "title": "Bayes AI",
    "section": "Example: Derek Jeter",
    "text": "Example: Derek Jeter\nDerek Jeter 2006 season versus individual pitchers.\n\n\n\nPitcher\nAt-bats\nHits\nObsAvg\nEstAvg\n95% Int\n\n\n\n\nR. Mendoza\n6\n5\n.833\n.322\n(.282, .394)\n\n\nH. Nomo\n20\n12\n.600\n.326\n(.289, .407)\n\n\nA.J.Burnett\n5\n3\n.600\n.320\n(.275, .381)\n\n\nE. Milton\n28\n14\n.500\n.324\n(.291, .397)\n\n\nD. Cone\n8\n4\n.500\n.320\n(.218, .381)\n\n\nR. Lopez\n45\n21\n.467\n.326\n(.291, .401)\n\n\nK. Escobar\n39\n16\n.410\n.322\n(.281, .386)\n\n\nJ. Wettland\n5\n2\n.400\n.318\n(.275, .375)"
  },
  {
    "objectID": "03-conjugate.html#example-derek-jeter-1",
    "href": "03-conjugate.html#example-derek-jeter-1",
    "title": "Bayes AI",
    "section": "Example: Derek Jeter",
    "text": "Example: Derek Jeter\nDerek Jeter 2006 season versus individual pitchers.\n\n\n\nPitcher\nAt-bats\nHits\nObsAvg\nEstAvg\n95% Int\n\n\n\n\nT. Wakefield\n81\n26\n.321\n.318\n(.279, .364)\n\n\nP. Martinez\n83\n21\n.253\n.312\n(.254, .347)\n\n\nK. Benson\n8\n2\n.250\n.317\n(.264, .368)\n\n\nT. Hudson\n24\n6\n.250\n.315\n(.260, .362)\n\n\nJ. Smoltz\n5\n1\n.200\n.314\n(.253, .355)\n\n\nF. Garcia\n25\n5\n.200\n.314\n(.253, .355)\n\n\nB. Radke\n41\n8\n.195\n.311\n(.247, .347)\n\n\nD. Kolb\n5\n0\n.000\n.316\n(.258, .363)\n\n\nJ. Julio\n13\n0\n.000\n.312\n(.243, .350 )\n\n\nTotal\n6530\n2061\n.316"
  },
  {
    "objectID": "03-conjugate.html#bayes-estimates",
    "href": "03-conjugate.html#bayes-estimates",
    "title": "Bayes AI",
    "section": "Bayes Estimates",
    "text": "Bayes Estimates\n\nStern estimates \\(\\hat{\\phi} = ( \\alpha + \\beta + 1 )^{-1} = 0.002\\) for Jeter\nDoesn’t vary much across the population of pitchers.\nThe extremes are shrunk the most also matchups with the smallest sample sizes.\nJeter had a season \\(.308\\) average.\n\nBayes estimates vary from \\(.311\\) to \\(.327\\)–he’s very consistent.\nIf all players had a similar record then a constant batting average would make sense."
  },
  {
    "objectID": "03-conjugate.html#bayes-elections-nate-silver-multinomial-dirichlet",
    "href": "03-conjugate.html#bayes-elections-nate-silver-multinomial-dirichlet",
    "title": "Bayes AI",
    "section": "Bayes Elections: Nate Silver: Multinomial-Dirichlet",
    "text": "Bayes Elections: Nate Silver: Multinomial-Dirichlet\nPredicting the Electoral Vote (EV)\n\nMultinomial-Dirichlet: \\((\\hat{p} | p) \\sim Multi (p), ( p | \\alpha ) \\sim Dir (\\alpha)\\)\n\n\\[\np_{Obama} = ( p_{1}, \\ldots ,p_{51} | \\hat{p}) \\sim Dir \\left ( \\alpha + \\hat{p} \\right )\n\\]\n\nFlat uninformative prior \\(\\alpha\\equiv 1\\).\n\nhttp://www.electoral-vote.com/evp2012/Pres/prespolls.csv"
  },
  {
    "objectID": "03-conjugate.html#bayes-elections-nate-silver-simulation",
    "href": "03-conjugate.html#bayes-elections-nate-silver-simulation",
    "title": "Bayes AI",
    "section": "Bayes Elections: Nate Silver: Simulation",
    "text": "Bayes Elections: Nate Silver: Simulation\nCalculate probabilities via simulation: rdirichlet \\[\np \\left ( p_{j,O} | {\\rm data} \\right )  \\;\\; {\\rm and} \\;  \\;\np \\left ( EV &gt;270 | {\\rm data} \\right )\n\\]\nThe election vote prediction is given by the sum \\[\nEV =\\sum_{j=1}^{51} EV(j) \\mathbb{E} \\left ( p_{j} | {\\rm data} \\right )\n\\] where \\(EV(j)\\) are for individual states"
  },
  {
    "objectID": "03-conjugate.html#polling-data",
    "href": "03-conjugate.html#polling-data",
    "title": "Bayes AI",
    "section": "Polling Data",
    "text": "Polling Data\nElectoral Vote (EV), Polling Data: Mitt and Obama percentages\n\n\n\n\n\n\n\n\n\nState\nM.pct\nO.pct\nEV\n\n\n\n\nAlabama\n58\n36\n9\n\n\nAlaska\n55\n37\n3\n\n\nArizona\n50\n46\n10\n\n\nArkansas\n51\n44\n6\n\n\nCalifornia\n33\n55\n55\n\n\nColorado\n45\n52\n9\n\n\nConnecticut\n31\n56\n7\n\n\nDelaware\n38\n56\n3\n\n\nD.C.\n13\n82\n3\n\n\nFlorida\n46\n50\n27\n\n\nGeorgia\n52\n47\n15\n\n\nHawaii\n32\n63\n4\n\n\nIdaho\n68\n26\n4\n\n\n\n\n\n\n\n\nState\nM.pct\nO.pct\nEV\n\n\n\n\nIllinois\n35\n59\n21\n\n\nIndiana\n48\n48\n11\n\n\nIowa\n37\n54\n7\n\n\nKansas\n63\n31\n6\n\n\nKentucky\n51\n42\n8\n\n\nLouisiana\n50\n43\n9\n\n\nMaine\n35\n56\n4\n\n\nMaryland\n39\n54\n10\n\n\nMassachusetts\n34\n53\n12\n\n\nMichigan\n37\n53\n17\n\n\nMinnesota\n42\n53\n10\n\n\nMississippi\n46\n33\n6"
  },
  {
    "objectID": "03-conjugate.html#polling-data-1",
    "href": "03-conjugate.html#polling-data-1",
    "title": "Bayes AI",
    "section": "Polling Data:",
    "text": "Polling Data:\n\n\n\n\n\n\n\n\n\nElection 2008 Prediction. Obama 370\n\n\n\n\n\n\n\nElection 2012 Prediction. Obama 332."
  },
  {
    "objectID": "03-conjugate.html#chicago-bears-2014-2015-season",
    "href": "03-conjugate.html#chicago-bears-2014-2015-season",
    "title": "Bayes AI",
    "section": "Chicago Bears 2014-2015 Season",
    "text": "Chicago Bears 2014-2015 Season\nBayes Learning: Update our beliefs in light of new information\n\nIn the 2014-2015 season.\n\nThe Bears suffered back-to-back 50-points defeats.\nPartiots-Bears \\(51-23\\)\nPackers-Bears \\(55-14\\)\n\nTheir next game was at home against the Minnesota Vikings.\n\nCurrent line against the Vikings was \\(-3.5\\) points.\nSlightly over a field goal\nWhat’s the Bayes approach to learning the line?"
  },
  {
    "objectID": "03-conjugate.html#hierarchical-model-1",
    "href": "03-conjugate.html#hierarchical-model-1",
    "title": "Bayes AI",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nHierarchical model for the current average win/lose this year \\[\n\\begin{aligned}\n\\bar{y} | \\theta & \\sim N \\left ( \\theta , \\frac{\\sigma^2}{n} \\right ) \\sim N \\left ( \\theta , \\frac{18.34^2}{9} \\right )\\\\\n\\theta & \\sim N( 0 , \\tau^2 )\n\\end{aligned}\n\\] Here \\(n =9\\) games so far. With \\(s = 18.34\\) points\nPre-season prior mean \\(\\mu_0 = 0\\), standard deviation \\(\\tau = 4\\).\nRecord so-far. Data \\(\\bar{y} = -9.22\\)."
  },
  {
    "objectID": "03-conjugate.html#chicago-bears",
    "href": "03-conjugate.html#chicago-bears",
    "title": "Bayes AI",
    "section": "Chicago Bears",
    "text": "Chicago Bears\nBayes Shrinkage estimator \\[\n\\mathbb{E} \\left ( \\theta | \\bar{y} , \\tau \\right ) = \\frac{ \\tau^2 }{ \\tau^2 + \\frac{\\sigma^2}{n} } \\bar{y}\n\\]\nThe Shrinkage factor is \\(0.3\\)!!\nThat’s quite a bit of shrinkage. Why?\n\nOur updated estimator is\n\n\\[\n\\mathbb{E} \\left ( \\theta | \\bar{y} , \\tau \\right ) = - 2.75 &gt; -.3.5\n\\] where current line is \\(-3.5\\).\n\nBased on our hierarchical model this is an over-reaction.\n\nOne point change on the line is about 3% on a probability scale.\nAlternatively, calculate a market-based \\(\\tau\\) given line \\(=-3.5\\)."
  },
  {
    "objectID": "03-conjugate.html#chicago-bears-1",
    "href": "03-conjugate.html#chicago-bears-1",
    "title": "Bayes AI",
    "section": "Chicago Bears",
    "text": "Chicago Bears\nLast two defeats were 50 points scored by opponent (2014-15)\n\n\nCode\nbears=c(-3,8,8,-21,-7,14,-13,-28,-41)\nmean(bears)\n\n\n[1] -9.222222\n\n\nCode\nsd(bears)\n\n\n[1] 18.34242\n\n\nCode\ntau=4\nsig2=sd(bears)*sd(bears)/9\ntau^2/(sig2+tau^2)\n\n\n[1] 0.2997225\n\n\nCode\npnorm(-2.76/18)\n\n\n[1] 0.4390677\n\n\nHome advantage is worth 3 points. Vikings an average record.\nResult: Bears 21, Vikings 13"
  },
  {
    "objectID": "03-conjugate.html#steins-paradox",
    "href": "03-conjugate.html#steins-paradox",
    "title": "Bayes AI",
    "section": "Stein’s Paradox",
    "text": "Stein’s Paradox\nStein paradox: possible to make a uniform improvement on the MLE in terms of MSE.\n\nMistrust of the statistical interpretation of Stein’s result.\n\nIn particular, the loss function.\n\nDifficulties in adapting the procedure to special cases\nLong familiarity with good properties for the MLE\n\nAny gains from a “complicated” procedure could not be worth the extra trouble (Tukey, savings not more than 10 % in practice)\nFor \\(k\\ge 3\\), we have the remarkable inequality \\[\nMSE(\\hat \\theta_{JS},\\theta) &lt; MSE(\\bar y,\\theta) \\; \\forall \\theta\n\\] Bias-variance explanation! Inadmissability of the classical stats."
  },
  {
    "objectID": "03-conjugate.html#baseball-batting-averages",
    "href": "03-conjugate.html#baseball-batting-averages",
    "title": "Bayes AI",
    "section": "Baseball Batting Averages",
    "text": "Baseball Batting Averages\nData: 18 major-league players after 45 at bats (1970 season)\n\n\n\nPlayer\n\\(\\bar{y}_i\\)\n\\(E ( p_i | D )\\)\naverage season\n\n\n\n\nClemente\n0.400\n0.290\n0.346\n\n\nRobinson\n0.378\n0.286\n0.298\n\n\nHoward\n0.356\n0.281\n0.276\n\n\nJohnstone\n0.333\n0.277\n0.222\n\n\nBerry\n0.311\n0.273\n0.273\n\n\nSpencer\n0.311\n0.273\n0.270\n\n\nKessinger\n0.311\n0.268\n0.263\n\n\nAlvarado\n0.267\n0.264\n0.210\n\n\nSanto\n0.244\n0.259\n0.269\n\n\nSwoboda\n0.244\n0.259\n0.230\n\n\nUnser\n0.222\n0.254\n0.264\n\n\nWilliams\n0.222\n0.254\n0.256\n\n\nScott\n0.222\n0.254\n0.303\n\n\nPetrocelli\n0.222\n0.254\n0.264\n\n\nRodriguez\n0.222\n0.254\n0.226\n\n\nCampanens\n0.200\n0.259\n0.285\n\n\nMunson\n0.178\n0.244\n0.316\n\n\nAlvis\n0.156\n0.239\n0.200"
  },
  {
    "objectID": "03-conjugate.html#baseball-data-first-shrinkage-estimator-efron-and-morris",
    "href": "03-conjugate.html#baseball-data-first-shrinkage-estimator-efron-and-morris",
    "title": "Bayes AI",
    "section": "Baseball Data: First Shrinkage Estimator: Efron and Morris",
    "text": "Baseball Data: First Shrinkage Estimator: Efron and Morris\n\nBaseball Shrinkage"
  },
  {
    "objectID": "03-conjugate.html#shrinkage",
    "href": "03-conjugate.html#shrinkage",
    "title": "Bayes AI",
    "section": "Shrinkage",
    "text": "Shrinkage\nLet \\(\\theta_i\\) denote the end of season average\n\nLindley: shrink to the overall grand mean\n\n\\[\nc = 1 - \\frac{ ( k - 3 ) \\sigma^2 }{ \\sum ( \\bar{y}_i - \\bar{y} )^2 }\n\\] where \\(\\bar{y}\\) is the overall grand mean and\n\\[\n\\hat{\\theta} = c \\bar{y}_i + ( 1 - c ) \\bar{y}\n\\]\n\nBaseball data: \\(c = 0.212\\) and \\(\\bar{y} = 0.265\\).\n\nCompute \\(\\sum ( \\hat{\\theta}_i - \\bar{y}^{obs}_i )^2\\) and see which is lower: \\[\nMLE = 0.077 \\; \\; STEIN = 0.022\n\\] That’s a factor of \\(3.5\\) times better!"
  },
  {
    "objectID": "03-conjugate.html#batting-averages",
    "href": "03-conjugate.html#batting-averages",
    "title": "Bayes AI",
    "section": "Batting Averages",
    "text": "Batting Averages"
  },
  {
    "objectID": "03-conjugate.html#baseball-paradoxes",
    "href": "03-conjugate.html#baseball-paradoxes",
    "title": "Bayes AI",
    "section": "Baseball Paradoxes",
    "text": "Baseball Paradoxes\nShrinkage on Clemente too severe: \\(z_{Cl} = 0.265 + 0.212 ( 0.400 - 0.265) = 0.294\\).\nThe \\(0.212\\) seems a little severe\n\nLimited translation rules, maximum shrinkage eg. 80%\nNot enough shrinkage eg O’Connor ( \\(y = 1 , n = 2\\)). \\(z_{O'C} = 0.265 + 0.212 ( 0.5  - 0.265 ) = 0.421\\).\n\nStill better than Ted Williams \\(0.406\\) in 1941.\n\nForeign car sales (\\(k = 19\\)) will further improve MSE performance! It will change the shrinkage factors.\nClearly an improvement over the Stein estimator is\n\n\\[\n\\hat{\\theta}_{S+} =\n\\max \\left ( \\left ( 1 - \\frac{k-2}{ \\sum \\bar{Y}_i^2 } \\right ) , 0 \\right ) \\bar{Y}_i\n\\]"
  },
  {
    "objectID": "03-conjugate.html#baseball-prior",
    "href": "03-conjugate.html#baseball-prior",
    "title": "Bayes AI",
    "section": "Baseball Prior",
    "text": "Baseball Prior\n\nInclude extra prior knowledge\nEmpirical distribution of all major league players \\[\n\\theta_i \\sim N ( 0.270 , 0.015 )\n\\]\nThe 0.270 provides another origin to shrink to and the prior variance 0.015 would give a different shrinkage factor.\nTo fully understand maybe we should build a probabilistic model and see what the posterior mean is as our estimator for the unknown parameters."
  },
  {
    "objectID": "03-conjugate.html#shrinkage-unequal-variances",
    "href": "03-conjugate.html#shrinkage-unequal-variances",
    "title": "Bayes AI",
    "section": "Shrinkage: Unequal Variances",
    "text": "Shrinkage: Unequal Variances\nModel \\(Y_i | \\theta_i  \\sim N ( \\theta_i , D_i )\\) where \\(\\theta_i \\sim N ( \\theta_0 , A ) \\sim N ( 0.270 , 0.015 )\\).\n\nThe \\(D_i\\) can be different – unequal variances\nBayes posterior means are given by\n\n\\[\nE ( \\theta_i | Y ) = ( 1 - B_i ) Y_i \\; \\; {\\rm where} \\; \\;\nB_i = \\frac{ D_i }{ D_i + A }\n\\] where \\(\\hat{A}\\) is estimated from the data, see Efron and Morris (1975).\n\nDifferent shrinkage factors as different variances \\(D_i\\).\n\n\\(D_i \\propto n_i^{-1}\\) and so smaller sample sizes are shrunk more.\nMakes sense."
  },
  {
    "objectID": "03-conjugate.html#example-toxoplasmosis-data",
    "href": "03-conjugate.html#example-toxoplasmosis-data",
    "title": "Bayes AI",
    "section": "Example: Toxoplasmosis Data",
    "text": "Example: Toxoplasmosis Data\nDisease of Blood that is endemic in tropical regions.\nData: 5000 people in El Salvador (varying sample sizes) from 36 cities.\n\nEstimate “true” prevalences \\(\\theta_i\\) for \\(1 \\leq i \\leq 36\\)\nAllocation of Resources: should we spend funds on the city with the highest observed occurrence of the disease? Same shrinkage factors?\nShrinkage Procedure (Efron and Morris, p315) \\[\nz_i = c_i y_i\n\\] where \\(y_i\\) are the observed relative rates (normalized so \\(\\bar{y} = 0\\) The smaller sample sizes will get shrunk more.\n\nThe most gentle are in the range \\(0.6 \\rightarrow 0.9\\) but some are \\(0.1 \\rightarrow 0.3\\)."
  },
  {
    "objectID": "03-conjugate.html#bayes-portfolio-selection",
    "href": "03-conjugate.html#bayes-portfolio-selection",
    "title": "Bayes AI",
    "section": "Bayes Portfolio Selection",
    "text": "Bayes Portfolio Selection\n\nde Finetti and Markowitz: Mean-variance portfolio shrinkage: \\(\\frac{1}{\\gamma} \\Sigma^{-1} \\mu\\)\nDifferent shrinkage factors for different history lengths.\nPortfolio Allocation in the SP500 index\nEntry/exit; splits; spin-offs etc. For example, 73 replacements to the SP500 index in period 1/1/94 to 12/31/96.\nAdvantage: \\(E ( \\alpha | D_t ) = 0.39\\), that is 39 bps per month which on an annual basis is \\(\\alpha = 468\\) bps.\nThe posterior mean for \\(\\beta\\) is \\(p ( \\beta | D_t ) = 0.745\\)\n\n\\(\\bar{x}_{M} = 12.25 \\%\\) and \\(\\bar{x}_{PT} = 14.05 \\%\\)."
  },
  {
    "objectID": "03-conjugate.html#sp-composition",
    "href": "03-conjugate.html#sp-composition",
    "title": "Bayes AI",
    "section": "SP Composition",
    "text": "SP Composition\n\n\n\nDate\nSymbol\n6/96\n12/89\n12/79\n12/69\n\n\n\n\nGeneral Electric\nGE\n2.800\n2.485\n1.640\n1.569\n\n\nCoca Cola\nKO\n2.342\n1.126\n0.606\n1.051\n\n\nExxon\nXON\n2.142\n2.672\n3.439\n2.957\n\n\nATT\nT\n2.030\n2.090\n5.197\n5.948\n\n\nPhilip Morris\nMO\n1.678\n1.649\n0.637\n*****\n\n\nRoyal Dutch\nRD\n1.636\n1.774\n1.191\n*****\n\n\nMerck\nMRK\n1.615\n1.308\n0.773\n0.906\n\n\nMicrosoft\nMSFT\n1.436\n*****\n*****\n*****\n\n\nJohnson/Johnson\nJNJ\n1.320\n0.845\n0.689\n*****\n\n\nIntel\nINTC\n1.262\n*****\n*****\n*****\n\n\nProcter and Gamble\nPG\n1.228\n1.040\n0.871\n0.993\n\n\nWalmart\nWMT\n1.208\n1.084\n*****\n*****\n\n\nIBM\nIBM\n1.181\n2.327\n5.341\n9.231\n\n\nHewlett Packard\nHWP\n1.105\n0.477\n0.497\n*****"
  },
  {
    "objectID": "03-conjugate.html#sp-composition-1",
    "href": "03-conjugate.html#sp-composition-1",
    "title": "Bayes AI",
    "section": "SP Composition",
    "text": "SP Composition\n\n\n\nDate\nSymbol\n6/96\n12/89\n12/79\n12/69\n\n\n\n\nPepsi\nPEP\n1.061\n0.719\n*****\n*****\n\n\nPfizer\nPFE\n0.918\n0.491\n0.408\n0.486\n\n\nDupont\nDD\n0.910\n1.229\n0.837\n1.101\n\n\nAIG\nAIG\n0.910\n0.723\n*****\n*****\n\n\nMobil\nMOB\n0.906\n1.093\n1.659\n1.040\n\n\nBristol Myers Squibb\nBMY\n0.878\n1.247\n*****\n0.484\n\n\nGTE\nGTE\n0.849\n0.975\n0.593\n0.705\n\n\nGeneral Motors\nGM\n0.848\n1.086\n2.079\n4.399\n\n\nDisney\nDIS\n0.839\n0.644\n*****\n*****\n\n\nCiticorp\nCCI\n0.831\n0.400\n0.418\n*****\n\n\nBellSouth\nBLS\n0.822\n1.190\n*****\n*****\n\n\nMotorola\nMOT\n0.804\n*****\n*****\n*****\n\n\nFord\nF\n0.798\n0.883\n0.485\n0.640\n\n\nChervon\nCHV\n0.794\n0.990\n1.370\n0.966"
  },
  {
    "objectID": "03-conjugate.html#sp-composition-2",
    "href": "03-conjugate.html#sp-composition-2",
    "title": "Bayes AI",
    "section": "SP Composition",
    "text": "SP Composition\n\n\n\nDate\nSymbol\n6/96\n12/89\n12/79\n12/69\n\n\n\n\nAmoco\nAN\n0.733\n1.198\n1.673\n0.758\n\n\nEli Lilly\nLLY\n0.720\n0.814\n*****\n*****\n\n\nAbbott Labs\nABT\n0.690\n0.654\n*****\n*****\n\n\nAmerHome Products\nAHP\n0.686\n0.716\n0.606\n0.793\n\n\nFedNatlMortgage\nFNM\n0.686\n*****\n*****\n*****\n\n\nMcDonald’s\nMCD\n0.686\n0.545\n*****\n*****\n\n\nAmeritech\nAIT\n0.639\n0.782\n*****\n*****\n\n\nCisco Systems\nCSCO\n0.633\n*****\n*****\n*****\n\n\nCMB\nCMB\n0.621\n*****\n*****\n*****\n\n\nSBC\nSBC\n0.612\n0.819\n*****\n*****\n\n\nBoeing\nBA\n0.598\n0.584\n0.462\n*****\n\n\nMMM\nMMM\n0.581\n0.762\n0.838\n1.331\n\n\nBankAmerica\nBAC\n0.560\n*****\n0.577\n*****\n\n\nBell Atlantic\nBEL\n0.556\n0.946\n*****\n*****"
  },
  {
    "objectID": "03-conjugate.html#sp-composition-3",
    "href": "03-conjugate.html#sp-composition-3",
    "title": "Bayes AI",
    "section": "SP Composition",
    "text": "SP Composition\n\n\n\nDate\nSymbol\n6/96\n12/89\n12/79\n12/69\n\n\n\n\nGillette\nG\n0.535\n*****\n*****\n*****\n\n\nKodak\nEK\n0.524\n0.570\n1.106\n*****\n\n\nChrysler\nC\n0.507\n*****\n*****\n0.367\n\n\nHome Depot\nHD\n0.497\n*****\n*****\n*****\n\n\nColgate\nCOL\n0.489\n0.499\n*****\n*****\n\n\nWells Fargo\nWFC\n0.478\n*****\n*****\n*****\n\n\nNations Bank\nNB\n0.453\n*****\n*****\n*****\n\n\nAmer Express\nAXP\n0.450\n0.621\n*****\n*****"
  },
  {
    "objectID": "03-conjugate.html#keynes-versus-buffett-capm",
    "href": "03-conjugate.html#keynes-versus-buffett-capm",
    "title": "Bayes AI",
    "section": "Keynes versus Buffett: CAPM",
    "text": "Keynes versus Buffett: CAPM\nkeynes = 15.08 + 1.83 market\nbuffett = 18.06 + 0.486 market\n\n\n\n\n\n\n\n\n\nYear\nKeynes\nMarket\n\n\n\n\n1928\n-3.4\n7.9\n\n\n1929\n0.8\n6.6\n\n\n1930\n-32.4\n-20.3\n\n\n1931\n-24.6\n-25.0\n\n\n1932\n44.8\n-5.8\n\n\n1933\n35.1\n21.5\n\n\n1934\n33.1\n-0.7\n\n\n1935\n44.3\n5.3\n\n\n1936\n56.0\n10.2\n\n\n\n\n\n\n\n\nYear\nKeynes\nMarket\n\n\n\n\n1937\n8.5\n-0.5\n\n\n1938\n-40.1\n-16.1\n\n\n1939\n12.9\n-7.2\n\n\n1940\n-15.6\n-12.9\n\n\n1941\n33.5\n12.5\n\n\n1942\n-0.9\n0.8\n\n\n1943\n53.9\n15.6\n\n\n1944\n14.5\n5.4\n\n\n1945\n14.6\n0.8"
  },
  {
    "objectID": "03-conjugate.html#kings-college-cambridge",
    "href": "03-conjugate.html#kings-college-cambridge",
    "title": "Bayes AI",
    "section": "King’s College Cambridge",
    "text": "King’s College Cambridge\n\nKeynes vs Cash"
  },
  {
    "objectID": "03-conjugate.html#superbowl-xlvii-ravens-vs-49ers",
    "href": "03-conjugate.html#superbowl-xlvii-ravens-vs-49ers",
    "title": "Bayes AI",
    "section": "SuperBowl XLVII: Ravens vs 49ers",
    "text": "SuperBowl XLVII: Ravens vs 49ers\nTradeSports.com\n\nSuperBowl XLVII"
  },
  {
    "objectID": "03-conjugate.html#super-bowl-xlvii-ravens-vs-49ers",
    "href": "03-conjugate.html#super-bowl-xlvii-ravens-vs-49ers",
    "title": "Bayes AI",
    "section": "Super Bowl XLVII: Ravens vs 49ers",
    "text": "Super Bowl XLVII: Ravens vs 49ers\n\nSuper Bowl XLVII was held at the Superdome in New Orleans on February 3, 2013.\nWe will track \\(X(t)\\) which corresponds to the Raven’s lead over the 49ers at each point in time. Table 3 provides the score at the end of each quarter.\n\n\\[\n\\begin{array}{c|ccccc}\nt & 0 & \\frac{1}{4} & \\frac{1}{2} & \\frac{3}{4} & 1  \\\\\\hline\nRavens & 0 & 7  & 21 & 28 & 34 \\\\\n49ers & 0 & 3 & 6 & 23 & 31\\\\\\hline\nX(t) & 0 & 4 & 15 & 5 & 3\\\\\n\\end{array}\n\\] SuperBowl XLVII by Quarter"
  },
  {
    "objectID": "03-conjugate.html#initial-market",
    "href": "03-conjugate.html#initial-market",
    "title": "Bayes AI",
    "section": "Initial Market",
    "text": "Initial Market\n\nInitial point spread Ravens being a four point underdog, \\(\\mu=-4\\). \\[\n\\mu = \\mathbb{E} \\left (X(1) \\right )=-4 .\n\\]\nThe Ravens upset the 49ers by \\(34-31\\) and \\(X(1)= 34-31=3\\) with the point spread being beaten by 7 points.\nTo determine the markets’ assessment of the probability that the Ravens would win at the beginning of the game we use the money-line odds."
  },
  {
    "objectID": "03-conjugate.html#initial-market-1",
    "href": "03-conjugate.html#initial-market-1",
    "title": "Bayes AI",
    "section": "Initial Market",
    "text": "Initial Market\n\nSan Francisco \\(-175\\)\nBaltimore Ravens \\(+155\\).\n\nThis implies that a bettor would have to place $175 to win $100 on the 49ers and a bet of $100 on the Ravens would lead to a win of $155.\nConvert both of these money-lines to implied probabilities of the each team winning\n\\[\np_{SF} = \\frac{175}{100+175} = 0.686 \\; \\; {\\rm and} \\; \\; p_{Bal} = \\frac{100}{100+155} = 0.392\n\\]"
  },
  {
    "objectID": "03-conjugate.html#probabilities-of-winning",
    "href": "03-conjugate.html#probabilities-of-winning",
    "title": "Bayes AI",
    "section": "Probabilities of Winning",
    "text": "Probabilities of Winning\nThe probabilities do not sum to one. This \"overound\" probability is talso known as the bookmaker’s edge. \\[\np_{SF} + p_{Bal} = 0.686+0.392 = 1.078\n\\] providing a \\(7.8\\)% edge for the bookmakers.\nthe \"market vig\" is the implied probability of the bookie making money on the bet.\n\n\nCode\no = 1.078-1\nv = o/(1+o)\nv\n\n\n[1] 0.07235622\n\n\nWe use the mid-point of the spread to determine \\(p\\) implying that\n\\[\np = \\frac{1}{2} p_{Bal} + \\frac{1}{2} (1 - p_{SF} ) = 0.353\n\\] From the Ravens perspective, we have \\(p = \\mathbb{P}(X(1)&gt;0) =0.353\\).\nBaltimore’s win probability started trading at \\(p^{mkt}_0 =0.38\\)"
  },
  {
    "objectID": "03-conjugate.html#half-time-analysis",
    "href": "03-conjugate.html#half-time-analysis",
    "title": "Bayes AI",
    "section": "Half Time Analysis",
    "text": "Half Time Analysis\nThe Ravens took a commanding \\(21-6\\) lead at half time. Market was trading at \\(p_{\\frac{1}{2}}^{mkt}= 0.90\\).\n\nDuring the 34 minute blackout 42760 contracts changed hands with Baltimore’s win probability ticking down from 95 to 94.\nThe win probability peak of 95% occurred after a third-quarter kickoff return for a touchdown.\nAt the end of the four quarter, however, when the 49ers nearly went into the lead with a touchdown, Baltimore’s win probability had dropped to 30%."
  },
  {
    "objectID": "03-conjugate.html#implied-volatility",
    "href": "03-conjugate.html#implied-volatility",
    "title": "Bayes AI",
    "section": "Implied Volatility",
    "text": "Implied Volatility\nTo calculate the implied volatility of the Superbowl we substitute the pair \\((\\mu,p) = (-4, .353)\\) into our definition and solve for \\(\\sigma_{IV}\\). \\[\n\\sigma = \\frac{\\mu}{\\Phi^{-1}(p)} \\, ,\n\\]\n\nWe obtain\n\n\\[\n\\sigma_{IV} = \\frac{\\mu}{\\Phi^{-1}(p)} = \\frac{-4}{-0.377}  = 10.60\n\\] where \\(\\Phi^{-1} ( p) = qnorm(0.353) = -0.377\\). The 4 point advantage assessed for the 49ers is under a \\(\\frac{1}{2} \\sigma\\) favorite.\n\nThe outcome \\(X(1)=3\\) was within one standard deviation of the pregame model which had an expectation of \\(\\mu=-4\\) and volatility of \\(\\sigma= 10.6\\)."
  },
  {
    "objectID": "03-conjugate.html#half-time-probabilities",
    "href": "03-conjugate.html#half-time-probabilities",
    "title": "Bayes AI",
    "section": "Half Time Probabilities",
    "text": "Half Time Probabilities\n\nWhat’s the probability of the Ravens winning given their lead at half time?\nAt half time Baltimore led by 15 points, 21 to 6.\nThe conditional mean for the final outcome is \\(15 +  0.5*(-4) = 13\\) and the conditional volatility is \\(10.6 \\sqrt{1-t}.\\)\nThese imply a probability of \\(.96\\) for Baltimore to win the game.\nA second estimate of the probability of winning given the half time lead can be obtained directly from the betting market.\nFrom the online betting market we also have traded contracts on TradeSports.com that yield a half time probability of \\(p_{\\frac{1}{2}} = 0.90\\)."
  },
  {
    "objectID": "03-conjugate.html#whats-the-implied-volatility-for-the-second-half",
    "href": "03-conjugate.html#whats-the-implied-volatility-for-the-second-half",
    "title": "Bayes AI",
    "section": "What’s the implied volatility for the second half?",
    "text": "What’s the implied volatility for the second half?\n\n\\(p_t^{mkt}\\) reflects all available information\nFor example, at half-time \\(t = \\frac{1}{2}\\) we would update\n\n\\[\n\\sigma_{IV,t=\\frac{1}{2}} = \\frac{ l + \\mu ( 1-t ) }{ \\Phi^{-1} ( p_t )  \\sqrt{1-t}} = \\frac{15-2}{ \\Phi^{-1}(0.9) / \\sqrt{2} } = 14\n\\] where \\(qnorm(0.9)=1.28\\).\n\nAs \\(14&gt; 10.6\\), the market was expecting a more volatile second half–possibly anticipating a comeback from the 49ers."
  },
  {
    "objectID": "03-conjugate.html#how-can-we-form-a-valid-betting-strategy",
    "href": "03-conjugate.html#how-can-we-form-a-valid-betting-strategy",
    "title": "Bayes AI",
    "section": "How can we form a valid betting strategy?",
    "text": "How can we form a valid betting strategy?\nGiven the initial implied volatility \\(\\sigma=10.6\\).\nAt half time with the Ravens having a \\(l +\\mu(1-t)=13\\) points edge\n\nWe would assess with \\(\\sigma = 10.6\\)\n\n\\[\np_{\\frac{1}{2}} = \\Phi \\left ( 13/ (10.6/\\sqrt{2}) \\right ) = 0.96\n\\] probability of winning versus the \\(p_{\\frac{1}{2}}^{mkt} = 0.90\\) rate.\n\nTo determine our optimal bet size, \\(\\omega_{bet}\\), on the Ravens we might appeal to the Kelly criterion (Kelly, 1956) which yields\n\n\\[\n\\omega_{bet} = p_{\\frac{1}{2}} - \\frac{q_{\\frac{1}{2}}}{O^{mkt}} = 0.96 - \\frac{0.1}{1/9} = 0.60\n\\]"
  },
  {
    "objectID": "03-conjugate.html#multivariate-normal",
    "href": "03-conjugate.html#multivariate-normal",
    "title": "Bayes AI",
    "section": "Multivariate Normal",
    "text": "Multivariate Normal\nIn the multivariate case, the normal-normal model is \\[\n\\theta \\sim N(\\mu_0,\\Sigma_0), \\quad y \\mid \\theta \\sim N(\\theta,\\Sigma).\n\\] The posterior distribution is \\[\n\\theta \\mid y \\sim N(\\mu_1,\\Sigma_1),\n\\] where \\[\n\\Sigma_1 = (\\Sigma_0^{-1} + \\Sigma^{-1})^{-1}, \\quad \\mu_1 = \\Sigma_1(\\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}y).\n\\] The predictive distribution is \\[\ny_{new} \\mid y \\sim N(\\mu_1,\\Sigma_1 + \\Sigma).\n\\]"
  },
  {
    "objectID": "03-conjugate.html#black-litterman",
    "href": "03-conjugate.html#black-litterman",
    "title": "Bayes AI",
    "section": "Black-Litterman",
    "text": "Black-Litterman\n\nBlack and Litterman (1991, 1992) work for combining investor views with market equilibrium.\nIn a multivariate returns setting the optimal allocation rule is \\[\n\\omega^\\star = \\frac{1}{\\gamma} \\Sigma^{-1} \\mu\n\\] The question is how to specify \\((\\mu, \\Sigma)\\) pairs?\nFor example, given \\(\\hat{\\Sigma}\\), BL derive Bayesian inference for \\(\\mu\\) given market equilibrium model and a priori views on the returns of pre-specified portfolios which take the form \\[\n( \\hat{\\mu} | \\mu ) \\sim \\mathcal{N} \\left ( \\mu , \\tau \\hat{\\Sigma} \\right ) \\; {\\rm and} \\;\n( Q | \\mu ) \\sim \\mathcal{N} \\left ( P \\mu , \\hat{\\Omega} \\right ) \\; .\n\\]"
  },
  {
    "objectID": "03-conjugate.html#posterior-views",
    "href": "03-conjugate.html#posterior-views",
    "title": "Bayes AI",
    "section": "Posterior Views",
    "text": "Posterior Views\n\nCombining views, the implied posterior is \\[\n( \\mu | \\hat{\\mu} , Q ) \\sim  \\mathcal{N} \\left ( B b , B \\right )\n\\]\nThe mean and variance are specified by\n\n\\[\nB = ( \\tau \\hat{\\Sigma} )^{-1} + P^\\prime \\hat{\\Omega}^{-1} P \\; {\\rm and} \\;    b = ( \\tau \\hat{\\Sigma} )^{-1} \\hat{\\mu}\n+ P^\\prime \\Omega^{-1} Q\n\\] These posterior moments then define the optimal allocation rule."
  },
  {
    "objectID": "03-conjugate.html#satya-nadella-ceo-of-microsoft",
    "href": "03-conjugate.html#satya-nadella-ceo-of-microsoft",
    "title": "Bayes AI",
    "section": "Satya Nadella: CEO of Microsoft",
    "text": "Satya Nadella: CEO of Microsoft\n\nIn 2014, Satya Nadella became the CEO of Microsoft.\nThe stock price of Microsoft has been on a steady rise since then.\nSuppose that you are a portfolio manager and you are interested in analyzing the returns of Microsoft stock compared to the market.\nSuppose you are managing a portfolio with two positions stock of Microsoft (MSFT) and an index fund that follows S&P500 index and tracks overall market performance.\nWhat is the mean returns of the positions in our portfolio?"
  },
  {
    "objectID": "03-conjugate.html#satya-nadella-ceo-of-microsoft-1",
    "href": "03-conjugate.html#satya-nadella-ceo-of-microsoft-1",
    "title": "Bayes AI",
    "section": "Satya Nadella: CEO of Microsoft",
    "text": "Satya Nadella: CEO of Microsoft\n\nAssume the prior for the mean returns is a bivariate normal distribution, let \\(\\mu_0 = (\\mu_{M}, \\mu_{S})\\) represent the prior mean returns for the stocks.\nThe covariance matrix \\(\\Sigma_0\\) captures your beliefs about the variability and the relationship between these stocks’ returns in the prior. \\[\n\\Sigma_0 = \\begin{bmatrix} \\sigma_{M}^2 & \\sigma_{MS} \\\\ \\sigma_{MS} & \\sigma_{S}^2 \\end{bmatrix},\n\\]\n\nWe will use the sample mean and covariance matrix of the historical returns as the prior mean and covariance matrix."
  },
  {
    "objectID": "03-conjugate.html#satya-nadella-ceo-of-microsoft-2",
    "href": "03-conjugate.html#satya-nadella-ceo-of-microsoft-2",
    "title": "Bayes AI",
    "section": "Satya Nadella: CEO of Microsoft",
    "text": "Satya Nadella: CEO of Microsoft\n\nThe likelihood of observing the data, given the mean returns, is also a bivariate normal distribution. \\[\n\\Sigma = \\begin{bmatrix} \\sigma_{M}^2 & \\sigma_{MS} \\\\ \\sigma_{MS} & \\sigma_{S}^2 \\end{bmatrix},\n\\] where \\(\\sigma_{M}^2\\) and \\(\\sigma_{S}^2\\) are the sample variances of the observed returns of MSFT and SPY, respectively, and \\(\\sigma_{MS}\\) is the sample covariance of the observed returns of MSFT and SPY. The likelihood mean is given by \\[\n\\mu = \\begin{bmatrix} \\mu_{M} \\\\ \\mu_{S} \\end{bmatrix},\n\\] where \\(\\mu_{M}\\) and \\(\\mu_{S}\\) are the sample means of the observed returns of MSFT and SPY, respectively."
  },
  {
    "objectID": "03-conjugate.html#satya-nadella-ceo-of-microsoft-3",
    "href": "03-conjugate.html#satya-nadella-ceo-of-microsoft-3",
    "title": "Bayes AI",
    "section": "Satya Nadella: CEO of Microsoft",
    "text": "Satya Nadella: CEO of Microsoft\n\nYou update your beliefs (prior) about the mean returns using the observed data (likelihood).\nThe posterior distribution, which combines your prior beliefs and the new information from the data, is also a bivariate normal distribution.\nThe mean \\(\\mu_{\\text{post}}\\) and covariance \\(\\Sigma_{\\text{post}}\\) of the posterior are calculated using Bayesian updating formulas, which involve \\(\\mu_0\\), \\(\\Sigma_0\\), \\(\\mu\\), and \\(\\Sigma\\).\nWe use observed returns prior to Nadella’s becoming CEO as our prior and analyze the returns post 2014."
  },
  {
    "objectID": "03-conjugate.html#satya-nadella-ceo-of-microsoft-4",
    "href": "03-conjugate.html#satya-nadella-ceo-of-microsoft-4",
    "title": "Bayes AI",
    "section": "Satya Nadella: CEO of Microsoft",
    "text": "Satya Nadella: CEO of Microsoft\n\n\nCode\npar(mar=c(4,4,0.7,0.5), bty='n', cex.lab=0.75, cex.axis=0.75,cex.main=0.75)\nlibrary(quantmod)\ngetSymbols(c(\"MSFT\", \"SPY\"), from = \"2001-01-01\", to = \"2023-12-31\")\n\n\n[1] \"MSFT\" \"SPY\" \n\n\nCode\ns = 3666 # 2015-07-30\nprior = 1:s\nobs = s:nrow(MSFT) # post covid\n# obs = 5476:nrow(MSFT) # 2022-10-06 bull run if 22-23\na = as.numeric(dailyReturn(MSFT))\nc = as.numeric(dailyReturn(SPY))\n# Prior\nmu0 = c(mean(a[prior]), mean(c[prior]))\nSigma0 = cov(data.frame(a=a[prior],c=c[prior]))\n# Data\nmu = c(mean(a[obs]), mean(c[obs]))\nSigma = cov(data.frame(a=a,c=c))\n# Posterior\nSigmaPost = solve(solve(Sigma0) + solve(Sigma))\nmuPost = SigmaPost %*% (solve(Sigma0) %*% mu0 + solve(Sigma) %*% mu)\n# Plot\nplot(a[obs], c[obs], xlab=\"MSFT\", ylab=\"SPY\", xlim=c(-0.005,0.005), ylim=c(-0.005,0.005), pch=16, cex=0.5)\nabline(v=0, h=0, col=\"grey\")\nabline(v=mu0[1], h=mu0[2], col=\"blue\",lwd=3) #prior\nabline(v=mu[1], h=mu[2], col=\"red\",lwd=3) #data\nabline(v=muPost[1], h=muPost[2], col=\"green\",lwd=3) #posterior\nlegend(\"bottomright\", c(\"Prior\", \"Likelihood\", \"Posterior\"), pch=15, col=c(\"blue\", \"red\", \"green\"), bty=\"n\")"
  },
  {
    "objectID": "03-conjugate.html#mixtures-of-conjugate-priors",
    "href": "03-conjugate.html#mixtures-of-conjugate-priors",
    "title": "Bayes AI",
    "section": "Mixtures of Conjugate Priors",
    "text": "Mixtures of Conjugate Priors\n\nThe mixture of conjugate priors is a powerful tool for modeling complex data. \\[\n\\theta \\sim p(\\theta) = \\sum_{k=1}^K \\pi_k p_k(\\theta).\n\\] Then the posterior is also a mixture of normal distributions, that is \\[\np(\\theta\\mid y) = p(y\\mid \\theta)\\sum_{k=1}^K \\pi_k p_k(\\theta)/C.\n\\]"
  },
  {
    "objectID": "03-conjugate.html#mixtures-of-conjugate-priors-1",
    "href": "03-conjugate.html#mixtures-of-conjugate-priors-1",
    "title": "Bayes AI",
    "section": "Mixtures of Conjugate Priors",
    "text": "Mixtures of Conjugate Priors\nWe introduce a normalizing constant for each component \\[\nC_k = \\int p(y\\mid \\theta)p_k(\\theta)d\\theta.\n\\] then \\[\np_k(\\theta\\mid y) =  p_k(\\theta)p(y\\mid \\theta)/C_k\n\\] is a proper distribution and our posterior is a mixture of these distributions \\[\np(\\theta\\mid y) = \\sum_{k=1}^K \\pi_k C_k p_k(\\theta\\mid y)/C.\n\\] Meaning that we need to require \\[\n\\dfrac{\\sum_{k=1}^K \\pi_k C_k}{C} = 1.\n\\] or \\[\nC = \\sum_{k=1}^K \\pi_k C_k\n\\]"
  },
  {
    "objectID": "03-conjugate.html#mixture-of-two-normal-distributions",
    "href": "03-conjugate.html#mixture-of-two-normal-distributions",
    "title": "Bayes AI",
    "section": "Mixture of two normal distributions",
    "text": "Mixture of two normal distributions\nThe prior distribution is a mixture of two normal distributions, that is \\[\n\\mu \\sim 0.5 N(0,1) + 0.5 N(5,1).\n\\] The likelihood is a normal distribution with mean \\(\\mu\\) and variance 1, that is \\[\ny \\mid \\mu \\sim N(\\mu,1).\n\\] The posterior distribution is a mixture of two normal distributions, that is \\[\np(\\mu \\mid y) \\propto \\phi(y\\mid \\mu,1) \\left(0.5 \\phi(\\mu\\mid 0,1) + 0.5 \\phi(\\mu\\mid 5,1)\\right),\n\\] where \\(\\phi(x\\mid \\mu,\\sigma^2)\\) is the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "03-conjugate.html#mixture-of-two-normal-distributions-1",
    "href": "03-conjugate.html#mixture-of-two-normal-distributions-1",
    "title": "Bayes AI",
    "section": "Mixture of two normal distributions",
    "text": "Mixture of two normal distributions\nWe can calculate it using property of a normal distribution \\[\n\\phi(x\\mid \\mu_1,\\sigma_1^2)\\phi(x\\mid \\mu_2,\\sigma_2^2) = \\phi(x\\mid \\mu_3,\\sigma_3^2)\\phi(\\mu_1-\\mu_2\\mid 0,\\sigma_1^2+\\sigma_2^2)\n\\] where \\[\n\\mu_3 = \\dfrac{\\mu_1/\\sigma_2^2 + \\mu_2/\\sigma_1^2}{1/\\sigma_1^2 + 1/\\sigma_2^2}, \\quad \\sigma_3^2 = \\dfrac{1}{1/\\sigma_1^2 + 1/\\sigma_2^2}.\n\\]"
  },
  {
    "objectID": "03-conjugate.html#mixture-of-two-normal-distributions-2",
    "href": "03-conjugate.html#mixture-of-two-normal-distributions-2",
    "title": "Bayes AI",
    "section": "Mixture of two normal distributions",
    "text": "Mixture of two normal distributions\nGiven, we observed \\(y = 2\\), we can calculate the posterior distribution for \\(\\mu\\)\n\n\nCode\nmu0 = c(0,5)\nsigma02 = c(1,1)\npi = c(0.5,0.5)\ny = 2\nmu3 = (mu0/sigma02 + y) / (1/sigma02 + 1)\nsigma3 = 1/(1/sigma02 + 1)\nC = dnorm(y-mu0,0,1+sigma02)*pi\nw = C/sum(C)\ncat(sprintf(\"Component parameters:\\nMean = (%1.1f,%2.1f)\\nVar = (%1.1f,%1.1f)\\nweights = (%1.2f,%1.2f)\", mu3[1],mu3[2], sigma3[1],sigma3[2],w[1],w[2]))\n\n\nComponent parameters:\nMean = (1.0,3.5)\nVar = (0.5,0.5)\nweights = (0.65,0.35)"
  },
  {
    "objectID": "03-conjugate.html#normal-with-unknown-variance",
    "href": "03-conjugate.html#normal-with-unknown-variance",
    "title": "Bayes AI",
    "section": "Normal With Unknown Variance",
    "text": "Normal With Unknown Variance\nConsider, another example, when mean \\(\\mu\\) is fixed and variance is a random variable which follows some distribution \\(\\sigma^2 \\sim p(\\sigma^2)\\). Given an observed sample \\(y\\), we can update the distribution over variance using the Bayes rule \\[\np(\\sigma^2 \\mid  y) = \\dfrac{p(y\\mid \\sigma^2 )p(\\sigma^2)}{p(y)}.\n\\] Now, the total probability in the denominator can be calculated as \\[\np(y) = \\int p(y\\mid \\sigma^2 )p(\\sigma^2) d\\sigma^2.\n\\]"
  },
  {
    "objectID": "03-conjugate.html#normal-with-unknown-variance-1",
    "href": "03-conjugate.html#normal-with-unknown-variance-1",
    "title": "Bayes AI",
    "section": "Normal With Unknown Variance",
    "text": "Normal With Unknown Variance\nA conjugate prior that leads to analytically calculable integral for variance under the normal likelihood is the inverse Gamma. Thus, if \\[\n\\sigma^2 \\mid  \\alpha,\\beta \\sim IG(\\alpha,\\beta) = \\dfrac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\sigma^{2(-\\alpha-1)}\\exp\\left(-\\dfrac{\\beta}{\\sigma^2}\\right)\n\\] and \\[\ny \\mid \\mu,\\sigma^2 \\sim N(\\mu,\\sigma^2)\n\\] Then the posterior distribution is another inverse Gamma \\(IG(\\alpha_{\\mathrm{posterior}},\\beta_{\\mathrm{posterior}})\\), with \\[\n\\alpha_{\\mathrm{posterior}} = \\alpha + 1/2, ~~\\beta_{\\mathrm{posterior}} = \\beta + \\dfrac{y-\\mu}{2}.\n\\]"
  },
  {
    "objectID": "03-conjugate.html#normal-with-unknown-variance-2",
    "href": "03-conjugate.html#normal-with-unknown-variance-2",
    "title": "Bayes AI",
    "section": "Normal With Unknown Variance",
    "text": "Normal With Unknown Variance\nNow, the predictive distribution over \\(y\\) can be calculated by \\[\np(y_{new}\\mid y) = \\int p(y_{new},\\sigma^2\\mid y)p(\\sigma^2\\mid y)d\\sigma^2.\n\\] Which happens to be a \\(t\\)-distribution with \\(2\\alpha_{\\mathrm{posterior}}\\) degrees of freedom, mean \\(\\mu\\) and variance \\(\\alpha_{\\mathrm{posterior}}/\\beta_{\\mathrm{posterior}}\\)."
  },
  {
    "objectID": "03-conjugate.html#the-normal-gamma-model",
    "href": "03-conjugate.html#the-normal-gamma-model",
    "title": "Bayes AI",
    "section": "The Normal-Gamma Model",
    "text": "The Normal-Gamma Model\n\nTo simplify the formulas, we use precision \\(\\rho = 1/\\sigma^2\\) instead of variance \\(\\sigma^2\\). The normal-Gamma distribution is a conjugate prior for the normal distribution, when we do not know the precision and the mean. Given the observed data \\(Y  = \\{y_1,\\ldots,y_n\\}\\), we assume normal likelihood \\[\ny_i \\mid \\theta, \\rho \\sim N(\\theta, 1/\\rho)\n\\]\n\nThe normal-gamma prior distribution is defined as \\[\n\\theta\\mid \\mu,\\rho,\\nu \\sim N(\\mu, 1/(\\rho \\nu)), \\quad \\rho \\mid \\alpha, \\beta \\sim \\text{Gamma}(\\alpha, \\beta).\n\\] - \\(1/\\rho\\) has inverse-Gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\). - Conditional on \\(\\rho\\), the mean \\(\\theta\\) has normal distribution with mean \\(\\mu\\) and precision \\(\\nu\\rho\\)."
  },
  {
    "objectID": "03-conjugate.html#the-normal-gamma-model-1",
    "href": "03-conjugate.html#the-normal-gamma-model-1",
    "title": "Bayes AI",
    "section": "The Normal-Gamma Model",
    "text": "The Normal-Gamma Model\n\\[\n\\theta\\mid \\mu,\\rho,\\nu \\sim N(\\mu, 1/(\\rho \\nu)), \\quad \\rho \\mid \\alpha, \\beta \\sim \\text{Gamma}(\\alpha, \\beta).\n\\]\n\nThe mean \\(\\theta\\) and precision \\(\\rho\\) are not independent.\nWhen the precision of observations \\(\\rho\\) is low, we are also less certain about the mean.\nWhen \\(\\nu=0\\), we have an improper uniform distribution over \\(\\theta\\), that is independent of \\(\\rho\\).\nThere is no conjugate distribution for \\(\\theta,\\rho\\) in which \\(\\theta\\) is independent of \\(\\rho\\)."
  },
  {
    "objectID": "03-conjugate.html#the-normal-gamma-model-2",
    "href": "03-conjugate.html#the-normal-gamma-model-2",
    "title": "Bayes AI",
    "section": "The Normal-Gamma Model",
    "text": "The Normal-Gamma Model\nGiven the normal likelihood \\[\np(y\\mid \\theta, \\rho) = \\left(\\dfrac{\\rho}{2\\pi}\\right)^{1/2}\\exp\\left(-\\dfrac{\\rho}{2}\\sum_{i=1}^n(y_i-\\theta)^2\\right)\n\\] and the normal-gamma prior \\[\np(\\theta, \\rho \\mid \\mu,\\nu,\\alpha,\\beta) = \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\nu\\rho^{\\alpha-1}\\exp(-\\beta\\rho)\\left(\\dfrac{\\nu\\rho}{2\\pi}\\right)^{1/2}\\exp\\left(-\\dfrac{\\nu\\rho}{2}(\\theta-\\mu)^2\\right)\n\\] the posterior distribution is given by \\[\np(\\theta, \\rho\\mid y) \\propto p(y\\mid \\theta, \\rho)p(\\theta, \\rho).\n\\] The posterior distribution is a normal-Gamma distribution with parameters \\[\n\\begin{aligned}\n\\mu_n &= \\dfrac{\\nu\\mu + n\\bar{y}}{\\nu+n},\\\\\n\\nu_n &= \\nu+n,\\\\\n\\alpha_n &= \\alpha + \\dfrac{n}{2},\\\\\n\\beta_n &= \\beta + \\dfrac{1}{2}\\sum_{i=1}^n(y_i-\\bar{y})^2 + \\dfrac{n\\nu}{2(\\nu+n)}(\\bar{y}-\\mu)^2.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "03-conjugate.html#the-normal-gamma-model-3",
    "href": "03-conjugate.html#the-normal-gamma-model-3",
    "title": "Bayes AI",
    "section": "The Normal-Gamma Model",
    "text": "The Normal-Gamma Model\n\n\\(\\bar{y} = n^{-1}\\sum_{i=1}^n y_i\\) is the sample mean and \\(n\\) is the sample size.\nThe posterior distribution is a normal-Gamma distribution with parameters \\(\\mu_n, \\nu_n, \\alpha_n, \\beta_n\\)."
  },
  {
    "objectID": "03-conjugate.html#credible-intervals-for-normal-gamma-model-posterior-parameters",
    "href": "03-conjugate.html#credible-intervals-for-normal-gamma-model-posterior-parameters",
    "title": "Bayes AI",
    "section": "Credible Intervals for Normal-Gamma Model Posterior Parameters",
    "text": "Credible Intervals for Normal-Gamma Model Posterior Parameters\n\nThe precission posterior follows a Gamma distribution with parameters \\(\\alpha_n, \\beta_n\\), thus we can use quantiles of the Gamma distribution to calculate credible intervals.\nA symmetric \\(100(1-c)%\\) credible interval \\([g_{c/2},g_{1-c/2}]\\) is given by \\(c/2\\) and \\(1-c/2\\) quantiles of the gamma distrinution. To find credible intterval for the variance \\(v = 1/\\rho\\), we simply use \\[\n[1/g_{1-c/2},1/g_{c/2}].\n\\] and for standard deviation \\(s = \\sqrt{v}\\) we use \\[\n[\\sqrt{1/g_{1-c/2}},\\sqrt{1/g_{c/2}}].\n\\]\n\nTo find credible interval over the mean \\(\\theta\\), we need to integrate out the precision \\(\\rho\\) from the posterior distribution. The marginal distribution of \\(\\theta\\) is a Student’s t-distribution with parameters center at \\(\\mu_n\\), variance \\(\\beta_n/(\\nu_n\\alpha_n)\\) and degrees of freedom \\(2\\alpha_n\\)."
  },
  {
    "objectID": "03-conjugate.html#exponential-gamma-model",
    "href": "03-conjugate.html#exponential-gamma-model",
    "title": "Bayes AI",
    "section": "Exponential-Gamma Model",
    "text": "Exponential-Gamma Model\n\nWaiting times between events: consecutive arrivals of a Poisson process is exponentially distributed with mean \\(1/\\lambda\\). \\[\nf(x;\\lambda) =  \\lambda e^{-\\lambda x}, ~ x \\geq 0\n\\]\n\\(\\lambda\\) is the rate parameter, which is the inverse of the mean\nspecial case of the Gamma distribution with shape 1 and scale \\(1/\\lambda\\).\n\n\n\n\nExponential Distribution\nParameters\n\n\n\n\nExpected value\n\\(\\mu = E{X} = 1/\\lambda\\)\n\n\nVariance\n\\(\\sigma^2 = Var{X} = 1/\\lambda^2\\)"
  },
  {
    "objectID": "03-conjugate.html#exponential-model-examples",
    "href": "03-conjugate.html#exponential-model-examples",
    "title": "Bayes AI",
    "section": "Exponential Model: Examples",
    "text": "Exponential Model: Examples\n\nLifespan of Electronic Components: The exponential distribution can model the time until a component fails in systems where the failure rate is constant over time.\nTime Between Arrivals: In a process where events (like customers arriving at a store or calls arriving at a call center) occur continuously and independently, the time between these events can often be modeled with an exponential distribution.\nRadioactive Decay: The time until a radioactive atom decays is often modeled with an exponential distribution."
  },
  {
    "objectID": "03-conjugate.html#exponential-gamma-model-1",
    "href": "03-conjugate.html#exponential-gamma-model-1",
    "title": "Bayes AI",
    "section": "Exponential-Gamma Model",
    "text": "Exponential-Gamma Model\nThe Exponential-Gamma model assumes that the data follows an exponential distribution (likelihood). - The Gamma distribution is a flexible two-parameter family of distributions and can model a wide range of shapes. \\[\\begin{align*}\n    \\lambda &\\sim \\text{Gamma}(\\alpha, \\beta) \\\\\n    x_i &\\sim \\text{Exponential}(\\lambda)\n\\end{align*}\\]\nThe posterior distribution of the rate parameter \\(\\lambda\\) is given by: \\[\np(\\lambda\\mid x_1, \\ldots, x_n) \\propto \\lambda^{\\alpha - 1} e^{-\\beta\\lambda} \\prod_{i=1}^n \\lambda e^{-\\lambda x_i} = \\lambda^{\\alpha + n - 1} e^{-(\\beta + \\sum_{i=1}^n x_i)\\lambda}\n\\]"
  },
  {
    "objectID": "03-conjugate.html#exponential-gamma-model-2",
    "href": "03-conjugate.html#exponential-gamma-model-2",
    "title": "Bayes AI",
    "section": "Exponential-Gamma Model",
    "text": "Exponential-Gamma Model\nPosterior is a Gamma distribution with shape parameter \\(\\alpha + n\\) and rate parameter \\(\\beta + \\sum_{i=1}^n x_i\\). The posterior mean and variance are given by: \\[\n\\mathbb{E}[\\lambda|x_1, \\ldots, x_n] = \\frac{\\alpha + n}{\\beta + \\sum_{i=1}^n x_i}, \\quad \\mathrm{Var}[\\lambda|x_1, \\ldots, x_n] = \\frac{\\alpha + n}{(\\beta + \\sum_{i=1}^n x_i)^2}.\n\\] Notice, that \\(\\sum x_i\\) is the sufficient statistic for inference about parameter \\(\\lambda\\)!"
  },
  {
    "objectID": "03-conjugate.html#exponential-gamma-model-3",
    "href": "03-conjugate.html#exponential-gamma-model-3",
    "title": "Bayes AI",
    "section": "Exponential-Gamma Model",
    "text": "Exponential-Gamma Model\n\nReliability Engineering: In situations where the failure rate of components or systems may not be constant and can vary, the Exponential-Gamma model can be used to estimate the time until failure, incorporating uncertainty in the failure rate.\nMedical Research: For modeling survival times of patients where the rate of mortality or disease progression is not constant and varies across a population. The variability in rates can be due to different factors like age, genetics, or environmental influences.\nEcology: In studying phenomena like the time between rare environmental events (e.g., extreme weather events), where the frequency of occurrence can vary due to changing climate conditions or other factors."
  },
  {
    "objectID": "03-conjugate.html#exploratory-data-analysis",
    "href": "03-conjugate.html#exploratory-data-analysis",
    "title": "Bayes AI",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nBefore deciding on a parametric model for a dataset. There are several tools that we use to choose the appropriate model. These include\n\nTheoretical assumptions underlying the distribution (our prior knowledge about the data)\nExploratory data analysis\nFormal goodness-of-fit tests\n\nThe two most common tools for exploratory data analysis are Q-Q plot, scatter plots and bar plots/histograms."
  },
  {
    "objectID": "03-conjugate.html#q-q-plot",
    "href": "03-conjugate.html#q-q-plot",
    "title": "Bayes AI",
    "section": "Q-Q plot",
    "text": "Q-Q plot\n\nQ-Q plot simply compares the quantiles of your data with the quantiles of a theoretical distribution (like normal, exponential, etc.).\nQuantile is the fraction (or percent) of points below the given value.\nThat is, the \\(i\\)-th quantile is the point \\(x\\) for which \\(i\\)% of the data lies below \\(x\\).\nOn a Q-Q plot, if the two data sets come from a population with the same distribution, we should see the points forming a line that’s roughly straight."
  },
  {
    "objectID": "03-conjugate.html#q-q-plot-1",
    "href": "03-conjugate.html#q-q-plot-1",
    "title": "Bayes AI",
    "section": "Q-Q plot",
    "text": "Q-Q plot\n\nIf the two data sets \\(x\\) and \\(y\\) come from the same distribution, then the points \\((x_{(i)}, y_{(i)})\\) should lie roughly on the line \\(y = x\\).\nIf \\(y\\) comes from a distribution that’s linear in \\(x\\), then the points \\((x_{(i)}, y_{(i)})\\) should lie roughly on a line, but not necessarily on the line \\(y = x\\)."
  },
  {
    "objectID": "03-conjugate.html#noraml-q-q-plot",
    "href": "03-conjugate.html#noraml-q-q-plot",
    "title": "Bayes AI",
    "section": "Noraml Q-Q plot",
    "text": "Noraml Q-Q plot\nQ-Q plot for the Data on birth weights of babies born in a Brisbane hospital on December 18, 1997. The data set contains 44 records. A more detailed description of the data set can be found in UsingR manual.\n\n\nCode\nrequire(UsingR)\nrequire(dplyr) \ndata(babyboom) \nqqnorm(babyboom$wt)\nqqline(babyboom$wt)\n\n\n\n\nFigure 1\nAre Birth Weights Normally Distributed?"
  },
  {
    "objectID": "03-conjugate.html#noraml-q-q-plot-1",
    "href": "03-conjugate.html#noraml-q-q-plot-1",
    "title": "Bayes AI",
    "section": "Noraml Q-Q plot",
    "text": "Noraml Q-Q plot\nThe Q-Q plots look different if we split the data based on the gender\n\nCode\ng = babyboom %&gt;% filter(gender==\"girl\") %&gt;% pull(wt) \nb = babyboom %&gt;% filter(gender==\"boy\")  %&gt;% pull(wt) \nqqnorm(g); qqline(g)\nqqnorm(b); qqline(b)\n\n\n\n\n\n\n\nGirls\n\n\n\n\n\n\n\nBoys\n\n\n\n\n\n\nHistogram of baby weights by gender"
  },
  {
    "objectID": "03-conjugate.html#noraml-q-q-plot-2",
    "href": "03-conjugate.html#noraml-q-q-plot-2",
    "title": "Bayes AI",
    "section": "Noraml Q-Q plot",
    "text": "Noraml Q-Q plot\nHow about the times in hours between births of babies?\n\n\nCode\nhr = ceiling(babyboom$running.time/60)\nBirthsByHour = tabulate(hr)\n# Number of hours with 0, 1, 2, 3, 4 births\nObservedCounts = table(BirthsByHour) \n# Average number of births per hour\nBirthRate=sum(BirthsByHour)/24    \n# Expected counts for Poisson distribution\nExpectedCounts=dpois(0:4,BirthRate)*24    \n# bind into matrix for plotting\nObsExp &lt;- rbind(ObservedCounts,ExpectedCounts) \nbarplot(ObsExp,names=0:4, beside=TRUE,legend=c(\"Observed\",\"Expected\"))"
  },
  {
    "objectID": "03-conjugate.html#exponential-q-q-plot",
    "href": "03-conjugate.html#exponential-q-q-plot",
    "title": "Bayes AI",
    "section": "Exponential Q-Q plot",
    "text": "Exponential Q-Q plot\nWhat about the Q-Q plot?\n\n\nCode\n# birth intervals\nbirthinterval=diff(babyboom$running.time) \n # quantiles of standard exponential distribution (rate=1)   \nexponential.quantiles = qexp(ppoints(43)) \nqqplot(exponential.quantiles, birthinterval)\nlmb=mean(birthinterval)\nlines(exponential.quantiles,exponential.quantiles*lmb) # Overlay a line\n\n\n\nHere\n\nppoints function computes the sequence of probability points\nqexp function computes the quantiles of the exponential distribution\ndiff function computes the difference between consecutive elements of a vector"
  },
  {
    "objectID": "03-conjugate.html#brief-list-of-conjugate-models",
    "href": "03-conjugate.html#brief-list-of-conjugate-models",
    "title": "Bayes AI",
    "section": "Brief List of Conjugate Models",
    "text": "Brief List of Conjugate Models\n\n\n\nLikelihood\nPrior\nPosterior\n\n\n\n\nBinomial\nBeta\nBeta\n\n\nNegative\nBinomial\nBeta\n\n\nPoisson\nGamma\nGamma\n\n\nGeometric\nBeta\nBeta\n\n\nExponential\nGamma\nGamma\n\n\nNormal (mean unknown)\nNormal\nNormal\n\n\nNormal (variance unknown)\nInverse Gamma\nInverse Gamma\n\n\nNormal (mean and variance unknown)\nNormal/Gamma\nNormal/Gamma\n\n\nMultinomial\nDirichlet\nDirichlet"
  },
  {
    "objectID": "05-sp.html#definition",
    "href": "05-sp.html#definition",
    "title": "Bayes AI",
    "section": "Definition",
    "text": "Definition\n\nAn instance (realization) of a process is a function \\(Y:~ U \\rightarrow S\\)\nFrom domain of index set \\(U\\)\nTo process values \\(S\\), called state-space.\n\nThe process then is the distribution over the space of functions from \\(U\\) to \\(S\\)."
  },
  {
    "objectID": "05-sp.html#brownian-motion",
    "href": "05-sp.html#brownian-motion",
    "title": "Bayes AI",
    "section": "Brownian Motion",
    "text": "Brownian Motion\n\nBrownian Motion, named after botanist Robert Brown\nIs a fundamental concept in the theory of stochastic processes.\nIt describes the random motion of particles suspended in a fluid (liquid or gas), as they are bombarded by the fast-moving molecules in the fluid.\n\nA one-dimensional Brownian Motion (also known as Wiener process) is a continuous time stochastic process \\(B(t)_{t\\ge 0}\\) with the following properties\n\n\\(B(0) = 0\\) almost surely\n\\(B(t)\\) has stationary independent increments: \\(B(t) - B(s) \\sim N(0, t-s)\\) for \\(0 \\le s &lt; t\\)\n\\(B(t)\\) is continuous function of \\(t\\)\nFor each time \\(t &gt; 0\\), the random variable \\(B(t)\\) is normally distributed with mean 0 and variance \\(t\\), i.e., \\(B(t) \\sim N(0, t)\\)."
  },
  {
    "objectID": "05-sp.html#formal-definition",
    "href": "05-sp.html#formal-definition",
    "title": "Bayes AI",
    "section": "Formal Definition",
    "text": "Formal Definition\nFormally brownian motion is a stochastic process \\(B(t)\\) is a family of real random variables indexed by the set of nonnegative real numbers \\(t\\).\n\n\nCode\n# Brownian Motion\nset.seed(92)\nt = seq(0, 1, 0.001)\nplot(t, cumsum(rnorm(1001, 0, sqrt(0.001))), type=\"l\", xlab=\"t\", ylab=\"B(t)\", lwd=2, ylim=c(-1.2, 2))\nlines(t, cumsum(rnorm(1001, 0, sqrt(0.001))), lwd=2, col=2)\nlines(t, cumsum(rnorm(1001, 0, sqrt(0.001))),lwd=2, col=3)\n\n\n\n\nFigure 1: Brownian Motion\nThus, for any times \\(0 \\leq t_1 &lt; t_2 &lt; ... &lt; t_n\\), the random variables \\(B(t_2) - B(t_1)\\), \\(B(t_3) - B(t_2)\\), …, \\(B(t_n) - B(t_{n-1})\\) are independent and the function \\(t \\mapsto B(t)\\) is continuous almost surely."
  },
  {
    "objectID": "05-sp.html#mertons-jump-diffusion-model",
    "href": "05-sp.html#mertons-jump-diffusion-model",
    "title": "Bayes AI",
    "section": "Merton’s Jump Diffusion Model",
    "text": "Merton’s Jump Diffusion Model\n\nIntroduced jumps to the model.\nThe additive jump term addresses the issues, asymmetry, and heavy tails in the distribution.\nMerton’s Jump Stochastic volatility model has a discrete-time version for log-returns, \\(y_t\\), with jump times, \\(J_t\\), jump sizes, \\(Z_t\\), and spot stochastic volatility, \\(V_t\\), given by the dynamics \\[\\begin{align*}\n  y_{t} & \\equiv \\log \\left( S_{t}/S_{t-1}\\right) =\\mu + V_t \\varepsilon_{t}+J_{t}Z_{t} \\\\V_{t+1} & = \\alpha_v + \\beta_v V_t + \\sigma_v \\sqrt{V_t} \\varepsilon_{t}^v\n\\end{align*}\\] where \\(\\mathbb{P} \\left ( J_t =1 \\right ) = \\lambda\\), \\(S_t\\) denote a stock or asset price and log-returns \\(y^t\\) is the log-return. The errors \\((\\varepsilon_{t},\\varepsilon_{t}^v)\\) are possibly correlated bivariate normals.\n\nThe investor must obtain optimal filters for \\((V_t,J_t,Z_t)\\), and learn the posterior densities of the parameters \\((\\mu, \\alpha_v, \\beta_v, \\sigma_v^2 , \\lambda )\\). These estimates will be conditional on the information available at each time."
  },
  {
    "objectID": "05-sp.html#gaussian-processes",
    "href": "05-sp.html#gaussian-processes",
    "title": "Bayes AI",
    "section": "Gaussian Processes",
    "text": "Gaussian Processes\n\nA Gaussian Process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution.\nUsed for modeling and predicting, e.g. regression and classification tasks in machine learning.\n\\(n\\) points from Gaussian Process is completely specified by its \\(n\\)-dimensional mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\).\nIndex of the GP is a real number \\(x\\) and values are also real numbers."
  },
  {
    "objectID": "05-sp.html#need-to-define-mean-and-covariance-functions",
    "href": "05-sp.html#need-to-define-mean-and-covariance-functions",
    "title": "Bayes AI",
    "section": "Need to define mean and covariance functions",
    "text": "Need to define mean and covariance functions\n\nTypically: \\(U = \\mathbb{R^d}\\), \\(S = \\mathbb{R}\\)\nMean \\(m(x):~ U \\rightarrow S\\) and covariance is defined by function \\(k(x, x'): :~ U\\times U \\rightarrow S\\), \\(x,x' \\in U\\)\nThe mean function defines the average value of the function at point \\(x\\)\nCovariance function, also known as the kernel, defines the extent to which the values of the function at two points \\(x\\) and \\(x'\\) are correlated.\nTypical notation is \\[\nf(x) \\sim \\mathcal{GP}(m(x), k(x, x')).\n\\]\n\n\\(k(x, x') = \\mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\\) describes the amount of dependence between the values of the function at two different points in the input space."
  },
  {
    "objectID": "05-sp.html#need-to-define-mean-and-covariance-functions-1",
    "href": "05-sp.html#need-to-define-mean-and-covariance-functions-1",
    "title": "Bayes AI",
    "section": "Need to define mean and covariance functions",
    "text": "Need to define mean and covariance functions\n\nTypically the mean function is less important than the covariance function.\nMost of the time data scientists will use a zero mean function, \\(m(x)=0\\), and focus on the covariance function.\nThe kernel function is often chosen to be a function of the distance between the two points \\(d = \\|x-x'\\|_2\\).\nTypically kernel function peaks at \\(d=0\\) and decays as \\(d\\) increases."
  },
  {
    "objectID": "05-sp.html#squared-exponential",
    "href": "05-sp.html#squared-exponential",
    "title": "Bayes AI",
    "section": "Squared exponential",
    "text": "Squared exponential\n\nThe most commonly used kernel function is the squared exponential kernel \\[\nk(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\|x-x'\\|_2^2}{2l^2}\\right)\n\\]\n\\(\\sigma^2\\) is the variance, controls the vertical variation (amplitude)\n\\(l\\) is the length scale parameter, controls horizontal variation (number of “bumps” or smoothness)\n\n\\(k(x,x) = \\sigma^2\\) and \\(k(x,x') \\rightarrow 0\\) as \\(\\|x-x'\\|_2 \\rightarrow \\infty\\)."
  },
  {
    "objectID": "05-sp.html#gaussian-process-demo",
    "href": "05-sp.html#gaussian-process-demo",
    "title": "Bayes AI",
    "section": "Gaussian Process Demo",
    "text": "Gaussian Process Demo\nGenerating a sequence 100 inputs (process indexes)\n\nx = seq(0,10, length.out = 100)\n\nand then define the mean function and the covariance function\n\nmean = rep(0, length(x))\nsqexpcov = function(x, x1, l=1, sigma=1) {\n  exp(-0.5 * (x - x1)^2 / l^2) * sigma^2\n}\n\nThe covariance matrix is then defined as\n\n\nCode\ncov_mat = outer(x, x, sqexpcov)"
  },
  {
    "objectID": "05-sp.html#gaussian-process-demo-1",
    "href": "05-sp.html#gaussian-process-demo-1",
    "title": "Bayes AI",
    "section": "Gaussian Process Demo",
    "text": "Gaussian Process Demo\nGenerate a sample from the GP using the mvrnorm function from the MASS package and plot a sample\n\n\nCode\nlibrary(MASS)\nset.seed(17)\nY = mvrnorm(1, mean, cov_mat)\nplot(x, Y, type=\"l\", xlab=\"x\", ylab=\"y\", ylim=c(-1.5,2), lwd=2)\n\n\n\n\nFigure 2: Sample from a Gaussian Process\nA collection of 100 points of function \\(f(x)\\) sampled from a Gaussian Process with zero mean and squared exponential kernel for the set of 100 indexes \\(x =(0,0.1,0.2,\\ldots,10)\\)."
  },
  {
    "objectID": "05-sp.html#gaussian-process-demo-2",
    "href": "05-sp.html#gaussian-process-demo-2",
    "title": "Bayes AI",
    "section": "Gaussian Process Demo",
    "text": "Gaussian Process Demo\nLet’s generate a few more samples from the same GP and plot them together\n\n\nCode\nYs = mvrnorm(3, mean, cov_mat)\nmatplot(x, t(Ys), type=\"l\", ylab=\"Y\", lwd=5)\n\n\n\n\nFigure 3: Samples from a Gaussian Process"
  },
  {
    "objectID": "05-sp.html#making-predictions-with-gaussian-processes",
    "href": "05-sp.html#making-predictions-with-gaussian-processes",
    "title": "Bayes AI",
    "section": "Making Predictions with Gaussian Processes",
    "text": "Making Predictions with Gaussian Processes\n\nAssuming input indexes \\(X = (x_1,\\ldots,x_n)\\) and outputs \\(Y = (y_1,\\ldots,y_n)\\) are a realization of a Gaussian Process\nCan we predict at new inputs \\(x_* \\in \\mathbb{R}^q\\). The joint distribution of the observed data \\(Y\\) and the new data \\(y_*\\) is given by \\[\n\\begin{bmatrix} Y \\\\ y_* \\end{bmatrix} \\sim \\mathcal{N} \\left ( \\begin{bmatrix} \\mu \\\\ \\mu_* \\end{bmatrix}, \\begin{bmatrix} K & K_* \\\\ K_*^T & K_{**} \\end{bmatrix} \\right )\n\\] where \\(K = k(X, X)\\in \\mathbb{R}^{n\\times n}\\), \\(K_* = k(X, x_*)\\in \\mathbb{R}^{n\\times q}\\), \\(K_{**} = k(x_*, x_*) \\in \\mathbb{R}^{q\\times q}\\), \\(\\mu = \\mathbb{E}[Y]\\), and \\(\\mu_* = \\mathbb{E}[y_*]\\). The conditional distribution of \\(y_*\\) given \\(y\\) is then given by \\[\ny_* \\mid Y \\sim \\mathcal{N}(\\mu_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}}).\n% y_* \\mid Y \\sim \\mathcal{N}(\\mu_* + K_* K^{-1} (y - \\mu), K_{**} - K_*^T K^{-1} K_*).\n\\]"
  },
  {
    "objectID": "05-sp.html#making-predictions-with-gaussian-processes-1",
    "href": "05-sp.html#making-predictions-with-gaussian-processes-1",
    "title": "Bayes AI",
    "section": "Making Predictions with Gaussian Processes",
    "text": "Making Predictions with Gaussian Processes\nThe mean of the conditional distribution is given by \\[\n\\mu_{\\mathrm{post}} = \\mu_* + K_*^TK^{-1} (Y - \\mu)\n\\qquad(1)\\] and the covariance is given by \\[\n\\Sigma_{\\mathrm{post}} = K_{**} - K_*^T K^{-1} K_*.\n\\qquad(2)\\]\nEquation 1 and Equation 2 are convenient properties of a multivariate normal distribution."
  },
  {
    "objectID": "05-sp.html#gaussian-process-for-sin-function",
    "href": "05-sp.html#gaussian-process-for-sin-function",
    "title": "Bayes AI",
    "section": "Gaussian Process for \\(\\sin\\) function",
    "text": "Gaussian Process for \\(\\sin\\) function\n\nn = 8; eps=1e-6\nX = matrix(seq(0, 2*pi, length=n), ncol=1)\nY = sin(X)\nK = outer(X[,1],X[,1], sqexpcov) + diag(eps, n)\n\n\nThe additive term diag(eps, n) \\(=\\epsilon I\\) adds a diagonal matrix with the small \\(\\epsilon\\) on the diagonal.\n\nWhy?"
  },
  {
    "objectID": "05-sp.html#gaussian-process-for-sin-function-1",
    "href": "05-sp.html#gaussian-process-for-sin-function-1",
    "title": "Bayes AI",
    "section": "Gaussian Process for \\(\\sin\\) function",
    "text": "Gaussian Process for \\(\\sin\\) function\nNow we generate a new set of inputs \\(x_*\\) and calculate the covariance matrices \\(K_*\\) and \\(K_{**}\\).\n\nq = 100\nXX = matrix(seq(-0.5, 2*pi + 0.5, length=q), ncol=1)\nKX = outer(X[,1], XX[,1],sqexpcov)\nKXX = outer(XX[,1],XX[,1], sqexpcov) + diag(eps, q)\n\nNotice, we did not add \\(\\epsilon I\\) to \\(K_*\\) = KX matrix, but to add it to \\(K_{**}\\) = KXX to guarantee that the resulting posterior covariance matrix is non-singular (invert able).\n\nSi = solve(K)\nmup = t(KX) %*% Si %*% Y # we assume mu is 0\nSigmap = KXX - t(KX) %*% Si %*% KX"
  },
  {
    "objectID": "05-sp.html#gaussian-process-for-sin-function-2",
    "href": "05-sp.html#gaussian-process-for-sin-function-2",
    "title": "Bayes AI",
    "section": "Gaussian Process for \\(\\sin\\) function",
    "text": "Gaussian Process for \\(\\sin\\) function\nNow, we can generate a sample from the posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\nCode\nplot_gp = function(mup, Sigmap, X, Y, XX, YY){\n  q1 = mup + qnorm(0.05, 0, sqrt(diag(Sigmap)))\n  q2 = mup + qnorm(0.95, 0, sqrt(diag(Sigmap)))\n  matplot(XX, t(YY), type=\"l\", col=\"gray\", lty=1, xlab=\"x\", ylab=\"y\")\n  points(X, Y, pch=20, cex=2)\n  lines(XX, sin(XX), col=\"blue\")\n  lines(XX, mup, lwd=2)\n  lines(XX, q1, lwd=2, lty=2, col=2)\n  lines(XX, q2, lwd=2, lty=2, col=2)\n}\nYY = mvrnorm(100, mup, Sigmap)\nplot_gp(mup, Sigmap, X, Y, XX, YY)"
  },
  {
    "objectID": "05-sp.html#mle-for-gaussian-process",
    "href": "05-sp.html#mle-for-gaussian-process",
    "title": "Bayes AI",
    "section": "MLE for Gaussian Process",
    "text": "MLE for Gaussian Process\n\nUse the observed data to estimate those to parameters.\nIn the context of GP models, they are call hyper-parameters.\n\nLikelihood: \\[\np(Y \\mid X, \\sigma, l) = \\frac{1}{(2\\pi)^{n/2} |K|^{1/2}} \\exp \\left ( -\\frac{1}{2} Y^T K^{-1} Y \\right )\n\\] where \\(K = K(X,X)\\) is the covariance matrix. We assume mean is zero, to simplify the formulas. The log-likelihood is given by \\[\n\\log p(Y \\mid X, \\sigma, l) = -\\frac{1}{2} \\log |K| - \\frac{1}{2} Y^T K^{-1} Y - \\frac{n}{2} \\log 2\\pi.\n\\]"
  },
  {
    "objectID": "05-sp.html#mle-for-gaussian-process-1",
    "href": "05-sp.html#mle-for-gaussian-process-1",
    "title": "Bayes AI",
    "section": "MLE for Gaussian Process",
    "text": "MLE for Gaussian Process\nLet’s implement a function that calculates the log-likelihood of the data given the hyper-parameters \\(\\sigma\\) and \\(l\\) and use optim function to find the maximum of the log-likelihood function.\n\nloglik = function(par, X, Y) {\n  sigma = par[1]\n  l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  return(-(-0.5 * log(det(K)) - 0.5 * t(Y) %*% Si %*% Y - (n/2)* log(2*pi)))\n}\npar = optim(c(1,1), loglik, X=X, Y=Y)$par\nprint(par)\n\n[1] 1.502246 2.398773"
  },
  {
    "objectID": "05-sp.html#mle-for-gaussian-process-2",
    "href": "05-sp.html#mle-for-gaussian-process-2",
    "title": "Bayes AI",
    "section": "MLE for Gaussian Process",
    "text": "MLE for Gaussian Process\nMake predictions about the output values at new inputs \\(x_*\\).\n\n\nCode\nl = par[2]; sigma = par[1]\npredplot = function(X, Y, XX, YY, l, sigma) {\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  KX = outer(X[,1], XX[,1],sqexpcov,l,sigma)\n  KXX = outer(XX[,1],XX[,1], sqexpcov,l,sigma) + diag(eps, q)\n  Si = solve(K)\n  mup = t(KX) %*% Si %*% Y # we assume mu is 0\n  Sigmap = KXX - t(KX) %*% Si %*% KX\n  YY = mvrnorm(100, mup, Sigmap)\n  plot_gp(mup, Sigmap, X, Y, XX, YY)\n}\npredplot(X, Y, XX, YY, l, sigma)\n\n\n\nWe can see that our uncertainty is much “tighter”"
  },
  {
    "objectID": "05-sp.html#log-likelihood-derivative",
    "href": "05-sp.html#log-likelihood-derivative",
    "title": "Bayes AI",
    "section": "Log-likelihood Derivative",
    "text": "Log-likelihood Derivative\nSome matrix calculus: \\[\n\\frac{\\partial  Y^T K^{-1} Y}{\\partial \\theta} =  Y^T \\frac{\\partial K^{-1}}{\\partial \\theta} Y.\n\\] The derivative of the inverse matrix \\[\n\\frac{\\partial K^{-1}}{\\partial \\theta} = -K^{-1} \\frac{\\partial K}{\\partial \\theta} K^{-1}.\n\\] and the log of the determinant of a matrix \\[\n\\frac{\\partial \\log |K|}{\\partial \\theta} = \\mathrm{tr} \\left ( K^{-1} \\frac{\\partial K}{\\partial \\theta} \\right ),\n\\]"
  },
  {
    "objectID": "05-sp.html#log-likelihood-derivative-1",
    "href": "05-sp.html#log-likelihood-derivative-1",
    "title": "Bayes AI",
    "section": "Log-likelihood Derivative",
    "text": "Log-likelihood Derivative\nCalculate the derivative of the log-likelihood function with respect to \\(\\theta\\) \\[\n\\frac{\\partial \\log p(Y \\mid X,\\theta)}{\\partial \\theta} = -\\frac{1}{2}\\frac{\\partial \\log |K|}{\\partial \\theta}  + \\frac{1}{2} Y^T \\frac{\\partial K^{-1}}{\\partial \\theta}  Y.\n\\] Putting it all together, we get \\[\n\\frac{\\partial \\log p(Y \\mid X,\\theta)}{\\partial \\theta} = -\\frac{1}{2} \\mathrm{tr} \\left ( K^{-1} \\frac{\\partial K}{\\partial \\theta} \\right ) + \\frac{1}{2} Y^T K^{-1} \\frac{\\partial K}{\\partial \\theta} K^{-1} Y.\n\\]"
  },
  {
    "objectID": "05-sp.html#log-likelihood-derivative-for-squared-exponential-kernel",
    "href": "05-sp.html#log-likelihood-derivative-for-squared-exponential-kernel",
    "title": "Bayes AI",
    "section": "Log-likelihood Derivative for squared exponential kernel",
    "text": "Log-likelihood Derivative for squared exponential kernel\n\\[\nK_{ij} = k(x_i, x_j) = \\sigma^2 \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right ).\n\\] The derivative of the covariance matrix with respect to \\(\\sigma\\) is given by \\[\n\\frac{\\partial K_{ij}}{\\partial \\sigma} = 2\\sigma \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right );~\\frac{\\partial K}{\\partial \\sigma} = \\dfrac{2}{\\sigma}K.\n\\] The derivative of the covariance matrix with respect to \\(l\\) is given by \\[\n\\frac{\\partial K_{ij}}{\\partial l} = \\sigma^2 \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right ) \\frac{(x_i - x_j)^2}{l^3};~ \\frac{\\partial K}{\\partial l}  = \\frac{(x_i - x_j)^2}{l^3} K.\n\\]"
  },
  {
    "objectID": "05-sp.html#log-likelihood-derivative-2",
    "href": "05-sp.html#log-likelihood-derivative-2",
    "title": "Bayes AI",
    "section": "Log-likelihood Derivative",
    "text": "Log-likelihood Derivative\nNow we can implement a function that calculates the derivative of the log-likelihood function with respect to \\(\\sigma\\) and \\(l\\).\n\n# Derivative of the log-likelihood function with respect to sigma\ndloglik_sigma = function(par, X, Y) {\n  sigma = par[1]; l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  dK = 2*K/sigma\n  tr = sum(diag(Si %*% dK))\n  return(-(-0.5 * tr + 0.5 * t(Y) %*% Si %*% dK %*% Si %*% Y))\n}\n# Derivative of the log-likelihood function with respect to l\ndloglik_l = function(par, X, Y) {\n  sigma = par[1]; l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov ,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  dK =   outer(X[,1],X[,1], function(x, x1) (x - x1)^2)/l^3 * K\n  tr = sum(diag(Si %*% dK))\n  return(-(-0.5 * tr + 0.5 * t(Y) %*% Si %*% dK %*% Si %*% Y))\n}\n# Gradient function that returns a vector of derivatives\ngnlg = function(par,X,Y) {\n  return(c(dloglik_sigma(par, X, Y), dloglik_l(par, X, Y)))\n}"
  },
  {
    "objectID": "05-sp.html#log-likelihood-derivative-3",
    "href": "05-sp.html#log-likelihood-derivative-3",
    "title": "Bayes AI",
    "section": "Log-likelihood Derivative",
    "text": "Log-likelihood Derivative\nNow we can use the optim function to find the maximum of the log-likelihood function and provide the derivative function we just implemented.\n\npar1 = optim(c(1,1), fn=loglik, gr=gnlg ,X=X, Y=Y,method=\"BFGS\")$par\nl = par1[2]; sigma = par1[1]\nprint(par1)\n\n[1] 1.48443 2.39151\n\n\nThe result is the same compared to when we called optim without the derivative function. Even execution time is the same for our small problem. However, at larger scale, the derivative-based optimization algorithm will be much faster."
  },
  {
    "objectID": "05-sp.html#use-r-package-for-gaussian-processes",
    "href": "05-sp.html#use-r-package-for-gaussian-processes",
    "title": "Bayes AI",
    "section": "Use R Package for Gaussian Processes",
    "text": "Use R Package for Gaussian Processes\n\n\nCode\nlibrary(laGP)\ngp = newGP(X, Y, 1, 0, dK = TRUE)\nres = mleGP(gp, tmax=20)\nl.laGP = sqrt(res$d/2)\nprint(l.laGP)\n\n\n[1] 2.36025\n\n\n\nCode\npredplot(X, Y, XX, YY, l, sigma)\npredplot(X, Y, XX, YY, l.laGP, 1)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) MLE Fit\n\n\n\n\n\n\n\n\n\n\n\n(b) laGP Fit\n\n\n\n\n\n\n\nFigure 4: Posterior distribution over \\(y_*\\), given \\(Y\\)"
  },
  {
    "objectID": "05-sp.html#prediction",
    "href": "05-sp.html#prediction",
    "title": "Bayes AI",
    "section": "Prediction",
    "text": "Prediction\n\nCode\nXX1 = matrix(seq(-4*pi, 6*pi + 0.5, length=q), ncol=1)\npredplot(X, Y, XX1, YY, l, sigma)\npredplot(X, Y, XX1, YY, l.laGP, 1)\n\n\n\n\n\n\n\nMLE Fit\n\n\n\n\n\n\n\nlaGP Fit\n\n\n\n\n\n\nExtrapolation: Posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\n\nWe can see that outside of the range of the observed data, the model with \\(\\sigma=1\\) is more “confident” in its predictions."
  },
  {
    "objectID": "05-sp.html#gaussian-process-for-motorcycle-accident-data",
    "href": "05-sp.html#gaussian-process-for-motorcycle-accident-data",
    "title": "Bayes AI",
    "section": "Gaussian Process for Motorcycle Accident Data",
    "text": "Gaussian Process for Motorcycle Accident Data\nWe first estimate the length scale parameter \\(l\\) using the laGP package.\n\n\nCode\nlibrary(MASS)\nlibrary(laGP)\nX = mcycle$times\nY = mcycle$accel\ngp = newGP(matrix(X), Y, 2, 1e-6, dK = TRUE);\nmleGP(gp, tmax=10);\n\n\nNow we plot the data and the fit using the estimated length scale parameter \\(l\\).\n\n\nCode\nlibrary(ggplot2)\nXX = matrix(seq(2.4, 55, length = 499), ncol=1)\np = predGP(gp, XX)\nN = 499\nq1 = qnorm(0.05, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nq2 = qnorm(0.95, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nq3 = qnorm(0.5, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nggplot() + geom_point(aes(x=X,y=Y)) + geom_line(aes(x=XX,y=q3)) + geom_ribbon(aes(x=XX,ymin=q1, ymax=q2), alpha=0.2)\n\n\n\nMotorcycle Accident Data. Black line is the mean of the posterior distribution over \\(y_*\\), given \\(Y\\). Blue lines are the 95% confidence interval."
  },
  {
    "objectID": "05-sp.html#gaussian-process-for-motorcycle-accident-data-1",
    "href": "05-sp.html#gaussian-process-for-motorcycle-accident-data-1",
    "title": "Bayes AI",
    "section": "Gaussian Process for Motorcycle Accident Data",
    "text": "Gaussian Process for Motorcycle Accident Data\n\n\nCode\nhist(X)\n\n\n\nHistogram of time valuesThe \\(\\sqrt{n}\\) decay in variance of the posterior distribution is a property of the squared exponential kernel."
  },
  {
    "objectID": "05-sp.html#definition-1",
    "href": "05-sp.html#definition-1",
    "title": "Bayes AI",
    "section": "Definition",
    "text": "Definition\n\nBayesian optimization is a sequential design strategy for global optimization of black-box functions.\nIt is particularly well-suited for optimization of high-cost functions, situations where the number of function evaluations is limited, and noisy evaluations.\nThe goal is to find the global minimum of an unknown function \\(f(x)\\), where \\(x \\in \\mathbb{R}^d\\).\nThe function is assumed to be a black-box, i.e., we can evaluate it at any point \\(x\\) but we do not have access to its analytical form."
  },
  {
    "objectID": "05-sp.html#basic-idea",
    "href": "05-sp.html#basic-idea",
    "title": "Bayes AI",
    "section": "Basic Idea",
    "text": "Basic Idea\n\n\nCode\ngraph LR\n    NS[Decision on Next Samples]--Collect--&gt;S[System Under Study]\n    S--Observe--&gt;NS\n\n\n\n\n\ngraph LR\n    NS[Decision on Next Samples]--Collect--&gt;S[System Under Study]\n    S--Observe--&gt;NS\n\n\n\n\n\n\n\nMulti-Armed Bandits\nQ-Learning\nActive Learning\nReinforcement Learning\nBayesian Optimization"
  },
  {
    "objectID": "05-sp.html#bayesian-optimization",
    "href": "05-sp.html#bayesian-optimization",
    "title": "Bayes AI",
    "section": "Bayesian Optimization",
    "text": "Bayesian Optimization\nGiven a function \\(f(x)\\), that is not known analytically, it can represent, for example, output of a complex computer program. The goal is to optimize \\[\nx^* = \\arg\\min_x f(x).\n\\]\nThe Bayesian approach to this problem is the following:\n\nDefine a prior distribution over \\(f(x)\\)\nCalculate \\(f\\) at a few points \\(x_1, \\ldots, x_n\\)\nRepeat until convergence:\n\nUpdate the prior to get the posterior distribution over \\(f(x)\\)\nChoose the next point \\(x^+\\) to evaluate \\(f(x)\\)\nCalculate \\(f(x^+)\\)\n\nPick \\(x^*\\) that corresponds to the smallest value of \\(f(x)\\) among evaluated points"
  },
  {
    "objectID": "05-sp.html#where-to-sample-next",
    "href": "05-sp.html#where-to-sample-next",
    "title": "Bayes AI",
    "section": "Where to sample next?",
    "text": "Where to sample next?\nThe expected improvement (EI) acquisition function \\[\nf^* = \\min y\n\\] Our current best. At a given point \\(x\\) and function value \\(y = f(x)\\), the expected improvement function is defined as \\[\na(x) = \\mathbb{E}\\left[\\max(0, f^* - y)\\right],\n\\] The function that we calculate expectation of \\[\nu(x) = \\max(0, f^* - y)\n\\] is the utility function. Thus, the acquisition function is the expected value of the utility function."
  },
  {
    "objectID": "05-sp.html#acquisition-function",
    "href": "05-sp.html#acquisition-function",
    "title": "Bayes AI",
    "section": "Acquisition function",
    "text": "Acquisition function\n\nGiven GP approximation for \\(f\\), we can calculate the acquisition function analytically.\nThe posterior distribution of Normal \\(f(x)\\mid x \\sim N(\\mu,\\sigma^2)\\), then the acquisition function is \\[\\begin{align*}\na(x) &= \\mathbb{E}\\left[\\max(0, f^* - y)\\right] \\\\\n&= \\int_{-\\infty}^{\\infty} \\max(0, f^* - y) \\phi(y,\\mu,\\sigma^2) dy \\\\\n&= \\int_{-\\infty}^{f^*} (f^* - y) \\phi(y,\\mu,\\sigma^2) dy\n\\end{align*}\\] where \\(\\phi(y,\\mu,\\sigma^2)\\) is the probability density function of the normal distribution. A useful identity is"
  },
  {
    "objectID": "05-sp.html#acquisition-function-1",
    "href": "05-sp.html#acquisition-function-1",
    "title": "Bayes AI",
    "section": "Acquisition function",
    "text": "Acquisition function\n\\[\n\\int y \\phi(y,\\mu,\\sigma^2) dy =\\frac{1}{2} \\mu  \\text{erf}\\left(\\frac{y-\\mu }{\\sqrt{2} \\sigma }\\right)-\\frac{\\sigma\n   e^{-\\frac{(y-\\mu )^2}{2 \\sigma ^2}}}{\\sqrt{2 \\pi }},\n\\] where \\(\\Phi(y,\\mu,\\sigma^2)\\) is the cumulative distribution function of the normal distribution. Thus, \\[\n\\int_{-\\infty}^{f^*} y \\phi(y,\\mu,\\sigma^2) dy = \\frac{1}{2} \\mu (1+\\text{erf}\\left(\\frac{f^*-\\mu }{\\sqrt{2} \\sigma\n   }\\right))-\\frac{\\sigma  e^{-\\frac{(f^*-\\mu )^2}{2 \\sigma ^2}}}{\\sqrt{2 \\pi}} = \\mu \\Phi(f^*,\\mu,\\sigma^2) + \\sigma^2 \\phi(f^*,\\mu,\\sigma^2).\n\\]\nwe can write the acquisition function as \\[\na(x) = \\dfrac{1}{2}\\left(\\sigma^2 \\phi(f^*,\\mu,\\sigma^2) + (f^*-\\mu)\\Phi(f^*,\\mu,\\sigma^2)\\right)\n\\]"
  },
  {
    "objectID": "05-sp.html#acquisition-function-2",
    "href": "05-sp.html#acquisition-function-2",
    "title": "Bayes AI",
    "section": "Acquisition function",
    "text": "Acquisition function\nLet’s implement it\n\nacq &lt;- function(xx,p, fstar) {\n  x = matrix(xx, ncol=1)\n  d = fstar - p$mean\n  s = sqrt(diag(p$Sigma))\n  return(s*dnorm(d) + d*pnorm(d))\n}"
  },
  {
    "objectID": "05-sp.html#taxi-fleet-optimisation",
    "href": "05-sp.html#taxi-fleet-optimisation",
    "title": "Bayes AI",
    "section": "Taxi Fleet Optimisation",
    "text": "Taxi Fleet Optimisation\n\nBlack box: taxi fleet simulator from Emukit project.\nInput: fleet size\nSimulates the taxi fleet operations and calculates the profit.\n\n\nTaxi Simulator Visualization"
  },
  {
    "objectID": "05-sp.html#taxi-fleet-optimisation-1",
    "href": "05-sp.html#taxi-fleet-optimisation-1",
    "title": "Bayes AI",
    "section": "Taxi Fleet Optimisation",
    "text": "Taxi Fleet Optimisation\n\nWe start with initial set of three designs \\(x = (10,30,90)\\), where \\(x\\) is the number of the taxis in\nObserve profit=(3.1,3.6,6.6)\n\nDefine a helper function to plot the GP emulator\n\nplotgp = function(x,y,XX,p) {\n  q1 = qnorm(0.05, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  q2 = qnorm(0.95, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  q3 = qnorm(0.5, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  ggplot() + geom_point(aes(x=x,y=y)) + geom_line(aes(x=XX,y=q3)) + geom_ribbon(aes(x=XX,ymin=q1, ymax=q2), alpha=0.2)\n}"
  },
  {
    "objectID": "05-sp.html#fit-gp",
    "href": "05-sp.html#fit-gp",
    "title": "Bayes AI",
    "section": "Fit GP",
    "text": "Fit GP\n\nlibrary(laGP)\nlibrary(mvtnorm)\nx = matrix(c(10,90,30), ncol=1)\nxx = seq(1,100, length=500)\nXX &lt;- matrix(xx, ncol = ncol(x))\nprofit = -c(3.1,3.6,6.6)\ngp &lt;- newGP(x, profit, 1000, 1e-6, dK = TRUE)\np &lt;- predGP(gp, XX)"
  },
  {
    "objectID": "05-sp.html#plot-the-fit",
    "href": "05-sp.html#plot-the-fit",
    "title": "Bayes AI",
    "section": "Plot the Fit",
    "text": "Plot the Fit\n\n\nCode\nplotgp(x,profit,XX,p)\n\n\n\nthere is potentially a better value at around 50 taxis"
  },
  {
    "objectID": "05-sp.html#run-optimisation",
    "href": "05-sp.html#run-optimisation",
    "title": "Bayes AI",
    "section": "Run Optimisation",
    "text": "Run Optimisation\n\nnextsample = function(){\n  ei = acq(xx,p,min(profit))\n  plot(xx,ei, type='l')\n  xnext = as.integer(xx[which.max(ei)])\n  return(xnext)\n}\nupdgp = function(xnext,f){\n  profit &lt;&lt;- c(profit, f)\n  x &lt;&lt;- c(x, xnext)\n  updateGP(gp, matrix(xnext,ncol=1), f)\n  p &lt;&lt;- predGP(gp, XX)\n  plotgp(x,profit,XX,p)\n}"
  },
  {
    "objectID": "05-sp.html#run-optimisation-1",
    "href": "05-sp.html#run-optimisation-1",
    "title": "Bayes AI",
    "section": "Run Optimisation",
    "text": "Run Optimisation\n\nCode\nnextsample(); #44\nupdgp(44, -8.4);\n\n\n\n\n\n\n\n[1] 44"
  },
  {
    "objectID": "05-sp.html#run-optimisation-2",
    "href": "05-sp.html#run-optimisation-2",
    "title": "Bayes AI",
    "section": "Run Optimisation",
    "text": "Run Optimisation\n\nCode\nnextsample(); # 57\nupdgp(57, -7.1);\n\n\n\n\n\n\n\n[1] 57"
  },
  {
    "objectID": "05-sp.html#run-optimisation-3",
    "href": "05-sp.html#run-optimisation-3",
    "title": "Bayes AI",
    "section": "Run Optimisation",
    "text": "Run Optimisation\n\nCode\nnextsample(); # 45\nupdgp(45, -8.5);\n\n\n\n\n\n\n\n[1] 45"
  },
  {
    "objectID": "05-sp.html#run-optimisation-4",
    "href": "05-sp.html#run-optimisation-4",
    "title": "Bayes AI",
    "section": "Run Optimisation",
    "text": "Run Optimisation\n\nCode\nnextsample(); # 100\nupdgp(100, -3.3);\n\n\n\n\n\n\n\n[1] 100"
  },
  {
    "objectID": "05-sp.html#run-optimisation-stop",
    "href": "05-sp.html#run-optimisation-stop",
    "title": "Bayes AI",
    "section": "Run Optimisation: Stop",
    "text": "Run Optimisation: Stop\nIf we run nextsample one more time, we get 47, close to our current best of 45. Further, the model is confident at this location. It means that we can stop the algorithm and “declare a victory”\n\n\nCode\nnextsample()\n\n\n\n[1] 47"
  },
  {
    "objectID": "07-regression.html#bayesian-regression",
    "href": "07-regression.html#bayesian-regression",
    "title": "Bayes AI",
    "section": "Bayesian Regression",
    "text": "Bayesian Regression\n\\[\ny_i \\sim D(f(\\eta_i),\\theta)\n\\] - \\(f\\): inverse of the link funciton - \\(\\eta_i\\): linear predictor - \\(D\\): distribution of the response variable (family) \\[\n\\eta = X\\beta + Zu\n\\] - \\(\\beta,u,\\theta\\): model parameters; \\(\\beta\\) - fixed effects, \\(u\\) - random effects"
  },
  {
    "objectID": "07-regression.html#random-intercepts-model",
    "href": "07-regression.html#random-intercepts-model",
    "title": "Bayes AI",
    "section": "Random Intercepts Model",
    "text": "Random Intercepts Model\nWe measure \\(Y_{ij}\\), subject \\(i\\), time \\(j\\). Want to model \\[\n\\mu_{ij} = E(Y_{ij}\\mid X_{ij},b_i);  ~g(\\mu_{ij})= \\beta_0 + b_i + \\beta_1 X_{ij}\n\\] - \\(b_i\\): random intercepts (subject-specific) - \\(X_{ij}\\): covariates - \\(g()\\): link function - \\(\\beta_0,\\beta_1\\): fixed effects"
  },
  {
    "objectID": "07-regression.html#random-intercepts-model-1",
    "href": "07-regression.html#random-intercepts-model-1",
    "title": "Bayes AI",
    "section": "Random Intercepts Model",
    "text": "Random Intercepts Model\nPriors: \\[\n\\beta_i \\sim N(0,10^2)\\\\\n\\] \\[\nb_i \\sim N(0,\\tau^2); ~ \\tau \\sim IG(1,0.02)\n\\] The likelihood is \\[\n\\prod_{ij} p(Y_{ij}\\mid \\mu_{ij})\n\\]"
  },
  {
    "objectID": "07-regression.html#random-intercepts-model-2",
    "href": "07-regression.html#random-intercepts-model-2",
    "title": "Bayes AI",
    "section": "Random Intercepts Model",
    "text": "Random Intercepts Model\nIn case of linear regression, the likelihood is \\[\n\\prod_{ij} N(\\mu_{ij},\\sigma^2)\n\\] \\[\n\\mu_{ij} = \\beta_0 + b_i + \\beta_1 X_{ij}\n\\]"
  },
  {
    "objectID": "07-regression.html#example-history-of-pandemics",
    "href": "07-regression.html#example-history-of-pandemics",
    "title": "Bayes AI",
    "section": "Example: History of Pandemics",
    "text": "Example: History of Pandemics\nBill Gates: 12/11/2009: “I’m most worried about a worldwide Pandemic”\n\n\n\n\n\n\n\n\n\nEarly-period Pandemics\nDates\nSize of Mortality\n\n\n\n\nPlague of Athens\n430 BC\n25% population.\n\n\nBlack Death\n1347\n30% Europe\n\n\nLondon Plague\n1666 2\n0% population\n\n\n\n\n\n\n\n\nRecent Flu Epidemics\nDates 1900-2010\nSize\n\n\n\n\nSpanish Flu\n1918-19 40-\n50 million\n\n\nAsian Flu\nH2N2, 1957-58 2\nmillion\n\n\nHong Kong Flu\nH3N2, 1968-69 6\nmillion\n\n\n\n\n\n\nSpanish Flu killed more than WW1\nH1N1 Flu 2009: \\(18,449\\) people killed World wide:"
  },
  {
    "objectID": "07-regression.html#seir-epidemic-models",
    "href": "07-regression.html#seir-epidemic-models",
    "title": "Bayes AI",
    "section": "SEIR Epidemic Models",
    "text": "SEIR Epidemic Models\nGrowth “self-reinforcing”: More likely if more infectants\n\nAn individual comes into contact with disease at rate \\(\\beta_1\\)\nThe susceptible individual contracts the disease with probability \\(\\beta_2\\)\nEach infectant becomes infectious with rate \\(\\alpha\\) per unit time\nEach infectant recovers with rate \\(\\gamma\\) per unit time\n\n\\(S_t + E_t + I_t + R_t = N\\)"
  },
  {
    "objectID": "07-regression.html#current-models-seir",
    "href": "07-regression.html#current-models-seir",
    "title": "Bayes AI",
    "section": "Current Models: SEIR",
    "text": "Current Models: SEIR\nsusceptible-exposed-infectious-recovered model\nDynamic models that extend earlier models to include exposure and recovery.\nThe coupled SEIR model:\n\\(\\dot{S} = -\\beta S I\\)\n\\(\\dot{E} = \\beta S I - \\alpha E\\)\n\\(\\dot{I} = \\alpha E -\\gamma I\\)\n\\(\\dot{R} = \\gamma I\\)"
  },
  {
    "objectID": "07-regression.html#infectious-disease-models",
    "href": "07-regression.html#infectious-disease-models",
    "title": "Bayes AI",
    "section": "Infectious disease models",
    "text": "Infectious disease models\nDaniel Bernoulli’s (1766) first model of disease transmission in smallpox:\n“I wish simply that, in matters which so closely concern the well being of the human race, no decision shall be made without all knowledge which a little analysis and calculation can provide”\n\nR.A. Ross, (Nobel Medicine winner, 1902) – math model of malaria transmission, which ultimately lead to malaria control.\n\nRoss-McDonald model\n\nKermack and McKendrick: susceptible-infectious-recovered (SIR)\n\nLondon Plague 1665-1666; Cholera: London 1865, Bombay, 1906."
  },
  {
    "objectID": "07-regression.html#example-london-plague-1666-village-eyam-nr.-sheffield",
    "href": "07-regression.html#example-london-plague-1666-village-eyam-nr.-sheffield",
    "title": "Bayes AI",
    "section": "Example: London Plague, 1666: Village Eyam nr. Sheffield",
    "text": "Example: London Plague, 1666: Village Eyam nr. Sheffield\nModel of transmission from Infectants, \\(I\\), to susceptibles, \\(S\\).\n\n\n\nDate 1666 Su\nsceptibles In\nfectives\n\n\n\n\nInitial\n254\n7\n\n\nJuly 3\n235 15\n\n\n\nJuly 19\n201 22\n\n\n\nAug 3 1\n53 29\n\n\n\nAug 19\n121 21\n\n\n\nSept 3\n108 8\n\n\n\nSept 19\n97 8\n\n\n\nOct 3\n–\n–\n\n\nOct 19\n83 0\n\n\n\n\nInitial Population \\(N=261=S_0\\); Final population \\(S_\\infty = 83\\)."
  },
  {
    "objectID": "07-regression.html#modeling-growth-si",
    "href": "07-regression.html#modeling-growth-si",
    "title": "Bayes AI",
    "section": "Modeling Growth: SI",
    "text": "Modeling Growth: SI\nCoupled Differential eqn \\(\\dot{S} = - \\beta SI , \\dot{I} = ( \\beta S - \\alpha ) I\\)\n\nEstimates \\(\\frac{\\beta}{\\alpha} = 6.54 \\times 10^{-3} , \\frac{\\alpha}{\\beta} = 1.53\\).\n\n\\[\n\\frac{ \\hat{\\beta} }{\\alpha} = \\frac{  \\ln ( S_0 / S_\\infty ) }{S_0 - S_\\infty}\n\\] Predicted maximum \\(30.4\\), very close to observed 29\nKey: \\(S\\) and \\(I\\) are observed and \\(\\alpha , \\beta\\) are estimated in hindsight"
  },
  {
    "objectID": "07-regression.html#transmission-rates-r_0-for-1918-episode",
    "href": "07-regression.html#transmission-rates-r_0-for-1918-episode",
    "title": "Bayes AI",
    "section": "Transmission Rates \\(R_0\\) for 1918 Episode",
    "text": "Transmission Rates \\(R_0\\) for 1918 Episode\n\n1918-19 influenza pandemic:\n\n\n\n\n\nMills et al. 2004:\n45 US cities\n3 (2-4)\n\n\nViboud et al. 2006:\nEngland and Wales\n1.8\n\n\nMassad et al. 2007:\nSao Paulo Brazil\n2.7\n\n\nNishiura, 2007:\nPrussia, Germany\n3.41\n\n\nChowell et al., 2006:\nGeneva, Switzerland\n2.7-3.8\n\n\nChowell et al., 2007:\nSan Francisco\n2.7-3.5\n\n\n\nThe larger the \\(R_0\\) the more severe the epidemic.\nTransmission parameters vary substantially from epidemic to epidemic"
  },
  {
    "objectID": "07-regression.html#boat-localization-example",
    "href": "07-regression.html#boat-localization-example",
    "title": "Bayes AI",
    "section": "Boat Localization Example",
    "text": "Boat Localization Example\nLocalization with measurement update\n\nA boat sails from one island to another\nBoat is trying to identify its location \\(\\theta \\sim N(m_0, C_0)\\)\nUsing a sequence of measurements to one of the islands \\(x_1,\\ldots,x_n\\)\n\nMeasurements are noisy due to dilution of precision http://www.sailingmates.com/your-gps-can-kill-you/"
  },
  {
    "objectID": "07-regression.html#reckoning",
    "href": "07-regression.html#reckoning",
    "title": "Bayes AI",
    "section": "Reckoning",
    "text": "Reckoning\nLocalization with no measurement updates is called reckoning\n\nsource: http://www.hakaimagazine.com/article-short/traversing-seas"
  },
  {
    "objectID": "07-regression.html#kalman-filter",
    "href": "07-regression.html#kalman-filter",
    "title": "Bayes AI",
    "section": "Kalman Filter",
    "text": "Kalman Filter\n\\[\n\\theta \\sim N(m_0, C_0)\n\\] \\[\nx_t = \\theta + w_t,~~~w_t \\sim N(0,\\sigma^2)\n\\] \\[\nx_1,x_2,\\ldots \\mid \\theta \\sim N(\\theta,\\sigma^2)\n\\] The prior variance \\(C_0\\) might be quite large if you are very uncertain about your guess \\(m_0\\)\nGiven the measurements \\(x^n = (x_1,\\ldots,x_n)\\), you update your opinion about \\(\\theta\\) computing its posterior density, using the Bayes formula"
  },
  {
    "objectID": "07-regression.html#normal-model",
    "href": "07-regression.html#normal-model",
    "title": "Bayes AI",
    "section": "Normal Model",
    "text": "Normal Model\n\\[\nf(x) = \\dfrac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp^{-\\dfrac{1}{2}\\dfrac{(x-\\mu)^2}{\\sigma^2}}\n\\] Or multivariate equivalent \\[\nf(x) = (2 \\pi)^{-k/2} |\\Sigma|^{-1/2}\\exp^{-0.5(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}\n\\]\n\n\nCode\ncurve(exp(-x*x),-2,2, bty=\"newdataframe &lt;- na.omit(dataframe)\")"
  },
  {
    "objectID": "07-regression.html#the-conjugate-prior-for-the-normal-distribution",
    "href": "07-regression.html#the-conjugate-prior-for-the-normal-distribution",
    "title": "Bayes AI",
    "section": "The Conjugate Prior for the Normal Distribution",
    "text": "The Conjugate Prior for the Normal Distribution\nWe will look at the Gaussian distribution from a Bayesian point of view. In the standard form, the likelihood has two parameters, the mean \\(\\mu\\) and the variance \\(\\sigma^2\\) \\[\np(x^n | \\mu, \\sigma^2) \\propto \\dfrac{1}{\\sigma^n}\\exp\\left(-\\dfrac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\right)\n\\]"
  },
  {
    "objectID": "07-regression.html#normal-prior",
    "href": "07-regression.html#normal-prior",
    "title": "Bayes AI",
    "section": "Normal Prior",
    "text": "Normal Prior\nIn case when we know the variance \\(\\sigma^2\\), but do not know mean \\(\\mu\\), we assume \\(\\mu\\) is random. To have conjugate prior we choose \\[\np(\\mu | \\mu_0, \\sigma_0) \\propto \\dfrac{1}{\\sigma_0}\\exp\\left(-\\dfrac{1}{2\\sigma_0^2}(\\mu-\\mu_0^2)\\right)\n\\] In practice, when little is known about \\(\\mu\\), it is common to set the location hyper-parameter to zero and the scale to some large value."
  },
  {
    "objectID": "07-regression.html#normal-model-with-unknown-mean-known-variance",
    "href": "07-regression.html#normal-model-with-unknown-mean-known-variance",
    "title": "Bayes AI",
    "section": "Normal Model with Unknown Mean, Known Variance",
    "text": "Normal Model with Unknown Mean, Known Variance\nSuppose we wish to estimate a model where the likelihood of the data is normal with an unknown mean \\(\\mu\\) and a known variance \\(\\sigma^2\\).\nOur parameter of interest is \\(\\mu\\). We can use a conjugate Normal prior on \\(\\mu\\), with mean \\(\\mu_0\\) and variance \\(\\sigma_0^2\\). \\[\n\\begin{aligned}\np(\\mu| x^n, \\sigma^2) & \\propto p(x^n | \\mu, \\sigma^2)p(\\mu) ~~~\\mbox{(Bayes rule)}\\\\\nN(\\mu_1,\\tau_1)     & = N(\\mu, \\sigma^2)\\times N(\\mu_0, \\sigma_0^2)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "07-regression.html#useful-identity",
    "href": "07-regression.html#useful-identity",
    "title": "Bayes AI",
    "section": "Useful Identity",
    "text": "Useful Identity\nOne of the most useful algebraic tricks for calculating posterior distribution is completing the square.\n\n\n\n\n\n\n\\[\n\\dfrac{(x-\\mu_1)^2}{\\sigma_1} + \\dfrac{(x-\\mu_2)^2}{\\sigma_2} = \\dfrac{(x - \\mu_3)^2}{\\sigma_3} + \\dfrac{(\\mu_1-\\mu_2)^2}{\\sigma_1 + \\sigma_2}\n\\] where \\[\n\\mu_3 = \\sigma_3 (\\mu_1/\\sigma_1 + \\mu_2/\\sigma_2)\n\\] and \\[\n\\sigma_3 = (1/\\sigma_1 + 1/\\sigma_2)^{-1}\n\\]\n\n\nPrior: \\[\n\\theta \\sim \\frac{e^{-\\frac{(\\theta-\\mu )^2}{2 \\sigma ^2}}}{\\sqrt{2 \\pi } \\sigma }\n\\] Likelihood: \\[\nx \\mid \\theta \\sim \\frac{e^{-\\frac{(\\theta-y )^2}{2 r^2}}}{\\sqrt{2 \\pi } r}\n\\] Posterior mean: \\[\n\\frac{x  \\sigma ^2+\\mu  r^2}{r^2+\\sigma ^2}\n\\] Posterior variance: \\[\n\\frac{1}{\\frac{1}{r^2}+\\frac{1}{\\sigma ^2}}\n\\]"
  },
  {
    "objectID": "07-regression.html#prior-likelihood-posterior",
    "href": "07-regression.html#prior-likelihood-posterior",
    "title": "Bayes AI",
    "section": "Prior, Likelihood, Posterior",
    "text": "Prior, Likelihood, Posterior"
  },
  {
    "objectID": "07-regression.html#after-n-steps",
    "href": "07-regression.html#after-n-steps",
    "title": "Bayes AI",
    "section": "After \\(n\\) steps",
    "text": "After \\(n\\) steps\n\\[\n\\begin{aligned}\np(\\mu  | x^n) & \\propto \\prod_{i=1}^{n}\\dfrac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac{(x_i - \\mu )^2}{2\\sigma^2}\\right)\\times\\dfrac{1}{\\sqrt{2\\pi \\sigma_0^2}}\\exp\\left(-\\frac{(\\mu  - \\mu_0)^2}{2\\sigma_0^2}\\right)\\\\\n& \\propto \\exp\\left(-\\sum_{i=1}^{n}\\frac{(x_i - \\mu )^2}{2\\sigma^2} - \\frac{(\\mu  - \\mu_0)^2}{2\\sigma_0^2}\\right)\\\\\n& = \\exp\\left(-\\dfrac{1}{2}\\left[\\sum_{i=1}^{n}\\frac{(x_i - \\mu )^2}{\\sigma^2} + \\frac{(\\mu  - \\mu_0)^2}{\\sigma_0^2}\\right]\\right)\\\\\n& = \\exp\\left(-\\dfrac{1}{2\\sigma^2 \\sigma_0^2}\\left[\\sigma_0^2\\sum_{i=1}^{n}(x_i - \\mu )^2 + \\sigma^2 (\\mu  - \\mu_0)^2\\right]\\right)\\\\\n& = \\exp\\left(-\\dfrac{1}{2\\sigma^2 \\sigma_0^2}\\left[\\sigma_0^2\\sum_{i=1}^{n}(x_i^2 - 2\\mu x_i+ \\mu ^2) + \\sigma^2 (\\mu ^2 - 2\\mu \\mu_0 + \\mu_0^2)\\right]\\right)\\\\\\end{aligned}\n\\]"
  },
  {
    "objectID": "07-regression.html#after-n-steps-1",
    "href": "07-regression.html#after-n-steps-1",
    "title": "Bayes AI",
    "section": "After \\(n\\) steps",
    "text": "After \\(n\\) steps\nWe can multiply the \\(2\\mu x_i\\) term in the summation by \\(n/n\\) in order to get the equations in terms of the sufficient statistic \\(\\bar{x}^n\\) \\[\n\\begin{aligned}\np(\\mu  | x^n) & \\propto \\exp\\left(-\\dfrac{1}{2\\sigma^2 \\sigma_0^2}\\left[\\sigma_0^2\\sum_{i=1}^{n}(x_i^2 - \\dfrac{n}{n}2\\mu x_i+ \\mu ^2) + \\sigma^2 (\\mu ^2 - 2\\mu \\mu_0 + \\mu_0^2)\\right]\\right)\\\\\n& = \\exp\\left(-\\dfrac{1}{2\\sigma^2 \\sigma_0^2}\\left[\\sigma_0^2\\sum_{i=1}^{n}x_i^2 - \\sigma_0^22\\mu n\\bar{x}^n+ \\tau_n^0n\\mu ^2 + \\sigma^2 \\mu ^2 - 2\\mu \\mu_0\\sigma^2 + \\mu_0^2\\sigma^2\\right]\\right)\\end{aligned}\n\\] set \\(k = \\sigma_0^2\\sum_{i=1}^{n}x_i^2 + \\mu_0^2\\sigma^2\\) (they do not contain \\(\\mu\\)) \\[\np(\\mu  | x^n)   \\propto \\exp\\left(-\\dfrac{1}{2}\\left[\\mu ^2\\left(\\dfrac{1}{\\sigma_0^2} + \\dfrac{n}{\\sigma^2}\\right) - 2\\mu\\left(\\dfrac{\\mu_0}{\\sigma_0^2} + \\dfrac{n\\bar{x}^n}{\\sigma^2}\\right) + k\\right]\\right)\n\\]"
  },
  {
    "objectID": "07-regression.html#after-n-steps-2",
    "href": "07-regression.html#after-n-steps-2",
    "title": "Bayes AI",
    "section": "After \\(n\\) steps",
    "text": "After \\(n\\) steps\nLet’s multiply by \\[\n\\dfrac{1/\\sigma_0^2 + n/\\sigma^2}{1/\\sigma_0^2 + n/\\sigma^2}\n\\] Now \\[\np(\\mu  | x^n)   \\propto \\exp\\left(-\\dfrac{1}{2}\\left(1/\\sigma_0^2 + n/\\sigma^2\\right)\\left(\\mu  - \\dfrac{\\mu_0/\\sigma_0^2 + n\\bar{x}^n/\\sigma^2}{1/\\sigma_0^2 + n/\\sigma^2}\\right)^2\\right)\n\\]\n\\[\np(\\mu  | x^n)   \\propto \\exp\\left(-\\dfrac{1}{2}\\left(1/\\sigma_0^2 + n/\\sigma^2\\right)\\left(\\mu  - \\dfrac{\\mu_0/\\sigma_0^2 + n\\bar{x}^n/\\sigma^2}{1/\\sigma_0^2 + n/\\sigma^2}\\right)^2\\right)\n\\]"
  },
  {
    "objectID": "07-regression.html#after-n-steps-3",
    "href": "07-regression.html#after-n-steps-3",
    "title": "Bayes AI",
    "section": "After \\(n\\) steps",
    "text": "After \\(n\\) steps\n\nPosterior mean: \\(\\mu_n  =  \\dfrac{\\mu_0/\\sigma_0^2 + n\\bar{x}^n/\\sigma^2}{1/\\sigma_0^2 + n/\\sigma^2}\\)\nPosterior variance: \\(\\sigma_n^2 = \\left(1/\\sigma_0^2 + n/\\sigma^2\\right)^{-1}\\)\nPosterior precision:: \\(\\tau_n^2 = 1/\\sigma_0^2 + n/\\sigma^2\\)\n\nPosterior Precision is just the sum of the prior precision and the data precision."
  },
  {
    "objectID": "07-regression.html#posterior-mean",
    "href": "07-regression.html#posterior-mean",
    "title": "Bayes AI",
    "section": "Posterior Mean",
    "text": "Posterior Mean\n\\[\n\\begin{aligned}\n\\mu_n  & =  \\dfrac{\\mu_0/\\sigma_0^2 + n\\bar{x}^n/\\sigma^2}{1/\\sigma_0^2 + n/\\sigma^2}\\\\\n& = \\dfrac{\\mu_0\\sigma^2}{\\sigma^2 + n\\sigma_0^2} + \\dfrac{\\sigma_0^2n\\bar{x}^n}{\\sigma^2 + n\\sigma_0^2}\\end{aligned}\n\\]\n\nAs \\(n\\) increases, data mean dominates prior mean.\nAs \\(\\sigma_0^2\\) decreases (less prior variance, greater prior precision), our prior mean becomes more important."
  },
  {
    "objectID": "07-regression.html#a-state-space-model",
    "href": "07-regression.html#a-state-space-model",
    "title": "Bayes AI",
    "section": "A state space model",
    "text": "A state space model\nA state space model consists of two equations: \\[\n\\begin{aligned}\nZ_t&=HS_t+w_t\\\\\nS_{t+1} &= FS_t + v_t\\end{aligned}\n\\] where \\(S_t\\) is a state vector of dimension \\(m\\), \\(Z_t\\) is the observed time series, \\(F\\), \\(G\\), \\(H\\) are matrices of parameters, \\(\\{w_t\\}\\) and \\(\\{v_t\\}\\) are \\(iid\\) random vectors satisfying \\[\n\\mbox{E}(w_t)=0, \\hspace{0.5cm} \\mbox{E}(v_t)=0, \\hspace{0.5cm}\\mathrm{cov}(v_t)=V, \\hspace{0.5cm} \\mathrm{cov}(w_t)=W\n\\] and \\(\\{w_t\\}\\) and \\(\\{v_t\\}\\) are independent."
  },
  {
    "objectID": "07-regression.html#state-space-models",
    "href": "07-regression.html#state-space-models",
    "title": "Bayes AI",
    "section": "State Space Models",
    "text": "State Space Models\n\nState space models consider a time series as the output of a dynamic system perturbed by random disturbances.\nNatural interpretation of a time series as the combination of several components, such as trend, seasonal or regressive components.\nComputations can be implemented by recursive algorithms."
  },
  {
    "objectID": "07-regression.html#types-of-inference",
    "href": "07-regression.html#types-of-inference",
    "title": "Bayes AI",
    "section": "Types of Inference",
    "text": "Types of Inference\n\nModel building versus inferring unknown variable. Assume a linear model \\(Z = HS + \\epsilon\\)\nModel building: know signal \\(S\\), observe \\(Z\\), infer \\(H\\) (a.k.a. model identification, learning)\nEstimation: know \\(H\\), observe \\(Z\\), estimate \\(S\\)\nHypothesis testing: unknown takes one of few possible values; aim at small probability of incorrect decision\nEstimation: aim at a small estimation error"
  },
  {
    "objectID": "07-regression.html#time-series-estimation-tasks",
    "href": "07-regression.html#time-series-estimation-tasks",
    "title": "Bayes AI",
    "section": "Time Series Estimation Tasks",
    "text": "Time Series Estimation Tasks\n\nFiltering: To recover the state vector \\(S_t\\) given \\(Z^t\\)\nPrediction: To predict \\(S_{t+h}\\) or \\(Z_{t+h}\\) for \\(h &gt; 0\\), given \\(Z^t\\)\nSmoothing: To estimate \\(S_t\\) given \\(Z^T\\) , where \\(T &gt; t\\)"
  },
  {
    "objectID": "07-regression.html#property-of-multivariate-normal",
    "href": "07-regression.html#property-of-multivariate-normal",
    "title": "Bayes AI",
    "section": "Property of Multivariate Normal",
    "text": "Property of Multivariate Normal\nUnder normality, we have\n\nthat normal prior plus normal likelihood results in a normal posterior,\nthat if the random vector \\((X, Y )\\) are jointly normal\n\n\\[\n\\begin{bmatrix}\nX\\\\\nY\n\\end{bmatrix}\n\\sim N\\left(\n\\begin{bmatrix}\n\\mu_x\\\\\n\\mu_y\n\\end{bmatrix},\n\\begin{bmatrix}\n\\Sigma_{xx}&\\Sigma_{xy}\\\\\n\\Sigma_{yx}&\\Sigma_{yy}\\\\\n\\end{bmatrix}\\right),\n\\]\n\nthen the conditional distribution of \\(X\\) given \\(Y = y\\) is normal\n\n\\[\nX|Y = y\\sim N\\left[\\mu_x+\\Sigma_{xy}\\Sigma_{yy}^{-1}(y-\\mu_y),\\Sigma_{xx}-\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx}\\right].\n\\]"
  },
  {
    "objectID": "07-regression.html#from-state-space-model",
    "href": "07-regression.html#from-state-space-model",
    "title": "Bayes AI",
    "section": "From State Space Model",
    "text": "From State Space Model\n\\[\n\\begin{aligned}\nS_{t+1}^t &= FS_t\\\\\nZ_{t+1}^t &=HS_{t+1}^t\\\\\nP_{t+1}^t&=FP_tF^T+GQG^T\\\\\nV_{t+1}^t&=HP_{t+1}^tH^T+R\\\\\nC_{t+1}^t&=HP_{t+1}^t\n\\end{aligned}\n\\]\n\n\\(P_{t+j}^t\\) = conditional covariance matrix of \\(S_{t+j}\\) given \\(\\{Z_t , Z_{t-1} , \\cdots\\}\\) for \\(j \\geq 0\\)\n\\(S_{t+j}^t\\) = conditional mean of \\(S_{t+j}\\) given \\(\\{Z_t , Z_{t-1} , \\cdots\\}\\)\n\\(V_{t+1}^t\\) = conditional variance of \\(Z_{t+1}\\) given \\(Z^t = \\{Z_t , Z_{t-1} , \\cdots\\}\\)\n\\(C_{t+1}^t\\) = conditional covariance between \\(Z_{t+1}\\) and \\(S_{t+1}\\)"
  },
  {
    "objectID": "07-regression.html#joint-conditional-distribution-ps_t1-z_t1-zt",
    "href": "07-regression.html#joint-conditional-distribution-ps_t1-z_t1-zt",
    "title": "Bayes AI",
    "section": "Joint conditional distribution \\(P(S_{t+1}, Z_{t+1} | Z^t)\\)",
    "text": "Joint conditional distribution \\(P(S_{t+1}, Z_{t+1} | Z^t)\\)\n\\[\n\\begin{bmatrix}\nS_{t+1}\\\\\nZ_{t+1}\n\\end{bmatrix}_t\n\\sim N\n\\left( \\begin{bmatrix}\nS_{t+1}^t\\\\\nZ_{t+1}^t\n\\end{bmatrix},\n\\begin{bmatrix}\nP_{t+1}^t & P_{t+1}^tH'\\\\\nHP_{t+1}^t & HP_{t+1}^tH'+R\n\\end{bmatrix} \\right)\n\\]"
  },
  {
    "objectID": "07-regression.html#ps_t1-z_t1",
    "href": "07-regression.html#ps_t1-z_t1",
    "title": "Bayes AI",
    "section": "\\(P(S_{t+1}| Z_{t+1})\\)",
    "text": "\\(P(S_{t+1}| Z_{t+1})\\)\nFinally, when \\(Z_{t+1}\\) becomes available, we may use the property of nromality to update the distribution of \\(S_{t+1}\\) . More specifically, \\[\nS_{t+1}=S_{t+1}^t+P_{t+1}^tH^T[HP_{t+1}^tH^T+R]^{-1}(Z_{t+1}-Z_{t+1}^t)\n\\] and \\[\nP_{t+1}=P_{t+1}^t-P_{t+1}^tH^T[HP_{t+1}^tH'+R]^{-1}HP_{t+1}^t.\n\\] Predictive residual: \\[\nR_{t+1}^t=Z_{t+1}-Z_{t+1}^t=Z_{t+1}-HS_{t+1}^t \\ne 0\n\\] means there is new information about the system so that the state vector should be modified. The contribution of \\(r_{t+1}^t\\) to the state vector, of course, needs to be weighted by the variance of \\(r_{t+1}^t\\) and the conditional covariance matrix of \\(S_{t+1}\\)."
  },
  {
    "objectID": "07-regression.html#kalman-filter-1",
    "href": "07-regression.html#kalman-filter-1",
    "title": "Bayes AI",
    "section": "Kalman filter",
    "text": "Kalman filter\n\nPredict: \\[\n\\begin{aligned}\nS_{t+1}^t &= FS_t\\\\\nZ_{t+1}^t &=HS_{t+1}^t\\\\\nP_{t+1}^t&=FP_tF^T+GQG^T\\\\\nV_{t+1}^t&=HP_{t+1}^tH^T+R\n\\end{aligned}\n\\]\nUpdate: \\[\n\\begin{aligned}\nS_{t+1|t+1}=& S_{t+1}^t+P_{t+1}^tH^T[HP_{t+1}^tH^T+R]^{-1}(Z_{t+1}-Z_{t+1}^t)\\\\\nP_{t+1|t+1}=& P_{t+1}^t-P_{t+1}^tH^T[HP_{t+1}^tH^T+R]^{-1}HP_{t+1}^t\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "07-regression.html#kalman-filter-2",
    "href": "07-regression.html#kalman-filter-2",
    "title": "Bayes AI",
    "section": "Kalman filter",
    "text": "Kalman filter\n\nstarts with initial prior information \\(S_{0}\\) and \\(P_{0}\\)\npredicts \\(Z_{1}^0\\) and \\(V_{1}^0\\)\nOnce the observation \\(Z_1\\) is available, uses the updating equations to compute \\(S_{1}\\) and \\(P_{1}\\)\n\n\\(S_{1|1}\\) and \\(P_{1|1}\\) is the prior for the next observation.\nThis is the Kalman recusion."
  },
  {
    "objectID": "07-regression.html#kalman-filter-3",
    "href": "07-regression.html#kalman-filter-3",
    "title": "Bayes AI",
    "section": "Kalman filter",
    "text": "Kalman filter\n\neffect of the initial values \\(S_{0}\\) and \\(P_{0}\\) is decresing as \\(t\\) increases\nfor a stationary time series, all eigenvalues of the coefficient matrix \\(F\\) are less than one in modulus\nKalman filter recursion ensures that the effect of the initial values indeed vanishes as \\(t\\) increases\nuncertainty about the state is always normal"
  },
  {
    "objectID": "07-regression.html#local-trend-model",
    "href": "07-regression.html#local-trend-model",
    "title": "Bayes AI",
    "section": "Local Trend Model",
    "text": "Local Trend Model\n\\[\n\\begin{aligned}\ny_t =& \\mu_t + e_t,~e_t \\sim N(0,\\sigma_e^2)\\\\\\\n\\mu_{t+1} =& \\mu_t + \\eta_t,~ \\eta_t \\sim N(0,\\sigma_{\\eta}^2 )\n\\end{aligned}\n\\]\n\n\\(\\{e_t\\}\\) and \\(\\{\\eta_t\\}\\) are iid Gaussian white noise\n\\(\\mu_0\\) is given (possible as a distributed value)\ntrend \\(\\mu_t\\) is not observable\nwe observe some noisy version of the trend \\(y_t\\)\nsuch a model can be used to analyze realized volatility: \\(\\mu_t\\) is the log volatility and \\(y_t\\) is constructed from high frequency transactions data"
  },
  {
    "objectID": "07-regression.html#local-trend-model-1",
    "href": "07-regression.html#local-trend-model-1",
    "title": "Bayes AI",
    "section": "Local Trend Model",
    "text": "Local Trend Model\n\\[\n\\begin{aligned}\ny_t =& \\mu_t + e_t,~e_t \\sim N(0,\\sigma_e^2)\\\\\\\n\\mu_{t+1} =& \\mu_t + \\eta_t,~ \\eta_t \\sim N(0,\\sigma_{\\eta}^2 )\n\\end{aligned}\n\\]\n\nif \\(\\sigma_e=0\\), then we have ARIMA(0,1,0) model\nif \\(\\sigma_e &gt; 0\\), then we have ARIMA(0,1,1) model, satisfying\n\n\\[\n(1-B)y_t = (1-\\theta B)a_t, ~ a_t \\sim N(0,\\sigma_a^2)\n\\] \\(\\sigma_a\\) and \\(\\theta\\) are determined by \\(\\sigma_e\\) and \\(\\sigma_{\\eta}\\) \\[\n(1-B)y_t = \\eta_{t-1} + e_t - e_{t-1}\n\\]"
  },
  {
    "objectID": "07-regression.html#liner-regression-time-dependent-parameters",
    "href": "07-regression.html#liner-regression-time-dependent-parameters",
    "title": "Bayes AI",
    "section": "Liner Regression (time dependent parameters)",
    "text": "Liner Regression (time dependent parameters)\n\\[\n\\begin{aligned}\ny_t  &= \\alpha_t + \\beta_t \\, x_t + \\epsilon_t   \\qquad & \\epsilon_t \\, \\sim N(0,\\sigma^2) \\\\\n\\alpha_t &= \\quad \\alpha_{t-1}  + \\epsilon_t^{\\alpha} \\qquad & \\epsilon_t^{\\alpha} \\sim N(0,\\sigma_{\\alpha}^2) \\\\\n\\beta_t  &= \\quad \\beta_{t-1}   + \\epsilon_t^{\\beta}   \\qquad & \\epsilon_t^{\\beta} \\sim N(0, \\sigma_{\\beta}^2) \\\\\n\\end{aligned}\n\\]\ndlm Package\n\ndlmModARMA: for an ARMA process, potentially multivariate\ndlmModPoly: for an \\(n^{th}\\) order polynomial\ndlmModReg : for Linear regression\ndlmModSeas: for periodic – Seasonal factors\ndlmModTrig: for periodic – Trigonometric form"
  },
  {
    "objectID": "07-regression.html#local-linear-trend",
    "href": "07-regression.html#local-linear-trend",
    "title": "Bayes AI",
    "section": "Local Linear Trend",
    "text": "Local Linear Trend\n\\[\n\\begin{aligned}\ny_t &= \\qquad \\quad \\mu_t  + \\upsilon_t  \\quad &\\upsilon_t \\sim N(0,V) \\\\\n\\mu_t &= \\mu_{t-1}  + \\delta_{t-1} + \\omega_t^{\\mu} \\quad & \\omega_t^{\\mu} \\sim N(0,W^{\\mu}) \\\\\n\\delta_t &= \\qquad \\,\\, \\, \\delta_{t-1} + \\omega_t^{\\delta} \\quad & \\omega_t^{\\delta} \\sim N(0,W^{\\delta}) \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "07-regression.html#simple-exponential-smoothing-with-additive-errors",
    "href": "07-regression.html#simple-exponential-smoothing-with-additive-errors",
    "title": "Bayes AI",
    "section": "Simple exponential smoothing with additive errors",
    "text": "Simple exponential smoothing with additive errors\n\\[\nx_t = \\ell_{t-1} + \\varepsilon_t\n\\] \\[\n\\ell_t=\\ell_{t-1}+\\alpha \\varepsilon_t.\n\\]"
  },
  {
    "objectID": "07-regression.html#holts-linear-method-with-additive-errors",
    "href": "07-regression.html#holts-linear-method-with-additive-errors",
    "title": "Bayes AI",
    "section": "Holt’s linear method with additive errors",
    "text": "Holt’s linear method with additive errors\n\\[\n\\begin{aligned}\ny_t&=\\ell_{t-1}+b_{t-1}+\\varepsilon_t\\\\\n\\ell_t&=\\ell_{t-1}+b_{t-1}+\\alpha \\varepsilon_t\\\\\nb_t&=b_{t-1}+\\beta \\varepsilon_t, \\end{aligned}\n\\]"
  },
  {
    "objectID": "07-regression.html#relation-to-arma-models",
    "href": "07-regression.html#relation-to-arma-models",
    "title": "Bayes AI",
    "section": "Relation to ARMA models",
    "text": "Relation to ARMA models\nConsider relation with ARMA models. The basic relations are\n\nan ARMA model can be put into a state space form in “infinite\" many ways;\nfor a given state space model in, there is an ARMA model."
  },
  {
    "objectID": "07-regression.html#state-space-model-to-arma-model",
    "href": "07-regression.html#state-space-model-to-arma-model",
    "title": "Bayes AI",
    "section": "State space model to ARMA model",
    "text": "State space model to ARMA model\nThe second possibility is that there is an observational noise. Then, the same argument gives \\[\n(1+\\alpha_1B+\\cdots+\\alpha_mB^m)(Z_{t+m}-\\epsilon_{t+m})=(1-\\theta_1B-\\cdots -\\theta_{m-1}B^{m-1})a_{t+m}\n\\] By combining \\(\\epsilon_t\\) with \\(a_t\\) , the above equation is an ARMA\\((m, m)\\) model."
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-ar2",
    "href": "07-regression.html#arma-model-to-state-space-model-ar2",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: AR(2)",
    "text": "ARMA model to state space model: AR(2)\n\\[\nZ_t=\\phi_1Z_{t-1}+\\phi_2Z_{t-2}+a_t\n\\] For such an AR(2) process, to compute the forecasts, we need \\(Z_{t-1}\\) and \\(Z_{t-2}\\) . Therefore, it is easily seen that \\[\n\\begin{bmatrix}\nZ_{t+1}\\\\\nZ_t\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\phi_1 & \\phi_2\\\\\n1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nZ_t\\\\\nZ_{t-1}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1\\\\\n0\n\\end{bmatrix}\ne_t,\n\\] where \\(e_t = a_{t+1}\\) and \\[\nZ_t=[1, 0]S_t\n\\] where \\(S_t = (Z_t , Z_{t-1})^T\\) and there is no observational noise."
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-ma2",
    "href": "07-regression.html#arma-model-to-state-space-model-ma2",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: MA(2)",
    "text": "ARMA model to state space model: MA(2)\n\\[\nZ_t=a_t-\\theta_1a_{t-1}-\\theta_2a_{t-2}\n\\] Method 1:\n\\[\n\\begin{bmatrix}\na_t\\\\\na_{t-1}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 & 0\\\\\n1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\na_{t-1}\\\\\na_{t-2}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1\\\\\n0\n\\end{bmatrix}\na_t\n\\]\n\\[\nZ_t=[-\\theta_1, -\\theta_2]S_{t-1} + a_t\n\\] Here the innovation \\(a_t\\) shows up in both the state transition equation and the observation equation. The state vector is of dimension 2."
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-ma2-1",
    "href": "07-regression.html#arma-model-to-state-space-model-ma2-1",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: MA(2)",
    "text": "ARMA model to state space model: MA(2)\nMethod 2: For an MA(2) model, we have \\[\n\\begin{aligned}\nZ_{t}^t&=Z_t\\\\\nZ_{t+1}^t&=-\\theta_1a_t-\\theta_2a_{t-1}\\\\\nZ_{t+2}^t&= -\\theta_2a_t\n\\end{aligned}\n\\] Let \\(S_t = (Z_t , -\\theta_1 a_t - \\theta_2 a_{t-1} , -\\theta_2 a_t )^T\\) . Then, \\[\nS_{t+1}=\n\\begin{bmatrix}\n0 & 1& 0\\\\\n0& 0& 1\\\\\n0& 0& 0\n\\end{bmatrix}\nS_t+\n\\begin{bmatrix}\n1\\\\\n-\\theta_1\\\\\n-\\theta_2\n\\end{bmatrix}\na_{t+1}\n\\] and \\[\nZ_t=[1,0,0]S_t\n\\] Here the state vector is of dimension 3, but there is no observational noise."
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-akaikes-approach",
    "href": "07-regression.html#arma-model-to-state-space-model-akaikes-approach",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: Akaike’s approach",
    "text": "ARMA model to state space model: Akaike’s approach\nConsider ARMA\\((p, q)\\) process, let \\(m = max\\{p, q + 1\\}\\), \\(\\phi_i = 0\\) for \\(i &gt; p\\) and \\(\\theta_j = 0\\) for \\(j &gt; q\\). \\[\nS_t = (Z_t , Z_{t+1}^t , Z_{t+2}^t ,\\cdots , Z_{t+m-1}^t )^T\n\\] where \\(Z_{t+\\ell}^t\\) is the conditional expectation of \\(Z_{t+\\ell}\\) given \\(\\Psi_t = \\{Z_t , Z_{t-1} , \\cdots\\}\\). By using the updating equation \\(f\\) forecasts (recall what we discussed before) \\[\nZ_{t+1}(\\ell -1)=Z_t(\\ell)+\\psi_{\\ell-1}a_{t+1},\n\\]"
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-akaikes-approach-1",
    "href": "07-regression.html#arma-model-to-state-space-model-akaikes-approach-1",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: Akaike’s approach",
    "text": "ARMA model to state space model: Akaike’s approach\n\\[\nS_t = (Z_t , Z_{t+1}^t , Z_{t+2}^t ,\\cdots , Z_{t+m-1}^t )^T\n\\]   \\[\nS_{t+1}=FS_t+Ga_{t+1}\n\\]\n\\[\nZ_t=[1,0, \\cdots ,0]S_t\n\\] where \\[\nF=\n\\left[\n\\begin{array}{c|cccc}\n0 &1& 0& \\cdots& 0\\\\\n0 & 0 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & & \\\\\n\\phi_m & \\phi_{m-1} & \\cdots & \\phi_2 & \\phi_1\n\\end{array}\\right], G=\n\\begin{bmatrix}\n1\\\\\n\\psi_1\\\\\n\\psi_2\\\\\n\\vdots\\\\\n\\psi_{m-1}\n\\end{bmatrix}\n\\] The matrix \\(F\\) is call a companion matrix of the polynomial \\(1 - \\phi_1 B - \\cdots - \\phi_m B^m\\)."
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-aokis-method",
    "href": "07-regression.html#arma-model-to-state-space-model-aokis-method",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: Aoki’s Method",
    "text": "ARMA model to state space model: Aoki’s Method\nTwo-step procedure: First, consider the MA\\((q)\\) part: \\[\nW_t = a_t - \\theta_1 a_{t-1} - \\cdots - \\theta_q a_{t-q}\n\\] \\[\n\\begin{bmatrix}\na_t\\\\\na_{t-1}\\\\\n\\vdots\\\\\na_{t-q+1}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0&0&\\cdots & 0&0\\\\\n1&0& \\cdots&0&0\\\\\n\\vdots & & & &\\\\\n0 & 0 & \\cdots & 1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\na_{t-1}\\\\\na_{t-2}\\\\\n\\vdots\\\\\na_{t-q}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{bmatrix}\na_t\n\\]\n\\[\nW_t=[-\\theta_1,-\\theta_2, \\cdots, -\\theta_q]S_t+a_t\n\\]"
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-aokis-method-1",
    "href": "07-regression.html#arma-model-to-state-space-model-aokis-method-1",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: Aoki’s Method",
    "text": "ARMA model to state space model: Aoki’s Method\nFirst, consider the AR\\((p)\\) part: \\[\nZ_t = \\phi_1 Z_{t-1} + ... + \\phi_p Z_{t-p} + W_t\n\\] Define state-space vector as \\[\nS_t=(Z_{t-1},Z_{t-2}, \\cdots , Z_{t-p},a_{t-1}, \\cdots, a_{t-q})'\n\\] Then, we have \\[\n\\begin{bmatrix}\nZ-t\\\\\nZ_{t-1}\\\\\n\\vdots\\\\\nZ_{t-p+1}\\\\\na_t\\\\\na_{t-1}\\\\\n\\vdots\\\\\na_{t-q+1}\n\\end{bmatrix}\n=\n\\left[\n\\begin{array}{cccc|cccc}\n\\phi_1&\\phi_2&\\cdots&\\phi_p&-\\theta_1&-\\theta_2&\\cdots&-\\theta_q\\\\\n1&0&\\cdots&0&0&0&\\cdots&0\\\\\n\\vdots &&&&\\vdots&&&\\\\\n0&\\cdots&1&0&0&0&\\cdots&0\\\\\n0&0&\\cdots&0&0&0&\\cdots&0\\\\\n0&0&\\cdots&0&1&0&\\cdots&0\\\\\n\\vdots &&&&0&&&\\\\\n0&0&\\cdots&0&0&\\cdots&1&0\n\\end{array}\\right]\n\\begin{bmatrix}\nZ_{t-1}\\\\\nZ_{t-2}\\\\\n\\vdots\\\\\nZ_{t-p}\\\\\na_{t-1}\\\\\na_{t-2}\\\\\n\\vdots\\\\\na_{t-q}\n\\end{bmatrix}+\n\\begin{bmatrix}\n1\\\\\n0\\\\\n\\vdots\\\\\n0\\\\\n1\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{bmatrix}a_t\n\\] and \\[\nZ_t=[\\phi_1,\\cdots,\\phi_p,-\\theta_1,\\cdots,-\\theta_q]S_t+a_t\n\\]"
  },
  {
    "objectID": "07-regression.html#mle-estimation",
    "href": "07-regression.html#mle-estimation",
    "title": "Bayes AI",
    "section": "MLE Estimation",
    "text": "MLE Estimation\nInnovations are given by \\[\n\\epsilon_t = Z_t - HS_t^{t-1}\n\\] can be shown that \\(\\mathrm{var}(\\epsilon_t) = \\Sigma_t\\), where \\[\n\\Sigma_t = HP_t^{t-1}H^T + R\n\\] Incomplete Data Likelihood: \\[\n-\\ln L(\\Theta) = \\dfrac{1}{2}\\sum_{t=1}^{n}\\log|\\Sigma_t(\\Theta)| + \\dfrac{1}{2}\\sum_{t=1}^{n}\\epsilon_t(\\Theta)^T\\Sigma(\\Theta)^{-1}\\epsilon_t(\\Theta)\n\\] Here \\(\\Theta = (F, Q, R)\\). Use BFGS to find a sequence of \\(\\Theta\\)’s and stop when stagnation happens."
  },
  {
    "objectID": "07-regression.html#kalman-smoother",
    "href": "07-regression.html#kalman-smoother",
    "title": "Bayes AI",
    "section": "Kalman Smoother",
    "text": "Kalman Smoother\n\nInput: initial distribution \\(X_0\\) and data \\(Z_1,...,Z_T\\)\nAlgorithm: forward-backward pass\nForward pass: Kalman filter: compute \\(S_{t+1}^t\\) and \\(S_{t+1}^{t+1}\\) for \\(0 \\le t &lt; T\\)\nBackward pass: Compute \\(S_t^T\\) for \\(0 \\le t &lt; T\\)"
  },
  {
    "objectID": "07-regression.html#backward-pass",
    "href": "07-regression.html#backward-pass",
    "title": "Bayes AI",
    "section": "Backward Pass",
    "text": "Backward Pass\n\nCompute \\(X_t^T\\) given \\(S_{t+1}^T \\sim N(m_{t+1}^T,C_{t+1}^T)\\)\nReverse arrow: \\(S_t^t \\leftarrow X_{t+1}^t\\)\nSame as incorporating measurement in filter\nCompute joint \\((S_t^t, S_{t+1}^t)\\)\nCompute conditional \\((S_t^t \\mid S_{t+1}^t )\\)\nNew: \\(S_{t+1}\\) is not “known”, we only know its distribution: \\(S_{t+1} \\sim S_{t+1}^T\\)\n“Uncondition” on \\(S_{t+1}\\) to compute \\(S_t^T\\) using laws of total expectation and variance"
  },
  {
    "objectID": "07-regression.html#kalman-smoother-1",
    "href": "07-regression.html#kalman-smoother-1",
    "title": "Bayes AI",
    "section": "Kalman Smoother",
    "text": "Kalman Smoother\nA smoothed version of data (an estimate, based on the entire data set) If \\(S_n\\) and \\(P_n\\) obtained via Kalman recursions, then for \\(t=n,..,1\\) \\[\n\\begin{aligned}\nS_{t-1}^t &= S_{t-1} + J_{t-1}(S_t^n - S_t^{t-1})\\\\\nP_{t-1}^n &= P^{t-1} + J_{t-1}(P_t^n - P_t^{t-1})J^T_{t-1}\\\\\nJ_{t-1} & = P_{t-1}F^T[P_t^{t-1}]^{-1}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "07-regression.html#kalman-and-histogran-filter-shortciomings",
    "href": "07-regression.html#kalman-and-histogran-filter-shortciomings",
    "title": "Bayes AI",
    "section": "Kalman and Histogran Filter Shortciomings",
    "text": "Kalman and Histogran Filter Shortciomings\nKalman:\n\nlinear dynamics\nlinear measurement model\nnormal errors\nunimodal uncertainty\n\nHistogram:\n\ndiscrete states\napproximation\ninefficient in memory"
  },
  {
    "objectID": "07-regression.html#mcmc-financial-econometrics",
    "href": "07-regression.html#mcmc-financial-econometrics",
    "title": "Bayes AI",
    "section": "MCMC Financial Econometrics",
    "text": "MCMC Financial Econometrics\nSet of tools for inference and pricing in continuous-time models.\n\nSimulation-based and provides a unified approach to state and parameter inference. Can also be applied sequentially.\nCan handle Estimation and Model risk. Important implications for financial decision making\nBayesian inference. Uses conditional probability to solve an inverse problem and estimates expectations using Monte Carlo."
  },
  {
    "objectID": "07-regression.html#filtering-smoothing-learning-and-prediction",
    "href": "07-regression.html#filtering-smoothing-learning-and-prediction",
    "title": "Bayes AI",
    "section": "Filtering, Smoothing, Learning and Prediction",
    "text": "Filtering, Smoothing, Learning and Prediction\nData \\(y_{t}\\) depends on a , \\(x_{t}\\). \\[\n\\begin{aligned}\n\\text{Observation equation} &  \\text{:\\ }y_{t}=f\\left(  x_{t},\\varepsilon\n_{t}^{y}\\right)  \\\\\n\\text{State evolution} &  \\text{: }x_{t+1}=g\\left(  x_{t},\\varepsilon\n_{t+1}^{x}\\right)  ,\\end{aligned}\n\\]\n\nPosterior distribution of \\(p\\left(x_{t}|y^{t}\\right)\\) where \\(y^{t}=\\left(\ny_{1},...,y_{t}\\right)\\)\nPrediction and Bayesian updating.\n\n\\[\np\\left(  x_{t+1}|y^{t}\\right)  =\\int p\\left(  x_{t+1}|x_{t}\\right)  p\\left(\nx_{t}|y^{t}\\right)  dx_{t},\\label{Predict}%\n\\] updated by Bayes rule\n\\[\n\\underset{\\text{Posterior}}{\\underbrace{p\\left(  x_{t+1}|y^{t+1}\\right)  }%\n}\\propto\\underset{\\text{Likelihood}}{\\underbrace{p\\left(  y_{t+1}%\n|x_{t+1}\\right)  }}\\underset{\\text{Prior}}{\\underbrace{p\\left(  x_{t+1}%\n|y^{t}\\right)  }}.\\label{Update}%\n\\]"
  },
  {
    "objectID": "07-regression.html#nonlinear-model",
    "href": "07-regression.html#nonlinear-model",
    "title": "Bayes AI",
    "section": "Nonlinear Model",
    "text": "Nonlinear Model\n\nThe observation and evolution dynamics are \\[\n\\begin{aligned}\ny_t & = \\frac{x_t}{1+ x_t^2} + v_t \\; , {\\rm where} \\; v_t \\sim N(0,1) \\\\\nx_t & = x_{t-1} + w_t  \\; , {\\rm where} \\; w_t \\sim N(0,0.5)\\end{aligned}\n\\]\nInitial condition \\(x_0 \\sim N( 1 , 10 )\\)\n\nFundamental question:\nHow do the filtering distributions \\(p(x_t|y^t)\\) propagate in time?"
  },
  {
    "objectID": "07-regression.html#nonlinear-y_t-x_t-1-x_t2-v_t",
    "href": "07-regression.html#nonlinear-y_t-x_t-1-x_t2-v_t",
    "title": "Bayes AI",
    "section": "Nonlinear: \\(y_t = x_t / (1+ x_t^2) + v_t\\)",
    "text": "Nonlinear: \\(y_t = x_t / (1+ x_t^2) + v_t\\)"
  },
  {
    "objectID": "07-regression.html#simulate-data",
    "href": "07-regression.html#simulate-data",
    "title": "Bayes AI",
    "section": "Simulate Data",
    "text": "Simulate Data\n\n\nCode\nset.seed(8)\n# MC sample size\nN = 1000\n\n#Posterior at time t=0 p(x[0]|y[0])=N(1,10)\nx = rnorm(N,1,sqrt(10))\nhist(x,main=\"Pr(x[0]|y[0])\")\n\n\n\n\n\n\n\n\n\nCode\n#Obtain draws from prior p(x[1]|y[0])\nx1 = x + rnorm(N,0,sqrt(0.5))\nhist(x1,main=\"Pr(x[1]|y[0])\")\n\n\n\n\n\n\n\n\n\nCode\n#Likelihood function p(y[1]|x[1])\ny1  = 5\nths = seq(-30,30,length=1000)\nplot(ths,dnorm(y1,ths/(1+ths^2),1),type=\"l\",xlab=\"\",ylab=\"\")\ntitle(paste(\"p(y[1]=\",y1,\"|x[1])\",sep=\"\"))"
  },
  {
    "objectID": "07-regression.html#nonlinear-filtering",
    "href": "07-regression.html#nonlinear-filtering",
    "title": "Bayes AI",
    "section": "Nonlinear Filtering",
    "text": "Nonlinear Filtering"
  },
  {
    "objectID": "07-regression.html#resampling",
    "href": "07-regression.html#resampling",
    "title": "Bayes AI",
    "section": "Resampling",
    "text": "Resampling\nKey: resample and propagate particles\n\n\nCode\n# Computing resampling weights\nw = dnorm(y1,x1/(1+x1^2),1)\n# Resample to obtain draws from p(x[1]|y[1])\nk = sample(1:N,size=N,replace=TRUE,prob=w)\nx = x1[k]\nhist(x,main=\"Pr(x[1]|y[1])\")\n\n\n\n\n\n\n\n\n\nCode\n#Obtain draws from prior p(x[2]|y[1])\nx2 = x + rnorm(N,0,sqrt(0.5))\nhist(x2,main=\"Pr(x[2]|y[1])\")"
  },
  {
    "objectID": "07-regression.html#propagation-of-mc-error",
    "href": "07-regression.html#propagation-of-mc-error",
    "title": "Bayes AI",
    "section": "Propagation of MC error",
    "text": "Propagation of MC error"
  },
  {
    "objectID": "07-regression.html#dynamic-linear-model-dlm-kalman-filter",
    "href": "07-regression.html#dynamic-linear-model-dlm-kalman-filter",
    "title": "Bayes AI",
    "section": "Dynamic Linear Model (DLM): Kalman Filter",
    "text": "Dynamic Linear Model (DLM): Kalman Filter\nKalman filter for linear Gaussian systems\n\nFFBS (Filter Forward Backwards Sample)\n\nThis determines the posterior distribution of the states\n\\[\np( x_t | y^t ) \\; {\\rm and} \\; p( x_t | y^T )\n\\] Also the joint distribution \\(p( x^T | y^T )\\) of the hidden states.\n\nDiscrete Hidden Markov Model HMM (Baum-Welch, Viterbi)\nWith parameters known the Kalman filter gives the exact recursions."
  },
  {
    "objectID": "07-regression.html#simulate-dlm",
    "href": "07-regression.html#simulate-dlm",
    "title": "Bayes AI",
    "section": "Simulate DLM",
    "text": "Simulate DLM\nDynamic Linear Models \\[\ny_t = x_t + v_t \\; \\; {\\rm and} \\; \\; x_t = \\alpha + \\beta x_{t-1} + w_t\n\\] Simulate Data\n\n\nCode\nT1=50; alpha=-0.01; beta=0.98; sW=0.1; sV=0.25; W=sW^2; V=sV^2\n\ny = rep(0,2*T1)\nht = rep(0,2*T1)\n\nfor (t in 2:(2*T1)){\n    ht[t] = rnorm(1,alpha+beta*ht[t-1],sW)\n    y[t] = rnorm(1,ht[t],sV)\n}\nht = ht[(T1+1):(2*T1)]\ny = y[(T1+1):(2*T1)]"
  },
  {
    "objectID": "07-regression.html#exact-calculations",
    "href": "07-regression.html#exact-calculations",
    "title": "Bayes AI",
    "section": "Exact calculations",
    "text": "Exact calculations\nKalman Filter recursions\n\n\nCode\nm=rep(0,T1);C=rep(0,T1); m0=0; C0=100\n\nR = C0+W; Q = R+V; A = R/Q\nm[1] = m0+A*(y[1]-m0); C[1] = R-A^2*Q\n\nfor (t in 2:T1){\n    R    = C[t-1]+W\n    Q    = R+V\n    A    = R/Q\n    m[t] = m[t-1]+A*(y[t]-m[t-1])\n    C[t] = R-A^2*Q\n}"
  },
  {
    "objectID": "07-regression.html#dlm-data",
    "href": "07-regression.html#dlm-data",
    "title": "Bayes AI",
    "section": "DLM Data",
    "text": "DLM Data\n\n\nCode\nplot(y,type=\"b\",col=\"blue\",xlab=\"Time\",ylab=\"y_t\", lwd=2, bty='n')\nlines(m,col=\"red\")\nlines(m+2*sqrt(C),col=\"grey\",lty=2)\nlines(m-2*sqrt(C),col=\"grey\",lty=2)"
  },
  {
    "objectID": "07-regression.html#bootstrap-filter",
    "href": "07-regression.html#bootstrap-filter",
    "title": "Bayes AI",
    "section": "Bootstrap Filter",
    "text": "Bootstrap Filter\n\n\nCode\nM = 1000\nh = rnorm(M,m0,sqrt(C0))\nhs = NULL\n\nfor (t in 1:T1){\n    h1 = rnorm(M,alpha+beta*h,sW)\n    w  = dnorm(y[t],h1,sV)\n    w  = w/sum(w)\n    h  = sample(h1,size=M,replace=T,prob=w)\n    hs = cbind(hs,h)\n}\n# Quantiles\nq025 = function(x){quantile(x,0.025)}\nq975 = function(x){quantile(x,0.975)}\nh025 = apply(hs,2,q025)\nh975 = apply(hs,2,q975)\nsd   = sqrt(apply(hs,2,var))\nm = colMeans(hs)\nplot(y,type=\"b\",col=\"blue\",xlab=\"Time\",ylab=\"y_t\", lwd=2, bty='n')\nlines(m,col=\"red\")\nlines(h025,col=\"grey\",lty=2)\nlines(h975,col=\"grey\",lty=2)"
  },
  {
    "objectID": "07-regression.html#streaming-data-how-do-parameter-distributions-change-in-time",
    "href": "07-regression.html#streaming-data-how-do-parameter-distributions-change-in-time",
    "title": "Bayes AI",
    "section": "Streaming Data: How do Parameter Distributions change in Time?",
    "text": "Streaming Data: How do Parameter Distributions change in Time?\n\n\n\n\n\n\n Bayes theorem: \\[\np(\\theta \\mid y^t) \\propto p(y_t \\mid \\theta) \\,\np(\\theta \\mid y^{t-1})\n\\]\n\n\nOnline Dynamic Learning\n\nReal-time surveillance\nBayes means sequential updating of information\nUpdate posterior density \\(p(\\theta \\mid y_t)\\) with every new observation (\\(t = 1, \\ldots, T\\)) - “sequential learning”"
  },
  {
    "objectID": "07-regression.html#galton-1877-first-particle-filter",
    "href": "07-regression.html#galton-1877-first-particle-filter",
    "title": "Bayes AI",
    "section": "Galton 1877: First Particle Filter",
    "text": "Galton 1877: First Particle Filter"
  },
  {
    "objectID": "07-regression.html#streaming-data-online-learning",
    "href": "07-regression.html#streaming-data-online-learning",
    "title": "Bayes AI",
    "section": "Streaming Data: Online Learning",
    "text": "Streaming Data: Online Learning\nConstruct an essential state vector \\(Z_{t+1}\\). \\[\n\\begin{aligned}\np(Z_{t+1}|y^{t+1}) &= \\int p(Z_{t+1}|Z_t, y_{t+1}) \\;d\\mathbb{P}(Z_t|y^{t+1}) \\\\\n&\\propto  \\int \\underbrace{p(Z_{t+1}|Z_t, y_{t+1})}_{propagate} \\overbrace{ \\underbrace{p(y_{t+1} | Z_t)}_{resample} \\;d\\mathbb{P}(Z_t|y^t)}\\end{aligned}\n\\]\n\nRe-sample with weights proportional to \\(p(y_{t+1} | Z_t^{(i)})\\) and generate \\(\\{Z_t^{\\zeta(i)}\\}_{i=1}^N\\)\nPropagate with \\(Z_{t+1}^{(i)} \\sim\np(Z_{t+1}|Z_t^{\\zeta(i)}, y_{t+1})\\) to obtain \\(\\{Z_{t+1}^{(i)}\\}_{i=1}^N\\)\n\nParameters: \\(p( \\theta | Z_{t+1} )\\) drawn “offline”"
  },
  {
    "objectID": "07-regression.html#sample-resample",
    "href": "07-regression.html#sample-resample",
    "title": "Bayes AI",
    "section": "Sample – Resample",
    "text": "Sample – Resample\n\nsample-resample"
  },
  {
    "objectID": "07-regression.html#resample-sample",
    "href": "07-regression.html#resample-sample",
    "title": "Bayes AI",
    "section": "Resample – Sample",
    "text": "Resample – Sample\n\nresample-sample"
  },
  {
    "objectID": "07-regression.html#particle-methods-blind-propagation",
    "href": "07-regression.html#particle-methods-blind-propagation",
    "title": "Bayes AI",
    "section": "Particle Methods: Blind Propagation",
    "text": "Particle Methods: Blind Propagation\n\nPropagate-Resample is replaced by Resample-Propagate"
  },
  {
    "objectID": "07-regression.html#traffic-problem",
    "href": "07-regression.html#traffic-problem",
    "title": "Bayes AI",
    "section": "Traffic Problem",
    "text": "Traffic Problem\n\nState-Space"
  },
  {
    "objectID": "07-regression.html#wave-speed-propagation-is-a-mixture-distribution",
    "href": "07-regression.html#wave-speed-propagation-is-a-mixture-distribution",
    "title": "Bayes AI",
    "section": "Wave Speed Propagation is a Mixture Distribution",
    "text": "Wave Speed Propagation is a Mixture Distribution\nShock wave propagation speed is a mixture, when calculated using Godunov scheme \\[\nw = \\frac{q(\\rho_l) - q(\\rho_r)}{\\rho_l-\\rho_r}  \\ \\left[\\frac{mi}{h}\\right] = \\left[\\frac{veh}{h}\\right]\\left[\\frac{mi}{veh}\\right].\n\\] Assume \\(\\rho_l \\sim TN(32, 16, 0, 320)\\) and \\(\\rho_r \\sim TN(48, 16, 0,320)\\)\n\\(q_c = 1600  \\ veh/h\\), \\(\\rho_c = 40 \\ veh/mi\\), and \\(\\rho_{jam} = 320 \\ veh/mi\\)"
  },
  {
    "objectID": "07-regression.html#traffic-flow-speed-forecast-is-a-mixtrue-dsitribution",
    "href": "07-regression.html#traffic-flow-speed-forecast-is-a-mixtrue-dsitribution",
    "title": "Bayes AI",
    "section": "Traffic Flow Speed Forecast is a Mixtrue Dsitribution",
    "text": "Traffic Flow Speed Forecast is a Mixtrue Dsitribution\nTheorem: The solution (including numerical) to the LWR model with stochastic initial conditions is a mixture distribution.\n\nA moment based filters such as Kalman Filter or Extended Kalman Filter would not capture the mixture."
  },
  {
    "objectID": "07-regression.html#problem-at-hand",
    "href": "07-regression.html#problem-at-hand",
    "title": "Bayes AI",
    "section": "Problem at Hand",
    "text": "Problem at Hand\nThe Parameter Learning and State Estimation Problem\n\nGoal: given sparse sensor measurements, find the distribution over traffic state and underlying traffic flow parameters \\(p(\\theta_t, \\phi|y_1, y_2,...,y_t); \\ \\phi=(q_c,\\rho_c)\\)\nParameters of the evolution equation (LWR) are stochastic\nDistribution over state is a mixture\nCan’t use moment based filters (KF, EKF,...)"
  },
  {
    "objectID": "07-regression.html#data-assimilation-state-space-representation",
    "href": "07-regression.html#data-assimilation-state-space-representation",
    "title": "Bayes AI",
    "section": "Data Assimilation: State Space Representation",
    "text": "Data Assimilation: State Space Representation\n\nState space formulation allows to combine knowledge from analytical model with the one from field measurements, while taking model and measurement errors into account"
  },
  {
    "objectID": "07-regression.html#state-space-representation",
    "href": "07-regression.html#state-space-representation",
    "title": "Bayes AI",
    "section": "State Space Representation",
    "text": "State Space Representation\n\nState vector \\(\\theta_t = ( \\rho_{1t} , \\ldots , \\rho_{nt} )\\)\nBoundary conditionals \\(\\rho_{0t}\\) and \\(\\rho_{(n+1)t}\\)\nUnderlying parameters \\(\\phi = (q_c, \\rho_c)\\) are stochastic\n\n\\[\\begin{align}\n\\mbox{Observation: }&y_{t+1} = H\\theta_{t+1}  + v; \\ v \\sim N(0,V) \\label{eqn-y}\\\\\n\\mbox{Evolution: }&\\theta_{t+1} = f_{\\phi}(\\theta_t) + w; \\ w \\sim N(0,W) \\label{(eqn-x)}\n\\end{align}\\]\n\\(H: \\mathbb{R}^{M} \\rightarrow \\mathbb{R}^k\\) in the measurement model. \\(\\phi  = (q_c,\\rho_c, \\rho_{max})\\).\nParameter priors: \\(q_c \\sim N(\\mu_q, \\sigma_c^2)\\), \\(\\rho_c = Uniform(\\rho_{min}, \\rho_{max})\\)"
  },
  {
    "objectID": "07-regression.html#particle-parameter-learning",
    "href": "07-regression.html#particle-parameter-learning",
    "title": "Bayes AI",
    "section": "Particle Parameter Learning",
    "text": "Particle Parameter Learning"
  },
  {
    "objectID": "07-regression.html#sample-based-pdf-representation",
    "href": "07-regression.html#sample-based-pdf-representation",
    "title": "Bayes AI",
    "section": "Sample-based PDF Representation",
    "text": "Sample-based PDF Representation\n\nRegions of high density: Many particles and Large weight of particles\nUneven partitioning\nDiscrete approximation for continuous pdf\n\n\\[\np^{N}\\left(  \\theta_{t+1}|y^{t+1}\\right) \\propto \\sum_{i=1}^{N}w_{t}^{\\left(  i\\right)  }p\\left(  \\theta_{t+1}|\\theta_t^{\\left(\ni\\right)  },y_{t+1}\\right)\n\\]"
  },
  {
    "objectID": "07-regression.html#particle-filter",
    "href": "07-regression.html#particle-filter",
    "title": "Bayes AI",
    "section": "Particle Filter",
    "text": "Particle Filter\nBayes Rule: \\[\np(y_{t+1},\\theta_{t+1}|\\theta_t)=p(y_{t+1}|\\theta_t)\\,p(\\theta_{t+1}|%\n\\theta_t,y_{t+1}).\n\\]\n\nGiven a particle approximation to \\(p^{N}\\left(  \\theta_t|y^{t}\\right)\\) \\[\n\\begin{aligned}\np^{N}\\left(  \\theta_{t+1}|y^{t+1}\\right)   &  \\propto\\sum_{i=1}^{N}p\\left(\ny_{t+1}|\\theta_t^{\\left(  i\\right)  }\\right)  p\\left(  \\theta_{t+1}|\\theta_t^{\\left(\ni\\right)  },y_{t+1}\\right) \\label{Mixture2}\\\\\n&  =\\sum_{i=1}^{N}w_{t}^{\\left(  i\\right)  }p\\left(  \\theta_{t+1}|\\theta_t^{\\left(\ni\\right)  },y_{t+1}\\right)  \\text{,}%\n\\end{aligned}\n\\] where\n\n\\[\nw_{t}^{\\left(  i\\right)  }=\\frac{p\\left(  y_{t+1}|\\theta_t^{\\left(  i\\right)\n}\\right)  }{\\sum_{i=1}^{N}p\\left(  y_{t+1}|\\theta_t^{\\left(  i\\right)  }\\right)\n}\\text{.}%\n\\]\n\nEssentially a mixture Kalman filter"
  },
  {
    "objectID": "07-regression.html#particle-parameter-learning-1",
    "href": "07-regression.html#particle-parameter-learning-1",
    "title": "Bayes AI",
    "section": "Particle Parameter Learning",
    "text": "Particle Parameter Learning\nGiven particles (a.k.a. random draws) \\((\\theta^{(i)}_t,\\phi^{(i)},s^{(i)}_t),\\) \\(i=1,\\ldots,N\\) \\[\np( \\theta_t | y_{1:t} ) = \\frac{1}{N} \\sum_{i=1}^N \\delta_{ \\theta^{(i)} } \\; .\n\\]\n\nFirst resample \\((\\theta^{k(i)}_t,\\phi^{k(i)},s^{k(i)}_t)\\) with weights proportional to \\(p(y_{t+1}|\\theta^{k(i)}_t,\\phi^{k(i)})\\) and \\(s_t^{k(i)}=S(s^{(i)}_t,\\theta^{k(i)}_t,y_{t+1})\\) and then propogate to \\(p(\\theta_{t+1}|y_{1:t+1})\\) by drawing \\(\\theta^{(i)}_{t+1}\\,\\)from \\(p(\\theta_{t+1}|\\theta^{k(i)}_t,\\phi^{k(i)},y_{t+1}),\\,i=1,\\ldots,N\\).\nNext we update the sufficient statistic as\n\n\\[\ns_{t+1}=S(s_t^{k(i)},\\theta^{(i)}_{t+1},y_{t+1}),\n\\] for \\(i=1,\\ldots,N\\), which represents a deterministic propogation.\n\nFinally, parameter learning is completed by drawing \\(\\phi^{(i)}\\) using \\(p(\\phi|s^{(i)}_{t+1})\\) for \\(i=1,\\ldots,N\\)."
  },
  {
    "objectID": "07-regression.html#streaming-data-online-learning-1",
    "href": "07-regression.html#streaming-data-online-learning-1",
    "title": "Bayes AI",
    "section": "Streaming Data: Online Learning",
    "text": "Streaming Data: Online Learning\nConstruct an essential state vector \\(Z_{t+1}\\). \\[\n\\begin{aligned}\np(Z_{t+1}|y^{t+1}) &= \\int p(Z_{t+1}|Z_t, y_{t+1}) \\;d\\mathbb{P}(Z_t|y^{t+1}) \\\\\n&\\propto  \\int \\underbrace{p(Z_{t+1}|Z_t, y_{t+1})}_{propagate} \\overbrace{ \\underbrace{p(y_{t+1} | Z_t)}_{resample} \\;d\\mathbb{P}(Z_t|y^t)}\n\\end{aligned}\n\\]\n\nRe-sample with weights proportional to \\(p(y_{t+1} | Z_t^{(i)})\\) and generate \\(\\{Z_t^{\\zeta(i)}\\}_{i=1}^N\\)\nPropagate with \\(Z_{t+1}^{(i)} \\sim\np(Z_{t+1}|Z_t^{\\zeta(i)}, y_{t+1})\\) to obtain \\(\\{Z_{t+1}^{(i)}\\}_{i=1}^N\\)\n\nParameters: \\(p( \\theta | Z_{t+1} )\\) drawn “offline”"
  },
  {
    "objectID": "07-regression.html#resample-propagate",
    "href": "07-regression.html#resample-propagate",
    "title": "Bayes AI",
    "section": "Resample – Propagate",
    "text": "Resample – Propagate"
  },
  {
    "objectID": "07-regression.html#algorithm",
    "href": "07-regression.html#algorithm",
    "title": "Bayes AI",
    "section": "Algorithm",
    "text": "Algorithm\nThese ingredients then define a particle filtering and learning algorithm for the sequence of joint posterior distributions \\(p( \\theta_t , \\phi | y_{1:t} )\\): \\[\n\\begin{aligned}\n&  \\text{Step 1. (Resample) Draw an index } k_t \\left(  i\\right)  \\sim\nMult_{N}\\left(  w_{t}^{\\left(  1\\right)  },...,w_{t}^{\\left(  N\\right)\n}\\right), \\\\\n& \\mbox{where the weights are given by } w_t^{(i)} \\propto p(y_{t+1}|(\\theta_t,\\phi)^{(i)}), \\   \\text{ for }i=1,...,N\\\\\n\\text{ }  &  \\text{Step 2. (Propagate) Draw }\\theta_{t+1}^{\\left(  i\\right)  }\\sim\np\\left(  \\theta_{t+1}|\\theta_t^{k_t \\left(  i\\right)  },y_{t+1}\\right)  \\text{ for\n}i=1,...,N.\\\\\n\\text{ }  &  \\text{Step 3. (Update) } s_{t+1}^{(i)} =S(s_t^{k_t(i)},\\theta_{t+1}^{(i)},y_{t+1})\\\\\n\\text{ }  &  \\text{Step 4. (Replenish) } \\phi^{(i)} \\sim p( \\phi | s_{t+1}^{(i)} )\n\\end{aligned}\n\\] There are a number of efficiency gains from such an approach, e.g. it does not suffer from degeneracy problems associated with traditional propagate-resample algorithms when \\(y_{t+1}\\) is an outliers."
  },
  {
    "objectID": "07-regression.html#obtaining-state-estimates-from-particles",
    "href": "07-regression.html#obtaining-state-estimates-from-particles",
    "title": "Bayes AI",
    "section": "Obtaining state estimates from particles",
    "text": "Obtaining state estimates from particles\n\nAny estimate of a function \\(f(\\theta_t)\\) can be calculated by discrete-approximation\n\n\\[\n\\mbox{E}(f(\\theta_t)) = \\dfrac{1}{N}\\sum_{j=1}^{N}w_t^{(j)}f(\\theta_t^{(j)})\n\\]\n\nMean:\n\n\\[\n\\mbox{E}(\\theta_t) = \\dfrac{1}{N}\\sum_{j=1}^{N}w_t^{(j)}\\theta_t^{(j)}\n\\]\n\nMAP-estimate: particle with largest weight\nRobust mean: mean within window around MAP-estimate"
  },
  {
    "objectID": "07-regression.html#particle-filters-pluses",
    "href": "07-regression.html#particle-filters-pluses",
    "title": "Bayes AI",
    "section": "Particle Filters: Pluses",
    "text": "Particle Filters: Pluses\n\nEstimation of full PDFs\nNon-Gaussian distributions (multi-modal)\nNon-linear state and observation model\nParallelizable"
  },
  {
    "objectID": "07-regression.html#particle-filters-minuses",
    "href": "07-regression.html#particle-filters-minuses",
    "title": "Bayes AI",
    "section": "Particle Filters: Minuses",
    "text": "Particle Filters: Minuses\n\nDegeneracy problem\nHigh number of particles needed\nComputationally expensive\nLinear-Gaussian assumption is often sufficient"
  },
  {
    "objectID": "07-regression.html#applications-localization",
    "href": "07-regression.html#applications-localization",
    "title": "Bayes AI",
    "section": "Applications: Localization",
    "text": "Applications: Localization\n\nTrack car position in given road map\nTrack car position from radio frequency measurements\nTrack aircraft position from estimated terrain elevation\nCollision Avoidance (Prediction)"
  },
  {
    "objectID": "07-regression.html#applications-model-estimation",
    "href": "07-regression.html#applications-model-estimation",
    "title": "Bayes AI",
    "section": "Applications: Model Estimation",
    "text": "Applications: Model Estimation\n\nTracking with multiple motion-models\nRecovery of signal from noisy measurements\nNeural Network model selection (on-line classification)"
  },
  {
    "objectID": "07-regression.html#applications-other",
    "href": "07-regression.html#applications-other",
    "title": "Bayes AI",
    "section": "Applications: Other",
    "text": "Applications: Other\n\nVisual Tracking\nPrediction of (financial) time series\nQuality control in semiconductor industry\nMilitary applications: Target recognition from single or multiple images, Guidance of missiles\nReinforcement Learning"
  },
  {
    "objectID": "07-regression.html#mixture-kalman-filter-for-traffic",
    "href": "07-regression.html#mixture-kalman-filter-for-traffic",
    "title": "Bayes AI",
    "section": "Mixture Kalman Filter For Traffic",
    "text": "Mixture Kalman Filter For Traffic\n\\[\n\\begin{aligned}\n\\mbox{Observation: }&y_{t+1} = Hx_{t+1}  + \\gamma^Tz_{t+1} + v_{t+1} , \\ v_{t+1} \\sim N(0, V_{t+1})\\\\\n\\mbox{Evolution: }&x_{t+1} = F_{\\alpha_{t+1}}x_t + (1-F_{\\alpha_{t+1}})\\mu + \\alpha_t\\beta_{t} + \\omega_{1} \\\\\n&\\beta_{t+1} = \\max(0,\\beta_{t} + \\omega_{2} \\label{eqn-beta})\\\\\n\\mbox{Switching Evolution: }&\\alpha_{t+1} \\sim p(\\alpha_{t+1} |\\alpha_{t},Z_{t})\n\\end{aligned}\n\\] where \\(z_t\\) is an exogenous variable that effects the sensor model, \\(\\mu\\) is an average free flow speed \\[\n\\alpha_t \\in \\{0,1,-1\\}\n\\] \\[\n\\omega = (\\omega_{1}, \\omega_{2})^T \\sim N(0, W), \\ v \\sim N(0,V)\n\\] \\[\nF_{\\alpha_t} = \\left\\{\n\\begin{aligned}\n&1,  \\ \\alpha_t \\in \\{1,-1\\}\\\\\n&F, \\  \\alpha_t = 0\\end{aligned}\n\\right.\n\\] No boundary conditions estimation is needed. No capacity/critical density is needed."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes AI Slides",
    "section": "",
    "text": "Unit 1: Introduction: AI Today and in the Past. Probability and Bayes Rule\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 2: Utility and Decision Theory\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 3: Bayesian Inference with Conjugate Pairs\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 4: Bayesian Hypothesis Tests\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 5: Stochastic Processes\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 6: Markov Chain Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 7: Bayesian Regression: Linear and Bayesian Trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 8: Quantile Neural Networks for Reinforcement Learning and Uncertainty Quantification\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 9: Bayesian Double Descent and Model Selection: Modern Approach to Bias-Variance Tradeoff\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 10: Bayesian Neural Networks and Deep Learning\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "03-conjugate.html#likelihood",
    "href": "03-conjugate.html#likelihood",
    "title": "Bayes AI",
    "section": "Likelihood",
    "text": "Likelihood\nThe likelihood is given by the Binomial distribution\n\n\nCode\nth = seq(0,1,length=100)\npar(mar=c(4,4,0,0), bty='n')\nll = th^30*(1-th)^70; ll = ll/sum(ll)\nplot(th,ll, type='l', col=1, ylab='likelihood', xlab='theta')\nll = th^3*(1-th)^7; ll = ll/sum(ll)\nlines(th,ll, type='l', col=2)\nlabels = c('n=100', 'n=10')\nlegend('topright', legend=labels, col=1:2, lty=1,bty='n')"
  },
  {
    "objectID": "03-conjugate.html#steins-estimator",
    "href": "03-conjugate.html#steins-estimator",
    "title": "Bayes AI",
    "section": "Stein’s Estimator",
    "text": "Stein’s Estimator\n\nPopularized by Efron’s 1970 paper\nDiscovered by Charles Stein in 1956 and further developed with James in 1961 (James-Stein estimator)\nWant to estimate \\(p\\) means of several independent normal distributions simultaneously. \\[\n\\mu_1, \\mu_2,\\ldots,\\mu_p\n\\]\nMLE: \\(\\mu_i\\) is simply the sample observation \\(x_i\\) from the i-th distribution, considering each mean individually."
  },
  {
    "objectID": "03-conjugate.html#the-shocking-result-stein-paradox",
    "href": "03-conjugate.html#the-shocking-result-stein-paradox",
    "title": "Bayes AI",
    "section": "The Shocking Result (Stein Paradox)",
    "text": "The Shocking Result (Stein Paradox)\n\nWhen \\(p&gt;3\\) there is different estimator that is uniformly better than using the individual sample means (\\(x_1\\), \\(x_2\\), …, \\(x_p\\)) when you consider the total squared error risk across all means.\n“Uniformly better” means it’s better for all possible true values of the means (\\(\\mu_1\\), \\(\\mu_2\\), …, \\(\\mu_p\\)).\nThe distributions are assumed to be independent.\n\nSample means are individually good estimators.\n\nCounterintuitive! How can we improve upon something that already seems so natural and optimal when considered in isolation?"
  },
  {
    "objectID": "03-conjugate.html#the-james-stein-estimator",
    "href": "03-conjugate.html#the-james-stein-estimator",
    "title": "Bayes AI",
    "section": "The James-Stein Estimator",
    "text": "The James-Stein Estimator\nThe estimator that Stein and James proposed (often called the James-Stein estimator or shrinkage estimator) is of the form: \\[\n\\mu_{JS} = (1 - b)\\hat x\n\\] - \\(b\\) is a “shrinkage factor” calculated based on the data, typically something like \\(b = (p - 2) * S^2 / ||x||^2\\), where \\(S^2\\) is the variance of the underlying distributions (often assumed to be known or estimated), and $||x||^2 = $x_1^2 + $x_2^2 + … + \\(x_p^2\\) is the squared Euclidean norm of x."
  },
  {
    "objectID": "03-conjugate.html#what-does-this-estimator-do",
    "href": "03-conjugate.html#what-does-this-estimator-do",
    "title": "Bayes AI",
    "section": "What does this estimator do?",
    "text": "What does this estimator do?\n\nIt shrinks the original estimate \\(\\mu\\) towards the origin (zero vector). The amount of shrinkage is determined by the data itself. If \\(||x||^2\\) is large (observations are far from zero), the shrinkage factor b is smaller, and less shrinkage is applied. If \\(||x||^2\\) is small, \\(b\\) is larger, and more shrinkage is applied towards zero.\nBrad Efron’s 1970 paper, “Stein’s Paradox in Statistics,” published in the American Statistician, simplified and clarified the paradox\nGeometric Interpretation: Efron emphasized a geometric perspective on the Stein Paradox.\nRisk Decomposition and Analysis: derivation cleverly decomposed the risk (expected squared error) of the estimators. This decomposition clearly showed how the James-Stein estimator reduces risk compared to the standard estimator, especially in higher dimensions."
  },
  {
    "objectID": "03-conjugate.html#risk-analysis-and-decomposition",
    "href": "03-conjugate.html#risk-analysis-and-decomposition",
    "title": "Bayes AI",
    "section": "Risk Analysis and Decomposition",
    "text": "Risk Analysis and Decomposition\n\\[\nRisk(\\mu_{JS}) = E[ ||\\mu_{JS}(x) - \\mu||^2 ] = E[ \\sum_{i=1}^p (\\mu_{JS,i}(x) - \\mu_i)^2 ]\n\\] - Each component of \\(x - \\mu\\) has variance \\(\\sigma^2\\) (let’s assume for simplicity all distributions have the same known variance ^2).\nSay \\(\\mu_{JS}(x) = x\\), the risk becomes: \\[\nRisk(x) = E[ ||x - \\mu||^2 ] = E[ \\sum_{i=1}^p (x_i - \\mu_i)^2 ] = \\sum_{i=1}^p Var(x) = p\\sigma^2\n\\] This is the risk of the standard MLE estimator. It scales linearly with the dimension p."
  },
  {
    "objectID": "03-conjugate.html#risk-of-a-shrinkage-estimator",
    "href": "03-conjugate.html#risk-of-a-shrinkage-estimator",
    "title": "Bayes AI",
    "section": "Risk of a Shrinkage Estimator",
    "text": "Risk of a Shrinkage Estimator\n\\[\n\\mu_{JS} = (1 - b)x\n\\] where b is a fixed constant between 0 and 1. Efron then analyzes the risk of this estimator: \\[\nRisk(\\mu_{JS}) = E[ ||(1 - b)x - \\mu||^2 ] = E[ ||(1 - b)(x - \\mu) - b\\mu||^2 ]\n\\] By expanding this using vector algebra and expectations, and making use of the independence and normality assumptions, Efron shows that you can decompose the risk into terms related to b and the dimension p."
  },
  {
    "objectID": "03-conjugate.html#risk-of-a-shrinkage-estimator-1",
    "href": "03-conjugate.html#risk-of-a-shrinkage-estimator-1",
    "title": "Bayes AI",
    "section": "Risk of a Shrinkage Estimator",
    "text": "Risk of a Shrinkage Estimator\nApproximately optimal fixed shrinkage constant b is around \\[\n(p - 2) / E[||x||^2]\n\\] or proportional to \\((p-2) / ||x||^2\\) in the data-dependent version."
  },
  {
    "objectID": "03-conjugate.html#random-effect-model",
    "href": "03-conjugate.html#random-effect-model",
    "title": "Bayes AI",
    "section": "Random Effect Model",
    "text": "Random Effect Model"
  }
]